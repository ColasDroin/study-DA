var __index = {"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Introduction","text":"<p>The study-DA package is a comprehensive suite of tools designed to analyze the dynamic aperture of particle accelerators built with Xsuite. It serves as an advanced replacement for the DA study template, offering enhanced capabilities for collider configuration, tracking, and analysis without requiring parametric scans.</p> <p>The package is organized into four key components:</p> <ol> <li> <p>Study Generation:    Enables the creation of dynamic aperture studies through template scripts and configuration files, structured as a multi-generational tree representing various layers of a parametric scan. Alternatively, single jobs can be created for tasks such as collider configuration and tracking, bypassing the need for a full scan.</p> </li> <li> <p>Study Submission:    Simplifies the submission of studies to local systems or computing clusters (primarily HTCondor) and automates the retrieval of results.</p> </li> <li> <p>Study Postprocessing:    Offers tools to transform raw results (typically <code>.parquet</code> files from tracking) into a Pandas <code>DataFrame</code> for easy analysis and aggregation.</p> </li> <li> <p>Study Plotting:    Provides functions to generate 2D and 3D heatmaps for visualizing postprocessed results effectively.</p> </li> </ol> <p>For installation instructions and tutorials, please refer to the Getting Started guide.</p>"},{"location":"contributing.html","title":"Contributing","text":"<p>We welcome contributions to the project. The contributing guidelines are as follows:</p> <ol> <li>Issues: If you find a bug or have a feature request, please open an issue on the GitHub repository</li> <li>Pull Requests: We welcome pull requests. Before submitting a pull request, please make sure that your changes are consistent with the project's coding style and that you have tested your changes. If you are adding a new feature, please also add documentation, and potentially tests for it.</li> <li>Documentation: You are very welcome to contribute to the documentation. If you find a typo, an actual error, or think that something is missing, please open an issue or submit a pull request.</li> <li>Feedback: We welcome feedback on the project. If you have any suggestions or comments, please open an issue.</li> <li>Questions: If you have any questions about contributing to the project, please contact the project maintainers.</li> <li>Thank you: Thank you for contributing to the project. Your contributions are greatly appreciated.</li> </ol>"},{"location":"contributing.html#pull-requests","title":"Pull requests","text":"<p>To do a pull request, please follow these steps:</p> <ol> <li>Fork the repository</li> <li>Create a new branch (<code>git checkout -b feature/yourfeature</code>)</li> <li>Make your changes</li> <li>Commit your changes (<code>git commit -am 'Add your feature'</code>)</li> <li>Push to the branch (<code>git push origin feature/yourfeature</code>)</li> <li>Create a new Pull Request</li> </ol>"},{"location":"getting_started.html","title":"Getting started","text":"<p>You can simply <code>pip install</code> the package:</p> <pre><code>pip install study-da\n</code></pre> <p>You will also have to install <code>xmask</code> somewhere as an external dependency since it requires pulling submodules:</p> <pre><code># You can install xmask in the directory of your choice (along with the optics, for instance)... \n# Just ensure it doesn't get deleted\ngit clone --recurse-submodules https://github.com/xsuite/xmask\npip install -e xmask\n</code></pre> <p>If you have some optics available on your machine or on AFS (e.g. HL-LHC v1.6), you can directly move to the tutorials to get started with the package, or to the case studies to see some actual useful examples of what can be done with the package.</p> <p>Otherwise, please refer to the installing the optics section to get the optics you need.</p> <p>Do not hesitate to refer to the detailed explanations in case you need more guidance to install Python, or if you want to install locally (for instance to contribute to the package).</p> <p>HTCondor must be installed and properly configured</p> <p>To submit large scans to HTCondor, the corresponding batch service must be installed and properly configured. If you don't have access, you can still run the script locally, but you won't be able to submit the jobs to the clusters. You can read more here.</p>"},{"location":"why_this_package.html","title":"Why this package?","text":"<p>Conducting a parametric scan for a dynamic aperture study can be a challenging and time-consuming task, especially when dealing with a large number of parameters. Previously, we developed the DA study template, which remains a solid starting point. However, it comes with several significant drawbacks:</p> <ul> <li> <p>Difficult maintenance:   Managing multiple versions of the template to accommodate different LHC configurations required extensive use of <code>git rebase</code> and <code>git cherry-pick</code>. This approach often led to frequent merge conflicts, synchronization issues, and duplication of largely similar code.</p> </li> <li> <p>Lack of user-friendliness:   Users needed to directly modify template scripts for new studies, increasing the likelihood of errors.</p> </li> <li> <p>Redundancy:   Parameters were often declared both in the configuration file and the generating script, creating opportunities for inconsistencies.</p> </li> <li> <p>Limited scalability:   The template was not well-suited for studies involving more than two generations, especially when using HTCondor.</p> </li> <li> <p>Cumbersome workflow:   Tasks like creating jobs, submitting them, retrieving results, postprocessing, and plotting had to be performed manually using separate scripts for each step.</p> </li> <li> <p>Restricted functionality:   The template did not explicitly support configuring and tracking a single collider without conducting a full scan.</p> </li> <li> <p>Lack of standardization:   The postprocessing workflow varied among users, leading to inconsistent results and plotting.</p> </li> <li> <p>No integrated analysis or plotting:   Users were responsible for performing their own analysis and visualization.</p> </li> <li> <p>Compatibility issues with Xsuite:   Frequent Xsuite updates made it challenging to maintain compatibility, as setting up a Continuous Integration (CI) pipeline for open scripts was not straightforward.</p> </li> <li> <p>Complex setup:   The template was not available on PyPI, complicating the installation process.</p> </li> </ul> <p>This package addresses all these issues and more, offering a more user-friendly, flexible, and standardized solution. It is also more robust, thanks to (a simple) CI pipeline testing, and easier to maintain due to the centralization of common code into a cohesive set of classes and functions.</p> <p>Standardization comes with trade-offs</p> <p>Adopting this package requires adherence to its conventions, which may feel more restrictive compared to the DA study template. Additionally, because functions for configuring, tracking, submitting, postprocessing, and plotting are centralized within the package, they are no longer as openly modifiable.</p> <p>That said, users are encouraged to implement their own functions and template scripts for their specific workflows. The package is also open to contributions to integrate workflows that may benefit the broader community.</p>"},{"location":"case_studies/index.html","title":"Case studies","text":"<p>The case studies in this section should provide you with almost ready-to-use solutions for your own projects. They are designed to be very generic, such that you should be able to use them as a starting point for your own projects.</p> <p>They also contain a lot of comments to help you understand what is going on in the code, and do complement nicely the tutorials section, as many tweaks and improvements to simulations are discussed there.</p>"},{"location":"case_studies/1_simple_collider.html","title":"Working with a single collider","text":"<p>The special function <code>create_single_job()</code> can be used to build and configure a collider without having to manually configure an irrelevant scan.</p> <p>Assuming you'd like to work with the template <code>hllhc16</code> configuration, along with the template scripts <code>generation_1.py</code> and <code>generation_2_level_by_nb.py</code>, saving the collider before tracking and only track for 1000 turns, you could proceed as follows:</p> single_collider.py<pre><code># ==================================================================================================\n# --- Imports\n# ==================================================================================================\n\n# Import standard library modules\nimport os\n\n# Import user-defined modules\nfrom study_da.utils import load_template_configuration_as_dic, write_dic_to_path\n\n# ==================================================================================================\n# --- Script to generate a study\n# ==================================================================================================\n\n# Load the configuration from hllhc16\nname_template_config = \"config_hllhc16.yaml\"\nconfig, ryaml = load_template_configuration_as_dic(name_template_config)\n\n# Update the location of acc-models\nconfig[\"config_mad\"][\"links\"][\"acc-models-lhc\"] = (\n    \"../../../../../external_dependencies/acc-models-lhc\"\n)\n\n# Adapt the number of turns\nconfig[\"config_simulation\"][\"n_turns\"] = 1000\n\n# Save the collider produced after the configuration step\nconfig[\"config_collider\"][\"save_output_collider\"] = True\n\n# Drop the configuration locally\nlocal_config_name = \"local_config.yaml\"\nwrite_dic_to_path(config, local_config_name, ryaml)\n\n# Now generate the study in the local directory\npath_tree = create_single_job(\n    name_main_configuration=local_config_name,\n    name_executable_generation_1=\"generation_1.py\",\n    name_executable_generation_2=\"generation_2_level_by_nb.py\",\n    name_study=\"single_job_study_hllhc16\",\n)\n\n# Delete the configuration file (it's copied in the study folder anyway)\nos.remove(local_config_name)\n</code></pre> <p>At this point, the following directory structure will be created:</p> <pre><code>\ud83d\udcc1 single_job_study_hllhc16/\n\u251c\u2500\u2574\ud83d\udcc1 generation_1/\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 generation_1.py\n\u2502   \u2514\u2500\u2500 \ud83d\udcc1 generation_2/\n\u2502        \u251c\u2500\u2500 \ud83d\udcc4 generation_2.py\n\u251c\u2500 \ud83d\udcc4 tree.yaml\n\u2514\u2500 \ud83d\udcc4 local_config.yaml\n</code></pre> <p>The <code>tree.yaml</code>, later used for submission, contains exactly this simple structure:</p> <pre><code>generation_1:\n  generation_1:\n    file: single_job_study_hllhc16/generation_1/generation_1.py\n  generation_2:\n    generation_2:\n      file: single_job_study_hllhc16/generation_1/generation_2/generation_2.py\n</code></pre> <p>From here, you can run the two generations one after the other using the <code>submit</code> function:</p> single_collider.py<pre><code>from study_da import submit\n\n# Define the variables of interest for the submission\npath_python_environment = \"path/to/python/environment\"\n\n# Preconfigure submission to local, so that you don't get prompted for the submission type\ndic_config_jobs = {\n    \"generation_1\" + \".py\": {\n        \"submission_type\": \"local\",\n    },\n    \"generation_2\" + \".py\": {\n        \"request_gpu\": False,\n        \"submission_type\": \"local\",\n    },\n}\n\n# Since gen_1 is submitted locally, add a command to remove unnecessary files\ndic_additional_commands_per_gen = {\n    1: \"rm -rf final_* modules optics_repository optics_toolkit tools tracking_tools temp \"\n    \"mad_collider.log __pycache__ twiss* errors fc* optics_orbit_at* \\n\",\n    2: \"\",\n}\n\n# Submit the study\nsubmit(\n    path_tree=path_tree, # path to the study tree\n    path_python_environment=path_python_environment, # path to the python environment\n    name_config=local_config_name, # configuration file for the execution\n    dic_config_jobs=dic_config_jobs, # preconfigure submission to local\n    dic_additional_commands_per_gen=dic_additional_commands_per_gen, # remove unnecessary files\n    keep_submit_until_done=True, # keep submitting until all jobs are done\n    wait_time=1, # wait 1mn before checking the status of the jobs and resubmitting\n)\n</code></pre> <p>This will complete the tree with the submission settings, generate the run files to launch the jobs, and the study will be executed in the local environment. Note that the <code>\"request_gpu\"</code> parameter is optional (<code>False</code> by default).</p> <p>Everytime a generation job is launched, the configuration file is mutated copied from the above generation folder, before being mutated at the end of the generation job, with the requested parameters (in this case, none, since no scan is being performed).</p> <p>After running the jobs, the directory should therefore looks like something like this (ignoring the intermediate and temporary files):</p> <pre><code>\ud83d\udcc1 single_job_study_hllhc16/\n\u251c\u2500\u2574\ud83d\udcc1 generation_1/\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 generation_1.py\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 run.sh\n\u2502   \u251c\u2500\u2500 \ud83d\udcc1 particles/\n\u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4 xx.parquet\n\u2502   \u251c\u2500\u2500 \ud83d\udcc4 local_config.yaml\n\u2502   \u2514\u2500\u2500 \ud83d\udcc1 generation_2/\n\u2502        \u251c\u2500\u2500 \ud83d\udcc4 generation_2.py\n\u2502        \u251c\u2500\u2500 \ud83d\udcc4 run.sh\n\u2502        \u251c\u2500\u2500 \ud83d\udcc4 local_config.yaml\n\u2502        \u251c\u2500\u2500 output_particles.parquet\n\u2502        \u2514\u2500\u2500 collider_file_for_tracking.json\n\u251c\u2500 \ud83d\udcc4 tree.yaml\n\u2514\u2500 \ud83d\udcc4 local_config.yaml\n</code></pre> <p>And the final tree will be (the paths will be different for you):</p> <pre><code>generation_1:\n  generation_1:\n    file: single_job_study_hllhc16/generation_1/generation_1.py\n    gpu: false\n    submission_type: local\n    status: finished\n    path_run: \n      /afs/cern.ch/work/u/user/private/study-DA/tests/generate_and_submit/single_collider_hllhc16/single_job_study_hllhc16/generation_1/run.sh\n  generation_2:\n    generation_2:\n      file: single_job_study_hllhc16/generation_1/generation_2/generation_2.py\n      gpu: false\n      submission_type: local\n      status: finished\n      path_run: \n        /afs/cern.ch/work/u/user/private/study-DA/tests/generate_and_submit/single_collider_hllhc16/single_job_study_hllhc16/generation_1/generation_2/run.sh\npython_environment: /afs/cern.ch/work/u/user/private/study-DA/.venv/bin/activate\ncontainer_image:\nabsolute_path: \n  /afs/cern.ch/work/u/user/private/study-DA/tests/generate_and_submit/single_collider_hllhc16\nstatus: finished\nconfigured: true\n</code></pre> <p>From here, one can load the collider file <code>collider_file_for_tracking.json</code> with Xsuite and/or check the result of the tracking using the file <code>output_particles.parquet</code>. </p> <p>If you want to inspect the collider object, you can use the collider-dashboard package, which should be fully compatible with the collider file produced here:</p> <pre><code>python -m collider_dashboard -c /path_to_adapt/single_collider_hllhc16/single_job_study_hllhc16/generation_1/generation_2/collider_file_for_configuration.json\n</code></pre>"},{"location":"case_studies/2_tune_scan.html","title":"Doing a tune scan","text":"<p>Tune scans are usually done in two generations: the first one allows to convert the Mad sequence to a Xsuite collider (single job), while the second one enables to configure the collider and do the tracking (scan, since many tunes are tested). Let's give an example with the <code>config_hllhc16.yaml</code> configuration template.</p>"},{"location":"case_studies/2_tune_scan.html#scan-configuration","title":"Scan configuration","text":"<p>First, let's configure our scan:</p> config_scan.yaml<pre><code># ==================================================================================================\n# --- Structure of the study ---\n# ==================================================================================================\nname: example_tune_scan\n\n# List all useful files that will be used by executable in generations below\n# These files are placed at the root of the study\ndependencies:\n  main_configuration: config_hllhc16.yaml\n\nstructure:\n  # First generation is always at the root of the study\n  # such that config_hllhc16.yaml is accessible as ../config_hllhc16.yaml\n  generation_1:\n    executable: generation_1.py\n    common_parameters:\n      # Needs to be redeclared as it's used for parallelization\n      # And re-used ine the second generation\n      n_split: 5\n\n  # Second generation depends on the config from the first generation\n  generation_2:\n    executable: generation_2_level_by_nb.py\n    scans:\n      distribution_file:\n        # Number of paths is set by n_split in the main config\n        path_list: [\"____.parquet\", n_split]\n      qx:\n        subvariables: [lhcb1, lhcb2]\n        linspace: [62.305, 62.330, 26]\n      qy:\n        subvariables: [lhcb1, lhcb2]\n        linspace: [60.305, 60.330, 26]\n        condition: qy &gt;= qx - 2 + 0.0039\n</code></pre> <p>Here, <code>generation_1.py</code> and <code>generation_2_level_by_nb.py</code> are the same template scripts as in the 1_simple_collider.md case study. The only difference is that the second generation will now be mutated in each job of generation 2 to scan the tunes.</p> <p>If no specific keyword is provided, it's the cartesian product of all the variables that will be scanned. In this case, since we added a condition on the tunes (because we're only interest in the working points above the super-diagonal), the scan will be done on the tune combinations that satisfy the condition.</p>"},{"location":"case_studies/2_tune_scan.html#study-generation","title":"Study generation","text":"<p>We can now write the script to generate the study:</p> tune_scan.py<pre><code># ==================================================================================================\n# --- Imports\n# ==================================================================================================\n\n# Import standard library modules\nimport os\n\n# Import user-defined modules\nfrom study_da import create\nfrom study_da.utils import write_dic_to_path, load_template_configuration_as_dic\n\n# ==================================================================================================\n# --- Script to generate a study\n# ==================================================================================================\n\n# Load the template configuration \nname_template_config = \"config_hllhc16.yaml\"\nconfig, ryaml = load_template_configuration_as_dic(name_template_config)\n\n# Update the location of acc-models since it's dumped in a different folder\nconfig[\"config_mad\"][\"links\"][\"acc-models-lhc\"] = \"path/to/acc-models-lhc\"\n\n# Drop the configuration locally\nwrite_dic_to_path(config, name_template_config, ryaml)\n\n# Now generate the study in the local directory\npath_tree, name_main_config = create(path_config_scan=\"config_scan.yaml\")\n\n# Delete the configuration\nos.remove(name_template_config)\n</code></pre> <p>As you can see, we load the template configuration from the package, update the location of the <code>acc-models-lhc</code> link (since it's dumped in a different folder), and then drop the configuration locally. We then generate the study in the local directory, and delete the configuration.</p> <p>How you modify the configuration is up to you </p> <p>We could also have used the configuration file of our liking, that we can for instance place in the study directory (but that's not even needed). In this case, no need to load it as a dict and modify it in a Python script, we can just modify the <code>yaml</code> file and provide the corresponding path in the <code>create</code> function.</p> <p>Anyway, at this point, the directory with all the jobs should be created, along with the corresponding tree file.</p>"},{"location":"case_studies/2_tune_scan.html#study-submission","title":"Study submission","text":"<p>We can now submit the jobs. Ideally, we would like to submit the first generation as a local job (since there's only one job, no need to queue on a cluster), and the second generation on a cluster. However, contrarily to the previous example, we're going to be lazy this time and not configure in advance the submission of the jobs. Therefore, <code>study-da</code> will ask you how you want to submit the jobs when you try to submit them. This is all in the following script (continuing from the previous one):</p> tune_scan.py<pre><code># ==================================================================================================\n# --- Script to submit the study\n# ==================================================================================================\n\n# In case gen_1 is submitted locally\ndic_additional_commands_per_gen = {\n    # To clean up the folder after the first generation if submitted locally\n    1: \"rm -rf final_* modules optics_repository optics_toolkit tools tracking_tools temp mad_collider.log __pycache__ twiss* errors fc* optics_orbit_at* \\n\"\n    # To copy back the particles folder from the first generation if submitted to HTC\n    # \"cp -r particles $path_job/particles \\n\",\n}\n\n# Dependencies for the executable of each generation. Only needed if one uses HTC or Slurm.\ndic_dependencies_per_gen = {\n    1: [\"acc-models-lhc\"],\n    2: [\"path_collider_file_for_configuration_as_input\", \"path_distribution_folder_input\"],\n}\n\n# Dic copy_back_per_gen (only matters for HTC)\ndic_copy_back_per_gen = {\n    1: {\"parquet\": True, \"yaml\": True, \"txt\": True, \"json\": True, \"zip\": True},\n    2: {\"parquet\": True, \"yaml\": True, \"txt\": True, \"json\": False, \"zip\": False},\n}\n\n# Submit the study\nsubmit(\n    path_tree=path_tree,\n    path_python_environment=\"/afs/cern.ch/work/u/user/private/study-DA/.venv\",\n    path_python_environment_container=\"/usr/local/DA_study/miniforge_docker\",\n    path_container_image=\"/cvmfs/unpacked.cern.ch/gitlab-registry.cern.ch/cdroin/da-study-docker:757f55da\",\n    dic_dependencies_per_gen=dic_dependencies_per_gen,\n    name_config=name_main_config,\n    dic_additional_commands_per_gen=dic_additional_commands_per_gen,\n    dic_copy_back_per_gen=dic_copy_back_per_gen,\n)\n</code></pre> <p>As usual, the <code>dic_additional_commands_per_gen</code> is used to clean up the folder after the first generation (since we're going to submit it locally), but it could also have been use to copy back the particles folder from the first generation if we were to submit to HTC. This is needed since HTC generations run indendently from each other, and the second generation doesn't have access to the files generated by the first generation.</p> <p>The <code>dic_dependencies_per_gen</code> is only needed if one uses HTC, and basically specifies which path should be mutated to be absolute in the configuration file of the executable, so that the executable can find the files it needs even if it's being run from a distant node.</p> <p>The <code>dic_copy_back_per_gen</code> is also only needed for HTC, and specifies which types files should be copied back to the local folder after the job is done (independently of whatever command has been inserted into the <code>dic_additional_commands_per_gen</code>, which just provides you with more freedom and, in this case, could have been needed for the particles forlder here since the <code>dic_copy_back_per_gen</code> doesn't handle folders).</p> <p>Finally, note that we must provide the paths to not only the local python environment (since we're submitting the first generation locally), but also to the container image (to pull the image from the registry) and the python environment in the container. This is because, in this example, we will decide to submit the second generation to HTC with Docker.</p> <p>Also note that in this case, we didn't specify <code>keep_submit_until_done=True</code> in the <code>submit</code> function, so the script will only submit one batch (most likely, generation) of jobs and stop running. However, you can run the script several times with no consequences: jobs that are already finished, or currently running or queuing, will not be resubmitted. When the study is finished, the script simply tells you (and warns you if some jobs were problematic).</p>"},{"location":"case_studies/2_tune_scan.html#study-post-processing-and-plotting","title":"Study post-processing and plotting","text":"<p>The following script shoud allow you to post-process the study (gather all the output data from each individual job) and plot the results. You might want to work in a Jupyter notebook from now on, since you will probably have to play with the parameters of the plot to get the best visualization:</p> postprocess_and_plot.py<pre><code># ==================================================================================================\n# --- Imports\n# ==================================================================================================\nfrom study_da.plot import get_title_from_configuration, plot_heatmap\nfrom study_da.postprocess import aggregate_output_data\n\n# ==================================================================================================\n# --- Postprocess the study\n# ==================================================================================================\n\ndf_final = aggregate_output_data(\n    \"example_tune_scan/tree.yaml\",\n    l_group_by_parameters=[\"qx_b1\", \"qy_b1\"],\n    generation_of_interest=2,\n    name_output=\"output_particles.parquet\",\n    write_output=True,\n    only_keep_lost_particles=True,\n)\n\n# ==================================================================================================\n# --- Plot\n# ==================================================================================================\n\ntitle = get_title_from_configuration(\n    df_final,\n    display_LHC_version=True,\n    display_energy=True,\n    display_bunch_index=True,\n    display_CC_crossing=True,\n    display_bunch_intensity=True,\n    display_beta=True,\n    display_crossing_IP_1=True,\n    display_crossing_IP_2=True,\n    display_crossing_IP_5=True,\n    display_crossing_IP_8=True,\n    display_bunch_length=True,\n    display_polarity_IP_2_8=True,\n    display_emittance=True,\n    display_chromaticity=True,\n    display_octupole_intensity=True,\n    display_coupling=True,\n    display_filling_scheme=True,\n    display_tune=False,\n    display_luminosity_1=True,\n    display_luminosity_2=True,\n    display_luminosity_5=True,\n    display_luminosity_8=True,\n    display_PU_1=True,\n    display_PU_2=True,\n    display_PU_5=True,\n    display_PU_8=True,\n)\n\nfig, ax = plot_heatmap(\n    df_final,\n    horizontal_variable=\"qx_b1\",\n    vertical_variable=\"qy_b1\",\n    color_variable=\"normalized amplitude in xy-plane\",\n    plot_contours=True,\n    xlabel=r\"Horizontal tune $Q_x$\",\n    ylabel=r\"Vertical tune $Q_y$\",\n    mask_lower_triangle=True,\n    symmetric_missing = True,\n    title=title,\n    vmin=4,\n    vmax=8,\n    green_contour=6.0,\n    label_cbar=\"Minimum DA (\" + r\"$\\sigma$\" + \")\",\n    output_path=\"output.png\",\n    vectorize=False,\n    fill_missing_value_with=\"interpolate\",\n)\n</code></pre> <p>Basically, the <code>aggregate_output_data</code> function will gather all the output data from each individual job of the second generation. You have to provide it yourself the parameters that you are scanning (<code>qx_b1</code> and <code>qy_b1</code> in this case), and the name of the output file that you want check for each individual jobs (if you didn't touch the configuration, it should be <code>output_particles.parquet</code>) of the generation that you are interested in (2 in this case).</p> <p>Variables name</p> <p>You might wonder why the name for the tunes are <code>qx_b1</code> and <code>qy_b1</code> when they're defined as <code>qx: lhcb1</code> and <code>qy: lhcb1</code> in the scan configuration file. This is just a simplification of the name. The whole dictionnary showing the final mapping for the name of the parameters is available here</p> <p><code>write_output</code> tells the function if it should write the aggregated data to a file (<code>da.parquet</code>  by default). It is useful since aggregating the data can be quite long, and you might want to save it for later.</p> <p>Finally,  <code>only_keep_lost_particles</code> tells the function if it should only keep the data from lost particles (if you're only interested in the DA, for example); this is useful since the output data can be quite large.</p> <p>From here, you will have to customize the title of the plot to your liking. I won't detail every single parameters as there are many, but they should be quite explicit.</p> <p>Finally, you can plot the result, with, again, many possibilities for customization. I only used the parameters that I thought were the most important, but you can find more in the documentation of the <code>plot_heatmap</code> function.</p> <p>Note that, since we only ran jobs on the super-diagonal, we have to mask the lower triangle of the heatmap and set <code>symmetric_missing</code> to True (for proper smoothing). Also note that, in this case, one job didn't work for some reason, so I had to fill the missing value with an interpolation. This is why I used the <code>fill_missing_value_with=\"interpolate\"</code> parameter.</p> <p>Just for illustration, here's the final plot that I obtained:</p> <p></p>"},{"location":"case_studies/3_octupole_scan_diagonal.html","title":"Doing an octupole scan along the tune superdiagonal","text":"<p>Octupole scans are an other type of scans that are frequently requested. Just like tune scan, they are usually done in second generation: the first one allows to convert the Mad sequence to a Xsuite collider (single job), while the second one enables to configure the collider and do the scan for various octupoles and tune values (tune values being on the superdiagonal). Let's give an example with the <code>config_runIII.yaml</code> configuration template.</p> <p>You're getting better at this! I'm getting lazy...</p> <p>You should have a reasonable understanding of the package by now. Therefore, details about the scripts and the configuration will not be explained in detail. If you need more information, please refer to the previous case studies.</p>"},{"location":"case_studies/3_octupole_scan_diagonal.html#scan-configuration","title":"Scan configuration","text":"<p>First, let's configure our scan. We want to do a two-dimensional scan, the first dimension being the tune on the superdiagonal (basically having <code>qx</code> and <code>qy</code> evolving together), and the second dimension being the octupole current.</p> config_scan.yaml<pre><code># ==================================================================================================\n# --- Structure of the study ---\n# ==================================================================================================\nname: example_oct_scan\n\n# List all useful files that will be used by executable in generations below\n# These files are placed at the root of the study\ndependencies:\n  main_configuration: config_runIII.yaml\n\nstructure:\n  # First generation is always at the root of the study\n  # such that config_runIII.yaml is accessible as ../config_runIII.yaml\n  generation_1:\n    executable: generation_1.py\n    scans:\n    common_parameters:\n      # Needs to be redeclared as it's used for parallelization\n      # And re-used ine the second generation\n      n_split: 5\n\n  # Second generation depends on the config from the first generation\n  generation_2:\n    executable: generation_2_level_by_nb.py\n    scans:\n      distribution_file:\n        # Number of paths is set by n_split in the main config\n        path_list: [\"____.parquet\", n_split]\n      qx:\n        subvariables: [lhcb1, lhcb2]\n        linspace: [62.305, 62.330, 26]\n      qy:\n        subvariables: [lhcb1, lhcb2]\n        expression: qx - 2 + 0.005\n        concomitant: [qx]\n      i_oct_b1:\n        linspace: [-300, 300, 21]\n      i_oct_b2:\n        linspace: [-300, 300, 21]\n        concomitant: [i_oct_b1]\n</code></pre> <p>As you can see, we use an <code>expression</code> to stipulate how <code>qy</code> should be generated from <code>qx</code>. In addition, we use the <code>concomitant</code> keyword to specify that <code>qy</code> should be scanned along with <code>qx</code>, i.e. not taking the cartesian product of the two variables. Similarly, for the octupole, we use the <code>concomitant</code> keyword to specify that the octupole current for beam 2 should be scanned along with the octupole current for beam 1. Note that, instead of a linspace for <code>i_oct_b2</code>, we could have used an <code>expression</code> to generate the octupole current from the octupole current for beam 1 (<code>expression: i_oct_b1</code>).</p>"},{"location":"case_studies/3_octupole_scan_diagonal.html#study-generation","title":"Study generation","text":"<p>We can now write the script to generate the study:</p> oct_scan.py<pre><code># ==================================================================================================\n# --- Imports\n# ==================================================================================================\n\n# Import standard library modules\nimport os\n\n# Import third-party modules\n# Import user-defined modules\nfrom study_da import create, submit\nfrom study_da.utils import load_template_configuration_as_dic, write_dic_to_path\n\n# ==================================================================================================\n# --- Script to generate a study\n# ==================================================================================================\n\n# Load the template configuration\nname_template_config = \"config_runIII.yaml\"\nconfig, ryaml = load_template_configuration_as_dic(name_template_config)\n\n# Do changes if needed\n# config[\"config_simulation\"][\"n_turns\"] = 1000000\n\n# Drop the configuration locally\nwrite_dic_to_path(config, \"config_runIII.yaml\", ryaml)\n\n# Now generate the study in the local directory\npath_tree, name_main_config = create(path_config_scan=\"config_scan.yaml\", force_overwrite=False)\n\n# Delete the configuration\nos.remove(\"config_runIII.yaml\")\n\n# ==================================================================================================\n# --- Script to submit the study\n# ==================================================================================================\n\n# In case gen_1 is submitted locally\ndic_additional_commands_per_gen = {\n    # To clean up the folder after the first generation if submitted locally\n    1: \"rm -rf final_* modules optics_repository optics_toolkit tools tracking_tools temp mad_collider.log __pycache__ twiss* errors fc* optics_orbit_at* \\n\"\n    # To copy back the particles folder from the first generation if submitted to HTC\n    # \"cp -r particles $path_job/particles \\n\",\n}\n\n# Dependencies for the executable of each generation. Only needed if one uses HTC or Slurm.\ndic_dependencies_per_gen = {\n    1: [\"acc-models-lhc\"],\n    2: [\"path_collider_file_for_configuration_as_input\", \"path_distribution_folder_input\"],\n}\n\n# Submit the study\nsubmit(\n    path_tree=path_tree,\n    path_python_environment=\"/afs/cern.ch/work/u/user/private/study-DA/.venv\",\n    path_python_environment_container=\"/usr/local/DA_study/miniforge_docker\",\n    path_container_image=\"/cvmfs/unpacked.cern.ch/gitlab-registry.cern.ch/cdroin/da-study-docker:757f55da\",\n    dic_dependencies_per_gen=dic_dependencies_per_gen,\n    name_config=name_main_config,\n    dic_additional_commands_per_gen=dic_additional_commands_per_gen,\n    one_generation_at_a_time=True,\n)\n</code></pre> <p>As you can see, this script is very similar to the one from the previous case study.</p> <p>One notable difference is that we don't modify the location of \"acc-models-lhc\" in the configuration since the path is already absolute.</p> <p>In addition, note that we've specified <code>one_generation_at_a_time=True</code>. Therefore, even if you relaunch the script while some jobs of the second generation are ready to be launched (because the first generation is already partially finished), no jobs will be submitted. In this case, a generation can only be submitted all at once, i.e. if all the jobs from the generation above are finished (or failed). This is not so relevant for this example since there are only two generations, and generation has only 1 job, but it can be very useful for more complex studies (e.g. to run checks on all jobs of a generation, between generations submission).</p>"},{"location":"case_studies/3_octupole_scan_diagonal.html#study-post-processing-and-plotting","title":"Study post-processing and plotting","text":"<p>The script for post-processing and plotting is also very similar to the one from the previous case study:</p> postprocess_and_plot.py<pre><code># ==================================================================================================\n# --- Imports\n# ==================================================================================================\nfrom study_da.plot import get_title_from_configuration, plot_heatmap\nfrom study_da.postprocess import aggregate_output_data\n\n# ==================================================================================================\n# --- Postprocess the study\n# ==================================================================================================\n\ndf_final = aggregate_output_data(\n    \"example_oct_scan/tree.yaml\",\n    l_group_by_parameters=[\"qx_b1\", \"qy_b1\", \"i_oct_b1\", \"i_oct_b2\"],\n    generation_of_interest=2,\n    name_output=\"output_particles.parquet\",\n    write_output=True,\n    only_keep_lost_particles=True,\n    force_overwrite=False,\n)\n\n# ==================================================================================================\n# --- Plot\n# ==================================================================================================\n\ntitle = get_title_from_configuration(\n    df_final,\n    crossing_type=\"vh\",\n    display_LHC_version=True,\n    display_energy=True,\n    display_bunch_index=True,\n    display_CC_crossing=False,\n    display_bunch_intensity=True,\n    display_beta=True,\n    display_crossing_IP_1=True,\n    display_crossing_IP_2=True,\n    display_crossing_IP_5=True,\n    display_crossing_IP_8=True,\n    display_bunch_length=True,\n    display_polarity_IP_2_8=True,\n    display_emittance=True,\n    display_chromaticity=True,\n    display_octupole_intensity=False,\n    display_coupling=True,\n    display_filling_scheme=True,\n    display_tune=False,\n    display_luminosity_1=True,\n    display_luminosity_2=True,\n    display_luminosity_5=True,\n    display_luminosity_8=True,\n    display_PU_1=True,\n    display_PU_2=True,\n    display_PU_5=True,\n    display_PU_8=True,\n)\n\nfig, ax = plot_heatmap(\n    df_final,\n    horizontal_variable=\"i_oct_b1\",\n    vertical_variable=\"qx_b1\",\n    color_variable=\"normalized amplitude in xy-plane\",\n    plot_contours=True,\n    xlabel=\"Octupole intensity [A]\",\n    ylabel=r\"$Q_x$\" + \"with \" + r\"$Q_y = Q_x -2 + 0.005$\",\n    title=title,\n    vmin=4.0,\n    vmax=8.0,\n    green_contour=6.0,\n    label_cbar=\"Minimum DA (\" + r\"$\\sigma$\" + \")\",\n    output_path=\"oct_scan.png\",\n    vectorize=False,\n    xaxis_ticks_on_top=False,\n    plot_diagonal_lines=False,\n)\n</code></pre> <p>Note that, in this case, the parameters we used to group by the output data with are both the tune and the octupole current for beam 1 and beam 2. Also, in this case, we have to specify the type of crossing for the plotting since it can't be parsed from the configuration. Finally, a few adjustements are needed in the <code>plot_heatmap</code> function, they should be self-explanatory.</p> <p>Just for illustration, here is the output of the plot (not vectorized):</p> <p></p>"},{"location":"case_studies/4_3D_scan.html","title":"Doing a three-dimensional scan","text":"<p>Three-dimensional scans are more rarely requested, but they can be useful when scanning several interdependent variables. For instance, the optimal DA is known to depend on the vertical and horizontal tunes, but also on the octupoles. Yet the optimal tune will depend on the octupole current. In this case, a three-dimensional scan might be helpful.</p> <p>Just like tune scans and octupole scans, they are usually done in two generations: the first one allows to convert the Mad sequence to a Xsuite collider (single job), while the second one enables to configure the collider and do the scan for whatever multidimensiontal parametric analysis you want to do. Let's give an example with the <code>config_hllhc13.yaml</code> configuration template (that you can find here).</p>"},{"location":"case_studies/4_3D_scan.html#scan-configuration","title":"Scan configuration","text":"<p>As usual, let's first configure our scan.</p> config_scan.yaml<pre><code># ==================================================================================================\n# --- Structure of the study ---\n# ==================================================================================================\nname: example_3D_scan\n\n# List all useful files that will be used by executable in generations below\n# These files are placed at the root of the study\ndependencies:\n  main_configuration: config_hllhc13.yaml\n\nstructure:\n  # First generation is always at the root of the study\n  # such that config_hllhc16.yaml is accessible as ../config_hllhc16.yaml\n  generation_1:\n    executable: generation_1.py\n    common_parameters:\n      # Needs to be redeclared as it's used for parallelization\n      # And re-used ine the second generation\n      n_split: 5\n\n  # Second generation depends on the config from the first generation\n  generation_2:\n    executable: generation_2_level_by_nb.py\n    scans:\n      distribution_file:\n        # Number of paths is set by n_split in the main config\n        path_list: [\"____.parquet\", n_split]\n      qx:\n        subvariables: [lhcb1, lhcb2]\n        linspace: [62.310, 62.325, 11]\n      qy:\n        subvariables: [lhcb1, lhcb2]\n        expression: qx - 2 + 0.005\n        concomitant: [qx]\n      i_oct_b1:\n        linspace: [-300, 300, 6]\n      i_oct_b2:\n        linspace: [-300, 300, 6]\n        concomitant: [i_oct_b1]\n      nemitt_x:\n        linspace: [2.e-6, 3.e-6, 6]\n      nemitt_y:\n        expression: nemitt_x\n        concomitant: [nemitt_x]\n</code></pre> <p>You might find the scan in the <code>generation_2</code> section a bit more complex than usual, but all the keywords should be relatively clear by now. Note that I used an <code>expression</code> for the definition of <code>nemitt_y</code>in term of <code>nemitt_x</code>. This is a way to define a variable in term of another one, which is useful when you want to keep the same value for both variables. It's simpler than defining a <code>linspace</code> with the same value for the two variables, as done with the octupoles.</p>"},{"location":"case_studies/4_3D_scan.html#study-generation","title":"Study generation","text":"<p>We can now write the script to generate the study:</p> tune_oct_emit_scan.py<pre><code># ==================================================================================================\n# --- Imports\n# ==================================================================================================\n\n# Import standard library modules\nimport os\n\n# Import third-party modules\n# Import user-defined modules\nfrom study_da import create, submit\nfrom study_da.utils import load_template_configuration_as_dic, write_dic_to_path\n\n# ==================================================================================================\n# --- Script to generate a study\n# ==================================================================================================\n\n# Load the template configuration\nname_template_config = \"config_hllhc13.yaml\"\nconfig, ryaml = load_template_configuration_as_dic(name_template_config)\n\n# Update the location of acc-models since it's copied in a different folder\nconfig[\"config_mad\"][\"links\"][\"acc-models-lhc\"] = (\n    \"../../../../../external_dependencies/acc-models-lhc-v13\"\n)\n\n# Drop the configuration locally\nwrite_dic_to_path(config, name_template_config, ryaml)\n\n# Now generate the study in the local directory\npath_tree, name_main_config = create(path_config_scan=\"config_scan.yaml\", force_overwrite=False)\n\n# Delete the configuration\nos.remove(name_template_config)\n\n# ==================================================================================================\n# --- Script to submit the study\n# ==================================================================================================\n\n# In case gen_1 is submitted locally\ndic_additional_commands_per_gen = {\n    # To clean up the folder after the first generation if submitted locally\n    1: \"rm -rf final_* modules optics_repository optics_toolkit tools tracking_tools temp mad_collider.log __pycache__ twiss* errors fc* optics_orbit_at* \\n\"\n}\n\n# Dependencies for the executable of each generation. Only needed if one uses HTC or Slurm.\ndic_dependencies_per_gen = {\n    1: [\"acc-models-lhc\"],\n    2: [\"path_collider_file_for_configuration_as_input\", \"path_distribution_folder_input\"],\n}\n\n# Submit the study\nsubmit(\n    path_tree=path_tree,\n    path_python_environment=\"/afs/cern.ch/work/c/cdroin/private/study-DA/.venv\",\n    path_python_environment_container=\"/usr/local/DA_study/miniforge_docker\",\n    path_container_image=\"/cvmfs/unpacked.cern.ch/gitlab-registry.cern.ch/cdroin/da-study-docker:757f55da\",\n    dic_dependencies_per_gen=dic_dependencies_per_gen,\n    name_config=name_main_config,\n    dic_additional_commands_per_gen=dic_additional_commands_per_gen,\n)\n</code></pre> <p>There shouldn't be anything new to understand here.</p>"},{"location":"case_studies/4_3D_scan.html#study-post-processing-and-plotting","title":"Study post-processing and plotting","text":"<p>The code for post-processing and plotting is a bit different:</p> postprocess_and_plot.py<pre><code># ==================================================================================================\n# --- Imports\n# ==================================================================================================\nimport pandas as pd\n\nfrom study_da.plot import plot_3D\nfrom study_da.postprocess import aggregate_output_data\n\n# ==================================================================================================\n# --- Postprocess the study\n# ==================================================================================================\n\ndf_final = aggregate_output_data(\n    \"example_3D_scan/tree.yaml\",\n    l_group_by_parameters=[\"qx_b1\", \"qy_b1\", \"i_oct_b1\", \"i_oct_b2\", \"nemitt_x\", \"nemitt_y\"],\n    generation_of_interest=2,\n    name_output=\"output_particles.parquet\",\n    write_output=True,\n    only_keep_lost_particles=True,\n)\n\n\n# Some more manual postprocessing to fill the missing values\ndf_final = df_final[[\"qx_b1\", \"i_oct_b1\", \"nemitt_x\", \"normalized amplitude in xy-plane\"]]\ndf_final = df_final.drop_duplicates()\ndf_final = df_final.set_index([\"qx_b1\", \"i_oct_b1\", \"nemitt_x\"])\nidx = pd.MultiIndex.from_product(\n    [df_final.index.levels[0], df_final.index.levels[1], df_final.index.levels[2]]\n)\ndf_final = df_final.reindex(idx, fill_value=None).reset_index()\n\n# Interpolate missing values in df_final for the column \"normalized amplitude in xy-plane\"\ndf_final[\"normalized amplitude in xy-plane\"] = df_final.groupby([\"qx_b1\", \"i_oct_b1\"])[\n    \"normalized amplitude in xy-plane\"\n].transform(lambda x: x.interpolate(method=\"linear\", limit_direction=\"both\"))\n\n# Fill remaining missing values with 8 as it corresponds to simulation with no lost particles\ndf_final[\"normalized amplitude in xy-plane\"] = df_final[\"normalized amplitude in xy-plane\"].fillna(\n    8\n)\n# ==================================================================================================\n# --- Plot\n# ==================================================================================================\n\n\nfig = plot_3D(\n    df_final,\n    \"qx_b1\",\n    \"i_oct_b1\",\n    \"nemitt_x\",\n    \"normalized amplitude in xy-plane\",\n    xlabel=r\"Qx\",\n    ylabel=r\"I [A]\",\n    z_label=r\"Normalized emittance [\u03bcm]\",\n    vmin=3.9,\n    vmax=8.1,\n    output_path=\"3D.png\",\n    output_path_html=\"3D.html\",\n    surface_count=10,\n    display_plot=False,\n    display_colormap=False,\n    figsize=(600, 600),\n)\n</code></pre> <p>As you can see, we need to aggregate the data for all the relevant variables in the <code>l_group_by_parameters</code> list. However, in this scan, several points are missing, due to either failed jobs (e.g. mismatched due to tune value) or no lost particles. Therefore, a bit more of postprocessing is needed before being able to do the plotting. This postprocessing will be very specific to your study, so you might need to adapt it.</p> <p>Finally, the 3D plotting function should be relatively straightforward to use, especially if you've used Plotly in the past. Note that a high number of <code>surface_count</code> will make the plot more detailed, but also heavier. Also note that the rendering of Latex equations is a bit capricious... So we didn't include a title in this plot(although it's possible, but you will have to play around with the latex string, and this is not the purpose of this tutorial).</p> <p>Just for illustration, here is the output of the plot (interactive, although the png is also available):</p>"},{"location":"case_studies/5_1_generational_scan.html","title":"Doing an 1-generational scan","text":"<p>Doing a 1-generational scan is usually not optimal as the step consisting in builing the Xsuite collider from the MAD-X files is time-consuming and identical for all the parametrization. However, you might want to proceed differently, so here's a demonstration of how you can adapt the different configuration files and scripts to do a 1-generational scan. We will work with the default run III ions configuration for this example, doing a small tune scan.</p>"},{"location":"case_studies/5_1_generational_scan.html#scan-configuration","title":"Scan configuration","text":"<p>The scan configuration is very simple, as we only have one generation.</p> config_scan.yaml<pre><code># ==================================================================================================\n# --- Structure of the study ---\n# ==================================================================================================\nname: example_scan_1_gen\n\n# List all useful files that will be used by executable in generations below\n# These files are placed at the root of the study\ndependencies:\n  main_configuration: config_runIII_ions.yaml\n\nstructure:\n  # First generation is always at the root of the study\n  # such that config_hllhc16.yaml is accessible as ../config_hllhc16.yaml\n  generation_1:\n    executable: custom_files/generation_1_custom.py\n    common_parameters:\n      # Needs to be redeclared as it's used for parallelization\n      # And re-used ine the second generation\n      n_split: 5\n    scans:\n      distribution_file:\n        # Number of paths is set by n_split in the main config\n        path_list: [\"____.parquet\", n_split]\n      qx:\n        subvariables: [lhcb1, lhcb2]\n        linspace: [62.305, 62.330, 11]\n      qy:\n        subvariables: [lhcb1, lhcb2]\n        linspace: [60.305, 60.330, 11]\n        condition: qy &gt;= qx - 2 + 0.0039\n</code></pre>"},{"location":"case_studies/5_1_generational_scan.html#template-script","title":"Template script","text":"<p>As you can see in the configuration scan, we need to create a template script for our job. We will do this by basically merging the <code>generation_1.py</code> and <code>generation_2_level_by_nb.py</code> scripts (see here).</p> custom_files/generation_1_custom.py<pre><code>\"\"\"This is a template script for generation 1 of simulation study, in which ones generates a\nparticle distribution and a collider from a MAD-X model.\"\"\"\n\n# ==================================================================================================\n# --- Imports\n# ==================================================================================================\n\n# Import standard library modules\nimport contextlib\nimport logging\nimport os\nimport time\n\n# Import third-party modules\nimport numpy as np\nimport pandas as pd\n\n# Import user-defined modules\nfrom study_da.generate import (\n    MadCollider,\n    ParticlesDistribution,\n    XsuiteCollider,\n    XsuiteTracking,\n)\nfrom study_da.utils import (\n    load_dic_from_path,\n    set_item_in_dic,\n    write_dic_to_path,\n)\n\n# Set up the logger here if needed\n\n\n# ==================================================================================================\n# --- Script functions\n# ==================================================================================================\ndef build_distribution(config_particles):\n    # Build object for generating particle distribution\n    distr = ParticlesDistribution(config_particles)\n\n    # Build particle distribution\n    particle_list = distr.return_distribution_as_list()\n\n    # Write particle distribution to file\n    distr.write_particle_distribution_to_disk(particle_list)\n\n\ndef build_collider(config_mad):\n    # Build object for generating collider from mad\n    mc = MadCollider(config_mad)\n\n    # Build mad model\n    mad_b1b2, mad_b4 = mc.prepare_mad_collider()\n\n    # Build collider from mad model\n    collider = mc.build_collider(mad_b1b2, mad_b4)\n\n    # Twiss to ensure everything is ok\n    mc.activate_RF_and_twiss(collider)\n\n    # Clean temporary files\n    mc.clean_temporary_files()\n\n    # Save collider to json\n    mc.write_collider_to_disk(collider)\n\n    # Return the collider\n    return collider\n\n\ndef configure_collider(full_configuration):\n    # Get configuration\n    config_collider = full_configuration[\"config_collider\"]\n    ver_hllhc_optics = full_configuration[\"config_mad\"][\"ver_hllhc_optics\"]\n    ver_lhc_run = full_configuration[\"config_mad\"][\"ver_lhc_run\"]\n    ions = full_configuration[\"config_mad\"][\"ions\"]\n    collider_filepath = full_configuration[\"config_collider\"][\n        \"path_collider_file_for_configuration_as_input\"\n    ]\n\n    # Build object for configuring collider\n    xc = XsuiteCollider(config_collider, collider_filepath, ver_hllhc_optics, ver_lhc_run, ions)\n\n    # Load collider\n    collider = xc.load_collider()\n\n    # Install beam-beam\n    xc.install_beam_beam_wrapper(collider)\n\n    # Build trackers\n    # For now, start with CPU tracker due to a bug with Xsuite\n    # Refer to issue https://github.com/xsuite/xsuite/issues/450\n    collider.build_trackers()  # (_context=context)\n\n    # Set knobs\n    xc.set_knobs(collider)\n\n    # Match tune and chromaticity\n    xc.match_tune_and_chroma(collider, match_linear_coupling_to_zero=True)\n\n    # Set filling scheme\n    xc.set_filling_and_bunch_tracked(ask_worst_bunch=False)\n\n    # Compute the number of collisions in the different IPs\n    n_collisions_ip1_and_5, n_collisions_ip2, n_collisions_ip8 = xc.compute_collision_from_scheme()\n\n    # Do the leveling if requested\n    if \"config_lumi_leveling\" in config_collider and not config_collider[\"skip_leveling\"]:\n        xc.level_all_by_separation(\n            n_collisions_ip1_and_5, n_collisions_ip2, n_collisions_ip8, collider\n        )\n    else:\n        logging.warning(\n            \"No leveling is done as no configuration has been provided, or skip_leveling\"\n            \" is set to True.\"\n        )\n\n    # Add linear coupling\n    xc.add_linear_coupling(collider)\n\n    # Rematch tune and chromaticity\n    xc.match_tune_and_chroma(collider, match_linear_coupling_to_zero=False)\n\n    # Assert that tune, chromaticity and linear coupling are correct one last time\n    xc.assert_tune_chroma_coupling(collider)\n\n    # Configure beam-beam if needed\n    if not xc.config_beambeam[\"skip_beambeam\"]:\n        xc.configure_beam_beam(collider)\n\n    # Update configuration with luminosity now that bb is known\n    l_n_collisions = [\n        n_collisions_ip1_and_5,\n        n_collisions_ip2,\n        n_collisions_ip1_and_5,\n        n_collisions_ip8,\n    ]\n    xc.record_final_luminosity(collider, l_n_collisions)\n\n    # Save collider to json (flag to save or not is inside function)\n    xc.write_collider_to_disk(collider, full_configuration)\n\n    # Get fingerprint\n    fingerprint = xc.return_fingerprint(collider)\n\n    return collider, fingerprint\n\n\ndef track_particles(full_configuration, collider, fingerprint):\n    # Get emittances\n    n_emitt_x = full_configuration[\"config_collider\"][\"config_beambeam\"][\"nemitt_x\"]\n    n_emitt_y = full_configuration[\"config_collider\"][\"config_beambeam\"][\"nemitt_y\"]\n    xst = XsuiteTracking(full_configuration[\"config_simulation\"], n_emitt_x, n_emitt_y)\n\n    # Prepare particle distribution\n    particles, particle_id, l_amplitude, l_angle = xst.prepare_particle_distribution_for_tracking(\n        collider\n    )\n\n    # Track\n    particles_dict = xst.track(collider, particles)\n\n    # Convert particles to dataframe\n    particles_df = pd.DataFrame(particles_dict)\n\n    # ! Very important, otherwise the particles will be mixed in each subset\n    # Sort by parent_particle_id\n    particles_df = particles_df.sort_values(\"parent_particle_id\")\n\n    # Assign the old id to the sorted dataframe\n    particles_df[\"particle_id\"] = particle_id\n\n    # Register the amplitude and angle in the dataframe\n    particles_df[\"normalized amplitude in xy-plane\"] = l_amplitude\n    particles_df[\"angle in xy-plane [deg]\"] = l_angle * 180 / np.pi\n\n    # Add some metadata to the output for better interpretability\n    particles_df.attrs[\"hash\"] = hash(fingerprint)\n    particles_df.attrs[\"fingerprint\"] = fingerprint\n    particles_df.attrs[\"configuration\"] = full_configuration\n    particles_df.attrs[\"date\"] = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    # Save output\n    particles_df.to_parquet(\n        full_configuration[\"config_simulation\"][\"path_distribution_file_output\"]\n    )\n\n\ndef clean():\n    # Remote the correction folder, and potential C files remaining\n    with contextlib.suppress(Exception):\n        os.system(\"rm -rf correction\")\n        os.system(\"rm -f *.cc\")\n\n\n# ==================================================================================================\n# --- Parameters placeholders definition\n# ==================================================================================================\ndict_mutated_parameters = {}  ###---parameters---###\npath_configuration = \"{}  ###---main_configuration---###\"\n# In case the placeholders have not been replaced, use default path\nif path_configuration.startswith(\"{}\"):\n    path_configuration = \"config.yaml\"\n\n# ==================================================================================================\n# --- Script for execution\n# ==================================================================================================\n\nif __name__ == \"__main__\":\n    logging.info(\"Starting script\")\n\n    # Load full configuration\n    full_configuration, ryaml = load_dic_from_path(path_configuration)\n\n    # Mutate parameters in configuration\n    for key, value in dict_mutated_parameters.items():\n        set_item_in_dic(full_configuration, key, value)\n\n    # Dump configuration\n    name_configuration = os.path.basename(path_configuration)\n    write_dic_to_path(full_configuration, name_configuration, ryaml)\n\n    # Build and save particle distribution\n    build_distribution(full_configuration[\"config_particles\"])\n\n    # Build and save collider\n    build_collider(full_configuration[\"config_mad\"])\n\n    # Configure collider\n    collider, fingerprint = configure_collider(full_configuration)\n\n    # Drop updated configuration\n    name_configuration = os.path.basename(path_configuration)\n    write_dic_to_path(full_configuration, name_configuration, ryaml)\n\n    # Track particles and save to disk\n    track_particles(full_configuration, collider, fingerprint)\n\n    # Clean temporary files\n    clean()\n\n    logging.info(\"Script finished\")\n</code></pre> <p>This is a long file but there's really nothing new in here: it's literally the combination of the two scripts we've seen before.</p>"},{"location":"case_studies/5_1_generational_scan.html#creating-and-submitting-the-study","title":"Creating and submitting the study","text":"<p>The script to create and submit the study might be the trickier one, as we'll have to do a few adjustments to make sure everything works as expected. Here's the script:</p> create_and_submit_study.py<pre><code># ==================================================================================================\n# --- Imports\n# ==================================================================================================\n\n# Import standard library modules\nimport logging\nimport os\n\n# Import third-party modules\n# Import user-defined modules\nfrom study_da import create, submit\nfrom study_da.utils import load_template_configuration_as_dic, write_dic_to_path\n\n# Set up the logger\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n\n# ==================================================================================================\n# --- Script to generate a study\n# ==================================================================================================\n# Load the template configuration\nname_template_config = \"config_runIII_ions.yaml\"\nconfig, ryaml = load_template_configuration_as_dic(name_template_config)\n\n# Update the location of the paths since everything is in the same directory in this scan\nconfig[\"config_collider\"][\"path_collider_file_for_configuration_as_input\"] = (\n    \"collider_file_for_configuration.json\"\n)\nconfig[\"config_simulation\"][\"path_collider_file_for_tracking_as_input\"] = (\n    \"collider_file_for_tracking.json\"\n)\n\nconfig[\"config_simulation\"][\"path_distribution_folder_input\"] = \"particles\"\n\n# Drop the configuration locally\nwrite_dic_to_path(config, name_template_config, ryaml)\n\n# Now generate the study in the local directory\npath_tree, name_main_config = create(path_config_scan=\"config_scan.yaml\", force_overwrite=False)\n\n# Delete the configuration\nos.remove(name_template_config)\n\n\n# Define the variables of interest for the submission\npath_python_environment = \"/afs/cern.ch/work/c/cdroin/private/study-DA/.venv\"\npath_python_environment_container = \"/usr/local/DA_study/miniforge_docker\"\npath_container_image = (\n    \"/cvmfs/unpacked.cern.ch/gitlab-registry.cern.ch/cdroin/da-study-docker:757f55da\"\n)\nforce_configure = False\n\n# Dependencies for the executable of each generation. Only needed if one uses HTC.\ndic_dependencies_per_gen = {\n    1: [\"acc-models-lhc\"],\n}\n\n\n# Preconfigure submission to HTC\ndic_config_jobs = {\n    \"generation_1\" + \".py\": {\n        \"request_gpu\": False,\n        \"submission_type\": \"htc_docker\",\n        \"htc_flavor\": \"testmatch\",\n    }\n}\n\n\n# Submit the study\nsubmit(\n    path_tree=path_tree,\n    path_python_environment=path_python_environment,\n    path_python_environment_container=path_python_environment_container,\n    path_container_image=path_container_image,\n    force_configure=force_configure,\n    dic_dependencies_per_gen=dic_dependencies_per_gen,\n    name_config=name_main_config,\n    dic_config_jobs=dic_config_jobs,\n)\n</code></pre> <p>The main thing to notice is that the paths for the collider or particles folders must be adapted since everything will now be in the same directory. Otherwise, the script is pretty much the same as the one we've seen before.</p>"},{"location":"case_studies/5_1_generational_scan.html#postprocessing-and-plotting","title":"Postprocessing and plotting","text":"postprocess_and_plot.py<pre><code># ==================================================================================================\n# --- Imports\n# ==================================================================================================\nfrom study_da.plot import get_title_from_configuration, plot_heatmap\nfrom study_da.postprocess import aggregate_output_data\n\n# ==================================================================================================\n# --- Postprocess the study\n# ==================================================================================================\n\ndf_final = aggregate_output_data(\n    \"example_scan_1_gen/tree.yaml\",\n    l_group_by_parameters=[\"qx_b1\", \"qy_b1\"],\n    generation_of_interest=1,\n    name_output=\"output_particles.parquet\",\n    write_output=True,\n    only_keep_lost_particles=True,\n)\n\n# ==================================================================================================\n# --- Plot\n# ==================================================================================================\n\ntitle = get_title_from_configuration(\n    df_final,\n    ions=True,\n    display_LHC_version=True,\n    display_energy=True,\n    display_bunch_index=True,\n    display_CC_crossing=True,\n    display_bunch_intensity=True,\n    display_beta=True,\n    display_crossing_IP_1=True,\n    display_crossing_IP_2=True,\n    display_crossing_IP_5=True,\n    display_crossing_IP_8=True,\n    display_bunch_length=True,\n    display_polarity_IP_2_8=True,\n    display_emittance=True,\n    display_chromaticity=True,\n    display_octupole_intensity=True,\n    display_coupling=True,\n    display_filling_scheme=True,\n    display_tune=False,\n    display_luminosity_1=True,\n    display_luminosity_2=True,\n    display_luminosity_5=True,\n    display_luminosity_8=True,\n    display_PU_1=False,\n    display_PU_2=False,\n    display_PU_5=False,\n    display_PU_8=False,\n)\n\nfig, ax = plot_heatmap(\n    df_final,\n    horizontal_variable=\"qx_b1\",\n    vertical_variable=\"qy_b1\",\n    color_variable=\"normalized amplitude in xy-plane\",\n    plot_contours=False,\n    xlabel=r\"Horizontal tune $Q_x$\",\n    ylabel=r\"Vertical tune $Q_y$\",\n    tick_interval=1,\n    symmetric_missing=True,\n    mask_lower_triangle=True,\n    shift_diagonal_lines=0,\n    title=title,\n    vmin=4,\n    vmax=16,\n    green_contour=2.0,\n    label_cbar=\"Minimum DA (\" + r\"$\\sigma$\" + \")\",\n    output_path=\"1_gen_tune_scan.png\",\n    vectorize=False,\n    fill_missing_value_with=\"interpolate\",\n)\n</code></pre> <p>The script to do the postprocessing is quite similar to the initial tune scan example. The only difference is that we now want to analyse the output of the generation 1.</p> <p>However, for the plotting, there are a few differences to consider:</p> <ul> <li>We have to update the beta values</li> <li>Since we're studying ions, we have to set the <code>ions</code> parameter to <code>True</code> in the <code>get_title_from_configuration</code> function.</li> <li>Since the corresponding pile-up values are very low, we don't want to display them in the title.</li> <li>We want to display all the ticks since there are only 11 points in the scan. We set <code>tick_interval=1</code>.</li> <li>The diagonal lines have been shifted for some reason (this is most likely due to the fact that the scan is smaller than usual, not sure exactly why), so we have to set <code>shift_diagonal_lines=0</code> in the <code>plot_heatmap</code> function (<code>1</code> is the default value).</li> <li>We don't want to display the contours since the scan is small</li> <li>Since we're tracking ions, the DA tends to be much higher. Therefore, we have to adapt the <code>vmin</code> and <code>vmax</code> values.</li> </ul> <p>And that's it! Just for the sake of completeness, here's the final plot:</p> <p></p>"},{"location":"case_studies/6_3_generational_scan.html","title":"Doing an 3-generational scan","text":"<p>In this case study, we will perform a 3-generational scan. Because the parameter space grows exponentially, we will just do a dummy scan with very few points, and no final plotting. The idea is just to demonstrate how to adapt the files to perform a 3-generational scan.</p>"},{"location":"case_studies/6_3_generational_scan.html#scan-configuration","title":"Scan configuration","text":"<p>The scan configuration is as follows:</p> config_scan.yaml<pre><code># ==================================================================================================\n# --- Structure of the study ---\n# ==================================================================================================\nname: example_scan_3_gen\n\n# List all useful files that will be used by executable in generations below\n# These files are placed at the root of the study\ndependencies:\n  main_configuration: custom_files/config_hllhc16_3_gen.yaml\n\nstructure:\n  # First generation is always at the root of the study\n  # such that config_hllhc16.yaml is accessible as ../config_hllhc16.yaml\n  generation_1:\n    executable: generation_1.py\n    common_parameters:\n      # Needs to be redeclared as it's used for parallelization\n      # And re-used ine the second generation\n      n_split: 2\n\n  # Second generation depends on the config from the first generation\n  generation_2:\n    executable: custom_files/generation_2_configure.py\n    scans:\n      distribution_file:\n        # Number of paths is set by n_split in the main config\n        path_list: [\"____.parquet\", n_split]\n      qx:\n        subvariables: [lhcb1, lhcb2]\n        linspace: [62.31, 62.32, 2]\n\n  # Third generation depends on the config from the second generation\n  generation_3:\n    executable: custom_files/generation_3_track.py\n    scans:\n      delta_max:\n        list: [27.e-5, 28.e-5]\n</code></pre> <p>Nothing fancy here, except that, as you can see, we now have split our usual generation_2 into two generations, <code>generation_2_configure.py</code> and <code>generation_3_track.py</code>. In the tracking generation, we will scan the <code>delta_max</code> (initial off-momentum) parameter.</p>"},{"location":"case_studies/6_3_generational_scan.html#template-scripts","title":"Template scripts","text":"<p>The first generation template script is the usual, so I'm not going to show it here. The second generation template script is as follows:</p> generation_2_configure.py<pre><code>\"\"\"This is a template script for generation 1 of simulation study, in which ones generates a\nparticle distribution and a collider from a MAD-X model.\"\"\"\n\n# ==================================================================================================\n# --- Imports\n# ==================================================================================================\n\n# Import standard library modules\nimport contextlib\nimport logging\nimport os\n\n# Import third-party modules\n# Import user-defined modules\nfrom study_da.generate import XsuiteCollider\nfrom study_da.utils import (\n    load_dic_from_path,\n    set_item_in_dic,\n    write_dic_to_path,\n)\n\n# Set up the logger here if needed\n\n\n# ==================================================================================================\n# --- Script functions\n# ==================================================================================================\ndef configure_collider(full_configuration):\n    # Get configuration\n    config_collider = full_configuration[\"config_collider\"]\n    ver_hllhc_optics = full_configuration[\"config_mad\"][\"ver_hllhc_optics\"]\n    ver_lhc_run = full_configuration[\"config_mad\"][\"ver_lhc_run\"]\n    ions = full_configuration[\"config_mad\"][\"ions\"]\n    collider_filepath = full_configuration[\"config_collider\"][\n        \"path_collider_file_for_configuration_as_input\"\n    ]\n\n    # Build object for configuring collider\n    xc = XsuiteCollider(config_collider, collider_filepath, ver_hllhc_optics, ver_lhc_run, ions)\n\n    # Load collider\n    collider = xc.load_collider()\n\n    # Install beam-beam\n    xc.install_beam_beam_wrapper(collider)\n\n    # Build trackers\n    # For now, start with CPU tracker due to a bug with Xsuite\n    # Refer to issue https://github.com/xsuite/xsuite/issues/450\n    collider.build_trackers()  # (_context=context)\n\n    # Set knobs\n    xc.set_knobs(collider)\n\n    # Match tune and chromaticity\n    xc.match_tune_and_chroma(collider, match_linear_coupling_to_zero=True)\n\n    # Set filling scheme\n    xc.set_filling_and_bunch_tracked(ask_worst_bunch=False)\n\n    # Compute the number of collisions in the different IPs\n    n_collisions_ip1_and_5, n_collisions_ip2, n_collisions_ip8 = xc.compute_collision_from_scheme()\n\n    # Do the leveling if requested\n    if \"config_lumi_leveling\" in config_collider and not config_collider[\"skip_leveling\"]:\n        xc.level_ip1_5_by_bunch_intensity(collider, n_collisions_ip1_and_5)\n        xc.level_ip2_8_by_separation(n_collisions_ip2, n_collisions_ip8, collider)\n    else:\n        logging.warning(\n            \"No leveling is done as no configuration has been provided, or skip_leveling\"\n            \" is set to True.\"\n        )\n\n    # Add linear coupling\n    xc.add_linear_coupling(collider)\n\n    # Rematch tune and chromaticity\n    xc.match_tune_and_chroma(collider, match_linear_coupling_to_zero=False)\n\n    # Assert that tune, chromaticity and linear coupling are correct one last time\n    xc.assert_tune_chroma_coupling(collider)\n\n    # Configure beam-beam if needed\n    if not xc.config_beambeam[\"skip_beambeam\"]:\n        xc.configure_beam_beam(collider)\n\n    # Update configuration with luminosity now that bb is known\n    l_n_collisions = [\n        n_collisions_ip1_and_5,\n        n_collisions_ip2,\n        n_collisions_ip1_and_5,\n        n_collisions_ip8,\n    ]\n    xc.record_final_luminosity(collider, l_n_collisions)\n\n    # Save collider to json (flag to save or not is inside function)\n    xc.write_collider_to_disk(collider, full_configuration)\n\n    # Get fingerprint\n    fingerprint = xc.return_fingerprint(collider)\n\n    return collider, fingerprint\n\n\ndef clean():\n    # Remote the correction folder, and potential C files remaining\n    with contextlib.suppress(Exception):\n        os.system(\"rm -rf correction\")\n        os.system(\"rm -f *.cc\")\n\n\n# ==================================================================================================\n# --- Parameters placeholders definition\n# ==================================================================================================\ndict_mutated_parameters = {}  ###---parameters---###\npath_configuration = \"{}  ###---main_configuration---###\"\n# In case the placeholders have not been replaced, use default path\nif path_configuration.startswith(\"{}\"):\n    path_configuration = \"config.yaml\"\n\n# ==================================================================================================\n# --- Script for execution\n# ==================================================================================================\n\nif __name__ == \"__main__\":\n    logging.info(\"Starting script to configure collider\")\n\n    # Load full configuration\n    full_configuration, ryaml = load_dic_from_path(path_configuration)\n\n    # Mutate parameters in configuration\n    for key, value in dict_mutated_parameters.items():\n        set_item_in_dic(full_configuration, key, value)\n\n    # Configure collider\n    collider, fingerprint = configure_collider(full_configuration)\n\n    # Drop updated configuration\n    name_configuration = os.path.basename(path_configuration)\n    write_dic_to_path(full_configuration, name_configuration, ryaml)\n\n    # Clean temporary files\n    clean()\n\n    logging.info(\"Script finished\")\n</code></pre> <p>As you can see, not much changed compared to the initial generation_2 script: we basically removed everything that relates to the tracking.</p> <p>The third generation template script is as follows:</p> generation_3_track.py<pre><code>\"\"\"This is a template script for generation 1 of simulation study, in which ones generates a\nparticle distribution and a collider from a MAD-X model.\"\"\"\n\n# ==================================================================================================\n# --- Imports\n# ==================================================================================================\n\n# Import standard library modules\nimport contextlib\nimport logging\nimport os\nimport time\n\n# Import third-party modules\nimport numpy as np\nimport pandas as pd\nimport xtrack as xt\n\n# Import user-defined modules\nfrom study_da.generate import XsuiteCollider, XsuiteTracking\nfrom study_da.utils import (\n    load_dic_from_path,\n    set_item_in_dic,\n    write_dic_to_path,\n)\n\n# Set up the logger here if needed\n\n\n# ==================================================================================================\n# --- Script functions\n# ==================================================================================================\n\n\ndef load_collider(full_configuration):\n    collider = XsuiteCollider._load_collider(\n        full_configuration[\"config_simulation\"][\"path_collider_file_for_tracking_as_input\"]\n    )\n    collider.build_trackers()\n    return collider\n\n\ndef track_particles(full_configuration, collider):\n    # Get emittances\n    n_emitt_x = full_configuration[\"config_collider\"][\"config_beambeam\"][\"nemitt_x\"]\n    n_emitt_y = full_configuration[\"config_collider\"][\"config_beambeam\"][\"nemitt_y\"]\n    xst = XsuiteTracking(full_configuration[\"config_simulation\"], n_emitt_x, n_emitt_y)\n\n    # Prepare particle distribution\n    particles, particle_id, l_amplitude, l_angle = xst.prepare_particle_distribution_for_tracking(\n        collider\n    )\n\n    # Track\n    particles_dict = xst.track(collider, particles)\n\n    # Convert particles to dataframe\n    particles_df = pd.DataFrame(particles_dict)\n\n    # ! Very important, otherwise the particles will be mixed in each subset\n    # Sort by parent_particle_id\n    particles_df = particles_df.sort_values(\"parent_particle_id\")\n\n    # Assign the old id to the sorted dataframe\n    particles_df[\"particle_id\"] = particle_id\n\n    # Register the amplitude and angle in the dataframe\n    particles_df[\"normalized amplitude in xy-plane\"] = l_amplitude\n    particles_df[\"angle in xy-plane [deg]\"] = l_angle * 180 / np.pi\n\n    # Add some metadata to the output for better interpretability\n    particles_df.attrs[\"configuration\"] = full_configuration\n    particles_df.attrs[\"date\"] = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    # Save output\n    particles_df.to_parquet(\n        full_configuration[\"config_simulation\"][\"path_distribution_file_output\"]\n    )\n\n\n# ==================================================================================================\n# --- Parameters placeholders definition\n# ==================================================================================================\ndict_mutated_parameters = {}  ###---parameters---###\npath_configuration = \"{}  ###---main_configuration---###\"\n# In case the placeholders have not been replaced, use default path\nif path_configuration.startswith(\"{}\"):\n    path_configuration = \"config.yaml\"\n\n# ==================================================================================================\n# --- Script for execution\n# ==================================================================================================\n\nif __name__ == \"__main__\":\n    logging.info(\"Starting script to configure collider and track\")\n\n    # Load full configuration\n    full_configuration, ryaml = load_dic_from_path(path_configuration)\n\n    # Mutate parameters in configuration\n    for key, value in dict_mutated_parameters.items():\n        set_item_in_dic(full_configuration, key, value)\n\n    # Configure collider\n    collider = load_collider(full_configuration)\n\n    # Drop updated configuration\n    name_configuration = os.path.basename(path_configuration)\n    write_dic_to_path(full_configuration, name_configuration, ryaml)\n\n    # Track particles and save to disk\n    track_particles(full_configuration, collider)\n\n    logging.info(\"Script finished\")\n</code></pre> <p>We had to do a tiny bit more of adaptation here compared to the usual generation 2: rewrite a function to load the collider, and remove the collider configuration configuration part.</p>"},{"location":"case_studies/6_3_generational_scan.html#template-configuration","title":"Template configuration","text":"<p>Finally, we have to ensure the template configuration is adapted to the new structure:</p> config_hllhc16_3_gen.yaml<pre><code>config_particles:\n  r_min: 4.0\n  r_max: 8.0\n  n_r: 256\n  n_angles: 5\n  n_split: 5\n  path_distribution_folder_output: particles\n\nconfig_mad:\n  # Links to be made for tools and scripts\n  links:\n    acc-models-lhc: ../../../../../external_dependencies/acc-models-lhc\n\n  # Optics file\n  optics_file: acc-models-lhc/strengths/round/opt_round_150_1500_optphases_thin.madx\n\n  # Beam parameters\n  beam_config:\n    lhcb1:\n      beam_energy_tot: 7000 # [GeV]\n    lhcb2:\n      beam_energy_tot: 7000 # [GeV]\n\n  # Ions being simulated\n  ions: false\n\n  # Enable machine imperfections\n  enable_imperfections: false\n\n  # Enable knob synthesis (for coupling correction, if no imperfections)\n  enable_knob_synthesis: true\n\n  # Rename the coupling knobs to avoid conflict between b1 and b2\n  # (for hllhc using old fortran code to generate the knobs)\n  rename_coupling_knobs: true\n\n  # Optics version, for choice of correction algorithms\n  # (ver_lhc_run or ver_hllhc_optics)\n  ver_hllhc_optics: 1.6\n  ver_lhc_run: null\n\n  # Parameters for machine imperfections\n  pars_for_imperfections:\n    par_myseed: 1\n    par_correct_for_D2: 0\n    par_correct_for_MCBX: 0\n    par_on_errors_LHC: 1\n    par_off_errors_Q4_inIP15: 0\n    par_off_errors_Q5_inIP15: 0\n    par_on_errors_MBH: 1\n    par_on_errors_Q4: 1\n    par_on_errors_D2: 1\n    par_on_errors_D1: 1\n    par_on_errors_IT: 1\n    par_on_errors_MCBRD: 0\n    par_on_errors_MCBXF: 0\n    par_on_errors_NLC: 0\n    par_write_errortable: 1\n\n  phasing:\n    # RF voltage and phases\n    vrf400: 16.0 # [MV]\n    lagrf400.b1: 0.5 # [rad]\n    lagrf400.b2: 0.5 # [rad]\n\n  # To make some specifics checks\n  sanity_checks: true\n\n  # Path of the collider file to be saved (usually at the end of the first generation)\n  path_collider_file_for_configuration_as_output: collider_file_for_configuration.json\n  compress: true # will compress the collider file, filename will end with .zip\n\n# Configuration for tuning of the collider\nconfig_collider:\n  # Even though the file doesn't end with .zip, scrip will first try to load it as a zip file\n  path_collider_file_for_configuration_as_input: ../collider_file_for_configuration.json\n  config_knobs_and_tuning:\n    knob_settings:\n      # Orbit knobs\n      on_x1: 250 # [urad]\n      on_sep1: 0 # [mm]\n      on_x2: -170 # [urad]\n      on_sep2: 0.138 # 0.1443593672910653 # 0.138 # [mm]\n      on_x5: 250 # [urad]\n      on_sep5: 0 # [mm]\n      on_x8h: 0.0\n      on_x8v: 170\n      on_sep8h: -0.01 # different from 0 so that the levelling algorithm is not stuck\n      on_sep8v: 0.01 # idem\n      on_a1: 0 # [urad]\n      on_o1: 0 # [mm]\n      on_a2: 0 # [urad]\n      on_o2: 0 # [mm]\n      on_a5: 0 # [urad]\n      on_o5: 0 # [mm]\n      on_a8: 0 # [urad]\n      on_o8: 0 # [mm]\n      on_disp: 1 # Value to choose could be optics-dependent\n\n      # Crab cavities\n      on_crab1: 0 # [urad]\n      on_crab5: 0 # [urad]\n\n      # Magnets of the experiments\n      on_alice_normalized: 1\n      on_lhcb_normalized: 1\n      on_sol_atlas: 0\n      on_sol_cms: 0\n      on_sol_alice: 0\n\n      # Octupoles\n      i_oct_b1: -60. # [A]\n      i_oct_b2: -60. # [A]\n\n    # Tunes and chromaticities\n    qx:\n      lhcb1: 62.31\n      lhcb2: 62.31\n    qy:\n      lhcb1: 60.32\n      lhcb2: 60.32\n    dqx:\n      lhcb1: 15\n      lhcb2: 15\n    dqy:\n      lhcb1: 15\n      lhcb2: 15\n\n    # Linear coupling\n    delta_cmr: 0.001\n    delta_cmi: 0.0\n\n    knob_names:\n      lhcb1:\n        q_knob_1: kqtf.b1\n        q_knob_2: kqtd.b1\n        dq_knob_1: ksf.b1\n        dq_knob_2: ksd.b1\n        c_minus_knob_1: c_minus_re_b1\n        c_minus_knob_2: c_minus_im_b1\n      lhcb2:\n        q_knob_1: kqtf.b2\n        q_knob_2: kqtd.b2\n        dq_knob_1: ksf.b2\n        dq_knob_2: ksd.b2\n        c_minus_knob_1: c_minus_re_b2\n        c_minus_knob_2: c_minus_im_b2\n\n  config_beambeam:\n    skip_beambeam: false\n    bunch_spacing_buckets: 10\n    num_slices_head_on: 11\n    num_long_range_encounters_per_side:\n      ip1: 25\n      ip2: 20\n      ip5: 25\n      ip8: 20\n    sigma_z: 0.0761\n    num_particles_per_bunch: 140000000000.0\n    nemitt_x: 2.5e-6\n    nemitt_y: 2.5e-6\n    mask_with_filling_pattern:\n      # If not already existing in the study-da package, pattern must have an absolute path or be\n      # added as a dependency for the run file\n      pattern_fname: 8b4e_1972b_1960_1178_1886_224bpi_12inj_800ns_bs200ns.json\n      i_bunch_b1: null # If not specified, the bunch with the worst schedule is chosen\n      i_bunch_b2: null # Same. A value for i_bunch_b1 and i_bunch_b2 must be specified if pattern_fname is specified\n    cross_section: 81e-27\n\n  config_lumi_leveling_ip1_5:\n    skip_leveling: false\n    luminosity: 5.0e+34\n    num_colliding_bunches: null # This will be set automatically according to the filling scheme\n    vary:\n      - num_particles_per_bunch\n    constraints:\n      max_intensity: 2.3e11\n      max_PU: 160\n\n  skip_leveling: false\n  config_lumi_leveling:\n    ip2:\n      separation_in_sigmas: 5\n      plane: x\n      impose_separation_orthogonal_to_crossing: false\n      knobs:\n        - on_sep2\n      bump_range:\n        lhcb1:\n          - e.ds.l2.b1\n          - s.ds.r2.b1\n        lhcb2:\n          - s.ds.r2.b2\n          - e.ds.l2.b2\n      preserve_angles_at_ip: true\n      preserve_bump_closure: true\n      corrector_knob_names:\n        # to preserve angles at ip\n        - corr_co_acbyvs4.l2b1\n        - corr_co_acbyhs4.l2b1\n        - corr_co_acbyvs4.r2b2\n        - corr_co_acbyhs4.r2b2\n          # to close the bumps\n        - corr_co_acbyvs4.l2b2\n        - corr_co_acbyhs4.l2b2\n        - corr_co_acbyvs4.r2b1\n        - corr_co_acbyhs4.r2b1\n        - corr_co_acbyhs5.l2b2\n        - corr_co_acbyvs5.l2b2\n        - corr_co_acbchs5.r2b1\n        - corr_co_acbcvs5.r2b1\n    ip8:\n      luminosity: 2.0e+33\n      num_colliding_bunches: null # This will be set automatically according to the filling scheme\n      impose_separation_orthogonal_to_crossing: true\n      knobs:\n        - on_sep8h\n        - on_sep8v\n      bump_range:\n        lhcb1:\n          - e.ds.l8.b1\n          - s.ds.r8.b1\n        lhcb2:\n          - s.ds.r8.b2\n          - e.ds.l8.b2\n      preserve_angles_at_ip: true\n      preserve_bump_closure: true\n      corrector_knob_names:\n        # to preserve angles at ip\n        - corr_co_acbyvs4.l8b1\n        - corr_co_acbyhs4.l8b1\n        - corr_co_acbyvs4.r8b2\n        - corr_co_acbyhs4.r8b2\n          # to close the bumps\n        - corr_co_acbyvs4.l8b2\n        - corr_co_acbyhs4.l8b2\n        - corr_co_acbyvs4.r8b1\n        - corr_co_acbyhs4.r8b1\n        - corr_co_acbcvs5.l8b2\n        - corr_co_acbchs5.l8b2\n        - corr_co_acbyvs5.r8b1\n        - corr_co_acbyhs5.r8b1\n\n  # Save collider or not (usually at the end of the collider tuning)\n  save_output_collider: true\n  path_collider_file_for_tracking_as_output: collider_file_for_tracking.json\n  compress: true # will compress the collider file, filename will end with .zip\n\nconfig_simulation:\n  # Collider file to load for the tracking\n  path_collider_file_for_tracking_as_input: ../collider_file_for_tracking.json\n\n  # Distribution in the normalized xy space\n  path_distribution_folder_input: ../../particles\n  distribution_file: 00.parquet\n\n  # Output particle file\n  path_distribution_file_output: output_particles.parquet\n\n  # Initial off-momentum\n  delta_max: 27.e-5\n\n  # Tracking\n  n_turns: 100 # number of turns to track\n\n  # Beam to track\n  beam: lhcb1 #lhcb1 or lhcb2\n\n  # Context for the simulation\n  context: \"cpu\" # 'cupy' # opencl\n\n  # Device number for GPU simulation\n  device_number: # 0\n</code></pre> <p>We did a couple changes here:</p> <ul> <li>save the output collider at the end of the configuration</li> <li>Adapt the folder for the particles</li> </ul> <p>Also note that we only use a 100 turns for the tracking as this is just a dummy simulation. And that's it! That's how easy it is.</p>"},{"location":"case_studies/6_3_generational_scan.html#study-generation-and-submission","title":"Study generation and submission","text":"<p>The script to generate and submit the study is also not so different from the usual. We're going to assume that we submit everything to HTCondor this time, without using Docker.</p> generate_and_submit.py<pre><code># ==================================================================================================\n# --- Imports\n# ==================================================================================================\n\n# Import standard library modules\nimport logging\n\n# Import third-party modules\n# Import user-defined modules\nfrom study_da import create, submit\n\n# Set up the logger\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n\n# ==================================================================================================\n# --- Script to generate a study\n# ==================================================================================================\n\n# Generate the study in the local directory\npath_tree, name_main_config = create(path_config_scan=\"config_scan.yaml\", force_overwrite=False)\n\n# Define the variables of interest for the submission\npath_python_environment = \"/afs/cern.ch/work/c/cdroin/private/study-DA/.venv\"\nforce_configure = False\n\n# Dependencies for the executable of each generation. Only needed if one uses HTC.\ndic_dependencies_per_gen = {\n    1: [\"acc-models-lhc\"],\n    2: [\"path_collider_file_for_configuration_as_input\"],\n    3: [\n        \"path_collider_file_for_tracking_as_input\",\n        \"path_distribution_folder_input\",\n    ],\n}\n\n# Dic copy_back_per_gen (only for HTC)\ndic_copy_back_per_gen = {\n    1: {\"parquet\": True, \"yaml\": True, \"txt\": True, \"json\": True, \"zip\": True},\n    2: {\"parquet\": True, \"yaml\": True, \"txt\": True, \"json\": True, \"zip\": True},\n    3: {\"parquet\": True, \"yaml\": True, \"txt\": True, \"json\": False, \"zip\": False},\n}\n\n# To bring back the \"particles\" folder from gen 1\ndic_additional_commands_per_gen = {\n    1: \"cp -r particles $path_job/particles \\n\",\n    2: \"\",\n}\n\n\n# Preconfigure submission to HTC\ndic_config_jobs = {\n    \"generation_1\" + \".py\": {\n        \"request_gpu\": False,\n        \"submission_type\": \"htc\",\n        \"htc_flavor\": \"microcentury\",\n    },\n    \"generation_2\" + \".py\": {\n        \"request_gpu\": False,\n        \"submission_type\": \"htc\",\n        \"htc_flavor\": \"microcentury\",\n    },\n    \"generation_3\" + \".py\": {\n        \"request_gpu\": False,\n        \"submission_type\": \"htc\",\n        \"htc_flavor\": \"espresso\",\n    },\n}\n\n\n# Submit the study\nsubmit(\n    path_tree=path_tree,\n    path_python_environment=path_python_environment,\n    force_configure=force_configure,\n    dic_dependencies_per_gen=dic_dependencies_per_gen,\n    name_config=name_main_config,\n    dic_config_jobs=dic_config_jobs,\n    dic_copy_back_per_gen=dic_copy_back_per_gen,\n    dic_additional_commands_per_gen=dic_additional_commands_per_gen,\n    keep_submit_until_done=True,\n    wait_time=15,\n)\n</code></pre> <p>The first thing you can notice is that we use a logger here as it's easier to debug. Then you can see that we add to adapt the <code>dic_dependencies_per_gen</code> to ensure all the intermediary files are copied back to the next generation, even though we run everything on HTCondor. We also added a <code>dic_copy_back_per_gen</code> to ensure the (large) files (such as the collider) are copied back to the next generation. We also added a <code>dic_additional_commands_per_gen</code> to ensure to copy back the particles folder from the first generation to the second generation.</p> <p>Finally, you can notice that we preconfigured the submission to HTC with the <code>dic_config_jobs</code> dictionary, to ease our life and not get prompted for each generation.</p> <p>Because we do three generations in a row, we set <code>keep_submit_until_done=True</code> to ensure we don't have to submit each generation manually. We also set <code>wait_time=15</code> to ensure we don't overload the system.</p> <p>And that's it! You can now run this script and let it run. It will take a bit of time, but it should run smoothly. Some jobs will fail because the matching of the tune and chroma could not be done, but that's expected, and other jobs will get to the end of generation three.</p> <p>You can now adapt this script to your needs and run your own 3-generational scan.</p>"},{"location":"installation/installing_locally.html","title":"Installing study-DA locally","text":""},{"location":"installation/installing_locally.html#cloning-the-repository","title":"Cloning the repository","text":"<p>If you encounter trouble with the pip installation, or if you want to modify and/or contribute to the package, you can install it locally by cloning the repository and installing the dependencies with Poetry. Note, that, if you plan to submit jobs to CERN HTCondor from a local Python installation, you should move to your AFS space first (e.g. <code>cd /afs/cern.ch/work/u/username/private</code>).:</p> <pre><code>git clone --recurse-submodules https://github.com/ColasDroin/study-DA.git\n</code></pre> <p>If you missed this step and clone the repository without the submodules, you can do a posteriori:</p> <pre><code>git submodule update --init --recursive\n</code></pre> <p>Cloning the submodules is important as this allows to download the optics for HL-LHC, which are necessary to run the tracking examples. Therefore, you should be able to skip the Installing the optics section.</p>"},{"location":"installation/installing_locally.html#installing-with-poetry","title":"Installing with Poetry","text":""},{"location":"installation/installing_locally.html#standard-installation-with-poetry","title":"Standard installation with Poetry","text":"<p>If not already done, install Poetry following the tutorial here.</p> <p>For easier submission later, impose the virtual environment to be created in the repository folder by running the following command:</p> <pre><code>poetry config virtualenvs.in-project true\n</code></pre> <p>Be careful where Python is installed</p> <p>If you're from CERN and intend to submit jobs to HTCondor from your local Python environment, ensure that the executable that Poetry will use to spawn a virtual environment is available on AFS.</p> <p>You can check the base executable of Python that Poetry is using by running the following command:</p> <pre><code>poetry env info\n</code></pre> <p>If needed (for instance, if your Python base executable is not on AFS), you can change the exectutable with e.g:</p> <pre><code>poetry env use /full/path/to/python\n</code></pre> <p>If you're not interested in using GPUs, you should jump directly to the Installing dependencies section. Otherwise, follow the next section.</p> <p>Are you sure you want to install study-DA locally?</p> <pre><code>If you're not planning to contribute to the package, it is recommended to install it with pip from PyPI, from a conda environment. Installing the package locally makes it much harder to have it compatible with GPUs.\n</code></pre>"},{"location":"installation/installing_locally.html#installing-with-poetry-for-gpus","title":"Installing with Poetry for GPUs","text":"<p>Using Poetry along with GPUs is a bit more complicated, as conda is not natively supported by Poetry. However, not all is lost as a simple trick allows to bypass this issue. First, from a conda-compatible Python environment (not the one you used to install Poetry), create a virtual environment with the following command:</p> <pre><code>conda create -n gpusim python=3.11\nconda activate gpusim\n</code></pre> <p>Now configure Poetry to use the virtual environment you just created:</p> <pre><code>poetry config virtualenvs.in-project false\npoetry config virtualenvs.path $CONDA_ENV_PATH\npoetry config virtualenvs.create false\n</code></pre> <p>Where <code>$CONDA_ENV_PATH</code> is the path to the base envs folder (e.g. <code>/home/user/miniforge3/envs</code>).  </p> <p>You can then install the CUDA toolkit and the necessary packages (e.g. <code>cupy</code>) in the virtual environment (from Xsuite documentation ):</p> <pre><code>conda install mamba -n base -c conda-forge\nmamba install cudatoolkit=11.8.0\n</code></pre> <p>Don't forget to select the path to your virtual environment for submission with the <code>submit()</code>method will be the conda one (e.g. <code>home/user/miniforge3/bin/activate</code>, and add right after <code>conda activate gpusim</code>), i.e.:</p> <pre><code>submit(\n    ...\n    path_python_environment=\"/home/user/miniforge3/bin/activate; conda activate gpusim\",\n    ...\n)\n</code></pre> <p>You're now good to go with the next section, as Poetry will automatically detect that the conda virtual environment is activated and use it to install the dependencies.</p>"},{"location":"installation/installing_locally.html#installing-dependencies","title":"Installing dependencies","text":"<p>Finally, install the dependencies by running the following command:</p> <pre><code>poetry install\n</code></pre> <p>Developer installation</p> <p>If you plan to contribute to the package, you can install the dependencies needed for development (tests, documentation) with:</p> <pre><code>poetry install --with test,docs,dev\n</code></pre> <p>At this point, ensure that a <code>.venv</code> folder has been created in the repository folder (except if you modified the procedure to use GPUs, as explained above). If not, follow the fix described in the next section.</p> <p>Nafflib dependencies</p> <p>If you encounter an error while installing nafflib, you can install it manually with: <pre><code>poetry run pip install nafflib\npoetry install\n</code></pre></p> <p>Conda compilers</p> <p>If you encounter an error while installing the dependencies, you may need to install the compilers and cmake with conda. You can do so by running the following commands: <pre><code>poetry shell\nconda install compilers cmake\n</code></pre></p> <p>Finally, you can make xsuite faster by precompiling the kernel, with:</p> <pre><code>poetry run xsuite-prebuild regenerate\n</code></pre> <p>To run any subsequent Python command, either activate the virtual environment (activate a shell within Poetry) with:</p> <pre><code>poetry shell\n# you then run a command as simply as `python my_script.py`\n</code></pre> <p>or run the command directly with Poetry:</p> <pre><code>poetry run python my_script.py\n</code></pre> <p>At this point, the only step left is to install xmask (which is normally already cloned), which is an external dependency that is not handled by Poetry since it requires cloning submodules. You can do so by running the following commands:</p> <pre><code>poetry shell\ncd external_dependencies\npip install -e xmask\n</code></pre>"},{"location":"installation/installing_locally.html#installing-locally-without-poetry","title":"Installing locally without Poetry","text":"<p>For local installations, it is strongly recommended to use Poetry as it will handle all the packages dependencies and the virtual environment for you. However, if you prefer to install the dependencies manually, you can do so by running the following commands (granted that you have Python installed along with pip):</p> <pre><code>pip install -r requirements.txt\n\n# Now install xmask\ncd external_dependencies\npip install -e xmask\n</code></pre>"},{"location":"installation/installing_python.html","title":"Installing Python","text":"<p>Python is probably already available on your machine. You can check by running the following commands:</p> <pre><code>which python # Tells you the path to the python executable\npython --version # Tells you the version of Python\n</code></pre> <p>If Python (&gt;=3.11) is not available, you can install it with, for instance, miniforge or miniconda.</p> <p>To install the latest version of Python with miniforge in your home directory, run the following commands:</p> <pre><code>cd\nwget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh\nbash Miniforge3-Linux-x86_64.sh -b  -p ./miniforge -f\nrm -f Miniforge3-Linux-x86_64.sh\nsource miniforge/bin/activate\n</code></pre> <p>Be careful where Python is installed</p> <p>If you're from CERN and intend to submit job to HTCondor without using a CVMFS environment or Docker environment, your environment must be available on AFS (i.e. must be installed on AFS).</p>"},{"location":"installation/installing_the_optics.html","title":"Installing the optics","text":"<p>Optics are the heart of any collider simulation. In this section, we will explain how to install the optics that are available for the package: HL-LHC v1.6, HL-LHC v1.3, Run III, and Run III ions.</p> <p>First, move to or create the directory where you want to install the optics.</p>"},{"location":"installation/installing_the_optics.html#hl-lhc-v16-optics","title":"HL-LHC v1.6 optics","text":"<p>To clone the HL-LHC optics v1.6, run the following command:</p> <pre><code>git clone https://gitlab.cern.ch/acc-models/acc-models-lhc.git -b hl16\n</code></pre>"},{"location":"installation/installing_the_optics.html#hl-lhc-v13-optics","title":"HL-LHC v1.3 optics","text":"<p>To clone the HL-LHC optics v1.3, run the following command:</p> <pre><code>git clone https://github.com/ColasDroin/hllhc13.git\n</code></pre>"},{"location":"installation/installing_the_optics.html#run-iii-and-run-iii-ions-optics","title":"Run III and Run III ions optics","text":"<p>Unfortunately, the Run III and Run III ions optics are not available as public repositories yet. They might be added to the https://github.com/lhcopt organization in the future.</p> <p>In the meanwhile, you will need to use the optics directly from AFS (e.g. from <code>/afs/cern.ch/eng/lhc/optics/runIII</code>). Of course, you can copy the directory wherever you like, but that's not needed.</p> <p>An example of configuration with the optics loaded from AFS if provided in the template configuration files.</p>"},{"location":"installation/using_HPC_clusters.html","title":"Using High-Performance Computing (HPC) Clusters","text":""},{"location":"installation/using_HPC_clusters.html#setting-up-the-clusters","title":"Setting up the clusters","text":"<p>study-DA should allow for an easy deployment of the simulations on HTCondor (CERN cluster) and Slurm (CNAF.INFN cluster). Please consult the corresponding tutorials (here, and here) to set up the clusters on your machine.</p> <p>You will get prompted on which machine the jobs should run (HTC, Slurm or locally) when submitting your study.</p>"},{"location":"installation/using_HPC_clusters.html#using-docker-images","title":"Using Docker images","text":"<p>For reproducibility purposes and/or limiting the load on AFS drives (for CERN user), one can use Docker images to run the simulations. A registry of Docker images is available at <code>/cvmfs/unpacked.cern.ch/gitlab-registry.cern.ch/</code>, and some ready-to-use for DA simulations Docker images on HTC or Slurm are available at <code>/cvmfs/unpacked.cern.ch/gitlab-registry.cern.ch/cdroin/da-study-docker</code>.</p> <p>To learn more about building Docker images and hosting them on the CERN registry, please consult the corresponding tutorial and the corresponding repository.</p>"},{"location":"installation/using_HPC_clusters.html#with-htcondor","title":"With HTCondor","text":"<p>When running simulations on HTCondor, Docker images are being pulled directly from CVMFS by the node. No more configuration than the usual arguments of the <code>submit</code> function is needed.</p>"},{"location":"installation/using_HPC_clusters.html#with-slurm","title":"With Slurm","text":"<p>Things are a bit tricker with Slurm, as the Docker image must first be manually pulled from CVMFS, and then loaded on the node after Singularity-ize it. The pulling of the image is only needed the first time, and can be done with e.g. (for the image <code>cdroin/da-study-docker</code>):</p> <pre><code>singularity pull docker://gitlab-registry.cern.ch/cdroin/da-study-docker:757f55da\n</code></pre> <p>Some nodes might not want to pull the image</p> <p>Due to unknown reason, only some nodes of INFN-CNAF will correctly execute this command. For instance, it didn't work on the default CPU CERN node (<code>hpc-201-11-01-a</code>), but it did on an alternative one (<code>hpc-201-11-02-a</code>). We recommand using either <code>hpc-201-11-02-a</code> or a GPU node (e.g. <code>hpc-201-11-35</code>) to pull the image. Once the image is pulled, it will be accessible from any node.</p> <p>For testing purposes, one can then run the image with Singularity directly on the node (not required):</p> <pre><code>singularity run da-study-docker_757f55da.sif\n</code></pre> <p>Once this is configured, you can just provide the path of the image to the <code>submit</code> function, and the script will take care of the rest.</p> <p>The slurm docker scripts are kind of experimental</p> <p>At the time of writing this documentation, symlinks path in the front-end node of INFN-CNAF are currently broken, meaning that some temporary fixs are implemented. This will hopefully be fixed byt the CNAF.INFN team in the future, and the fixs should not prevent the scripts from running anyway.</p>"},{"location":"reference/SUMMARY.html","title":"SUMMARY","text":"<ul> <li>study_da<ul> <li>assets<ul> <li>template_scripts<ul> <li>generation_1</li> <li>generation_2_level_by_nb</li> <li>generation_2_level_by_sep</li> </ul> </li> </ul> </li> <li>generate<ul> <li>generate_scan</li> <li>master_classes<ul> <li>mad_collider</li> <li>particles_distribution</li> <li>scheme_utils</li> <li>utils</li> <li>xsuite_collider</li> <li>xsuite_leveling</li> <li>xsuite_tracking</li> </ul> </li> <li>parameter_space</li> <li>version_specific_files<ul> <li>hllhc13<ul> <li>crab_fix</li> <li>optics_specific_tools</li> <li>orbit_correction</li> </ul> </li> <li>hllhc16<ul> <li>optics_specific_tools</li> <li>orbit_correction</li> </ul> </li> <li>runIII<ul> <li>optics_specific_tools</li> </ul> </li> <li>runIII_ions<ul> <li>optics_specific_tools</li> </ul> </li> </ul> </li> </ul> </li> <li>plot<ul> <li>build_title</li> <li>plot_study</li> <li>utils<ul> <li>maplotlib_utils</li> </ul> </li> </ul> </li> <li>postprocess<ul> <li>postprocess</li> </ul> </li> <li>study_da</li> <li>submit<ul> <li>ask_user_config</li> <li>cluster_submission<ul> <li>cluster_submission</li> <li>submission_statements</li> </ul> </li> <li>config_jobs</li> <li>dependency_graph</li> <li>generate_run</li> <li>submit_scan</li> </ul> </li> <li>utils<ul> <li>dic_utils</li> <li>template_utils</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/study_da/index.html","title":"study_da","text":""},{"location":"reference/study_da/index.html#study_da.GenerateScan","title":"<code>GenerateScan</code>","text":"<p>A class to generate a study (along with the corresponding tree) from a parameter file, and potentially a set of template files.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>dict</code> <p>The configuration dictionary.</p> <code>ryaml</code> <code>YAML</code> <p>The YAML parser.</p> <code>dic_common_parameters</code> <code>dict</code> <p>Dictionary of common parameters across generations.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initializes the generation scan with a configuration file or dictionary.</p> <code>render</code> <p>Renders the study file using a template.</p> <code>write</code> <p>Writes the study file to disk.</p> <code>generate_render_write</code> <p>Generates, renders, and writes the study file.</p> <code>get_dic_parametric_scans</code> <p>Retrieves dictionaries of parametric scan values.</p> <code>parse_parameter_space</code> <p>Parses the parameter space for a given parameter.</p> <code>browse_and_collect_parameter_space</code> <p>Browses and collects the parameter space for a given generation.</p> <code>postprocess_parameter_lists</code> <p>Postprocesses the parameter lists.</p> <code>create_scans</code> <p>Creates study files for parametric scans.</p> <code>complete_tree</code> <p>Completes the tree structure of the study dictionary.</p> <code>write_tree</code> <p>Writes the study tree structure to a YAML file.</p> <code>create_study_for_current_gen</code> <p>Creates study files for the current generation.</p> <code>create_study</code> <p>Creates study files for the entire study.</p> <code>eval_conditions</code> <p>Evaluates the conditions to filter out some parameter values.</p> <code>filter_for_concomitant_parameters</code> <p>Filters the conditions for concomitant parameters.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>class GenerateScan:\n    \"\"\"\n    A class to generate a study (along with the corresponding tree) from a parameter file,\n    and potentially a set of template files.\n\n    Attributes:\n        config (dict): The configuration dictionary.\n        ryaml (yaml.YAML): The YAML parser.\n        dic_common_parameters (dict): Dictionary of common parameters across generations.\n\n    Methods:\n        __init__(): Initializes the generation scan with a configuration file or dictionary.\n        render(): Renders the study file using a template.\n        write(): Writes the study file to disk.\n        generate_render_write(): Generates, renders, and writes the study file.\n        get_dic_parametric_scans(): Retrieves dictionaries of parametric scan values.\n        parse_parameter_space(): Parses the parameter space for a given parameter.\n        browse_and_collect_parameter_space(): Browses and collects the parameter space for a given\n            generation.\n        postprocess_parameter_lists(): Postprocesses the parameter lists.\n        create_scans(): Creates study files for parametric scans.\n        complete_tree(): Completes the tree structure of the study dictionary.\n        write_tree(): Writes the study tree structure to a YAML file.\n        create_study_for_current_gen(): Creates study files for the current generation.\n        create_study(): Creates study files for the entire study.\n        eval_conditions(): Evaluates the conditions to filter out some parameter values.\n        filter_for_concomitant_parameters(): Filters the conditions for concomitant parameters.\n    \"\"\"\n\n    def __init__(\n        self, path_config: Optional[str] = None, dic_scan: Optional[dict[str, Any]] = None\n    ):  # sourcery skip: remove-redundant-if\n        \"\"\"\n        Initialize the generation scan with a configuration file or dictionary.\n\n        Args:\n            path_config (Optional[str]): Path to the configuration file for the scan.\n                Default is None.\n            dic_scan (Optional[dict[str, Any]]): Dictionary containing the scan configuration.\n                Default is None.\n\n        Raises:\n            ValueError: If neither or both of `path_config` and `dic_scan` are provided.\n        \"\"\"\n        # Load the study configuration from file or dictionary\n        if dic_scan is None and path_config is None:\n            raise ValueError(\n                \"Either a path to the configuration file or a dictionary must be provided.\"\n            )\n        elif dic_scan is not None and path_config is not None:\n            raise ValueError(\"Only one of the configuration file or dictionary must be provided.\")\n        elif path_config is not None:\n            self.config, self.ryaml = load_dic_from_path(path_config)\n        elif dic_scan is not None:\n            self.config = dic_scan\n            self.ryaml = yaml.YAML()\n        else:\n            raise ValueError(\"An unexpected error occurred.\")\n\n        # Parameters common across all generations (e.g. for parallelization)\n        self.dic_common_parameters: dict[str, Any] = {}\n\n        # Path to the tree file\n        self.path_tree = self.config[\"name\"] + \"/\" + \"tree.yaml\"\n\n    def render(\n        self,\n        str_parameters: str,\n        template_path: str,\n        path_main_configuration: str,\n        study_path: Optional[str] = None,\n        str_dependencies: Optional[dict[str, str]] = None,\n    ) -&gt; str:\n        \"\"\"\n        Renders the study file using a template.\n\n        Args:\n            str_parameters (str): The string representation of parameters to declare/mutate.\n            template_path (str): The path to the template file.\n            path_main_configuration (str): The path to the main configuration file.\n            study_path (str, optional): The path to the root of the study. Defaults to None.\n            dependencies (dict[str, str], optional): The dictionary of dependencies. Defaults to {}.\n\n        Returns:\n            str: The rendered study file.\n        \"\"\"\n\n        # Handle mutable default argument\n        if str_dependencies is None:\n            dependencies = \"\"\n        if study_path is None:\n            study_path = \"\"\n\n        # Generate generations from template\n        directory_path = os.path.dirname(template_path)\n        template_name = os.path.basename(template_path)\n        environment = Environment(\n            loader=FileSystemLoader(directory_path),\n            variable_start_string=\"{}  ###---\",\n            variable_end_string=\"---###\",\n        )\n        template = environment.get_template(template_name)\n\n        # Better not to render the dependencies path this way, as it becomes too cumbersome to\n        # handle the paths when using clusters\n\n        return template.render(\n            parameters=str_parameters,\n            main_configuration=path_main_configuration,\n            path_root_study=study_path,\n            # dependencies = str_dependencies,\n        )\n\n    def write(self, study_str: str, file_path: str, format_with_black: bool = True):\n        \"\"\"\n        Writes the study file to disk.\n\n        Args:\n            study_str (str): The study file string.\n            file_path (str): The path to write the study file.\n            format_with_black (bool, optional): Whether to format the output file with black.\n                Defaults to True.\n        \"\"\"\n\n        # Format the string with black\n        if format_with_black:\n            study_str = format_str(study_str, mode=FileMode())\n\n        # Make folder if it doesn't exist\n        folder = os.path.dirname(file_path)\n        if folder != \"\":\n            os.makedirs(folder, exist_ok=True)\n\n        with open(file_path, mode=\"w\", encoding=\"utf-8\") as file:\n            file.write(study_str)\n\n    def generate_render_write(\n        self,\n        gen_name: str,\n        job_directory_path: str,\n        template_path: str,\n        depth_gen: int,\n        dic_mutated_parameters: dict[str, Any] = {},\n    ) -&gt; list[str]:  # sourcery skip: default-mutable-arg\n        \"\"\"\n        Generates, renders, and writes the study file.\n\n        Args:\n            gen_name (str): The name of the generation.\n            study_path (str): The path to the job folder.\n            template_path (str): The path to the template folder.\n            depth_gen (int): The depth of the generation in the tree.\n            dic_mutated_parameters (dict[str, Any], optional): The dictionary of mutated parameters.\n                Defaults to {}.\n\n        Returns:\n            tuple[str, list[str]]: The study file string and the list of study paths.\n        \"\"\"\n\n        directory_path_gen = f\"{job_directory_path}\"\n        if not directory_path_gen.endswith(\"/\"):\n            directory_path_gen += \"/\"\n        file_path_gen = f\"{directory_path_gen}{gen_name}.py\"\n        logging.info(f'Now rendering generation \"{file_path_gen}\"')\n\n        # Generate the string of parameters\n        str_parameters = \"{\"\n        for key, value in dic_mutated_parameters.items():\n            if isinstance(value, str):\n                str_parameters += f\"'{key}' : '{value}', \"\n            else:\n                str_parameters += f\"'{key}' : {value}, \"\n        str_parameters += \"}\"\n\n        # Adapt the dict of dependencies to the current generation\n        dic_dependencies = self.config[\"dependencies\"] if \"dependencies\" in self.config else {}\n\n        # Unpacking list of dependencies\n        dic_dependencies = {\n            **{\n                key: value for key, value in dic_dependencies.items() if not isinstance(value, list)\n            },\n            **{\n                f\"{key}_{str(i).zfill(len(str(len(value))))}\": i_value\n                for key, value in dic_dependencies.items()\n                if isinstance(value, list)\n                for i, i_value in enumerate(value)\n            },\n        }\n        self.config[\"dependencies\"] = dic_dependencies\n\n        # Initial dependencies are always copied at the root of the study (hence value.split(\"/\")[-1])\n        dic_dependencies = {\n            key: \"../\" * depth_gen + value.split(\"/\")[-1] for key, value in dic_dependencies.items()\n        }\n\n        # Always load configuration from above generation, and remove the path from dependencies\n        path_main_configuration = \"../\" + dic_dependencies.pop(\"main_configuration\").split(\"/\")[-1]\n\n        # Create the str for the dependencies\n        str_dependencies = \"{\"\n        for key, value in dic_dependencies.items():\n            str_dependencies += f\"'{key}' : '{value}', \"\n        str_dependencies += \"}\"\n\n        # Render and write the study file\n        study_str = self.render(\n            str_parameters,\n            template_path=template_path,\n            path_main_configuration=path_main_configuration,\n            study_path=os.path.abspath(self.config[\"name\"]),\n            str_dependencies=str_dependencies,\n        )\n\n        self.write(study_str, file_path_gen)\n        return [directory_path_gen]\n\n    def get_dic_parametric_scans(\n        self, generation: str\n    ) -&gt; tuple[dict[str, Any], dict[str, Any], np.ndarray | None]:\n        \"\"\"\n        Retrieves dictionaries of parametric scan values.\n\n        Args:\n            generation: The generation name.\n\n        Returns:\n            tuple[dict[str, Any], dict[str, Any], np.ndarray|None]: The dictionaries of parametric\n                scan values, another dictionnary with better naming for the tree creation, and an\n                array of conditions to filter out some parameter values.\n        \"\"\"\n\n        if generation == \"base\":\n            raise ValueError(\"Generation 'base' should not have scans.\")\n\n        # Remember common parameters as they might be used across generations\n        if \"common_parameters\" in self.config[\"structure\"][generation]:\n            self.dic_common_parameters[generation] = {}\n            for parameter in self.config[\"structure\"][generation][\"common_parameters\"]:\n                self.dic_common_parameters[generation][parameter] = self.config[\"structure\"][\n                    generation\n                ][\"common_parameters\"][parameter]\n\n        # Check that the generation has scans\n        if (\n            \"scans\" not in self.config[\"structure\"][generation]\n            or self.config[\"structure\"][generation][\"scans\"] is None\n        ):\n            dic_parameter_lists = {\"\": [generation]}\n            dic_parameter_lists_for_naming = {\"\": [generation]}\n            array_conditions = None\n            ll_concomitant_parameters = []\n        else:\n            # Browse and collect the parameter space for the generation\n            (\n                dic_parameter_lists,\n                dic_parameter_lists_for_naming,\n                dic_subvariables,\n                ll_concomitant_parameters,\n                l_conditions,\n            ) = self.browse_and_collect_parameter_space(generation)\n\n            # Get the dimension corresponding to each parameter\n            dic_dimension_indices = {\n                parameter: idx for idx, parameter in enumerate(dic_parameter_lists)\n            }\n\n            # Generate array of conditions to filter out some of the values later\n            # Is an array of True values if no conditions are present\n            array_conditions = self.eval_conditions(l_conditions, dic_parameter_lists)\n\n            # Filter for concomitant parameters\n            array_conditions = self.filter_for_concomitant_parameters(\n                array_conditions, ll_concomitant_parameters, dic_dimension_indices\n            )\n\n            # Postprocess the parameter lists and update the dictionaries\n            dic_parameter_lists, dic_parameter_lists_for_naming = self.postprocess_parameter_lists(\n                dic_parameter_lists, dic_parameter_lists_for_naming, dic_subvariables\n            )\n\n        return (\n            dic_parameter_lists,\n            dic_parameter_lists_for_naming,\n            array_conditions,\n        )\n\n    def parse_parameter_space(\n        self,\n        parameter: str,\n        dic_curr_parameter: dict[str, Any],\n        dic_parameter_lists: dict[str, Any],\n        dic_parameter_lists_for_naming: dict[str, Any],\n    ) -&gt; tuple[dict[str, Any], dict[str, Any]]:\n        \"\"\"\n        Parses the parameter space for a given parameter.\n\n        Args:\n            parameter (str): The parameter name.\n            dic_curr_parameter (dict[str, Any]): The dictionary of current parameter values.\n            dic_parameter_lists (dict[str, Any]): The dictionary of parameter lists.\n            dic_parameter_lists_for_naming (dict[str, Any]): The dictionary of parameter lists for naming.\n\n        Returns:\n            tuple[dict[str, Any], dict[str, Any]]: The updated dictionaries of parameter lists.\n        \"\"\"\n\n        if \"linspace\" in dic_curr_parameter:\n            parameter_list = linspace(dic_curr_parameter[\"linspace\"])\n            dic_parameter_lists_for_naming[parameter] = parameter_list\n        elif \"logspace\" in dic_curr_parameter:\n            parameter_list = logspace(dic_curr_parameter[\"logspace\"])\n            dic_parameter_lists_for_naming[parameter] = parameter_list\n        elif \"path_list\" in dic_curr_parameter:\n            l_values_path_list = dic_curr_parameter[\"path_list\"]\n            parameter_list = list_values_path(l_values_path_list, self.dic_common_parameters)\n            dic_parameter_lists_for_naming[parameter] = [\n                f\"{n:02d}\" for n, path in enumerate(parameter_list)\n            ]\n        elif \"list\" in dic_curr_parameter:\n            parameter_list = dic_curr_parameter[\"list\"]\n            dic_parameter_lists_for_naming[parameter] = parameter_list\n        elif \"expression\" in dic_curr_parameter:\n            parameter_list = np.round(\n                eval(dic_curr_parameter[\"expression\"], copy.deepcopy(dic_parameter_lists)),\n                8,\n            )\n            dic_parameter_lists_for_naming[parameter] = parameter_list\n        else:\n            raise ValueError(f\"Scanning method for parameter {parameter} is not recognized.\")\n\n        dic_parameter_lists[parameter] = np.array(parameter_list)\n        return dic_parameter_lists, dic_parameter_lists_for_naming\n\n    def browse_and_collect_parameter_space(\n        self,\n        generation: str,\n    ) -&gt; tuple[\n        dict[str, Any],\n        dict[str, Any],\n        dict[str, Any],\n        list[list[str]],\n        list[str],\n    ]:\n        \"\"\"\n        Browses and collects the parameter space for a given generation.\n\n        Args:\n            generation (str): The generation name.\n\n        Returns:\n            tuple[dict[str, Any], dict[str, Any], dict[str, Any], list[list[str]]]: The updated\n                dictionaries of parameter lists.\n        \"\"\"\n\n        l_conditions = []\n        ll_concomitant_parameters = []\n        dic_subvariables = {}\n        dic_parameter_lists = {}\n        dic_parameter_lists_for_naming = {}\n        for parameter in self.config[\"structure\"][generation][\"scans\"]:\n            dic_curr_parameter = self.config[\"structure\"][generation][\"scans\"][parameter]\n\n            # Parse the parameter space\n            dic_parameter_lists, dic_parameter_lists_for_naming = self.parse_parameter_space(\n                parameter, dic_curr_parameter, dic_parameter_lists, dic_parameter_lists_for_naming\n            )\n\n            # Store potential subvariables\n            if \"subvariables\" in dic_curr_parameter:\n                dic_subvariables[parameter] = dic_curr_parameter[\"subvariables\"]\n\n            # Save the condition if it exists\n            if \"condition\" in dic_curr_parameter:\n                l_conditions.append(dic_curr_parameter[\"condition\"])\n\n            # Save the concomitant parameters if they exist\n            if \"concomitant\" in dic_curr_parameter:\n                if not isinstance(dic_curr_parameter[\"concomitant\"], list):\n                    dic_curr_parameter[\"concomitant\"] = [dic_curr_parameter[\"concomitant\"]]\n                for concomitant_parameter in dic_curr_parameter[\"concomitant\"]:\n                    # Assert that the parameters list have the same size\n                    assert len(dic_parameter_lists[parameter]) == len(\n                        dic_parameter_lists[concomitant_parameter]\n                    ), (\n                        f\"Parameters {parameter} and {concomitant_parameter} must have the \"\n                        \"same size.\"\n                    )\n                # Add to the list for filtering later\n                ll_concomitant_parameters.append([parameter] + dic_curr_parameter[\"concomitant\"])\n\n        return (\n            dic_parameter_lists,\n            dic_parameter_lists_for_naming,\n            dic_subvariables,\n            ll_concomitant_parameters,\n            l_conditions,\n        )\n\n    def postprocess_parameter_lists(\n        self,\n        dic_parameter_lists: dict[str, Any],\n        dic_parameter_lists_for_naming: dict[str, Any],\n        dic_subvariables: dict[str, Any],\n    ) -&gt; tuple[dict[str, Any], dict[str, Any]]:\n        \"\"\"\n        Post-processes parameter lists by ensuring values are not numpy types and handling nested\n        parameters.\n\n        Args:\n            dic_parameter_lists (dict[str, Any]): Dictionary containing parameter lists.\n            dic_parameter_lists_for_naming (dict[str, Any]): Dictionary containing parameter lists\n                for naming.\n            dic_subvariables (dict[str, Any]): Dictionary containing subvariables for nested\n                parameters.\n\n        Returns:\n            tuple[dict[str, Any], dict[str, Any]]: Updated dictionaries of parameter lists and\n                parameter lists for naming.\n        \"\"\"\n        for parameter, parameter_list in dic_parameter_lists.items():\n            parameter_list_for_naming = dic_parameter_lists_for_naming[parameter]\n\n            # Ensure that all values are not numpy types (to avoid serialization issues)\n            parameter_list = [x.item() if isinstance(x, np.generic) else x for x in parameter_list]\n\n            # Handle nested parameters\n            parameter_list_updated = (\n                convert_for_subvariables(dic_subvariables[parameter], parameter_list)\n                if parameter in dic_subvariables\n                else parameter_list\n            )\n            # Update the dictionaries\n            dic_parameter_lists[parameter] = parameter_list_updated\n            dic_parameter_lists_for_naming[parameter] = parameter_list_for_naming\n\n        return dic_parameter_lists, dic_parameter_lists_for_naming\n\n    def create_scans(\n        self,\n        generation: str,\n        generation_path: str,\n        template_path: str,\n        depth_gen: int,\n        dic_parameter_lists: Optional[dict[str, Any]] = None,\n        dic_parameter_lists_for_naming: Optional[dict[str, Any]] = None,\n        add_prefix_to_folder_names: bool = False,\n    ) -&gt; list[str]:\n        \"\"\"\n        Creates study files for parametric scans.\n        If a dictionary of parameter lists is provided, the scan will be done on the parameter\n        lists (no cartesian product). Otherwise, the scan will be done on the cartesian product of\n        the parameters defined in the scan configuration file.\n\n        Args:\n            generation (str): The generation name.\n            generation_path (str): The (relative) path to the generation folder.\n            template_path (str): The path to the template folder.\n            depth_gen (int): The depth of the generation in the tree.\n            dic_parameter_lists (Optional[dict[str, Any]]): The dictionary of parameter lists.\n                Defaults to None.\n            dic_parameter_lists_for_naming (Optional[dict[str, Any]]): The dictionary of parameter\n                lists for naming. Defaults to None.\n            add_prefix_to_folder_names (bool): Whether to add a prefix to the folder names. Defaults\n                to False.\n\n        Returns:\n            tuple[list[str], list[str]]: The list of study file strings and the list of study paths.\n        \"\"\"\n        if dic_parameter_lists is None:\n            # Get dictionnary of parametric values being scanned\n            dic_parameter_lists, dic_parameter_lists_for_naming, array_conditions = (\n                self.get_dic_parametric_scans(generation)\n            )\n            apply_cartesian_product = True\n        else:\n            if dic_parameter_lists_for_naming is None:\n                dic_parameter_lists_for_naming = copy.deepcopy(dic_parameter_lists)\n            array_conditions = None\n            apply_cartesian_product = False\n\n        # Generate render write for the parameters parameters\n        l_study_path = []\n        if apply_cartesian_product:\n            logging.info(\n                f\"Now generation cartesian product of all parameters for generation: {generation}\"\n            )\n            array_param_values = itertools.product(*dic_parameter_lists.values())\n            array_param_values_for_naming = itertools.product(\n                *dic_parameter_lists_for_naming.values()\n            )\n            array_idx = itertools.product(*[range(len(x)) for x in dic_parameter_lists.values()])\n        else:\n            logging.info(f\"Now generation parameters for generation: {generation}\")\n            array_param_values = [list(x) for x in zip(*dic_parameter_lists.values())]\n            array_param_values_for_naming = [\n                list(x) for x in zip(*dic_parameter_lists_for_naming.values())\n            ]\n            array_idx = range(len(array_param_values))\n\n        # Loop over the parameters\n        to_disk_len = np.sum(array_conditions) if array_conditions is not None else 1\n        to_disk_idx = 0\n        for idx, (l_values, l_values_for_naming, l_idx) in enumerate(\n            zip(array_param_values, array_param_values_for_naming, array_idx)\n        ):\n            # Check the idx to keep if conditions are present\n            if array_conditions is not None and not array_conditions[l_idx]:\n                continue\n\n            # Create the path for the study\n            dic_mutated_parameters = dict(zip(dic_parameter_lists.keys(), l_values))\n            dic_mutated_parameters_for_naming = dict(\n                zip(dic_parameter_lists.keys(), l_values_for_naming)\n            )\n\n            # Handle prefix\n            prefix_path = \"\"\n            if add_prefix_to_folder_names:\n                prefix_path = f\"ID_{str(to_disk_idx).zfill(len(str(to_disk_len)))}_\"\n                to_disk_idx += 1\n\n            # Handle suffix\n            suffix_path = \"_\".join(\n                [\n                    f\"{parameter}_{value}\"\n                    for parameter, value in dic_mutated_parameters_for_naming.items()\n                ]\n            )\n            suffix_path = suffix_path.removeprefix(\"_\")\n\n            # Create final path\n            path = generation_path + prefix_path + suffix_path + \"/\"\n\n            # Add common parameters\n            if generation in self.dic_common_parameters:\n                dic_mutated_parameters |= self.dic_common_parameters[generation]\n\n            # Remove \"\" from mutated parameters, if it's in the dictionary\n            # as it's only used when no scan is done\n            if \"\" in dic_mutated_parameters:\n                dic_mutated_parameters.pop(\"\")\n\n            # Generate the study for current generation\n            self.generate_render_write(\n                generation,\n                path,\n                template_path,\n                depth_gen,\n                dic_mutated_parameters=dic_mutated_parameters,\n            )\n\n            # Append the list of study paths to build the tree later on\n            l_study_path.append(path)\n\n        if not l_study_path:\n            logging.warning(\n                f\"No study paths were created for generation {generation}.\"\n                \"Please check the conditions.\"\n            )\n\n        return l_study_path\n\n    def complete_tree(\n        self, dictionary_tree: dict, l_study_path_next_gen: list[str], gen: str\n    ) -&gt; dict:\n        \"\"\"\n        Completes the tree structure of the study dictionary.\n\n        Args:\n            dictionary_tree (dict): The dictionary representing the study tree structure.\n            l_study_path_next_gen (list[str]): The list of study paths for the next gen.\n            gen (str): The generation name.\n\n        Returns:\n            dict: The updated dictionary representing the study tree structure.\n        \"\"\"\n        logging.info(f\"Completing the tree structure for generation: {gen}\")\n        for path_next in l_study_path_next_gen:\n            nested_set(\n                dictionary_tree,\n                path_next.split(\"/\")[1:-1] + [gen],\n                {\"file\": f\"{path_next}{gen}.py\"},\n            )\n\n        return dictionary_tree\n\n    def write_tree(self, dictionary_tree: dict):\n        \"\"\"\n        Writes the study tree structure to a YAML file.\n\n        Args:\n            dictionary_tree (dict): The dictionary representing the study tree structure.\n        \"\"\"\n        logging.info(\"Writing the tree structure to a YAML file.\")\n        ryaml = yaml.YAML()\n        with open(self.path_tree, \"w\") as yaml_file:\n            ryaml.indent(sequence=4, offset=2)\n            ryaml.dump(dictionary_tree, yaml_file)\n\n    def create_study_for_current_gen(\n        self,\n        generation: str,\n        generation_path: str,\n        depth_gen: int,\n        dic_parameter_lists: Optional[dict[str, Any]] = None,\n        dic_parameter_lists_for_naming: Optional[dict[str, Any]] = None,\n        add_prefix_to_folder_names: bool = False,\n    ) -&gt; list[str]:\n        \"\"\"\n        Creates study files for the current generation.\n\n        Args:\n            generation (str): The name of the current generation.\n            directory_path (str): The (relative) path to the directory folder for the current\n                generation.\n            depth_gen (int): The depth of the generation in the tree.\n            dic_parameter_lists (Optional[dict[str, Any]]): The dictionary of parameter lists.\n                Defaults to None.\n            dic_parameter_lists_for_naming (Optional[dict[str, Any]]): The dictionary of parameter\n                lists for naming. Defaults to None.\n            add_prefix_to_folder_names (bool): Whether to add a prefix to the folder names. Defaults\n                to False.\n\n        Returns:\n            tuple[list[str], list[str]]: The list of study file strings and the list of study paths.\n        \"\"\"\n        executable_path = self.config[\"structure\"][generation][\"executable\"]\n        path_local_template = (\n            f\"{os.path.dirname(inspect.getfile(GenerateScan))}/../assets/template_scripts/\"\n        )\n\n        # Check if the executable path corresponds to a file\n        if not os.path.isfile(executable_path):\n            # Check if the executable path corresponds to a file in the template folder\n            executable_path_template = f\"{path_local_template}{executable_path}\"\n            if not os.path.isfile(executable_path_template):\n                raise FileNotFoundError(\n                    f\"Executable file {executable_path} not found locally nor in the study-da \"\n                    \"template folder.\"\n                )\n            else:\n                executable_path = executable_path_template\n\n        # Ensure that the values in dic_parameter_lists can be dumped with ryaml\n        if dic_parameter_lists is not None:\n            # Recursively convert all numpy types to standard types\n            clean_dic(dic_parameter_lists)\n            logging.info(\"An external dictionary of parameters was provided.\")\n        else:\n            logging.info(\"Creating the dictionnary of parameters from the configuration file.\")\n\n        return self.create_scans(\n            generation,\n            generation_path,\n            executable_path,\n            depth_gen,\n            dic_parameter_lists,\n            dic_parameter_lists_for_naming,\n            add_prefix_to_folder_names,\n        )\n\n    def browse_and_creat_study(\n        self,\n        dic_parameter_all_gen: Optional[dict[str, dict[str, Any]]],\n        dic_parameter_all_gen_naming: Optional[dict[str, dict[str, Any]]],\n        add_prefix_to_folder_names: bool,\n    ) -&gt; dict:\n        l_study_path = [self.config[\"name\"] + \"/\"]\n        dictionary_tree = {}\n\n        # Browse through the generations\n        l_generations = list(self.config[\"structure\"].keys())\n        for idx, generation in enumerate(l_generations):\n            l_study_path_all_next_generation = []\n            logging.info(f\"Taking care of generation: {generation}\")\n            for study_path in l_study_path:\n                if dic_parameter_all_gen is None or generation not in dic_parameter_all_gen:\n                    dic_parameter_current_gen = None\n                    dic_parameter_naming_current_gen = None\n                else:\n                    dic_parameter_current_gen = dic_parameter_all_gen[generation]\n                    if (\n                        dic_parameter_all_gen_naming is not None\n                        and generation in dic_parameter_all_gen_naming\n                    ):\n                        dic_parameter_naming_current_gen = dic_parameter_all_gen_naming[generation]\n                    else:\n                        dic_parameter_naming_current_gen = None\n\n                # Get list of paths for the children of the current study\n                l_study_path_next_generation = self.create_study_for_current_gen(\n                    generation,\n                    study_path,\n                    idx + 1,\n                    dic_parameter_current_gen,\n                    dic_parameter_naming_current_gen,\n                    add_prefix_to_folder_names,\n                )\n                # Update tree\n                dictionary_tree = self.complete_tree(\n                    dictionary_tree, l_study_path_next_generation, generation\n                )\n                # Complete list of paths for the children of all studies (of the current generation)\n                l_study_path_all_next_generation.extend(l_study_path_next_generation)\n\n            # Update study path for next later\n            l_study_path = l_study_path_all_next_generation\n\n        return dictionary_tree\n\n    def create_study(\n        self,\n        tree_file: bool = True,\n        force_overwrite: bool = False,\n        dic_parameter_all_gen: Optional[dict[str, dict[str, Any]]] = None,\n        dic_parameter_all_gen_naming: Optional[dict[str, dict[str, Any]]] = None,\n        add_prefix_to_folder_names: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Creates study files for the entire study.\n\n        Args:\n            tree_file (bool, optional): Whether to write the study tree structure to a YAML file.\n                Defaults to True.\n            force_overwrite (bool, optional): Whether to overwrite existing study files.\n                Defaults to False.\n            dic_parameter_all_gen (Optional[dict[str, dict[str, Any]]]): The dictionary of parameter\n                lists for all generations. Defaults to None.\n            dic_parameter_all_gen_naming (Optional[dict[str, dict[str, Any]]]): The dictionary of\n                parameter lists for all generations for naming. Defaults to None.\n            add_prefix_to_folder_names (bool): Whether to add a prefix to the folder names. Defaults\n                to False.\n\n        Returns:\n            list[str]: The list of study file strings.\n        \"\"\"\n\n        # Raise an error if dic_parameter_all_gen_naming is not None while dic_parameter_all_gen is None\n        if dic_parameter_all_gen is None and dic_parameter_all_gen_naming is not None:\n            raise ValueError(\n                \"If dic_parameter_all_gen_naming is defined, dic_parameter_all_gen must be defined.\"\n            )\n\n        # Remove existing study if force_overwrite\n        if os.path.exists(self.config[\"name\"]):\n            if not force_overwrite:\n                logging.info(\n                    f\"Study {self.config['name']} already exists. Set force_overwrite to True to \"\n                    \"overwrite. Continuing without overwriting.\"\n                )\n                return\n            shutil.rmtree(self.config[\"name\"])\n\n        # Browse through the generations and create the study\n        dictionary_tree = self.browse_and_creat_study(\n            dic_parameter_all_gen,\n            dic_parameter_all_gen_naming,\n            add_prefix_to_folder_names,\n        )\n\n        # Add dependencies to root of the study\n        if \"dependencies\" in self.config:\n            for dependency, path in self.config[\"dependencies\"].items():\n                # Check if the dependency exists as a file\n                if not os.path.isfile(path):\n                    # Check if the dependency exists as a file in the template folder\n                    path_template = f\"{os.path.dirname(inspect.getfile(GenerateScan))}/../assets/configurations/{path}\"\n                    if not os.path.isfile(path_template):\n                        raise FileNotFoundError(\n                            f\"Dependency file {path} not found locally nor in the study-da \"\n                            \"template folder.\"\n                        )\n                    else:\n                        path = path_template\n                shutil.copy2(path, self.config[\"name\"])\n\n        if tree_file:\n            self.write_tree(dictionary_tree)\n\n    @staticmethod\n    def eval_conditions(l_condition: list[str], dic_parameter_lists: dict[str, Any]) -&gt; np.ndarray:\n        \"\"\"\n        Evaluates the conditions to filter out some parameter values.\n\n        Args:\n            l_condition (list[str]): The list of conditions.\n            dic_parameter_lists (dict[str: Any]): The dictionary of parameter lists.\n\n        Returns:\n            np.ndarray: The array of conditions.\n        \"\"\"\n        # Initialize the array of parameters as a meshgrid of all parameters\n        l_parameters = list(dic_parameter_lists.values())\n        meshgrid = np.meshgrid(*l_parameters, indexing=\"ij\")\n\n        # Associate the parameters to their names\n        dic_param_mesh = dict(zip(dic_parameter_lists.keys(), meshgrid))\n\n        # Evaluate the conditions and take the intersection of all conditions\n        array_conditions = np.ones_like(meshgrid[0], dtype=bool)\n        for condition in l_condition:\n            array_conditions = array_conditions &amp; eval(condition, dic_param_mesh)\n\n        return array_conditions\n\n    @staticmethod\n    def filter_for_concomitant_parameters(\n        array_conditions: np.ndarray,\n        ll_concomitant_parameters: list[list[str]],\n        dic_dimension_indices: dict[str, int],\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Filters the conditions for concomitant parameters.\n\n        Args:\n            array_conditions (np.ndarray): The array of conditions.\n            ll_concomitant_parameters (list[list[str]]): The list of concomitant parameters.\n            dic_dimension_indices (dict[str, int]): The dictionary of dimension indices.\n\n        Returns:\n            np.ndarray: The filtered array of conditions.\n        \"\"\"\n\n        # Return the array of conditions if no concomitant parameters\n        if not ll_concomitant_parameters:\n            return array_conditions\n\n        # Get the indices of the concomitant parameters\n        ll_idx_concomitant_parameters = [\n            [dic_dimension_indices[parameter] for parameter in concomitant_parameters]\n            for concomitant_parameters in ll_concomitant_parameters\n        ]\n\n        # Browse all the values of array_conditions\n        for idx, _ in np.ndenumerate(array_conditions):\n            # Check if the value is on the diagonal of the concomitant parameters\n            for l_idx_concomitant_parameter in ll_idx_concomitant_parameters:\n                if any(\n                    idx[i] != idx[j]\n                    for i, j in itertools.combinations(l_idx_concomitant_parameter, 2)\n                ):\n                    array_conditions[idx] = False\n                    break\n\n        return array_conditions\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.GenerateScan.__init__","title":"<code>__init__(path_config=None, dic_scan=None)</code>","text":"<p>Initialize the generation scan with a configuration file or dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>path_config</code> <code>Optional[str]</code> <p>Path to the configuration file for the scan. Default is None.</p> <code>None</code> <code>dic_scan</code> <code>Optional[dict[str, Any]]</code> <p>Dictionary containing the scan configuration. Default is None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither or both of <code>path_config</code> and <code>dic_scan</code> are provided.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def __init__(\n    self, path_config: Optional[str] = None, dic_scan: Optional[dict[str, Any]] = None\n):  # sourcery skip: remove-redundant-if\n    \"\"\"\n    Initialize the generation scan with a configuration file or dictionary.\n\n    Args:\n        path_config (Optional[str]): Path to the configuration file for the scan.\n            Default is None.\n        dic_scan (Optional[dict[str, Any]]): Dictionary containing the scan configuration.\n            Default is None.\n\n    Raises:\n        ValueError: If neither or both of `path_config` and `dic_scan` are provided.\n    \"\"\"\n    # Load the study configuration from file or dictionary\n    if dic_scan is None and path_config is None:\n        raise ValueError(\n            \"Either a path to the configuration file or a dictionary must be provided.\"\n        )\n    elif dic_scan is not None and path_config is not None:\n        raise ValueError(\"Only one of the configuration file or dictionary must be provided.\")\n    elif path_config is not None:\n        self.config, self.ryaml = load_dic_from_path(path_config)\n    elif dic_scan is not None:\n        self.config = dic_scan\n        self.ryaml = yaml.YAML()\n    else:\n        raise ValueError(\"An unexpected error occurred.\")\n\n    # Parameters common across all generations (e.g. for parallelization)\n    self.dic_common_parameters: dict[str, Any] = {}\n\n    # Path to the tree file\n    self.path_tree = self.config[\"name\"] + \"/\" + \"tree.yaml\"\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.GenerateScan.browse_and_collect_parameter_space","title":"<code>browse_and_collect_parameter_space(generation)</code>","text":"<p>Browses and collects the parameter space for a given generation.</p> <p>Parameters:</p> Name Type Description Default <code>generation</code> <code>str</code> <p>The generation name.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Any], dict[str, Any], dict[str, Any], list[list[str]], list[str]]</code> <p>tuple[dict[str, Any], dict[str, Any], dict[str, Any], list[list[str]]]: The updated dictionaries of parameter lists.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def browse_and_collect_parameter_space(\n    self,\n    generation: str,\n) -&gt; tuple[\n    dict[str, Any],\n    dict[str, Any],\n    dict[str, Any],\n    list[list[str]],\n    list[str],\n]:\n    \"\"\"\n    Browses and collects the parameter space for a given generation.\n\n    Args:\n        generation (str): The generation name.\n\n    Returns:\n        tuple[dict[str, Any], dict[str, Any], dict[str, Any], list[list[str]]]: The updated\n            dictionaries of parameter lists.\n    \"\"\"\n\n    l_conditions = []\n    ll_concomitant_parameters = []\n    dic_subvariables = {}\n    dic_parameter_lists = {}\n    dic_parameter_lists_for_naming = {}\n    for parameter in self.config[\"structure\"][generation][\"scans\"]:\n        dic_curr_parameter = self.config[\"structure\"][generation][\"scans\"][parameter]\n\n        # Parse the parameter space\n        dic_parameter_lists, dic_parameter_lists_for_naming = self.parse_parameter_space(\n            parameter, dic_curr_parameter, dic_parameter_lists, dic_parameter_lists_for_naming\n        )\n\n        # Store potential subvariables\n        if \"subvariables\" in dic_curr_parameter:\n            dic_subvariables[parameter] = dic_curr_parameter[\"subvariables\"]\n\n        # Save the condition if it exists\n        if \"condition\" in dic_curr_parameter:\n            l_conditions.append(dic_curr_parameter[\"condition\"])\n\n        # Save the concomitant parameters if they exist\n        if \"concomitant\" in dic_curr_parameter:\n            if not isinstance(dic_curr_parameter[\"concomitant\"], list):\n                dic_curr_parameter[\"concomitant\"] = [dic_curr_parameter[\"concomitant\"]]\n            for concomitant_parameter in dic_curr_parameter[\"concomitant\"]:\n                # Assert that the parameters list have the same size\n                assert len(dic_parameter_lists[parameter]) == len(\n                    dic_parameter_lists[concomitant_parameter]\n                ), (\n                    f\"Parameters {parameter} and {concomitant_parameter} must have the \"\n                    \"same size.\"\n                )\n            # Add to the list for filtering later\n            ll_concomitant_parameters.append([parameter] + dic_curr_parameter[\"concomitant\"])\n\n    return (\n        dic_parameter_lists,\n        dic_parameter_lists_for_naming,\n        dic_subvariables,\n        ll_concomitant_parameters,\n        l_conditions,\n    )\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.GenerateScan.complete_tree","title":"<code>complete_tree(dictionary_tree, l_study_path_next_gen, gen)</code>","text":"<p>Completes the tree structure of the study dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary_tree</code> <code>dict</code> <p>The dictionary representing the study tree structure.</p> required <code>l_study_path_next_gen</code> <code>list[str]</code> <p>The list of study paths for the next gen.</p> required <code>gen</code> <code>str</code> <p>The generation name.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The updated dictionary representing the study tree structure.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def complete_tree(\n    self, dictionary_tree: dict, l_study_path_next_gen: list[str], gen: str\n) -&gt; dict:\n    \"\"\"\n    Completes the tree structure of the study dictionary.\n\n    Args:\n        dictionary_tree (dict): The dictionary representing the study tree structure.\n        l_study_path_next_gen (list[str]): The list of study paths for the next gen.\n        gen (str): The generation name.\n\n    Returns:\n        dict: The updated dictionary representing the study tree structure.\n    \"\"\"\n    logging.info(f\"Completing the tree structure for generation: {gen}\")\n    for path_next in l_study_path_next_gen:\n        nested_set(\n            dictionary_tree,\n            path_next.split(\"/\")[1:-1] + [gen],\n            {\"file\": f\"{path_next}{gen}.py\"},\n        )\n\n    return dictionary_tree\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.GenerateScan.create_scans","title":"<code>create_scans(generation, generation_path, template_path, depth_gen, dic_parameter_lists=None, dic_parameter_lists_for_naming=None, add_prefix_to_folder_names=False)</code>","text":"<p>Creates study files for parametric scans. If a dictionary of parameter lists is provided, the scan will be done on the parameter lists (no cartesian product). Otherwise, the scan will be done on the cartesian product of the parameters defined in the scan configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>generation</code> <code>str</code> <p>The generation name.</p> required <code>generation_path</code> <code>str</code> <p>The (relative) path to the generation folder.</p> required <code>template_path</code> <code>str</code> <p>The path to the template folder.</p> required <code>depth_gen</code> <code>int</code> <p>The depth of the generation in the tree.</p> required <code>dic_parameter_lists</code> <code>Optional[dict[str, Any]]</code> <p>The dictionary of parameter lists. Defaults to None.</p> <code>None</code> <code>dic_parameter_lists_for_naming</code> <code>Optional[dict[str, Any]]</code> <p>The dictionary of parameter lists for naming. Defaults to None.</p> <code>None</code> <code>add_prefix_to_folder_names</code> <code>bool</code> <p>Whether to add a prefix to the folder names. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>tuple[list[str], list[str]]: The list of study file strings and the list of study paths.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def create_scans(\n    self,\n    generation: str,\n    generation_path: str,\n    template_path: str,\n    depth_gen: int,\n    dic_parameter_lists: Optional[dict[str, Any]] = None,\n    dic_parameter_lists_for_naming: Optional[dict[str, Any]] = None,\n    add_prefix_to_folder_names: bool = False,\n) -&gt; list[str]:\n    \"\"\"\n    Creates study files for parametric scans.\n    If a dictionary of parameter lists is provided, the scan will be done on the parameter\n    lists (no cartesian product). Otherwise, the scan will be done on the cartesian product of\n    the parameters defined in the scan configuration file.\n\n    Args:\n        generation (str): The generation name.\n        generation_path (str): The (relative) path to the generation folder.\n        template_path (str): The path to the template folder.\n        depth_gen (int): The depth of the generation in the tree.\n        dic_parameter_lists (Optional[dict[str, Any]]): The dictionary of parameter lists.\n            Defaults to None.\n        dic_parameter_lists_for_naming (Optional[dict[str, Any]]): The dictionary of parameter\n            lists for naming. Defaults to None.\n        add_prefix_to_folder_names (bool): Whether to add a prefix to the folder names. Defaults\n            to False.\n\n    Returns:\n        tuple[list[str], list[str]]: The list of study file strings and the list of study paths.\n    \"\"\"\n    if dic_parameter_lists is None:\n        # Get dictionnary of parametric values being scanned\n        dic_parameter_lists, dic_parameter_lists_for_naming, array_conditions = (\n            self.get_dic_parametric_scans(generation)\n        )\n        apply_cartesian_product = True\n    else:\n        if dic_parameter_lists_for_naming is None:\n            dic_parameter_lists_for_naming = copy.deepcopy(dic_parameter_lists)\n        array_conditions = None\n        apply_cartesian_product = False\n\n    # Generate render write for the parameters parameters\n    l_study_path = []\n    if apply_cartesian_product:\n        logging.info(\n            f\"Now generation cartesian product of all parameters for generation: {generation}\"\n        )\n        array_param_values = itertools.product(*dic_parameter_lists.values())\n        array_param_values_for_naming = itertools.product(\n            *dic_parameter_lists_for_naming.values()\n        )\n        array_idx = itertools.product(*[range(len(x)) for x in dic_parameter_lists.values()])\n    else:\n        logging.info(f\"Now generation parameters for generation: {generation}\")\n        array_param_values = [list(x) for x in zip(*dic_parameter_lists.values())]\n        array_param_values_for_naming = [\n            list(x) for x in zip(*dic_parameter_lists_for_naming.values())\n        ]\n        array_idx = range(len(array_param_values))\n\n    # Loop over the parameters\n    to_disk_len = np.sum(array_conditions) if array_conditions is not None else 1\n    to_disk_idx = 0\n    for idx, (l_values, l_values_for_naming, l_idx) in enumerate(\n        zip(array_param_values, array_param_values_for_naming, array_idx)\n    ):\n        # Check the idx to keep if conditions are present\n        if array_conditions is not None and not array_conditions[l_idx]:\n            continue\n\n        # Create the path for the study\n        dic_mutated_parameters = dict(zip(dic_parameter_lists.keys(), l_values))\n        dic_mutated_parameters_for_naming = dict(\n            zip(dic_parameter_lists.keys(), l_values_for_naming)\n        )\n\n        # Handle prefix\n        prefix_path = \"\"\n        if add_prefix_to_folder_names:\n            prefix_path = f\"ID_{str(to_disk_idx).zfill(len(str(to_disk_len)))}_\"\n            to_disk_idx += 1\n\n        # Handle suffix\n        suffix_path = \"_\".join(\n            [\n                f\"{parameter}_{value}\"\n                for parameter, value in dic_mutated_parameters_for_naming.items()\n            ]\n        )\n        suffix_path = suffix_path.removeprefix(\"_\")\n\n        # Create final path\n        path = generation_path + prefix_path + suffix_path + \"/\"\n\n        # Add common parameters\n        if generation in self.dic_common_parameters:\n            dic_mutated_parameters |= self.dic_common_parameters[generation]\n\n        # Remove \"\" from mutated parameters, if it's in the dictionary\n        # as it's only used when no scan is done\n        if \"\" in dic_mutated_parameters:\n            dic_mutated_parameters.pop(\"\")\n\n        # Generate the study for current generation\n        self.generate_render_write(\n            generation,\n            path,\n            template_path,\n            depth_gen,\n            dic_mutated_parameters=dic_mutated_parameters,\n        )\n\n        # Append the list of study paths to build the tree later on\n        l_study_path.append(path)\n\n    if not l_study_path:\n        logging.warning(\n            f\"No study paths were created for generation {generation}.\"\n            \"Please check the conditions.\"\n        )\n\n    return l_study_path\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.GenerateScan.create_study","title":"<code>create_study(tree_file=True, force_overwrite=False, dic_parameter_all_gen=None, dic_parameter_all_gen_naming=None, add_prefix_to_folder_names=False)</code>","text":"<p>Creates study files for the entire study.</p> <p>Parameters:</p> Name Type Description Default <code>tree_file</code> <code>bool</code> <p>Whether to write the study tree structure to a YAML file. Defaults to True.</p> <code>True</code> <code>force_overwrite</code> <code>bool</code> <p>Whether to overwrite existing study files. Defaults to False.</p> <code>False</code> <code>dic_parameter_all_gen</code> <code>Optional[dict[str, dict[str, Any]]]</code> <p>The dictionary of parameter lists for all generations. Defaults to None.</p> <code>None</code> <code>dic_parameter_all_gen_naming</code> <code>Optional[dict[str, dict[str, Any]]]</code> <p>The dictionary of parameter lists for all generations for naming. Defaults to None.</p> <code>None</code> <code>add_prefix_to_folder_names</code> <code>bool</code> <p>Whether to add a prefix to the folder names. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>list[str]: The list of study file strings.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def create_study(\n    self,\n    tree_file: bool = True,\n    force_overwrite: bool = False,\n    dic_parameter_all_gen: Optional[dict[str, dict[str, Any]]] = None,\n    dic_parameter_all_gen_naming: Optional[dict[str, dict[str, Any]]] = None,\n    add_prefix_to_folder_names: bool = False,\n) -&gt; None:\n    \"\"\"\n    Creates study files for the entire study.\n\n    Args:\n        tree_file (bool, optional): Whether to write the study tree structure to a YAML file.\n            Defaults to True.\n        force_overwrite (bool, optional): Whether to overwrite existing study files.\n            Defaults to False.\n        dic_parameter_all_gen (Optional[dict[str, dict[str, Any]]]): The dictionary of parameter\n            lists for all generations. Defaults to None.\n        dic_parameter_all_gen_naming (Optional[dict[str, dict[str, Any]]]): The dictionary of\n            parameter lists for all generations for naming. Defaults to None.\n        add_prefix_to_folder_names (bool): Whether to add a prefix to the folder names. Defaults\n            to False.\n\n    Returns:\n        list[str]: The list of study file strings.\n    \"\"\"\n\n    # Raise an error if dic_parameter_all_gen_naming is not None while dic_parameter_all_gen is None\n    if dic_parameter_all_gen is None and dic_parameter_all_gen_naming is not None:\n        raise ValueError(\n            \"If dic_parameter_all_gen_naming is defined, dic_parameter_all_gen must be defined.\"\n        )\n\n    # Remove existing study if force_overwrite\n    if os.path.exists(self.config[\"name\"]):\n        if not force_overwrite:\n            logging.info(\n                f\"Study {self.config['name']} already exists. Set force_overwrite to True to \"\n                \"overwrite. Continuing without overwriting.\"\n            )\n            return\n        shutil.rmtree(self.config[\"name\"])\n\n    # Browse through the generations and create the study\n    dictionary_tree = self.browse_and_creat_study(\n        dic_parameter_all_gen,\n        dic_parameter_all_gen_naming,\n        add_prefix_to_folder_names,\n    )\n\n    # Add dependencies to root of the study\n    if \"dependencies\" in self.config:\n        for dependency, path in self.config[\"dependencies\"].items():\n            # Check if the dependency exists as a file\n            if not os.path.isfile(path):\n                # Check if the dependency exists as a file in the template folder\n                path_template = f\"{os.path.dirname(inspect.getfile(GenerateScan))}/../assets/configurations/{path}\"\n                if not os.path.isfile(path_template):\n                    raise FileNotFoundError(\n                        f\"Dependency file {path} not found locally nor in the study-da \"\n                        \"template folder.\"\n                    )\n                else:\n                    path = path_template\n            shutil.copy2(path, self.config[\"name\"])\n\n    if tree_file:\n        self.write_tree(dictionary_tree)\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.GenerateScan.create_study_for_current_gen","title":"<code>create_study_for_current_gen(generation, generation_path, depth_gen, dic_parameter_lists=None, dic_parameter_lists_for_naming=None, add_prefix_to_folder_names=False)</code>","text":"<p>Creates study files for the current generation.</p> <p>Parameters:</p> Name Type Description Default <code>generation</code> <code>str</code> <p>The name of the current generation.</p> required <code>directory_path</code> <code>str</code> <p>The (relative) path to the directory folder for the current generation.</p> required <code>depth_gen</code> <code>int</code> <p>The depth of the generation in the tree.</p> required <code>dic_parameter_lists</code> <code>Optional[dict[str, Any]]</code> <p>The dictionary of parameter lists. Defaults to None.</p> <code>None</code> <code>dic_parameter_lists_for_naming</code> <code>Optional[dict[str, Any]]</code> <p>The dictionary of parameter lists for naming. Defaults to None.</p> <code>None</code> <code>add_prefix_to_folder_names</code> <code>bool</code> <p>Whether to add a prefix to the folder names. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>tuple[list[str], list[str]]: The list of study file strings and the list of study paths.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def create_study_for_current_gen(\n    self,\n    generation: str,\n    generation_path: str,\n    depth_gen: int,\n    dic_parameter_lists: Optional[dict[str, Any]] = None,\n    dic_parameter_lists_for_naming: Optional[dict[str, Any]] = None,\n    add_prefix_to_folder_names: bool = False,\n) -&gt; list[str]:\n    \"\"\"\n    Creates study files for the current generation.\n\n    Args:\n        generation (str): The name of the current generation.\n        directory_path (str): The (relative) path to the directory folder for the current\n            generation.\n        depth_gen (int): The depth of the generation in the tree.\n        dic_parameter_lists (Optional[dict[str, Any]]): The dictionary of parameter lists.\n            Defaults to None.\n        dic_parameter_lists_for_naming (Optional[dict[str, Any]]): The dictionary of parameter\n            lists for naming. Defaults to None.\n        add_prefix_to_folder_names (bool): Whether to add a prefix to the folder names. Defaults\n            to False.\n\n    Returns:\n        tuple[list[str], list[str]]: The list of study file strings and the list of study paths.\n    \"\"\"\n    executable_path = self.config[\"structure\"][generation][\"executable\"]\n    path_local_template = (\n        f\"{os.path.dirname(inspect.getfile(GenerateScan))}/../assets/template_scripts/\"\n    )\n\n    # Check if the executable path corresponds to a file\n    if not os.path.isfile(executable_path):\n        # Check if the executable path corresponds to a file in the template folder\n        executable_path_template = f\"{path_local_template}{executable_path}\"\n        if not os.path.isfile(executable_path_template):\n            raise FileNotFoundError(\n                f\"Executable file {executable_path} not found locally nor in the study-da \"\n                \"template folder.\"\n            )\n        else:\n            executable_path = executable_path_template\n\n    # Ensure that the values in dic_parameter_lists can be dumped with ryaml\n    if dic_parameter_lists is not None:\n        # Recursively convert all numpy types to standard types\n        clean_dic(dic_parameter_lists)\n        logging.info(\"An external dictionary of parameters was provided.\")\n    else:\n        logging.info(\"Creating the dictionnary of parameters from the configuration file.\")\n\n    return self.create_scans(\n        generation,\n        generation_path,\n        executable_path,\n        depth_gen,\n        dic_parameter_lists,\n        dic_parameter_lists_for_naming,\n        add_prefix_to_folder_names,\n    )\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.GenerateScan.eval_conditions","title":"<code>eval_conditions(l_condition, dic_parameter_lists)</code>  <code>staticmethod</code>","text":"<p>Evaluates the conditions to filter out some parameter values.</p> <p>Parameters:</p> Name Type Description Default <code>l_condition</code> <code>list[str]</code> <p>The list of conditions.</p> required <code>dic_parameter_lists</code> <code>dict[str</code> <p>Any]): The dictionary of parameter lists.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The array of conditions.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>@staticmethod\ndef eval_conditions(l_condition: list[str], dic_parameter_lists: dict[str, Any]) -&gt; np.ndarray:\n    \"\"\"\n    Evaluates the conditions to filter out some parameter values.\n\n    Args:\n        l_condition (list[str]): The list of conditions.\n        dic_parameter_lists (dict[str: Any]): The dictionary of parameter lists.\n\n    Returns:\n        np.ndarray: The array of conditions.\n    \"\"\"\n    # Initialize the array of parameters as a meshgrid of all parameters\n    l_parameters = list(dic_parameter_lists.values())\n    meshgrid = np.meshgrid(*l_parameters, indexing=\"ij\")\n\n    # Associate the parameters to their names\n    dic_param_mesh = dict(zip(dic_parameter_lists.keys(), meshgrid))\n\n    # Evaluate the conditions and take the intersection of all conditions\n    array_conditions = np.ones_like(meshgrid[0], dtype=bool)\n    for condition in l_condition:\n        array_conditions = array_conditions &amp; eval(condition, dic_param_mesh)\n\n    return array_conditions\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.GenerateScan.filter_for_concomitant_parameters","title":"<code>filter_for_concomitant_parameters(array_conditions, ll_concomitant_parameters, dic_dimension_indices)</code>  <code>staticmethod</code>","text":"<p>Filters the conditions for concomitant parameters.</p> <p>Parameters:</p> Name Type Description Default <code>array_conditions</code> <code>ndarray</code> <p>The array of conditions.</p> required <code>ll_concomitant_parameters</code> <code>list[list[str]]</code> <p>The list of concomitant parameters.</p> required <code>dic_dimension_indices</code> <code>dict[str, int]</code> <p>The dictionary of dimension indices.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The filtered array of conditions.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>@staticmethod\ndef filter_for_concomitant_parameters(\n    array_conditions: np.ndarray,\n    ll_concomitant_parameters: list[list[str]],\n    dic_dimension_indices: dict[str, int],\n) -&gt; np.ndarray:\n    \"\"\"\n    Filters the conditions for concomitant parameters.\n\n    Args:\n        array_conditions (np.ndarray): The array of conditions.\n        ll_concomitant_parameters (list[list[str]]): The list of concomitant parameters.\n        dic_dimension_indices (dict[str, int]): The dictionary of dimension indices.\n\n    Returns:\n        np.ndarray: The filtered array of conditions.\n    \"\"\"\n\n    # Return the array of conditions if no concomitant parameters\n    if not ll_concomitant_parameters:\n        return array_conditions\n\n    # Get the indices of the concomitant parameters\n    ll_idx_concomitant_parameters = [\n        [dic_dimension_indices[parameter] for parameter in concomitant_parameters]\n        for concomitant_parameters in ll_concomitant_parameters\n    ]\n\n    # Browse all the values of array_conditions\n    for idx, _ in np.ndenumerate(array_conditions):\n        # Check if the value is on the diagonal of the concomitant parameters\n        for l_idx_concomitant_parameter in ll_idx_concomitant_parameters:\n            if any(\n                idx[i] != idx[j]\n                for i, j in itertools.combinations(l_idx_concomitant_parameter, 2)\n            ):\n                array_conditions[idx] = False\n                break\n\n    return array_conditions\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.GenerateScan.generate_render_write","title":"<code>generate_render_write(gen_name, job_directory_path, template_path, depth_gen, dic_mutated_parameters={})</code>","text":"<p>Generates, renders, and writes the study file.</p> <p>Parameters:</p> Name Type Description Default <code>gen_name</code> <code>str</code> <p>The name of the generation.</p> required <code>study_path</code> <code>str</code> <p>The path to the job folder.</p> required <code>template_path</code> <code>str</code> <p>The path to the template folder.</p> required <code>depth_gen</code> <code>int</code> <p>The depth of the generation in the tree.</p> required <code>dic_mutated_parameters</code> <code>dict[str, Any]</code> <p>The dictionary of mutated parameters. Defaults to {}.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>tuple[str, list[str]]: The study file string and the list of study paths.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def generate_render_write(\n    self,\n    gen_name: str,\n    job_directory_path: str,\n    template_path: str,\n    depth_gen: int,\n    dic_mutated_parameters: dict[str, Any] = {},\n) -&gt; list[str]:  # sourcery skip: default-mutable-arg\n    \"\"\"\n    Generates, renders, and writes the study file.\n\n    Args:\n        gen_name (str): The name of the generation.\n        study_path (str): The path to the job folder.\n        template_path (str): The path to the template folder.\n        depth_gen (int): The depth of the generation in the tree.\n        dic_mutated_parameters (dict[str, Any], optional): The dictionary of mutated parameters.\n            Defaults to {}.\n\n    Returns:\n        tuple[str, list[str]]: The study file string and the list of study paths.\n    \"\"\"\n\n    directory_path_gen = f\"{job_directory_path}\"\n    if not directory_path_gen.endswith(\"/\"):\n        directory_path_gen += \"/\"\n    file_path_gen = f\"{directory_path_gen}{gen_name}.py\"\n    logging.info(f'Now rendering generation \"{file_path_gen}\"')\n\n    # Generate the string of parameters\n    str_parameters = \"{\"\n    for key, value in dic_mutated_parameters.items():\n        if isinstance(value, str):\n            str_parameters += f\"'{key}' : '{value}', \"\n        else:\n            str_parameters += f\"'{key}' : {value}, \"\n    str_parameters += \"}\"\n\n    # Adapt the dict of dependencies to the current generation\n    dic_dependencies = self.config[\"dependencies\"] if \"dependencies\" in self.config else {}\n\n    # Unpacking list of dependencies\n    dic_dependencies = {\n        **{\n            key: value for key, value in dic_dependencies.items() if not isinstance(value, list)\n        },\n        **{\n            f\"{key}_{str(i).zfill(len(str(len(value))))}\": i_value\n            for key, value in dic_dependencies.items()\n            if isinstance(value, list)\n            for i, i_value in enumerate(value)\n        },\n    }\n    self.config[\"dependencies\"] = dic_dependencies\n\n    # Initial dependencies are always copied at the root of the study (hence value.split(\"/\")[-1])\n    dic_dependencies = {\n        key: \"../\" * depth_gen + value.split(\"/\")[-1] for key, value in dic_dependencies.items()\n    }\n\n    # Always load configuration from above generation, and remove the path from dependencies\n    path_main_configuration = \"../\" + dic_dependencies.pop(\"main_configuration\").split(\"/\")[-1]\n\n    # Create the str for the dependencies\n    str_dependencies = \"{\"\n    for key, value in dic_dependencies.items():\n        str_dependencies += f\"'{key}' : '{value}', \"\n    str_dependencies += \"}\"\n\n    # Render and write the study file\n    study_str = self.render(\n        str_parameters,\n        template_path=template_path,\n        path_main_configuration=path_main_configuration,\n        study_path=os.path.abspath(self.config[\"name\"]),\n        str_dependencies=str_dependencies,\n    )\n\n    self.write(study_str, file_path_gen)\n    return [directory_path_gen]\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.GenerateScan.get_dic_parametric_scans","title":"<code>get_dic_parametric_scans(generation)</code>","text":"<p>Retrieves dictionaries of parametric scan values.</p> <p>Parameters:</p> Name Type Description Default <code>generation</code> <code>str</code> <p>The generation name.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Any], dict[str, Any], ndarray | None]</code> <p>tuple[dict[str, Any], dict[str, Any], np.ndarray|None]: The dictionaries of parametric scan values, another dictionnary with better naming for the tree creation, and an array of conditions to filter out some parameter values.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def get_dic_parametric_scans(\n    self, generation: str\n) -&gt; tuple[dict[str, Any], dict[str, Any], np.ndarray | None]:\n    \"\"\"\n    Retrieves dictionaries of parametric scan values.\n\n    Args:\n        generation: The generation name.\n\n    Returns:\n        tuple[dict[str, Any], dict[str, Any], np.ndarray|None]: The dictionaries of parametric\n            scan values, another dictionnary with better naming for the tree creation, and an\n            array of conditions to filter out some parameter values.\n    \"\"\"\n\n    if generation == \"base\":\n        raise ValueError(\"Generation 'base' should not have scans.\")\n\n    # Remember common parameters as they might be used across generations\n    if \"common_parameters\" in self.config[\"structure\"][generation]:\n        self.dic_common_parameters[generation] = {}\n        for parameter in self.config[\"structure\"][generation][\"common_parameters\"]:\n            self.dic_common_parameters[generation][parameter] = self.config[\"structure\"][\n                generation\n            ][\"common_parameters\"][parameter]\n\n    # Check that the generation has scans\n    if (\n        \"scans\" not in self.config[\"structure\"][generation]\n        or self.config[\"structure\"][generation][\"scans\"] is None\n    ):\n        dic_parameter_lists = {\"\": [generation]}\n        dic_parameter_lists_for_naming = {\"\": [generation]}\n        array_conditions = None\n        ll_concomitant_parameters = []\n    else:\n        # Browse and collect the parameter space for the generation\n        (\n            dic_parameter_lists,\n            dic_parameter_lists_for_naming,\n            dic_subvariables,\n            ll_concomitant_parameters,\n            l_conditions,\n        ) = self.browse_and_collect_parameter_space(generation)\n\n        # Get the dimension corresponding to each parameter\n        dic_dimension_indices = {\n            parameter: idx for idx, parameter in enumerate(dic_parameter_lists)\n        }\n\n        # Generate array of conditions to filter out some of the values later\n        # Is an array of True values if no conditions are present\n        array_conditions = self.eval_conditions(l_conditions, dic_parameter_lists)\n\n        # Filter for concomitant parameters\n        array_conditions = self.filter_for_concomitant_parameters(\n            array_conditions, ll_concomitant_parameters, dic_dimension_indices\n        )\n\n        # Postprocess the parameter lists and update the dictionaries\n        dic_parameter_lists, dic_parameter_lists_for_naming = self.postprocess_parameter_lists(\n            dic_parameter_lists, dic_parameter_lists_for_naming, dic_subvariables\n        )\n\n    return (\n        dic_parameter_lists,\n        dic_parameter_lists_for_naming,\n        array_conditions,\n    )\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.GenerateScan.parse_parameter_space","title":"<code>parse_parameter_space(parameter, dic_curr_parameter, dic_parameter_lists, dic_parameter_lists_for_naming)</code>","text":"<p>Parses the parameter space for a given parameter.</p> <p>Parameters:</p> Name Type Description Default <code>parameter</code> <code>str</code> <p>The parameter name.</p> required <code>dic_curr_parameter</code> <code>dict[str, Any]</code> <p>The dictionary of current parameter values.</p> required <code>dic_parameter_lists</code> <code>dict[str, Any]</code> <p>The dictionary of parameter lists.</p> required <code>dic_parameter_lists_for_naming</code> <code>dict[str, Any]</code> <p>The dictionary of parameter lists for naming.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Any], dict[str, Any]]</code> <p>tuple[dict[str, Any], dict[str, Any]]: The updated dictionaries of parameter lists.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def parse_parameter_space(\n    self,\n    parameter: str,\n    dic_curr_parameter: dict[str, Any],\n    dic_parameter_lists: dict[str, Any],\n    dic_parameter_lists_for_naming: dict[str, Any],\n) -&gt; tuple[dict[str, Any], dict[str, Any]]:\n    \"\"\"\n    Parses the parameter space for a given parameter.\n\n    Args:\n        parameter (str): The parameter name.\n        dic_curr_parameter (dict[str, Any]): The dictionary of current parameter values.\n        dic_parameter_lists (dict[str, Any]): The dictionary of parameter lists.\n        dic_parameter_lists_for_naming (dict[str, Any]): The dictionary of parameter lists for naming.\n\n    Returns:\n        tuple[dict[str, Any], dict[str, Any]]: The updated dictionaries of parameter lists.\n    \"\"\"\n\n    if \"linspace\" in dic_curr_parameter:\n        parameter_list = linspace(dic_curr_parameter[\"linspace\"])\n        dic_parameter_lists_for_naming[parameter] = parameter_list\n    elif \"logspace\" in dic_curr_parameter:\n        parameter_list = logspace(dic_curr_parameter[\"logspace\"])\n        dic_parameter_lists_for_naming[parameter] = parameter_list\n    elif \"path_list\" in dic_curr_parameter:\n        l_values_path_list = dic_curr_parameter[\"path_list\"]\n        parameter_list = list_values_path(l_values_path_list, self.dic_common_parameters)\n        dic_parameter_lists_for_naming[parameter] = [\n            f\"{n:02d}\" for n, path in enumerate(parameter_list)\n        ]\n    elif \"list\" in dic_curr_parameter:\n        parameter_list = dic_curr_parameter[\"list\"]\n        dic_parameter_lists_for_naming[parameter] = parameter_list\n    elif \"expression\" in dic_curr_parameter:\n        parameter_list = np.round(\n            eval(dic_curr_parameter[\"expression\"], copy.deepcopy(dic_parameter_lists)),\n            8,\n        )\n        dic_parameter_lists_for_naming[parameter] = parameter_list\n    else:\n        raise ValueError(f\"Scanning method for parameter {parameter} is not recognized.\")\n\n    dic_parameter_lists[parameter] = np.array(parameter_list)\n    return dic_parameter_lists, dic_parameter_lists_for_naming\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.GenerateScan.postprocess_parameter_lists","title":"<code>postprocess_parameter_lists(dic_parameter_lists, dic_parameter_lists_for_naming, dic_subvariables)</code>","text":"<p>Post-processes parameter lists by ensuring values are not numpy types and handling nested parameters.</p> <p>Parameters:</p> Name Type Description Default <code>dic_parameter_lists</code> <code>dict[str, Any]</code> <p>Dictionary containing parameter lists.</p> required <code>dic_parameter_lists_for_naming</code> <code>dict[str, Any]</code> <p>Dictionary containing parameter lists for naming.</p> required <code>dic_subvariables</code> <code>dict[str, Any]</code> <p>Dictionary containing subvariables for nested parameters.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Any], dict[str, Any]]</code> <p>tuple[dict[str, Any], dict[str, Any]]: Updated dictionaries of parameter lists and parameter lists for naming.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def postprocess_parameter_lists(\n    self,\n    dic_parameter_lists: dict[str, Any],\n    dic_parameter_lists_for_naming: dict[str, Any],\n    dic_subvariables: dict[str, Any],\n) -&gt; tuple[dict[str, Any], dict[str, Any]]:\n    \"\"\"\n    Post-processes parameter lists by ensuring values are not numpy types and handling nested\n    parameters.\n\n    Args:\n        dic_parameter_lists (dict[str, Any]): Dictionary containing parameter lists.\n        dic_parameter_lists_for_naming (dict[str, Any]): Dictionary containing parameter lists\n            for naming.\n        dic_subvariables (dict[str, Any]): Dictionary containing subvariables for nested\n            parameters.\n\n    Returns:\n        tuple[dict[str, Any], dict[str, Any]]: Updated dictionaries of parameter lists and\n            parameter lists for naming.\n    \"\"\"\n    for parameter, parameter_list in dic_parameter_lists.items():\n        parameter_list_for_naming = dic_parameter_lists_for_naming[parameter]\n\n        # Ensure that all values are not numpy types (to avoid serialization issues)\n        parameter_list = [x.item() if isinstance(x, np.generic) else x for x in parameter_list]\n\n        # Handle nested parameters\n        parameter_list_updated = (\n            convert_for_subvariables(dic_subvariables[parameter], parameter_list)\n            if parameter in dic_subvariables\n            else parameter_list\n        )\n        # Update the dictionaries\n        dic_parameter_lists[parameter] = parameter_list_updated\n        dic_parameter_lists_for_naming[parameter] = parameter_list_for_naming\n\n    return dic_parameter_lists, dic_parameter_lists_for_naming\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.GenerateScan.render","title":"<code>render(str_parameters, template_path, path_main_configuration, study_path=None, str_dependencies=None)</code>","text":"<p>Renders the study file using a template.</p> <p>Parameters:</p> Name Type Description Default <code>str_parameters</code> <code>str</code> <p>The string representation of parameters to declare/mutate.</p> required <code>template_path</code> <code>str</code> <p>The path to the template file.</p> required <code>path_main_configuration</code> <code>str</code> <p>The path to the main configuration file.</p> required <code>study_path</code> <code>str</code> <p>The path to the root of the study. Defaults to None.</p> <code>None</code> <code>dependencies</code> <code>dict[str, str]</code> <p>The dictionary of dependencies. Defaults to {}.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The rendered study file.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def render(\n    self,\n    str_parameters: str,\n    template_path: str,\n    path_main_configuration: str,\n    study_path: Optional[str] = None,\n    str_dependencies: Optional[dict[str, str]] = None,\n) -&gt; str:\n    \"\"\"\n    Renders the study file using a template.\n\n    Args:\n        str_parameters (str): The string representation of parameters to declare/mutate.\n        template_path (str): The path to the template file.\n        path_main_configuration (str): The path to the main configuration file.\n        study_path (str, optional): The path to the root of the study. Defaults to None.\n        dependencies (dict[str, str], optional): The dictionary of dependencies. Defaults to {}.\n\n    Returns:\n        str: The rendered study file.\n    \"\"\"\n\n    # Handle mutable default argument\n    if str_dependencies is None:\n        dependencies = \"\"\n    if study_path is None:\n        study_path = \"\"\n\n    # Generate generations from template\n    directory_path = os.path.dirname(template_path)\n    template_name = os.path.basename(template_path)\n    environment = Environment(\n        loader=FileSystemLoader(directory_path),\n        variable_start_string=\"{}  ###---\",\n        variable_end_string=\"---###\",\n    )\n    template = environment.get_template(template_name)\n\n    # Better not to render the dependencies path this way, as it becomes too cumbersome to\n    # handle the paths when using clusters\n\n    return template.render(\n        parameters=str_parameters,\n        main_configuration=path_main_configuration,\n        path_root_study=study_path,\n        # dependencies = str_dependencies,\n    )\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.GenerateScan.write","title":"<code>write(study_str, file_path, format_with_black=True)</code>","text":"<p>Writes the study file to disk.</p> <p>Parameters:</p> Name Type Description Default <code>study_str</code> <code>str</code> <p>The study file string.</p> required <code>file_path</code> <code>str</code> <p>The path to write the study file.</p> required <code>format_with_black</code> <code>bool</code> <p>Whether to format the output file with black. Defaults to True.</p> <code>True</code> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def write(self, study_str: str, file_path: str, format_with_black: bool = True):\n    \"\"\"\n    Writes the study file to disk.\n\n    Args:\n        study_str (str): The study file string.\n        file_path (str): The path to write the study file.\n        format_with_black (bool, optional): Whether to format the output file with black.\n            Defaults to True.\n    \"\"\"\n\n    # Format the string with black\n    if format_with_black:\n        study_str = format_str(study_str, mode=FileMode())\n\n    # Make folder if it doesn't exist\n    folder = os.path.dirname(file_path)\n    if folder != \"\":\n        os.makedirs(folder, exist_ok=True)\n\n    with open(file_path, mode=\"w\", encoding=\"utf-8\") as file:\n        file.write(study_str)\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.GenerateScan.write_tree","title":"<code>write_tree(dictionary_tree)</code>","text":"<p>Writes the study tree structure to a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary_tree</code> <code>dict</code> <p>The dictionary representing the study tree structure.</p> required Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def write_tree(self, dictionary_tree: dict):\n    \"\"\"\n    Writes the study tree structure to a YAML file.\n\n    Args:\n        dictionary_tree (dict): The dictionary representing the study tree structure.\n    \"\"\"\n    logging.info(\"Writing the tree structure to a YAML file.\")\n    ryaml = yaml.YAML()\n    with open(self.path_tree, \"w\") as yaml_file:\n        ryaml.indent(sequence=4, offset=2)\n        ryaml.dump(dictionary_tree, yaml_file)\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.SubmitScan","title":"<code>SubmitScan</code>","text":"Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>class SubmitScan:\n    def __init__(\n        self,\n        path_tree: str,\n        path_python_environment: str = \"\",\n        path_python_environment_container: str = \"\",\n        path_container_image: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the SubmitScan class.\n\n        Args:\n            path_tree (str): The path to the tree structure.\n            path_python_environment (str): The path to the Python environment. Defaults to \"\".\n            path_python_environment_container (str, optional): The path to the Python environment\n                in the container. Defaults to \"\".\n            path_container_image (Optional[str], optional): The path to the container image.\n                Defaults to None.\n        \"\"\"\n        # Path to study files\n        self.path_tree = path_tree\n\n        # Absolute path to the tree\n        self.abs_path_tree = os.path.abspath(path_tree)\n\n        # Name of the study folder\n        self.study_name = os.path.dirname(path_tree)\n\n        # Absolute path to the study folder (get from the path_tree)\n        self.abs_path = os.path.abspath(self.study_name).split(f\"/{self.study_name}\")[0]\n\n        # Check that the current working directory is one step above the study folder\n        if os.getcwd() != self.abs_path:\n            raise ValueError(\n                \"The current working directory must be the parent folder of the study folder, \"\n                \"i.e. the folder from which the study was generated. \"\n                \"Please submit from there.\"\n            )\n\n        # Container image (Docker or Singularity, if any)\n        # Turn to absolute path if it is not already\n        if path_container_image is None:\n            self.path_container_image = None\n        elif not os.path.isabs(path_container_image):\n            self.path_container_image = os.path.abspath(path_container_image)\n        else:\n            self.path_container_image = path_container_image\n\n        # Python environment for the container\n        self.path_python_environment_container = path_python_environment_container\n\n        # Ensure that the container image is set if the python environment is set\n        if self.path_container_image and not self.path_python_environment_container:\n            raise ValueError(\n                \"The path to the python environment in the container must be set if the container\"\n                \"image is set.\"\n            )\n\n        # Add /bin/activate to the path_python_environment if needed\n        if not self.path_python_environment_container.endswith(\"/bin/activate\"):\n            # Remove potential / at the end of the path\n            if (\n                self.path_python_environment_container\n                and self.path_python_environment_container[-1] == \"/\"\n            ):\n                self.path_python_environment_container = self.path_python_environment_container[:-1]\n            self.path_python_environment_container += \"/bin/activate\"\n\n        # Ensure the path to the python environment is not \"\" if the container image is not set\n        if not self.path_container_image and not path_python_environment:\n            raise ValueError(\n                \"The path to the python environment must be set if the container image is not set.\"\n            )\n\n        # Path to the python environment, activate with `source path_python_environment`\n        if not path_python_environment:\n            logging.warning(\"No local python environment provided.\")\n            self.path_python_environment = \"\"\n\n        else:\n            # Ensure that the path is not of the form path/bin/activate environment_name\n            split_path = path_python_environment.split(\" \")\n            real_path = split_path[0]\n            env_name = split_path[1] if len(split_path) &gt; 1 else \"\"\n\n            # Turn to absolute path if it is not already\n            self.path_python_environment = (\n                real_path if os.path.isabs(real_path) else os.path.abspath(real_path)\n            )\n\n            # Add /bin/activate to the path_python_environment if needed\n            if \"bin/activate\" not in self.path_python_environment:\n                # Ensure there's no / at the end of the path\n                if self.path_python_environment and self.path_python_environment[-1] == \"/\":\n                    self.path_python_environment = self.path_python_environment[:-1]\n                self.path_python_environment += \"/bin/activate\"\n\n            # Add environment name to the path_python_environment if needed\n            if env_name:\n                self.path_python_environment += f\" {env_name}\"\n        # Lock file to avoid concurrent access (softlock as several platforms are used)\n        self.lock = SoftFileLock(f\"{self.path_tree}.lock\", timeout=60)\n\n    # dic_tree as a property so that it is reloaded every time it is accessed\n    @property\n    def dic_tree(self) -&gt; dict:\n        \"\"\"\n        Loads the dictionary tree from the path.\n\n        Returns:\n            dict: The loaded dictionary tree.\n        \"\"\"\n        logging.info(f\"Loading tree from {self.path_tree}\")\n        return load_dic_from_path(self.path_tree)[0]\n\n    # Setter for the dic_tree property\n    @dic_tree.setter\n    def dic_tree(self, value: dict) -&gt; None:\n        \"\"\"\n        Writes the dictionary tree to the path.\n\n        Args:\n            value (dict): The dictionary tree to write.\n        \"\"\"\n        logging.info(f\"Writing tree to {self.path_tree}\")\n        write_dic_to_path(value, self.path_tree)\n\n    def configure_jobs(\n        self,\n        force_configure: bool = False,\n        dic_config_jobs: Optional[dict[str, dict[str, Any]]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Configures the jobs by modifying the tree structure and creating the run files for each job.\n\n        Args:\n            force_configure (bool, optional): Whether to force reconfiguration. Defaults to False.\n            dic_config_jobs (Optional[dict[str, dict[str, Any]]], optional): A dictionary containing\n                the configuration of the jobs. Defaults to None.\n        \"\"\"\n        # Lock since we are modifying the tree\n        logging.info(\"Acquiring lock to configure jobs\")\n        with self.lock:\n            # Get the tree\n            dic_tree = self.dic_tree\n\n            # Ensure jobs have not been configured already\n            if (\"configured\" in dic_tree and dic_tree[\"configured\"]) and not force_configure:\n                logging.warning(\"Jobs have already been configured. Skipping.\")\n                return\n\n            # Configure the jobs (add generation and job keys, set status to \"To finish\")\n            dic_tree = ConfigJobs(dic_tree,starting_depth=-len(Path(self.path_tree).parts) + 2).find_and_configure_jobs(dic_config_jobs)\n\n            # Add the python environment, container image and absolute path of the study to the tree\n            dic_tree[\"python_environment\"] = self.path_python_environment\n            dic_tree[\"container_image\"] = self.path_container_image\n            dic_tree[\"absolute_path\"] = self.abs_path\n            dic_tree[\"status\"] = \"to_finish\"\n            dic_tree[\"configured\"] = True\n\n            # Explicitly set the dic_tree property to force rewrite\n            self.dic_tree = dic_tree\n\n        logging.info(\"Jobs have been configured. Lock released.\")\n\n    def get_all_jobs(self) -&gt; dict:\n        \"\"\"\n        Retrieves all jobs from the configuration, without modifying the tree.\n\n        Returns:\n            dict: A dictionary containing all jobs.\n        \"\"\"\n        # Get a copy of the tree as it's safer\n        with self.lock:\n            dic_tree = self.dic_tree\n        return ConfigJobs(dic_tree,starting_depth=-len(Path(self.path_tree).parts) + 2).find_all_jobs()\n\n    def generate_run_files(\n        self,\n        dic_tree: dict[str, Any],\n        l_jobs: list[str],\n        dic_additional_commands_per_gen: dict[int, str],\n        dic_dependencies_per_gen: dict[int, list[str]],\n        dic_copy_back_per_gen: dict[int, dict[str, bool]],\n        name_config: str,\n    ) -&gt; dict:\n        \"\"\"\n        Generates run files for the specified jobs.\n\n        Args:\n            dic_tree (dict): The dictionary tree structure.\n            l_jobs (list[str]): List of jobs to submit.\n            dic_additional_commands_per_gen (dict[int, str], optional): Additional commands per\n                generation. Defaults to {}.\n            dic_dependencies_per_gen (dict[int, list[str]], optional): Dependencies per generation.\n                Only used when doing a HTC submission.\n            dic_copy_back_per_gen (Optional[dict[int, dict[str, bool]]], optional): A dictionary\n                containing the files to copy back per generation. Accepted keys are \"parquet\",\n                \"yaml\", \"txt\", \"json\", \"zip\" and \"all\".\n            name_config (str, optional): The name of the configuration file for the study.\n\n        Returns:\n            dict: The updated dictionary tree structure.\n        \"\"\"\n\n        logging.info(\"Generating run files for the jobs to submit\")\n        # Generate the run files for the jobs to submit\n        dic_all_jobs = self.get_all_jobs()\n        for job in l_jobs:\n            l_keys = dic_all_jobs[job][\"l_keys\"]\n            job_name = os.path.basename(job)\n            relative_job_folder = os.path.dirname(job)\n            absolute_job_folder = f\"{self.abs_path}/{relative_job_folder}\"\n            generation_number = dic_all_jobs[job][\"gen\"]\n            submission_type = nested_get(dic_tree, l_keys + [\"submission_type\"])\n            singularity = \"docker\" in submission_type\n            path_python_environment = (\n                self.path_python_environment_container\n                if singularity\n                else self.path_python_environment\n            )\n\n            # Ensure that the run file does not already exist\n            if \"path_run\" in nested_get(dic_tree, l_keys):\n                path_run_curr = nested_get(dic_tree, l_keys + [\"path_run\"])\n                if path_run_curr is not None and os.path.exists(path_run_curr):\n                    logging.info(f\"Run file already exists for job {job}. Skipping.\")\n                    continue\n\n            # Build l_dependencies and add to the kwargs\n            l_dependencies = dic_dependencies_per_gen.get(generation_number, [])\n\n            # Get arguments of current generation\n            dic_args = dic_copy_back_per_gen.get(generation_number, {})\n\n            # Mutate the keys\n            dic_args = {f\"copy_back_{key}\": value for key, value in dic_args.items()}\n\n            # Build kwargs for the run file\n            kwargs_htc = {\n                \"l_dependencies\": l_dependencies,\n                \"name_config\": name_config,\n            } | dic_args\n\n            run_str = generate_run_file(\n                absolute_job_folder,\n                job_name,\n                path_python_environment,\n                htc=\"htc\" in submission_type,\n                additionnal_command=dic_additional_commands_per_gen.get(generation_number, \"\"),\n                **kwargs_htc,\n            )\n            # Write the run file\n            path_run_job = f\"{absolute_job_folder}/run.sh\"\n            with open(path_run_job, \"w\") as f:\n                f.write(run_str)\n\n            # Change permissions to make the file executable\n            os.chmod(path_run_job, 0o755)\n\n            # Record the path to the run file in the tree\n            nested_set(dic_tree, l_keys + [\"path_run\"], path_run_job)\n\n        return dic_tree\n\n    def check_and_update_all_jobs_status(self) -&gt; tuple[dict[str, Any], str]:\n        \"\"\"\n        Checks the status of all jobs and updates their status in the job dictionary.\n\n        This method iterates through all jobs, checks if a \".finished\" or a \".failed\" file exists in\n        the job's folder, and updates the job's status accordingly. If at least one job is not\n        finished or failed, the overall status is set to \"to_finish\". If all jobs are finished or\n        failed, the overall status is set to \"finished\".\n\n        Returns:\n            tuple[dict[str, Any], str]: A tuple containing:\n            - A dictionary with all jobs and their updated statuses.\n            - A string representing the final status (\"to_finish\" or \"finished\").\n        \"\"\"\n        dic_all_jobs = self.get_all_jobs()\n        at_least_one_job_to_finish = False\n        final_status = \"to_finish\"\n        with self.lock:\n            # Get dic tree once to avoid reloading it for every job\n            dic_tree = self.dic_tree\n\n            # First pass to update the state of the tree\n            for job in dic_all_jobs:\n                # Skip jobs that are already finished, failed or unsubmittable\n                if nested_get(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"]) in [\n                    \"finished\",\n                    \"failed\",\n                    \"unsubmittable\",\n                ]:\n                    continue\n\n                # Check the state of the others\n                relative_job_folder = os.path.dirname(job)\n                absolute_job_folder = f\"{self.abs_path}/{relative_job_folder}\"\n                if os.path.exists(f\"{absolute_job_folder}/.finished\"):\n                    nested_set(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"], \"finished\")\n                # Check if the job failed otherwise (not to resubmit it again)\n                elif os.path.exists(f\"{absolute_job_folder}/.failed\"):\n                    nested_set(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"], \"failed\")\n                # else:\n                #     at_least_one_job_to_finish = True\n\n            # Second pass to update the state of the tree with unreachable jobs\n            dependency_graph = DependencyGraph(dic_tree, dic_all_jobs)\n            for job in dic_all_jobs:\n                # Get all failed dependencies across the tree\n                l_dep_failed = dependency_graph.get_failed_dependency(job)\n                if len(l_dep_failed) &gt; 0:\n                    nested_set(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"], \"unsubmittable\")\n                elif nested_get(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"]) == \"to_submit\":\n                    at_least_one_job_to_finish = True\n\n            if not at_least_one_job_to_finish:\n                # No more jobs to submit so finished\n                dic_tree[\"status\"] = final_status = \"finished\"\n                # Last pass to check if all jobs are properly finished\n                for job in dic_all_jobs:\n                    if nested_get(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"]) != \"finished\":\n                        dic_tree[\"status\"] = final_status = \"finished with issues\"\n                        break\n\n            # Update dic_tree from cluster_submission\n            self.dic_tree = dic_tree\n\n        return dic_all_jobs, final_status\n\n    def reset_failed_jobs(self, dic_tree: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"\n        Resets the status of jobs that have failed to \"to_submit\".\n\n        Args:\n            dic_tree (dict[str, Any]): The dictionary tree structure.\n\n        Returns:\n            dict[str, Any]: The updated dictionary tree structure.\n        \"\"\"\n\n        dic_all_jobs = self.get_all_jobs()\n        # First pass to update the state of the tree\n        for job in dic_all_jobs:\n            # Skip jobs that are not failed\n            if nested_get(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"]) != \"failed\":\n                continue\n\n            # Reset the state of the others\n            relative_job_folder = os.path.dirname(job)\n            absolute_job_folder = f\"{self.abs_path}/{relative_job_folder}\"\n\n            # Remove failed tag\n            if os.path.exists(f\"{absolute_job_folder}/.failed\"):\n                os.remove(f\"{absolute_job_folder}/.failed\")\n            else:\n                logging.warning(f\"Failed file not found for job {job}.\")\n\n            # Remove run file\n            if \"path_run\" in nested_get(dic_tree, dic_all_jobs[job][\"l_keys\"]):\n                path_run_curr = nested_get(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"path_run\"])\n                if path_run_curr is not None and os.path.exists(path_run_curr):\n                    os.remove(path_run_curr)\n                else:\n                    logging.warning(f\"Run file not found for job {job}.\")\n\n            # Reset the status of the job\n            nested_set(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"], \"to_submit\")\n\n        return dic_tree\n\n    def submit(\n        self,\n        one_generation_at_a_time: bool = False,\n        dic_additional_commands_per_gen: Optional[dict[int, str]] = None,\n        dic_dependencies_per_gen: Optional[dict[int, list[str]]] = None,\n        dic_copy_back_per_gen: Optional[dict[int, dict[str, bool]]] = None,\n        name_config: str = \"config.yaml\",\n        force_submit: bool = False,\n    ) -&gt; str:\n        \"\"\"\n        Submits the jobs to the cluster. Note that copying back large files (e.g. json colliders)\n        can trigger a throttling mechanism in AFS.\n\n        The following arguments are only used for HTC jobs submission:\n        - dic_additional_commands_per_gen\n        - dic_dependencies_per_gen\n        - dic_copy_back_per_gen\n        - name_config\n\n        Args:\n            one_generation_at_a_time (bool, optional): Whether to submit one full generation at a\n                time. Defaults to False.\n            dic_additional_commands_per_gen (dict[int, str], optional): Additional commands per\n                generation. Defaults to None.\n            dic_dependencies_per_gen (dict[int, list[str]], optional): Dependencies per generation.\n                Only used when doing a HTC submission. Defaults to None.\n            dic_copy_back_per_gen (Optional[dict[int, dict[str, bool]]], optional): A dictionary\n                containing the files to copy back per generation. Accepted keys are \"parquet\",\n                \"yaml\", \"txt\", \"json\", \"zip\" and \"all\". Defaults to None, corresponding to copying\n                back only \"light\" files, i.e. parquet, yaml and txt.\n            name_config (str, optional): The name of the configuration file for the study.\n                Defaults to \"config.yaml\".\n            force_submit (bool, optional): If True, jobs are resubmitted even though they failed.\n                Defaults to False.\n\n        Returns:\n            str: The final status of the jobs.\n        \"\"\"\n        # Handle mutable default arguments\n        if dic_additional_commands_per_gen is None:\n            dic_additional_commands_per_gen = {}\n        if dic_dependencies_per_gen is None:\n            dic_dependencies_per_gen = {}\n        if dic_copy_back_per_gen is None:\n            dic_copy_back_per_gen = {}\n\n        # Handle force submit\n        if force_submit:\n            logging.warning(\"Forcing resubmission of all failed jobs.\")\n            with self.lock:\n                # Acquire tree from disk\n                dic_tree = self.dic_tree\n\n                # Reset the tree by deleting the failed tags\n                dic_tree = self.reset_failed_jobs(dic_tree)\n                dic_tree[\"status\"] = \"to_finish\"\n                # Write the tree back to disk\n                self.dic_tree = dic_tree\n\n        # Update the status of all jobs before submitting\n        dic_all_jobs, final_status = self.check_and_update_all_jobs_status()\n        if final_status == \"finished\":\n            print(\"All jobs are finished.\")\n            return final_status\n        elif final_status == \"finished with issues\":\n            print(\"All jobs are finished but some did not run properly.\")\n            return final_status\n\n        logging.info(\"Acquiring lock to submit jobs\")\n        with self.lock:\n            # Get dic tree once to avoid reloading it for every job\n            dic_tree = self.dic_tree\n\n            # Submit the jobs\n            self._submit(\n                dic_tree,\n                dic_all_jobs,\n                one_generation_at_a_time,\n                dic_additional_commands_per_gen,\n                dic_dependencies_per_gen,\n                dic_copy_back_per_gen,\n                name_config,\n            )\n\n            # Update dic_tree from cluster_submission\n            self.dic_tree = dic_tree\n        logging.info(\"Jobs have been submitted. Lock released.\")\n        return final_status\n\n    def _submit(\n        self,\n        dic_tree: dict[str, Any],\n        dic_all_jobs: dict[str, dict[str, Any]],\n        one_generation_at_a_time: bool,\n        dic_additional_commands_per_gen: dict[int, str],\n        dic_dependencies_per_gen: dict[int, list[str]],\n        dic_copy_back_per_gen: dict[int, dict[str, bool]],\n        name_config: str,\n    ) -&gt; None:\n        \"\"\"\n        Submits the jobs to the cluster.\n\n        Args:\n            dic_tree (dict[str, Any]): The dictionary tree structure.\n            dic_all_jobs (dict[str, dict[str,Any]]): A dictionary containing all jobs.\n            one_generation_at_a_time (bool): Whether to submit one full generation at a time.\n            dic_additional_commands_per_gen (dict[int, str], optional): Additional commands per\n                generation.\n\n            The following arguments are only used for HTC jobs submission:\n\n            dic_dependencies_per_gen (dict[int, list[str]], optional): Dependencies per generation.\n                Only used when doing a HTC submission. Defaults to None.\n            dic_copy_back_per_gen (Optional[dict[int, dict[str, bool]]], optional): A dictionary\n                containing the files to copy back per generation.\n            name_config (str, optional): The name of the configuration file for the study.\n        \"\"\"\n        # Collect dict of list of unfinished jobs for every tree branch and every gen\n        dic_to_submit_by_gen = {}\n        dic_summary_by_gen = {}\n        dependency_graph = DependencyGraph(dic_tree, dic_all_jobs)\n        for job in dic_all_jobs:\n            dic_to_submit_by_gen, dic_summary_by_gen = self._check_job_submit_status(\n                job,\n                dic_tree,\n                dic_all_jobs,\n                dic_to_submit_by_gen,\n                dic_summary_by_gen,\n                dependency_graph,\n            )\n\n        # Only keep the topmost generation if one_generation_at_a_time is True\n        if one_generation_at_a_time:\n            logging.info(\n                \"Cropping list of jobs to submit to ensure only one generation is submitted at \"\n                \"a time.\"\n            )\n            min_gen = min(k for k, l_jobs in dic_to_submit_by_gen.items() if l_jobs)\n            dic_to_submit_by_gen = {min_gen: dic_to_submit_by_gen[min_gen]}\n\n        # Convert dic_to_submit_by_gen to contain all requested information\n        l_jobs_to_submit = [job for dic_gen in dic_to_submit_by_gen.values() for job in dic_gen]\n\n        # Generate run files for the jobs to submit\n        # ! Run files are generated at submit and not at configuration as the configuration\n        # ! files are created at the end of each generation\n        dic_tree = self.generate_run_files(\n            dic_tree,\n            l_jobs_to_submit,\n            dic_additional_commands_per_gen,\n            dic_dependencies_per_gen=dic_dependencies_per_gen,\n            dic_copy_back_per_gen=dic_copy_back_per_gen,\n            name_config=name_config,\n        )\n\n        # Create the ClusterSubmission object\n        path_submission_file = f\"{self.abs_path}/{self.study_name}/submission/submission_file.sub\"\n        cluster_submission = ClusterSubmission(\n            self.study_name,\n            l_jobs_to_submit,\n            dic_all_jobs,\n            dic_tree,\n            path_submission_file,\n            self.abs_path,\n        )\n\n        # Write and submit the submission files\n        logging.info(\"Writing and submitting submission files\")\n        dic_submission_files = cluster_submission.write_sub_files(dic_summary_by_gen)\n\n        # Log the state of the jobs\n        self.log_jobs_state(dic_summary_by_gen)\n        for submission_type, (\n            list_of_jobs,\n            l_submission_filenames,\n        ) in dic_submission_files.items():\n            cluster_submission.submit(list_of_jobs, l_submission_filenames, submission_type)\n\n    @staticmethod\n    def log_jobs_state(dic_summary_by_gen: dict[int, dict[str, int]]) -&gt; None:\n        \"\"\"\n        Logs the state of jobs for each generation.\n\n        Args:\n            dic_summary_by_gen (dict): A dictionary where the keys are generation numbers\n                and the values are dictionaries summarizing job states.\n                Each summary dictionary should contain the following keys:\n                - 'to_submit_later': int, number of jobs left to submit later\n                - 'running_or_queuing': int, number of jobs running or queuing\n                - 'submitted_now': int, number of jobs submitted now\n                - 'finished': int, number of jobs finished\n                - 'failed': int, number of jobs failed\n                - 'dependency_failed': int, number of jobs on hold due to failed dependencies\n\n        Returns:\n            None\n        \"\"\"\n        print(\"State of the jobs:\")\n        for gen, dic_summary in dic_summary_by_gen.items():\n            print(\"********************************\")\n            print(f\"Generation {gen}\")\n            print(f\"Jobs left to submit later: {dic_summary['to_submit_later']}\")\n            print(f\"Jobs running or queuing: {dic_summary['running_or_queuing']}\")\n            print(f\"Jobs submitted now: {dic_summary['submitted_now']}\")\n            print(f\"Jobs finished: {dic_summary['finished']}\")\n            print(f\"Jobs failed: {dic_summary['failed']}\")\n            print(f\"Jobs on hold due to failed dependencies: {dic_summary['dependency_failed']}\")\n            print(\"********************************\")\n\n    @staticmethod\n    def _check_job_submit_status(\n        job: str,\n        dic_tree: dict[str, Any],\n        dic_all_jobs: dict[str, dict[str, Any]],\n        dic_to_submit_by_gen: dict[int, list[str]],\n        dic_summary_by_gen: dict[int, dict[str, int]],\n        dependency_graph: DependencyGraph,\n    ) -&gt; tuple[dict[int, list[str]], dict[int, dict[str, int]]]:\n        \"\"\"\n        Checks the status and dependencies of a job and updates the submission and summary\n        dictionaries.\n\n        Args:\n            job (str): The job identifier.\n            dic_tree (dict[str, Any]): The dictionary tree structure.\n            dic_all_jobs (dict[str, dict[str,Any]]): A dictionary containing all jobs.\n            dic_to_submit_by_gen (dict[int, list[str]]): A dictionary where keys are generation\n                numbers and values are lists of jobs to submit for each generation.\n            dic_summary_by_gen (dict[int, dict[str, int]]): A dictionary where keys are generation\n                numbers and values are dictionaries summarizing job states.\n            dependency_graph (DependencyGraph): An object to check job dependencies.\n\n        Returns:\n            tuple[dict[int, list[str]], dict[int, dict[str, int]]]: Updated dictionaries for jobs to\n                submit and job summaries.\n        \"\"\"\n        gen = dic_all_jobs[job][\"gen\"]\n        if gen not in dic_to_submit_by_gen:\n            dic_to_submit_by_gen[gen] = []\n            dic_summary_by_gen[gen] = {\n                \"finished\": 0,\n                \"failed\": 0,\n                \"dependency_failed\": 0,\n                \"running_or_queuing\": 0,\n                \"submitted_now\": 0,\n                \"to_submit_later\": 0,\n            }\n        logging.info(f\"Checking job {job} dependencies and status in tree\")\n        l_dep = dependency_graph.get_unfinished_dependency(job)\n        l_dep_failed = dependency_graph.get_failed_dependency(job)\n\n        # Job will be on hold as it has failed dependencies\n        if len(l_dep_failed) &gt; 0:\n            logging.warning(\n                f\"Job {job} has failed dependencies: {l_dep_failed}, it won't be submitted.\"\n            )\n            dic_summary_by_gen[gen][\"dependency_failed\"] += 1\n\n        # Jobs is waiting for dependencies to finish\n        elif len(l_dep) &gt; 0:\n            dic_summary_by_gen[gen][\"to_submit_later\"] += 1\n\n        # Job dependencies are ok\n        elif len(l_dep) == 0:\n            # But job has failed already\n            if nested_get(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"]) == \"failed\":\n                dic_summary_by_gen[gen][\"failed\"] += 1\n\n            # Or job has finished already\n            elif nested_get(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"]) == \"finished\":\n                dic_summary_by_gen[gen][\"finished\"] += 1\n\n            # Else everything is ok, added to the submit dict\n            else:\n                logging.info(f\"Job {job} is added for submission.\")\n                dic_to_submit_by_gen[gen].append(job)\n                # We'll determine which jobs actually have to be submitted and which jobs\n                # are running at the end of the function, after querying the cluster or the local pc\n\n        return dic_to_submit_by_gen, dic_summary_by_gen\n\n    def keep_submit_until_done(\n        self,\n        one_generation_at_a_time: bool = False,\n        wait_time: float = 30,\n        max_try=100,\n        dic_additional_commands_per_gen: Optional[dict[int, str]] = None,\n        dic_dependencies_per_gen: Optional[dict[int, list[str]]] = None,\n        dic_copy_back_per_gen: Optional[dict[int, dict[str, bool]]] = None,\n        name_config: str = \"config.yaml\",\n        force_submit: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Keeps submitting jobs until all jobs are finished or failed.\n\n        The following arguments are only used for HTC jobs submission:\n        - dic_additional_commands_per_gen\n        - dic_dependencies_per_gen\n        - dic_copy_back_per_gen\n        - name_config\n\n        Args:\n            one_generation_at_a_time (bool, optional): Whether to submit one full generation at a\n                time. Defaults to False.\n            wait_time (float, optional): The wait time between submissions in minutes.\n                Defaults to 30.\n            max_try (int, optional): The maximum number of tries before stopping the submission.\n            dic_additional_commands_per_gen (dict[int, str], optional): Additional commands per\n                generation. Defaults to None.\n            dic_dependencies_per_gen (dict[int, list[str]], optional): Dependencies per generation.\n                Only used when doing a HTC submission. Defaults to None.\n            dic_copy_back_per_gen (Optional[dict[int, dict[str, bool]]], optional): A dictionary\n                containing the files to copy back per generation. Accepted keys are \"parquet\",\n                \"yaml\", \"txt\", \"json\", \"zip\" and \"all\". Defaults to None, corresponding to copying\n                back only \"light\" files, i.e. parquet, yaml and txt.\n            name_config (str, optional): The name of the configuration file for the study.\n                Defaults to \"config.yaml\".\n            force_submit (bool, optional): If True, jobs are resubmitted even though they failed.\n                Defaults to False.\n\n\n        Returns:\n            None\n        \"\"\"\n        # Handle mutable default arguments\n        if dic_additional_commands_per_gen is None:\n            dic_additional_commands_per_gen = {}\n        if dic_dependencies_per_gen is None:\n            dic_dependencies_per_gen = {}\n\n        if wait_time &lt; 1 / 20:\n            logging.warning(\"Wait time should be at least 10 seconds to prevent locking errors.\")\n            logging.warning(\"Setting wait time to 10 seconds.\")\n            wait_time = 10 / 60\n\n        # I don't need to lock the tree here since the status cheking is read only and\n        # the lock is acquired in the submit method for the submission\n        while (\n            self.submit(\n                one_generation_at_a_time,\n                dic_additional_commands_per_gen,\n                dic_dependencies_per_gen,\n                dic_copy_back_per_gen,\n                name_config,\n                force_submit=force_submit,\n            )\n            not in [\"finished\", \"finished with issues\"]\n            and max_try &gt; 0\n        ):\n            # Wait for a certain amount of time before checking again\n            logging.info(f\"Waiting {wait_time} minutes before checking again.\")\n            time.sleep(wait_time * 60)\n            max_try -= 1\n\n        if max_try == 0:\n            print(\"Maximum number of tries reached. Stopping submission.\")\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.SubmitScan.dic_tree","title":"<code>dic_tree: dict</code>  <code>property</code> <code>writable</code>","text":"<p>Loads the dictionary tree from the path.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The loaded dictionary tree.</p>"},{"location":"reference/study_da/index.html#study_da.SubmitScan.__init__","title":"<code>__init__(path_tree, path_python_environment='', path_python_environment_container='', path_container_image=None)</code>","text":"<p>Initializes the SubmitScan class.</p> <p>Parameters:</p> Name Type Description Default <code>path_tree</code> <code>str</code> <p>The path to the tree structure.</p> required <code>path_python_environment</code> <code>str</code> <p>The path to the Python environment. Defaults to \"\".</p> <code>''</code> <code>path_python_environment_container</code> <code>str</code> <p>The path to the Python environment in the container. Defaults to \"\".</p> <code>''</code> <code>path_container_image</code> <code>Optional[str]</code> <p>The path to the container image. Defaults to None.</p> <code>None</code> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>def __init__(\n    self,\n    path_tree: str,\n    path_python_environment: str = \"\",\n    path_python_environment_container: str = \"\",\n    path_container_image: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes the SubmitScan class.\n\n    Args:\n        path_tree (str): The path to the tree structure.\n        path_python_environment (str): The path to the Python environment. Defaults to \"\".\n        path_python_environment_container (str, optional): The path to the Python environment\n            in the container. Defaults to \"\".\n        path_container_image (Optional[str], optional): The path to the container image.\n            Defaults to None.\n    \"\"\"\n    # Path to study files\n    self.path_tree = path_tree\n\n    # Absolute path to the tree\n    self.abs_path_tree = os.path.abspath(path_tree)\n\n    # Name of the study folder\n    self.study_name = os.path.dirname(path_tree)\n\n    # Absolute path to the study folder (get from the path_tree)\n    self.abs_path = os.path.abspath(self.study_name).split(f\"/{self.study_name}\")[0]\n\n    # Check that the current working directory is one step above the study folder\n    if os.getcwd() != self.abs_path:\n        raise ValueError(\n            \"The current working directory must be the parent folder of the study folder, \"\n            \"i.e. the folder from which the study was generated. \"\n            \"Please submit from there.\"\n        )\n\n    # Container image (Docker or Singularity, if any)\n    # Turn to absolute path if it is not already\n    if path_container_image is None:\n        self.path_container_image = None\n    elif not os.path.isabs(path_container_image):\n        self.path_container_image = os.path.abspath(path_container_image)\n    else:\n        self.path_container_image = path_container_image\n\n    # Python environment for the container\n    self.path_python_environment_container = path_python_environment_container\n\n    # Ensure that the container image is set if the python environment is set\n    if self.path_container_image and not self.path_python_environment_container:\n        raise ValueError(\n            \"The path to the python environment in the container must be set if the container\"\n            \"image is set.\"\n        )\n\n    # Add /bin/activate to the path_python_environment if needed\n    if not self.path_python_environment_container.endswith(\"/bin/activate\"):\n        # Remove potential / at the end of the path\n        if (\n            self.path_python_environment_container\n            and self.path_python_environment_container[-1] == \"/\"\n        ):\n            self.path_python_environment_container = self.path_python_environment_container[:-1]\n        self.path_python_environment_container += \"/bin/activate\"\n\n    # Ensure the path to the python environment is not \"\" if the container image is not set\n    if not self.path_container_image and not path_python_environment:\n        raise ValueError(\n            \"The path to the python environment must be set if the container image is not set.\"\n        )\n\n    # Path to the python environment, activate with `source path_python_environment`\n    if not path_python_environment:\n        logging.warning(\"No local python environment provided.\")\n        self.path_python_environment = \"\"\n\n    else:\n        # Ensure that the path is not of the form path/bin/activate environment_name\n        split_path = path_python_environment.split(\" \")\n        real_path = split_path[0]\n        env_name = split_path[1] if len(split_path) &gt; 1 else \"\"\n\n        # Turn to absolute path if it is not already\n        self.path_python_environment = (\n            real_path if os.path.isabs(real_path) else os.path.abspath(real_path)\n        )\n\n        # Add /bin/activate to the path_python_environment if needed\n        if \"bin/activate\" not in self.path_python_environment:\n            # Ensure there's no / at the end of the path\n            if self.path_python_environment and self.path_python_environment[-1] == \"/\":\n                self.path_python_environment = self.path_python_environment[:-1]\n            self.path_python_environment += \"/bin/activate\"\n\n        # Add environment name to the path_python_environment if needed\n        if env_name:\n            self.path_python_environment += f\" {env_name}\"\n    # Lock file to avoid concurrent access (softlock as several platforms are used)\n    self.lock = SoftFileLock(f\"{self.path_tree}.lock\", timeout=60)\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.SubmitScan.check_and_update_all_jobs_status","title":"<code>check_and_update_all_jobs_status()</code>","text":"<p>Checks the status of all jobs and updates their status in the job dictionary.</p> <p>This method iterates through all jobs, checks if a \".finished\" or a \".failed\" file exists in the job's folder, and updates the job's status accordingly. If at least one job is not finished or failed, the overall status is set to \"to_finish\". If all jobs are finished or failed, the overall status is set to \"finished\".</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>tuple[dict[str, Any], str]: A tuple containing:</p> <code>str</code> <ul> <li>A dictionary with all jobs and their updated statuses.</li> </ul> <code>tuple[dict[str, Any], str]</code> <ul> <li>A string representing the final status (\"to_finish\" or \"finished\").</li> </ul> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>def check_and_update_all_jobs_status(self) -&gt; tuple[dict[str, Any], str]:\n    \"\"\"\n    Checks the status of all jobs and updates their status in the job dictionary.\n\n    This method iterates through all jobs, checks if a \".finished\" or a \".failed\" file exists in\n    the job's folder, and updates the job's status accordingly. If at least one job is not\n    finished or failed, the overall status is set to \"to_finish\". If all jobs are finished or\n    failed, the overall status is set to \"finished\".\n\n    Returns:\n        tuple[dict[str, Any], str]: A tuple containing:\n        - A dictionary with all jobs and their updated statuses.\n        - A string representing the final status (\"to_finish\" or \"finished\").\n    \"\"\"\n    dic_all_jobs = self.get_all_jobs()\n    at_least_one_job_to_finish = False\n    final_status = \"to_finish\"\n    with self.lock:\n        # Get dic tree once to avoid reloading it for every job\n        dic_tree = self.dic_tree\n\n        # First pass to update the state of the tree\n        for job in dic_all_jobs:\n            # Skip jobs that are already finished, failed or unsubmittable\n            if nested_get(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"]) in [\n                \"finished\",\n                \"failed\",\n                \"unsubmittable\",\n            ]:\n                continue\n\n            # Check the state of the others\n            relative_job_folder = os.path.dirname(job)\n            absolute_job_folder = f\"{self.abs_path}/{relative_job_folder}\"\n            if os.path.exists(f\"{absolute_job_folder}/.finished\"):\n                nested_set(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"], \"finished\")\n            # Check if the job failed otherwise (not to resubmit it again)\n            elif os.path.exists(f\"{absolute_job_folder}/.failed\"):\n                nested_set(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"], \"failed\")\n            # else:\n            #     at_least_one_job_to_finish = True\n\n        # Second pass to update the state of the tree with unreachable jobs\n        dependency_graph = DependencyGraph(dic_tree, dic_all_jobs)\n        for job in dic_all_jobs:\n            # Get all failed dependencies across the tree\n            l_dep_failed = dependency_graph.get_failed_dependency(job)\n            if len(l_dep_failed) &gt; 0:\n                nested_set(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"], \"unsubmittable\")\n            elif nested_get(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"]) == \"to_submit\":\n                at_least_one_job_to_finish = True\n\n        if not at_least_one_job_to_finish:\n            # No more jobs to submit so finished\n            dic_tree[\"status\"] = final_status = \"finished\"\n            # Last pass to check if all jobs are properly finished\n            for job in dic_all_jobs:\n                if nested_get(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"]) != \"finished\":\n                    dic_tree[\"status\"] = final_status = \"finished with issues\"\n                    break\n\n        # Update dic_tree from cluster_submission\n        self.dic_tree = dic_tree\n\n    return dic_all_jobs, final_status\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.SubmitScan.configure_jobs","title":"<code>configure_jobs(force_configure=False, dic_config_jobs=None)</code>","text":"<p>Configures the jobs by modifying the tree structure and creating the run files for each job.</p> <p>Parameters:</p> Name Type Description Default <code>force_configure</code> <code>bool</code> <p>Whether to force reconfiguration. Defaults to False.</p> <code>False</code> <code>dic_config_jobs</code> <code>Optional[dict[str, dict[str, Any]]]</code> <p>A dictionary containing the configuration of the jobs. Defaults to None.</p> <code>None</code> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>def configure_jobs(\n    self,\n    force_configure: bool = False,\n    dic_config_jobs: Optional[dict[str, dict[str, Any]]] = None,\n) -&gt; None:\n    \"\"\"\n    Configures the jobs by modifying the tree structure and creating the run files for each job.\n\n    Args:\n        force_configure (bool, optional): Whether to force reconfiguration. Defaults to False.\n        dic_config_jobs (Optional[dict[str, dict[str, Any]]], optional): A dictionary containing\n            the configuration of the jobs. Defaults to None.\n    \"\"\"\n    # Lock since we are modifying the tree\n    logging.info(\"Acquiring lock to configure jobs\")\n    with self.lock:\n        # Get the tree\n        dic_tree = self.dic_tree\n\n        # Ensure jobs have not been configured already\n        if (\"configured\" in dic_tree and dic_tree[\"configured\"]) and not force_configure:\n            logging.warning(\"Jobs have already been configured. Skipping.\")\n            return\n\n        # Configure the jobs (add generation and job keys, set status to \"To finish\")\n        dic_tree = ConfigJobs(dic_tree,starting_depth=-len(Path(self.path_tree).parts) + 2).find_and_configure_jobs(dic_config_jobs)\n\n        # Add the python environment, container image and absolute path of the study to the tree\n        dic_tree[\"python_environment\"] = self.path_python_environment\n        dic_tree[\"container_image\"] = self.path_container_image\n        dic_tree[\"absolute_path\"] = self.abs_path\n        dic_tree[\"status\"] = \"to_finish\"\n        dic_tree[\"configured\"] = True\n\n        # Explicitly set the dic_tree property to force rewrite\n        self.dic_tree = dic_tree\n\n    logging.info(\"Jobs have been configured. Lock released.\")\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.SubmitScan.generate_run_files","title":"<code>generate_run_files(dic_tree, l_jobs, dic_additional_commands_per_gen, dic_dependencies_per_gen, dic_copy_back_per_gen, name_config)</code>","text":"<p>Generates run files for the specified jobs.</p> <p>Parameters:</p> Name Type Description Default <code>dic_tree</code> <code>dict</code> <p>The dictionary tree structure.</p> required <code>l_jobs</code> <code>list[str]</code> <p>List of jobs to submit.</p> required <code>dic_additional_commands_per_gen</code> <code>dict[int, str]</code> <p>Additional commands per generation. Defaults to {}.</p> required <code>dic_dependencies_per_gen</code> <code>dict[int, list[str]]</code> <p>Dependencies per generation. Only used when doing a HTC submission.</p> required <code>dic_copy_back_per_gen</code> <code>Optional[dict[int, dict[str, bool]]]</code> <p>A dictionary containing the files to copy back per generation. Accepted keys are \"parquet\", \"yaml\", \"txt\", \"json\", \"zip\" and \"all\".</p> required <code>name_config</code> <code>str</code> <p>The name of the configuration file for the study.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The updated dictionary tree structure.</p> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>def generate_run_files(\n    self,\n    dic_tree: dict[str, Any],\n    l_jobs: list[str],\n    dic_additional_commands_per_gen: dict[int, str],\n    dic_dependencies_per_gen: dict[int, list[str]],\n    dic_copy_back_per_gen: dict[int, dict[str, bool]],\n    name_config: str,\n) -&gt; dict:\n    \"\"\"\n    Generates run files for the specified jobs.\n\n    Args:\n        dic_tree (dict): The dictionary tree structure.\n        l_jobs (list[str]): List of jobs to submit.\n        dic_additional_commands_per_gen (dict[int, str], optional): Additional commands per\n            generation. Defaults to {}.\n        dic_dependencies_per_gen (dict[int, list[str]], optional): Dependencies per generation.\n            Only used when doing a HTC submission.\n        dic_copy_back_per_gen (Optional[dict[int, dict[str, bool]]], optional): A dictionary\n            containing the files to copy back per generation. Accepted keys are \"parquet\",\n            \"yaml\", \"txt\", \"json\", \"zip\" and \"all\".\n        name_config (str, optional): The name of the configuration file for the study.\n\n    Returns:\n        dict: The updated dictionary tree structure.\n    \"\"\"\n\n    logging.info(\"Generating run files for the jobs to submit\")\n    # Generate the run files for the jobs to submit\n    dic_all_jobs = self.get_all_jobs()\n    for job in l_jobs:\n        l_keys = dic_all_jobs[job][\"l_keys\"]\n        job_name = os.path.basename(job)\n        relative_job_folder = os.path.dirname(job)\n        absolute_job_folder = f\"{self.abs_path}/{relative_job_folder}\"\n        generation_number = dic_all_jobs[job][\"gen\"]\n        submission_type = nested_get(dic_tree, l_keys + [\"submission_type\"])\n        singularity = \"docker\" in submission_type\n        path_python_environment = (\n            self.path_python_environment_container\n            if singularity\n            else self.path_python_environment\n        )\n\n        # Ensure that the run file does not already exist\n        if \"path_run\" in nested_get(dic_tree, l_keys):\n            path_run_curr = nested_get(dic_tree, l_keys + [\"path_run\"])\n            if path_run_curr is not None and os.path.exists(path_run_curr):\n                logging.info(f\"Run file already exists for job {job}. Skipping.\")\n                continue\n\n        # Build l_dependencies and add to the kwargs\n        l_dependencies = dic_dependencies_per_gen.get(generation_number, [])\n\n        # Get arguments of current generation\n        dic_args = dic_copy_back_per_gen.get(generation_number, {})\n\n        # Mutate the keys\n        dic_args = {f\"copy_back_{key}\": value for key, value in dic_args.items()}\n\n        # Build kwargs for the run file\n        kwargs_htc = {\n            \"l_dependencies\": l_dependencies,\n            \"name_config\": name_config,\n        } | dic_args\n\n        run_str = generate_run_file(\n            absolute_job_folder,\n            job_name,\n            path_python_environment,\n            htc=\"htc\" in submission_type,\n            additionnal_command=dic_additional_commands_per_gen.get(generation_number, \"\"),\n            **kwargs_htc,\n        )\n        # Write the run file\n        path_run_job = f\"{absolute_job_folder}/run.sh\"\n        with open(path_run_job, \"w\") as f:\n            f.write(run_str)\n\n        # Change permissions to make the file executable\n        os.chmod(path_run_job, 0o755)\n\n        # Record the path to the run file in the tree\n        nested_set(dic_tree, l_keys + [\"path_run\"], path_run_job)\n\n    return dic_tree\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.SubmitScan.get_all_jobs","title":"<code>get_all_jobs()</code>","text":"<p>Retrieves all jobs from the configuration, without modifying the tree.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing all jobs.</p> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>def get_all_jobs(self) -&gt; dict:\n    \"\"\"\n    Retrieves all jobs from the configuration, without modifying the tree.\n\n    Returns:\n        dict: A dictionary containing all jobs.\n    \"\"\"\n    # Get a copy of the tree as it's safer\n    with self.lock:\n        dic_tree = self.dic_tree\n    return ConfigJobs(dic_tree,starting_depth=-len(Path(self.path_tree).parts) + 2).find_all_jobs()\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.SubmitScan.keep_submit_until_done","title":"<code>keep_submit_until_done(one_generation_at_a_time=False, wait_time=30, max_try=100, dic_additional_commands_per_gen=None, dic_dependencies_per_gen=None, dic_copy_back_per_gen=None, name_config='config.yaml', force_submit=False)</code>","text":"<p>Keeps submitting jobs until all jobs are finished or failed.</p> <p>The following arguments are only used for HTC jobs submission: - dic_additional_commands_per_gen - dic_dependencies_per_gen - dic_copy_back_per_gen - name_config</p> <p>Parameters:</p> Name Type Description Default <code>one_generation_at_a_time</code> <code>bool</code> <p>Whether to submit one full generation at a time. Defaults to False.</p> <code>False</code> <code>wait_time</code> <code>float</code> <p>The wait time between submissions in minutes. Defaults to 30.</p> <code>30</code> <code>max_try</code> <code>int</code> <p>The maximum number of tries before stopping the submission.</p> <code>100</code> <code>dic_additional_commands_per_gen</code> <code>dict[int, str]</code> <p>Additional commands per generation. Defaults to None.</p> <code>None</code> <code>dic_dependencies_per_gen</code> <code>dict[int, list[str]]</code> <p>Dependencies per generation. Only used when doing a HTC submission. Defaults to None.</p> <code>None</code> <code>dic_copy_back_per_gen</code> <code>Optional[dict[int, dict[str, bool]]]</code> <p>A dictionary containing the files to copy back per generation. Accepted keys are \"parquet\", \"yaml\", \"txt\", \"json\", \"zip\" and \"all\". Defaults to None, corresponding to copying back only \"light\" files, i.e. parquet, yaml and txt.</p> <code>None</code> <code>name_config</code> <code>str</code> <p>The name of the configuration file for the study. Defaults to \"config.yaml\".</p> <code>'config.yaml'</code> <code>force_submit</code> <code>bool</code> <p>If True, jobs are resubmitted even though they failed. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>def keep_submit_until_done(\n    self,\n    one_generation_at_a_time: bool = False,\n    wait_time: float = 30,\n    max_try=100,\n    dic_additional_commands_per_gen: Optional[dict[int, str]] = None,\n    dic_dependencies_per_gen: Optional[dict[int, list[str]]] = None,\n    dic_copy_back_per_gen: Optional[dict[int, dict[str, bool]]] = None,\n    name_config: str = \"config.yaml\",\n    force_submit: bool = False,\n) -&gt; None:\n    \"\"\"\n    Keeps submitting jobs until all jobs are finished or failed.\n\n    The following arguments are only used for HTC jobs submission:\n    - dic_additional_commands_per_gen\n    - dic_dependencies_per_gen\n    - dic_copy_back_per_gen\n    - name_config\n\n    Args:\n        one_generation_at_a_time (bool, optional): Whether to submit one full generation at a\n            time. Defaults to False.\n        wait_time (float, optional): The wait time between submissions in minutes.\n            Defaults to 30.\n        max_try (int, optional): The maximum number of tries before stopping the submission.\n        dic_additional_commands_per_gen (dict[int, str], optional): Additional commands per\n            generation. Defaults to None.\n        dic_dependencies_per_gen (dict[int, list[str]], optional): Dependencies per generation.\n            Only used when doing a HTC submission. Defaults to None.\n        dic_copy_back_per_gen (Optional[dict[int, dict[str, bool]]], optional): A dictionary\n            containing the files to copy back per generation. Accepted keys are \"parquet\",\n            \"yaml\", \"txt\", \"json\", \"zip\" and \"all\". Defaults to None, corresponding to copying\n            back only \"light\" files, i.e. parquet, yaml and txt.\n        name_config (str, optional): The name of the configuration file for the study.\n            Defaults to \"config.yaml\".\n        force_submit (bool, optional): If True, jobs are resubmitted even though they failed.\n            Defaults to False.\n\n\n    Returns:\n        None\n    \"\"\"\n    # Handle mutable default arguments\n    if dic_additional_commands_per_gen is None:\n        dic_additional_commands_per_gen = {}\n    if dic_dependencies_per_gen is None:\n        dic_dependencies_per_gen = {}\n\n    if wait_time &lt; 1 / 20:\n        logging.warning(\"Wait time should be at least 10 seconds to prevent locking errors.\")\n        logging.warning(\"Setting wait time to 10 seconds.\")\n        wait_time = 10 / 60\n\n    # I don't need to lock the tree here since the status cheking is read only and\n    # the lock is acquired in the submit method for the submission\n    while (\n        self.submit(\n            one_generation_at_a_time,\n            dic_additional_commands_per_gen,\n            dic_dependencies_per_gen,\n            dic_copy_back_per_gen,\n            name_config,\n            force_submit=force_submit,\n        )\n        not in [\"finished\", \"finished with issues\"]\n        and max_try &gt; 0\n    ):\n        # Wait for a certain amount of time before checking again\n        logging.info(f\"Waiting {wait_time} minutes before checking again.\")\n        time.sleep(wait_time * 60)\n        max_try -= 1\n\n    if max_try == 0:\n        print(\"Maximum number of tries reached. Stopping submission.\")\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.SubmitScan.log_jobs_state","title":"<code>log_jobs_state(dic_summary_by_gen)</code>  <code>staticmethod</code>","text":"<p>Logs the state of jobs for each generation.</p> <p>Parameters:</p> Name Type Description Default <code>dic_summary_by_gen</code> <code>dict</code> <p>A dictionary where the keys are generation numbers and the values are dictionaries summarizing job states. Each summary dictionary should contain the following keys: - 'to_submit_later': int, number of jobs left to submit later - 'running_or_queuing': int, number of jobs running or queuing - 'submitted_now': int, number of jobs submitted now - 'finished': int, number of jobs finished - 'failed': int, number of jobs failed - 'dependency_failed': int, number of jobs on hold due to failed dependencies</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>@staticmethod\ndef log_jobs_state(dic_summary_by_gen: dict[int, dict[str, int]]) -&gt; None:\n    \"\"\"\n    Logs the state of jobs for each generation.\n\n    Args:\n        dic_summary_by_gen (dict): A dictionary where the keys are generation numbers\n            and the values are dictionaries summarizing job states.\n            Each summary dictionary should contain the following keys:\n            - 'to_submit_later': int, number of jobs left to submit later\n            - 'running_or_queuing': int, number of jobs running or queuing\n            - 'submitted_now': int, number of jobs submitted now\n            - 'finished': int, number of jobs finished\n            - 'failed': int, number of jobs failed\n            - 'dependency_failed': int, number of jobs on hold due to failed dependencies\n\n    Returns:\n        None\n    \"\"\"\n    print(\"State of the jobs:\")\n    for gen, dic_summary in dic_summary_by_gen.items():\n        print(\"********************************\")\n        print(f\"Generation {gen}\")\n        print(f\"Jobs left to submit later: {dic_summary['to_submit_later']}\")\n        print(f\"Jobs running or queuing: {dic_summary['running_or_queuing']}\")\n        print(f\"Jobs submitted now: {dic_summary['submitted_now']}\")\n        print(f\"Jobs finished: {dic_summary['finished']}\")\n        print(f\"Jobs failed: {dic_summary['failed']}\")\n        print(f\"Jobs on hold due to failed dependencies: {dic_summary['dependency_failed']}\")\n        print(\"********************************\")\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.SubmitScan.reset_failed_jobs","title":"<code>reset_failed_jobs(dic_tree)</code>","text":"<p>Resets the status of jobs that have failed to \"to_submit\".</p> <p>Parameters:</p> Name Type Description Default <code>dic_tree</code> <code>dict[str, Any]</code> <p>The dictionary tree structure.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: The updated dictionary tree structure.</p> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>def reset_failed_jobs(self, dic_tree: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"\n    Resets the status of jobs that have failed to \"to_submit\".\n\n    Args:\n        dic_tree (dict[str, Any]): The dictionary tree structure.\n\n    Returns:\n        dict[str, Any]: The updated dictionary tree structure.\n    \"\"\"\n\n    dic_all_jobs = self.get_all_jobs()\n    # First pass to update the state of the tree\n    for job in dic_all_jobs:\n        # Skip jobs that are not failed\n        if nested_get(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"]) != \"failed\":\n            continue\n\n        # Reset the state of the others\n        relative_job_folder = os.path.dirname(job)\n        absolute_job_folder = f\"{self.abs_path}/{relative_job_folder}\"\n\n        # Remove failed tag\n        if os.path.exists(f\"{absolute_job_folder}/.failed\"):\n            os.remove(f\"{absolute_job_folder}/.failed\")\n        else:\n            logging.warning(f\"Failed file not found for job {job}.\")\n\n        # Remove run file\n        if \"path_run\" in nested_get(dic_tree, dic_all_jobs[job][\"l_keys\"]):\n            path_run_curr = nested_get(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"path_run\"])\n            if path_run_curr is not None and os.path.exists(path_run_curr):\n                os.remove(path_run_curr)\n            else:\n                logging.warning(f\"Run file not found for job {job}.\")\n\n        # Reset the status of the job\n        nested_set(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"], \"to_submit\")\n\n    return dic_tree\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.SubmitScan.submit","title":"<code>submit(one_generation_at_a_time=False, dic_additional_commands_per_gen=None, dic_dependencies_per_gen=None, dic_copy_back_per_gen=None, name_config='config.yaml', force_submit=False)</code>","text":"<p>Submits the jobs to the cluster. Note that copying back large files (e.g. json colliders) can trigger a throttling mechanism in AFS.</p> <p>The following arguments are only used for HTC jobs submission: - dic_additional_commands_per_gen - dic_dependencies_per_gen - dic_copy_back_per_gen - name_config</p> <p>Parameters:</p> Name Type Description Default <code>one_generation_at_a_time</code> <code>bool</code> <p>Whether to submit one full generation at a time. Defaults to False.</p> <code>False</code> <code>dic_additional_commands_per_gen</code> <code>dict[int, str]</code> <p>Additional commands per generation. Defaults to None.</p> <code>None</code> <code>dic_dependencies_per_gen</code> <code>dict[int, list[str]]</code> <p>Dependencies per generation. Only used when doing a HTC submission. Defaults to None.</p> <code>None</code> <code>dic_copy_back_per_gen</code> <code>Optional[dict[int, dict[str, bool]]]</code> <p>A dictionary containing the files to copy back per generation. Accepted keys are \"parquet\", \"yaml\", \"txt\", \"json\", \"zip\" and \"all\". Defaults to None, corresponding to copying back only \"light\" files, i.e. parquet, yaml and txt.</p> <code>None</code> <code>name_config</code> <code>str</code> <p>The name of the configuration file for the study. Defaults to \"config.yaml\".</p> <code>'config.yaml'</code> <code>force_submit</code> <code>bool</code> <p>If True, jobs are resubmitted even though they failed. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The final status of the jobs.</p> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>def submit(\n    self,\n    one_generation_at_a_time: bool = False,\n    dic_additional_commands_per_gen: Optional[dict[int, str]] = None,\n    dic_dependencies_per_gen: Optional[dict[int, list[str]]] = None,\n    dic_copy_back_per_gen: Optional[dict[int, dict[str, bool]]] = None,\n    name_config: str = \"config.yaml\",\n    force_submit: bool = False,\n) -&gt; str:\n    \"\"\"\n    Submits the jobs to the cluster. Note that copying back large files (e.g. json colliders)\n    can trigger a throttling mechanism in AFS.\n\n    The following arguments are only used for HTC jobs submission:\n    - dic_additional_commands_per_gen\n    - dic_dependencies_per_gen\n    - dic_copy_back_per_gen\n    - name_config\n\n    Args:\n        one_generation_at_a_time (bool, optional): Whether to submit one full generation at a\n            time. Defaults to False.\n        dic_additional_commands_per_gen (dict[int, str], optional): Additional commands per\n            generation. Defaults to None.\n        dic_dependencies_per_gen (dict[int, list[str]], optional): Dependencies per generation.\n            Only used when doing a HTC submission. Defaults to None.\n        dic_copy_back_per_gen (Optional[dict[int, dict[str, bool]]], optional): A dictionary\n            containing the files to copy back per generation. Accepted keys are \"parquet\",\n            \"yaml\", \"txt\", \"json\", \"zip\" and \"all\". Defaults to None, corresponding to copying\n            back only \"light\" files, i.e. parquet, yaml and txt.\n        name_config (str, optional): The name of the configuration file for the study.\n            Defaults to \"config.yaml\".\n        force_submit (bool, optional): If True, jobs are resubmitted even though they failed.\n            Defaults to False.\n\n    Returns:\n        str: The final status of the jobs.\n    \"\"\"\n    # Handle mutable default arguments\n    if dic_additional_commands_per_gen is None:\n        dic_additional_commands_per_gen = {}\n    if dic_dependencies_per_gen is None:\n        dic_dependencies_per_gen = {}\n    if dic_copy_back_per_gen is None:\n        dic_copy_back_per_gen = {}\n\n    # Handle force submit\n    if force_submit:\n        logging.warning(\"Forcing resubmission of all failed jobs.\")\n        with self.lock:\n            # Acquire tree from disk\n            dic_tree = self.dic_tree\n\n            # Reset the tree by deleting the failed tags\n            dic_tree = self.reset_failed_jobs(dic_tree)\n            dic_tree[\"status\"] = \"to_finish\"\n            # Write the tree back to disk\n            self.dic_tree = dic_tree\n\n    # Update the status of all jobs before submitting\n    dic_all_jobs, final_status = self.check_and_update_all_jobs_status()\n    if final_status == \"finished\":\n        print(\"All jobs are finished.\")\n        return final_status\n    elif final_status == \"finished with issues\":\n        print(\"All jobs are finished but some did not run properly.\")\n        return final_status\n\n    logging.info(\"Acquiring lock to submit jobs\")\n    with self.lock:\n        # Get dic tree once to avoid reloading it for every job\n        dic_tree = self.dic_tree\n\n        # Submit the jobs\n        self._submit(\n            dic_tree,\n            dic_all_jobs,\n            one_generation_at_a_time,\n            dic_additional_commands_per_gen,\n            dic_dependencies_per_gen,\n            dic_copy_back_per_gen,\n            name_config,\n        )\n\n        # Update dic_tree from cluster_submission\n        self.dic_tree = dic_tree\n    logging.info(\"Jobs have been submitted. Lock released.\")\n    return final_status\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.aggregate_output_data","title":"<code>aggregate_output_data(path_tree, l_group_by_parameters, function_to_aggregate=min, generation_of_interest=2, name_output='output_particles.parquet', write_output=True, path_output=None, only_keep_lost_particles=True, dic_parameters_of_interest=None, l_parameters_to_keep=None, name_template_parameters='parameters_lhc.yaml', path_template_parameters=None, force_overwrite=False)</code>","text":"<p>Aggregates output data from simulation files.</p> <p>Parameters:</p> Name Type Description Default <code>path_tree</code> <code>str</code> <p>The path to the tree file.</p> required <code>l_group_by_parameters</code> <code>list</code> <p>List of parameters to group by.</p> required <code>function_to_aggregate</code> <code>callable</code> <p>Function to aggregate the grouped data.</p> <code>min</code> <code>generation_of_interest</code> <code>int</code> <p>The generation of interest. Defaults to 2.</p> <code>2</code> <code>name_output</code> <code>str</code> <p>The name of the output file. Defaults to \"output_particles.parquet\".</p> <code>'output_particles.parquet'</code> <code>write_output</code> <code>bool</code> <p>Flag to indicate if the output should be written to a file. Defaults to True.</p> <code>True</code> <code>path_output</code> <code>str</code> <p>The path to the output file. If not provided, the default output file will be in the study folder as 'da.parquet'. Defaults to None.</p> <code>None</code> <code>only_keep_lost_particles</code> <code>bool</code> <p>Flag to indicate if only lost particles should be kept. Defaults to True.</p> <code>True</code> <code>dic_parameters_of_interest</code> <code>dict</code> <p>Dictionary of parameters of interest. Defaults to None.</p> <code>None</code> <code>l_parameters_to_keep</code> <code>list</code> <p>List of parameters to keep. Defaults to None.</p> <code>None</code> <code>name_template_parameters</code> <code>str</code> <p>The name of the template parameters file associating each parameter to a list of keys. Defaults to \"parameters_lhc.yaml\", which is already contained in the study-da package, and includes the main usual parameters.</p> <code>'parameters_lhc.yaml'</code> <code>path_template_parameters</code> <code>str</code> <p>The path to the template parameters file. Must be provided if a no template already contained in study-da is provided through the argument name_template_parameters. Defaults to None.</p> <code>None</code> <code>force_overwrite</code> <code>bool</code> <p>Flag to indicate if the output file should be overwritten if it already exists. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The final aggregated DataFrame.</p> Source code in <code>study_da/postprocess/postprocess.py</code> <pre><code>def aggregate_output_data(\n    path_tree: str,\n    l_group_by_parameters: List[str],\n    function_to_aggregate: Callable = min,\n    generation_of_interest: int = 2,\n    name_output: str = \"output_particles.parquet\",\n    write_output: bool = True,\n    path_output: Optional[str] = None,\n    only_keep_lost_particles: bool = True,\n    dic_parameters_of_interest: Optional[Dict[str, List[str]]] = None,\n    l_parameters_to_keep: Optional[List[str]] = None,\n    name_template_parameters: str = \"parameters_lhc.yaml\",\n    path_template_parameters: Optional[str] = None,\n    force_overwrite: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Aggregates output data from simulation files.\n\n    Args:\n        path_tree (str): The path to the tree file.\n        l_group_by_parameters (list): List of parameters to group by.\n        function_to_aggregate (callable, optional): Function to aggregate the grouped data.\n        generation_of_interest (int, optional): The generation of interest. Defaults to 2.\n        name_output (str, optional): The name of the output file. Defaults to \"output_particles.parquet\".\n        write_output (bool, optional): Flag to indicate if the output should be written to a file.\n            Defaults to True.\n        path_output (str, optional): The path to the output file. If not provided, the default\n            output file will be in the study folder as 'da.parquet'. Defaults to None.\n        only_keep_lost_particles (bool, optional): Flag to indicate if only lost particles should be\n            kept. Defaults to True.\n        dic_parameters_of_interest (dict, optional): Dictionary of parameters of interest. Defaults\n            to None.\n        l_parameters_to_keep (list, optional): List of parameters to keep. Defaults to None.\n        name_template_parameters (str, optional): The name of the template parameters file\n            associating each parameter to a list of keys. Defaults to \"parameters_lhc.yaml\", which\n            is already contained in the study-da package, and includes the main usual parameters.\n        path_template_parameters (str, optional): The path to the template parameters file. Must\n            be provided if a no template already contained in study-da is provided through the\n            argument name_template_parameters. Defaults to None.\n        force_overwrite (bool, optional): Flag to indicate if the output file should be overwritten\n            if it already exists. Defaults to False.\n\n    Returns:\n        pd.DataFrame: The final aggregated DataFrame.\n    \"\"\"\n    # Check it the output doesn't already exist and ask for confirmation to overwrite\n    dic_tree, _ = load_dic_from_path(path_tree)\n    absolute_path_study = dic_tree[\"absolute_path\"]\n    if path_output is None:\n        path_output = os.path.join(absolute_path_study, \"da.parquet\")\n    if os.path.exists(path_output) and not force_overwrite:\n        input_user = input(\n            f\"The output file {path_output} already exists. Do you want to overwrite it? (y/n) \"\n        )\n        if input_user.lower() != \"y\":\n            logging.warning(\"Output file not overwritten\")\n            return pd.read_parquet(path_output)\n\n    logging.info(\"Analysis of output simulation files started\")\n\n    dic_all_jobs = ConfigJobs(dic_tree,starting_depth=-len(Path(path_tree).parts) + 2).find_all_jobs()\n\n    l_df_sim = get_particles_data(\n        dic_all_jobs, absolute_path_study, generation_of_interest, name_output\n    )\n\n    default_path_template_parameters = False\n    if dic_parameters_of_interest is None:\n        if path_template_parameters is not None:\n            logging.info(\"Loading parameters of interest from the provided configuration file\")\n        else:\n            if name_template_parameters is None:\n                raise ValueError(\n                    \"No template configuration file provided for the parameters of interest\"\n                )\n            logging.info(\"Loading parameters of interest from the template configuration file\")\n            path_template_parameters = os.path.join(\n                os.path.dirname(inspect.getfile(aggregate_output_data)),\n                \"configs\",\n                name_template_parameters,\n            )\n            default_path_template_parameters = True\n        dic_parameters_of_interest, _ = load_dic_from_path(path_template_parameters)\n\n    l_df_output = add_parameters_from_config(\n        l_df_sim, dic_parameters_of_interest, default_path_template_parameters\n    )\n\n    df_final = merge_and_group_by_parameters_of_interest(\n        l_df_output,\n        l_group_by_parameters,\n        only_keep_lost_particles,\n        l_parameters_to_keep,\n        function_to_aggregate,\n    )\n\n    # Fix the LHC version type\n    df_final = fix_LHC_version(df_final)\n\n    if write_output:\n        df_final.to_parquet(path_output)\n    elif path_output is not None:\n        logging.warning(\"Output path provided but write_output set to False, no output saved\")\n\n    logging.info(\"Final dataframe for current set of simulations: %s\", df_final)\n    return df_final\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.create","title":"<code>create(path_config_scan='config_scan.yaml', force_overwrite=False, dic_parameter_all_gen=None, dic_parameter_all_gen_naming=None, add_prefix_to_folder_names=False)</code>","text":"<p>Create a study based on the configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>path_config_scan</code> <code>str</code> <p>Path to the configuration file for the scan. Defaults to \"config_scan.yaml\".</p> <code>'config_scan.yaml'</code> <code>force_overwrite</code> <code>bool</code> <p>Flag to force overwrite the study. Defaults to False.</p> <code>False</code> <code>dic_parameter_all_gen</code> <code>Optional[dict[str, dict[str, Any]]]</code> <p>Dictionary of parameters for the scan, if not provided through the scan config. Defaults to None.</p> <code>None</code> <code>dic_parameter_all_gen_naming</code> <code>Optional[dict[str, dict[str, Any]]]</code> <p>Dictionary of parameters for the naming of the scan subfolders, if not provided through the scan config. Defaults to None.</p> <code>None</code> <code>add_prefix_to_folder_names</code> <code>bool</code> <p>Whether to add a prefix to the folder names. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[str, str]</code> <p>tuple[str, str]: The path to the tree file and the name of the main configuration file.</p> Source code in <code>study_da/study_da.py</code> <pre><code>def create(\n    path_config_scan: str = \"config_scan.yaml\",\n    force_overwrite: bool = False,\n    dic_parameter_all_gen: Optional[dict[str, dict[str, Any]]] = None,\n    dic_parameter_all_gen_naming: Optional[dict[str, dict[str, Any]]] = None,\n    add_prefix_to_folder_names: bool = False,\n) -&gt; tuple[str, str]:\n    \"\"\"\n    Create a study based on the configuration file.\n\n    Args:\n        path_config_scan (str, optional): Path to the configuration file for the scan.\n            Defaults to \"config_scan.yaml\".\n        force_overwrite (bool, optional): Flag to force overwrite the study. Defaults to False.\n        dic_parameter_all_gen (Optional[dict[str, dict[str, Any]]], optional): Dictionary of\n            parameters for the scan, if not provided through the scan config. Defaults to None.\n        dic_parameter_all_gen_naming (Optional[dict[str, dict[str, Any]]], optional): Dictionary of\n            parameters for the naming of the scan subfolders, if not provided through the scan\n            config. Defaults to None.\n        add_prefix_to_folder_names (bool, optional): Whether to add a prefix to the folder names.\n            Defaults to False.\n\n    Returns:\n        tuple[str, str]: The path to the tree file and the name of the main configuration file.\n    \"\"\"\n    logging.info(f\"Create study from configuration file: {path_config_scan}\")\n    study = GenerateScan(path_config=path_config_scan)\n    study.create_study(\n        force_overwrite=force_overwrite,\n        dic_parameter_all_gen=dic_parameter_all_gen,\n        dic_parameter_all_gen_naming=dic_parameter_all_gen_naming,\n        add_prefix_to_folder_names=add_prefix_to_folder_names,\n    )\n\n    # Get variables of interest for the submission\n    path_tree = study.path_tree\n    name_main_configuration = study.config[\"dependencies\"][\"main_configuration\"]\n\n    return path_tree, name_main_configuration\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.create_single_job","title":"<code>create_single_job(name_main_configuration, name_executable_generation_1, name_executable_generation_2=None, name_executable_generation_3=None, name_study='single_job_study', force_overwrite=False)</code>","text":"<p>Create a single job study (not a parametric scan) with the specified configuration and executables. Limited to three generations.</p> <p>Parameters:</p> Name Type Description Default <code>name_main_configuration</code> <code>str</code> <p>The name of the main configuration file for the study.</p> required <code>name_executable_generation_1</code> <code>str</code> <p>The name of the executable for the first generation.</p> required <code>name_executable_generation_2</code> <code>Optional[str]</code> <p>The name of the executable for the second generation. Defaults to None.</p> <code>None</code> <code>name_executable_generation_3</code> <code>Optional[str]</code> <p>The name of the executable for the third generation. Defaults to None.</p> <code>None</code> <code>name_study</code> <code>str</code> <p>The name of the study. Defaults to \"single_job_study\".</p> <code>'single_job_study'</code> <code>force_overwrite</code> <code>bool</code> <p>Whether to force overwrite existing files. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The path to the tree file.</p> Source code in <code>study_da/study_da.py</code> <pre><code>def create_single_job(\n    name_main_configuration: str,\n    name_executable_generation_1: str,\n    name_executable_generation_2: Optional[str] = None,\n    name_executable_generation_3: Optional[str] = None,\n    name_study: str = \"single_job_study\",\n    force_overwrite: bool = False,\n) -&gt; str:\n    \"\"\"\n    Create a single job study (not a parametric scan) with the specified configuration and\n    executables. Limited to three generations.\n\n    Args:\n        name_main_configuration (str): The name of the main configuration file for the study.\n        name_executable_generation_1 (str): The name of the executable for the first generation.\n        name_executable_generation_2 (Optional[str], optional): The name of the executable for the\n            second generation. Defaults to None.\n        name_executable_generation_3 (Optional[str], optional): The name of the executable for the\n            third generation. Defaults to None.\n        name_study (str, optional): The name of the study. Defaults to \"single_job_study\".\n        force_overwrite (bool, optional): Whether to force overwrite existing files.\n            Defaults to False.\n\n    Returns:\n        str: The path to the tree file.\n    \"\"\"\n    # Generate the scan dictionnary\n    dic_scan = {\n        \"name\": name_study,\n        \"dependencies\": {\"main_configuration\": name_main_configuration},\n        \"structure\": {\n            \"generation_1\": {\n                \"executable\": name_executable_generation_1,\n            },\n        },\n    }\n\n    if name_executable_generation_2 is not None:\n        dic_scan[\"structure\"][\"generation_2\"] = {\n            \"executable\": name_executable_generation_2,\n        }\n\n    if name_executable_generation_3 is not None:\n        dic_scan[\"structure\"][\"generation_3\"] = {\n            \"executable\": name_executable_generation_3,\n        }\n\n    # Create the study\n    logging.info(f\"Create single job study: {name_study}\")\n    study = GenerateScan(dic_scan=dic_scan)\n    study.create_study(\n        force_overwrite=force_overwrite,\n    )\n\n    return study.path_tree\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.get_title_from_configuration","title":"<code>get_title_from_configuration(dataframe_data, ions=False, crossing_type=None, display_LHC_version=True, display_energy=True, display_bunch_index=True, display_CC_crossing=True, display_bunch_intensity=True, display_beta=True, display_crossing_IP_1=True, display_crossing_IP_2=True, display_crossing_IP_5=True, display_crossing_IP_8=True, display_bunch_length=True, display_polarity_IP_2_8=True, display_emittance=True, display_chromaticity=True, display_octupole_intensity=True, display_coupling=True, display_filling_scheme=True, display_horizontal_tune=None, display_vertical_tune=None, display_tune=True, display_luminosity_1=True, display_luminosity_2=True, display_luminosity_5=True, display_luminosity_8=True, display_PU_1=True, display_PU_2=True, display_PU_5=True, display_PU_8=True, display_number_of_turns=False)</code>","text":"<p>Generates a title string from the configuration data.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing configuration data.</p> required <code>ions</code> <code>bool</code> <p>Whether the beam is composed of ions. Defaults to False.</p> <code>False</code> <code>crossing_type</code> <code>str</code> <p>The type of crossing: 'vh' or 'hv'. Defaults to None, meaning it will try to be inferred from the optics file name. Back to 'hv' if not found.</p> <code>None</code> <code>display_betx_bety</code> <code>bool</code> <p>Whether to display the beta functions. Defaults to True.</p> required <code>display_LHC_version</code> <code>bool</code> <p>Whether to display the LHC version. Defaults to True.</p> <code>True</code> <code>display_energy</code> <code>bool</code> <p>Whether to display the energy. Defaults to True.</p> <code>True</code> <code>display_bunch_index</code> <code>bool</code> <p>Whether to display the bunch index. Defaults to True.</p> <code>True</code> <code>display_CC_crossing</code> <code>bool</code> <p>Whether to display the CC crossing. Defaults to True.</p> <code>True</code> <code>display_bunch_intensity</code> <code>bool</code> <p>Whether to display the bunch intensity. Defaults to True.</p> <code>True</code> <code>display_beta</code> <code>bool</code> <p>Whether to display the beta function. Defaults to True.</p> <code>True</code> <code>display_crossing_IP_1</code> <code>bool</code> <p>Whether to display the crossing at IP1. Defaults to True.</p> <code>True</code> <code>display_crossing_IP_2</code> <code>bool</code> <p>Whether to display the crossing at IP2. Defaults to True.</p> <code>True</code> <code>display_crossing_IP_5</code> <code>bool</code> <p>Whether to display the crossing at IP5. Defaults to True.</p> <code>True</code> <code>display_crossing_IP_8</code> <code>bool</code> <p>Whether to display the crossing at IP8. Defaults to True.</p> <code>True</code> <code>display_bunch_length</code> <code>bool</code> <p>Whether to display the bunch length. Defaults to True.</p> <code>True</code> <code>display_polarity_IP_2_8</code> <code>bool</code> <p>Whether to display the polarity at IP2 and IP8. Defaults to True.</p> <code>True</code> <code>display_emittance</code> <code>bool</code> <p>Whether to display the emittance. Defaults to True.</p> <code>True</code> <code>display_chromaticity</code> <code>bool</code> <p>Whether to display the chromaticity. Defaults to True.</p> <code>True</code> <code>display_octupole_intensity</code> <code>bool</code> <p>Whether to display the octupole intensity. Defaults to True.</p> <code>True</code> <code>display_coupling</code> <code>bool</code> <p>Whether to display the coupling. Defaults to True.</p> <code>True</code> <code>display_filling_scheme</code> <code>bool</code> <p>Whether to display the filling scheme. Defaults to True.</p> <code>True</code> <code>display_horizontal_tune</code> <code>bool</code> <p>Whether to display the horizontal tune. Defaults to None. Takes precedence over display_tune.</p> <code>None</code> <code>display_vertical_tune</code> <code>bool</code> <p>Whether to display the vertical tune. Defaults to None. Takes precedence over display_tune.</p> <code>None</code> <code>display_tune</code> <code>bool</code> <p>Whether to display the tune. Defaults to True.</p> <code>True</code> <code>display_luminosity_1</code> <code>bool</code> <p>Whether to display the luminosity at IP1. Defaults to True.</p> <code>True</code> <code>display_luminosity_2</code> <code>bool</code> <p>Whether to display the luminosity at IP2. Defaults to True.</p> <code>True</code> <code>display_luminosity_5</code> <code>bool</code> <p>Whether to display the luminosity at IP5. Defaults to True.</p> <code>True</code> <code>display_luminosity_8</code> <code>bool</code> <p>Whether to display the luminosity at IP8. Defaults to True.</p> <code>True</code> <code>display_PU_1</code> <code>bool</code> <p>Whether to display the PU at IP1. Defaults to True.</p> <code>True</code> <code>display_PU_2</code> <code>bool</code> <p>Whether to display the PU at IP2. Defaults to True.</p> <code>True</code> <code>display_PU_5</code> <code>bool</code> <p>Whether to display the PU at IP5. Defaults to True.</p> <code>True</code> <code>display_PU_8</code> <code>bool</code> <p>Whether to display the PU at IP8. Defaults to True.</p> <code>True</code> <code>display_number_of_turns</code> <code>bool</code> <p>Whether to display the number of turns. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The generated title string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_title_from_configuration(\n    dataframe_data: pd.DataFrame,\n    ions: bool = False,\n    crossing_type: Optional[str] = None,\n    display_LHC_version: bool = True,\n    display_energy: bool = True,\n    display_bunch_index: bool = True,\n    display_CC_crossing: bool = True,\n    display_bunch_intensity: bool = True,\n    display_beta: bool = True,\n    display_crossing_IP_1: bool = True,\n    display_crossing_IP_2: bool = True,\n    display_crossing_IP_5: bool = True,\n    display_crossing_IP_8: bool = True,\n    display_bunch_length: bool = True,\n    display_polarity_IP_2_8: bool = True,\n    display_emittance: bool = True,\n    display_chromaticity: bool = True,\n    display_octupole_intensity: bool = True,\n    display_coupling: bool = True,\n    display_filling_scheme: bool = True,\n    display_horizontal_tune: Optional[bool] = None,\n    display_vertical_tune: Optional[bool] = None,\n    display_tune: bool = True,\n    display_luminosity_1: bool = True,\n    display_luminosity_2: bool = True,\n    display_luminosity_5: bool = True,\n    display_luminosity_8: bool = True,\n    display_PU_1: bool = True,\n    display_PU_2: bool = True,\n    display_PU_5: bool = True,\n    display_PU_8: bool = True,\n    display_number_of_turns=False,\n) -&gt; str:\n    \"\"\"\n    Generates a title string from the configuration data.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing configuration data.\n        ions (bool, optional): Whether the beam is composed of ions. Defaults to False.\n        crossing_type (str, optional): The type of crossing: 'vh' or 'hv'. Defaults to None, meaning\n            it will try to be inferred from the optics file name. Back to 'hv' if not found.\n        display_betx_bety (bool, optional): Whether to display the beta functions. Defaults to True.\n        display_LHC_version (bool, optional): Whether to display the LHC version. Defaults to True.\n        display_energy (bool, optional): Whether to display the energy. Defaults to True.\n        display_bunch_index (bool, optional): Whether to display the bunch index. Defaults to True.\n        display_CC_crossing (bool, optional): Whether to display the CC crossing. Defaults to True.\n        display_bunch_intensity (bool, optional): Whether to display the bunch intensity. Defaults\n            to True.\n        display_beta (bool, optional): Whether to display the beta function. Defaults to True.\n        display_crossing_IP_1 (bool, optional): Whether to display the crossing at IP1. Defaults to\n            True.\n        display_crossing_IP_2 (bool, optional): Whether to display the crossing at IP2. Defaults to\n            True.\n        display_crossing_IP_5 (bool, optional): Whether to display the crossing at IP5. Defaults to\n            True.\n        display_crossing_IP_8 (bool, optional): Whether to display the crossing at IP8. Defaults to\n            True.\n        display_bunch_length (bool, optional): Whether to display the bunch length. Defaults to\n            True.\n        display_polarity_IP_2_8 (bool, optional): Whether to display the polarity at IP2 and IP8.\n            Defaults to True.\n        display_emittance (bool, optional): Whether to display the emittance. Defaults to True.\n        display_chromaticity (bool, optional): Whether to display the chromaticity.\n            Defaults to True.\n        display_octupole_intensity (bool, optional): Whether to display the octupole intensity.\n            Defaults to True.\n        display_coupling (bool, optional): Whether to display the coupling. Defaults to True.\n        display_filling_scheme (bool, optional): Whether to display the filling scheme. Defaults to\n            True.\n        display_horizontal_tune (bool, optional): Whether to display the horizontal tune. Defaults to\n            None. Takes precedence over display_tune.\n        display_vertical_tune (bool, optional): Whether to display the vertical tune. Defaults to\n            None. Takes precedence over display_tune.\n        display_tune (bool, optional): Whether to display the tune. Defaults to True.\n        display_luminosity_1 (bool, optional): Whether to display the luminosity at IP1. Defaults to\n            True.\n        display_luminosity_2 (bool, optional): Whether to display the luminosity at IP2. Defaults to\n            True.\n        display_luminosity_5 (bool, optional): Whether to display the luminosity at IP5. Defaults to\n            True.\n        display_luminosity_8 (bool, optional): Whether to display the luminosity at IP8. Defaults to\n            True.\n        display_PU_1 (bool, optional): Whether to display the PU at IP1. Defaults to True.\n        display_PU_2 (bool, optional): Whether to display the PU at IP2. Defaults to True.\n        display_PU_5 (bool, optional): Whether to display the PU at IP5. Defaults to True.\n        display_PU_8 (bool, optional): Whether to display the PU at IP8. Defaults to True.\n        display_number_of_turns (bool, optional): Whether to display the number of turns. Defaults to\n            False.\n\n    Returns:\n        str: The generated title string.\n    \"\"\"\n\n    # Warn about tune definition\n    if (\n        display_horizontal_tune is not None or display_vertical_tune is not None\n    ) and not display_tune:\n        logging.warning(\n            \"You have defined display_horizontal_tune or display_vertical_tune, but not \"\n            \"display_tune. The horizontal and/or vertical tunes will still be displayed.\"\n        )\n\n    # Find out what is the crossing type\n    if crossing_type is None:\n        crossing_type = get_crossing_type(dataframe_data)\n\n    # Collect all the information to display\n    LHC_version_str = get_LHC_version_str(dataframe_data, ions)\n    energy_str = get_energy_str(dataframe_data, ions)\n    bunch_index_str = get_bunch_index_str(dataframe_data)\n    CC_crossing_str = get_CC_crossing_str(dataframe_data)\n    bunch_intensity_str = get_bunch_intensity_str(dataframe_data)\n    beta_str = get_beta_str(dataframe_data)\n    xing_IP1_str, xing_IP5_str = get_crossing_IP_1_5_str(dataframe_data, crossing_type)\n    xing_IP2_str, xing_IP8_str = get_crossing_IP_2_8_str(dataframe_data)\n    bunch_length_str = get_bunch_length_str(dataframe_data)\n    polarity_str = get_polarity_IP_2_8_str(dataframe_data)\n    emittance_str = get_normalized_emittance_str(dataframe_data)\n    chromaticity_str = get_chromaticity_str(dataframe_data)\n    octupole_intensity_str = get_octupole_intensity_str(dataframe_data)\n    coupling_str = get_linear_coupling_str(dataframe_data)\n    filling_scheme_str = get_filling_scheme_str(dataframe_data)\n    tune_str = get_tune_str(dataframe_data, display_horizontal_tune, display_vertical_tune)\n    n_turns_str = get_number_of_turns_str(dataframe_data)\n\n    # Collect luminosity and PU strings at each IP\n    dic_lumi_PU_str = {\n        \"with_beam_beam\": {\"lumi\": {}, \"PU\": {}},\n        \"without_beam_beam\": {\"lumi\": {}, \"PU\": {}},\n    }\n    for beam_beam in [\"with_beam_beam\", \"without_beam_beam\"]:\n        for ip in [1, 2, 5, 8]:\n            dic_lumi_PU_str[beam_beam][\"lumi\"][ip] = get_luminosity_at_ip_str(\n                dataframe_data, ip, beam_beam=True\n            )\n            dic_lumi_PU_str[beam_beam][\"PU\"][ip] = get_PU_at_IP_str(\n                dataframe_data, ip, beam_beam=True\n            )\n\n    def test_if_empty_and_add_period(string: str) -&gt; str:\n        \"\"\"\n        Test if a string is empty and add a period if not.\n\n        Args:\n            string (str): The string to test.\n\n        Returns:\n            str: The string with a period if not empty.\n        \"\"\"\n        return f\"{string}. \" if string != \"\" else \"\"\n\n    # Make the final title (order is the same as in the past)\n    title = \"\"\n    if display_LHC_version:\n        title += test_if_empty_and_add_period(LHC_version_str)\n    if display_energy:\n        title += test_if_empty_and_add_period(energy_str)\n    if display_CC_crossing:\n        title += test_if_empty_and_add_period(CC_crossing_str)\n    if display_bunch_intensity:\n        title += test_if_empty_and_add_period(bunch_intensity_str)\n    # Jump to the next line\n    title += \"\\n\"\n    if display_luminosity_1:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"lumi\"][1])\n    if display_PU_1:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"PU\"][1])\n    if display_luminosity_5:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"lumi\"][5])\n    if display_PU_5:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"PU\"][5])\n    # Jump to the next line\n    title += \"\\n\"\n    if display_luminosity_2:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"lumi\"][2])\n    if display_PU_2:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"PU\"][2])\n    if display_luminosity_8:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"lumi\"][8])\n    if display_PU_8:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"PU\"][8])\n    # Jump to the next line\n    title += \"\\n\"\n    if display_beta:\n        title += test_if_empty_and_add_period(beta_str)\n    if display_polarity_IP_2_8:\n        title += test_if_empty_and_add_period(polarity_str)\n    if display_bunch_length:\n        title += test_if_empty_and_add_period(bunch_length_str)\n    # Jump to the next line\n    title += \"\\n\"\n    if display_crossing_IP_1:\n        title += test_if_empty_and_add_period(xing_IP1_str)\n    if display_crossing_IP_5:\n        title += test_if_empty_and_add_period(xing_IP5_str)\n    if display_crossing_IP_2:\n        title += test_if_empty_and_add_period(xing_IP2_str)\n    if display_crossing_IP_8:\n        title += test_if_empty_and_add_period(xing_IP8_str)\n\n    # Jump to the next line\n    title += \"\\n\"\n    if display_emittance:\n        title += test_if_empty_and_add_period(emittance_str)\n    if display_chromaticity:\n        title += test_if_empty_and_add_period(chromaticity_str)\n    if display_octupole_intensity:\n        title += test_if_empty_and_add_period(octupole_intensity_str)\n    if display_coupling:\n        title += test_if_empty_and_add_period(coupling_str)\n    if display_tune:\n        title += test_if_empty_and_add_period(tune_str)\n    # Jump to the next line\n    title += \"\\n\"\n    if display_filling_scheme:\n        title += test_if_empty_and_add_period(filling_scheme_str)\n    if display_bunch_index:\n        title += test_if_empty_and_add_period(bunch_index_str)\n    # Jump to the next line\n    if display_number_of_turns:\n        title += \"\\n\"\n        title += test_if_empty_and_add_period(n_turns_str)\n\n    # Filter final title for empty lines\n    title = \"\\n\".join([line for line in title.split(\"\\n\") if line.strip() != \"\"])\n\n    return title\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.plot_3D","title":"<code>plot_3D(dataframe_data, x_variable, y_variable, z_variable, color_variable, xlabel=None, ylabel=None, z_label=None, title='', vmin=4.5, vmax=7.5, surface_count=30, opacity=0.2, figsize=(1000, 1000), colormap='RdBu', colorbar_title_text='Minimum DA (\u03c3)', display_colormap=False, output_path='output.png', output_path_html='output.html', display_plot=True, dark_theme=False)</code>","text":"<p>Plots a 3D volume rendering from the given dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing the data to plot.</p> required <code>x_variable</code> <code>str</code> <p>The variable to plot on the x-axis.</p> required <code>y_variable</code> <code>str</code> <p>The variable to plot on the y-axis.</p> required <code>z_variable</code> <code>str</code> <p>The variable to plot on the z-axis.</p> required <code>color_variable</code> <code>str</code> <p>The variable to use for the color scale.</p> required <code>xlabel</code> <code>Optional[str]</code> <p>The label for the x-axis. Defaults to None.</p> <code>None</code> <code>ylabel</code> <code>Optional[str]</code> <p>The label for the y-axis. Defaults to None.</p> <code>None</code> <code>z_label</code> <code>Optional[str]</code> <p>The label for the z-axis. Defaults to None.</p> <code>None</code> <code>title</code> <code>str</code> <p>The title of the plot. Defaults to \"\".</p> <code>''</code> <code>vmin</code> <code>float</code> <p>The minimum value for the color scale. Defaults to 4.5.</p> <code>4.5</code> <code>vmax</code> <code>float</code> <p>The maximum value for the color scale. Defaults to 7.5.</p> <code>7.5</code> <code>surface_count</code> <code>int</code> <p>The number of surfaces for volume rendering. Defaults to 30.</p> <code>30</code> <code>opacity</code> <code>float</code> <p>The opacity of the volume rendering. Defaults to 0.2.</p> <code>0.2</code> <code>figsize</code> <code>tuple[float, float]</code> <p>The size of the figure. Defaults to (1000, 1000).</p> <code>(1000, 1000)</code> <code>colormap</code> <code>str</code> <p>The colormap to use. Defaults to \"RdBu\".</p> <code>'RdBu'</code> <code>colorbar_title_text</code> <code>str</code> <p>The label for the colorbar. Defaults to \"Minimum DA (\u03c3)\".</p> <code>'Minimum DA (\u03c3)'</code> <code>display_colormap</code> <code>bool</code> <p>Whether to display the colormap. Defaults to False.</p> <code>False</code> <code>output_path</code> <code>str</code> <p>The path to save the plot image. Defaults to \"output.png\".</p> <code>'output.png'</code> <code>output_path_html</code> <code>str</code> <p>The path to save the plot HTML. Defaults to \"output.html\".</p> <code>'output.html'</code> <code>display_plot</code> <code>bool</code> <p>Whether to display the plot. Defaults to True.</p> <code>True</code> <code>dark_theme</code> <code>bool</code> <p>Whether to use a dark theme. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>go.Figure: The plotly figure object.</p> Source code in <code>study_da/plot/plot_study.py</code> <pre><code>def plot_3D(\n    dataframe_data: pd.DataFrame,\n    x_variable: str,\n    y_variable: str,\n    z_variable: str,\n    color_variable: str,\n    xlabel: Optional[str] = None,\n    ylabel: Optional[str] = None,\n    z_label: Optional[str] = None,\n    title: str = \"\",\n    vmin: float = 4.5,\n    vmax: float = 7.5,\n    surface_count: int = 30,\n    opacity: float = 0.2,\n    figsize: tuple[float, float] = (1000, 1000),\n    colormap: str = \"RdBu\",\n    colorbar_title_text: str = \"Minimum DA (\u03c3)\",\n    display_colormap: bool = False,\n    output_path: str = \"output.png\",\n    output_path_html: str = \"output.html\",\n    display_plot: bool = True,\n    dark_theme: bool = False,\n) -&gt; Any:\n    \"\"\"\n    Plots a 3D volume rendering from the given dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing the data to plot.\n        x_variable (str): The variable to plot on the x-axis.\n        y_variable (str): The variable to plot on the y-axis.\n        z_variable (str): The variable to plot on the z-axis.\n        color_variable (str): The variable to use for the color scale.\n        xlabel (Optional[str], optional): The label for the x-axis. Defaults to None.\n        ylabel (Optional[str], optional): The label for the y-axis. Defaults to None.\n        z_label (Optional[str], optional): The label for the z-axis. Defaults to None.\n        title (str, optional): The title of the plot. Defaults to \"\".\n        vmin (float, optional): The minimum value for the color scale. Defaults to 4.5.\n        vmax (float, optional): The maximum value for the color scale. Defaults to 7.5.\n        surface_count (int, optional): The number of surfaces for volume rendering. Defaults to 30.\n        opacity (float, optional): The opacity of the volume rendering. Defaults to 0.2.\n        figsize (tuple[float, float], optional): The size of the figure. Defaults to (1000, 1000).\n        colormap (str, optional): The colormap to use. Defaults to \"RdBu\".\n        colorbar_title_text (str, optional): The label for the colorbar. Defaults to \"Minimum DA (\u03c3)\".\n        display_colormap (bool, optional): Whether to display the colormap. Defaults to False.\n        output_path (str, optional): The path to save the plot image. Defaults to \"output.png\".\n        output_path_html (str, optional): The path to save the plot HTML. Defaults to \"output.html\".\n        display_plot (bool, optional): Whether to display the plot. Defaults to True.\n        dark_theme (bool, optional): Whether to use a dark theme. Defaults to False.\n\n    Returns:\n        go.Figure: The plotly figure object.\n    \"\"\"\n    # Check if plotly is installed\n    try:\n        import plotly.graph_objects as go\n    except ImportError as e:\n        raise ImportError(\"Please install plotly to use this function\") from e\n\n    X = np.array(dataframe_data[x_variable])\n    Y = np.array(dataframe_data[y_variable])\n    Z = np.array(dataframe_data[z_variable])\n    values = np.array(dataframe_data[color_variable])\n    fig = go.Figure(\n        data=go.Volume(\n            x=X.flatten(),\n            y=Y.flatten(),\n            z=Z.flatten(),\n            value=values.flatten(),\n            isomin=vmin,\n            isomax=vmax,\n            opacity=opacity,  # needs to be small to see through all surfaces\n            surface_count=surface_count,  # needs to be a large number for good volume rendering\n            colorscale=colormap,\n            colorbar_title_text=colorbar_title_text,\n        )\n    )\n\n    fig.update_layout(\n        scene_xaxis_title_text=xlabel,\n        scene_yaxis_title_text=ylabel,\n        scene_zaxis_title_text=z_label,\n        title=title,\n    )\n\n    # Get a good initial view, dezoomed\n    fig.update_layout(scene_camera=dict(eye=dict(x=1.5, y=1.5, z=1.5)))\n\n    # Center the title\n    fig.update_layout(title_x=0.5, title_y=0.9, title_xanchor=\"center\", title_yanchor=\"top\")\n\n    # Specify the width and height of the figure\n    fig.update_layout(width=figsize[0], height=figsize[1])\n\n    # Remove margins and padding\n    fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\n\n    # Display the colormap\n    if not display_colormap:\n        fig.update_layout(coloraxis_showscale=False)\n        fig.update_traces(showscale=False)\n    else:\n        # Make colorbar smaller\n        fig.update_layout(coloraxis_colorbar=dict(thickness=10, len=0.5))\n\n    # Set the theme\n    if dark_theme:\n        fig.update_layout(template=\"plotly_dark\")\n\n    # Display/save/return the figure\n    if output_path is not None:\n        fig.write_image(output_path)\n\n    if output_path_html is not None:\n        fig.write_html(output_path_html)\n\n    if display_plot:\n        fig.show()\n\n    return fig\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.plot_heatmap","title":"<code>plot_heatmap(dataframe_data, horizontal_variable, vertical_variable, color_variable, link=None, position_qr='top-right', plot_contours=True, xlabel=None, ylabel=None, tick_interval=2, round_xticks=None, round_yticks=None, symmetric_missing=False, mask_lower_triangle=False, mask_upper_triangle=False, plot_diagonal_lines=True, shift_diagonal_lines=1, xaxis_ticks_on_top=True, title='', vmin=4.5, vmax=7.5, k_masking=-1, green_contour=6.0, min_level_contours=1, max_level_contours=15, delta_levels_contours=0.5, figsize=None, label_cbar='Minimum DA (' + '$\\\\sigma$' + ')', colormap='coolwarm_r', style='ggplot', output_path='output.png', display_plot=True, latex_fonts=True, vectorize=False, fill_missing_value_with=None, dpi=300)</code>","text":"<p>Plots a heatmap from the given dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing the data to plot.</p> required <code>horizontal_variable</code> <code>str</code> <p>The variable to plot on the horizontal axis.</p> required <code>vertical_variable</code> <code>str</code> <p>The variable to plot on the vertical axis.</p> required <code>color_variable</code> <code>str</code> <p>The variable to use for the color scale.</p> required <code>link</code> <code>Optional[str]</code> <p>A link to encode in a QR code. Defaults to None.</p> <code>None</code> <code>plot_contours</code> <code>bool</code> <p>Whether to plot contours. Defaults to True.</p> <code>True</code> <code>xlabel</code> <code>Optional[str]</code> <p>The label for the x-axis. Defaults to None.</p> <code>None</code> <code>ylabel</code> <code>Optional[str]</code> <p>The label for the y-axis. Defaults to None.</p> <code>None</code> <code>tick_interval</code> <code>int</code> <p>The interval for the ticks. Defaults to 2.</p> <code>2</code> <code>round_xticks</code> <code>Optional[int]</code> <p>The number of decimal places to round the x-ticks to. Defaults to None.</p> <code>None</code> <code>round_yticks</code> <code>Optional[int]</code> <p>The number of decimal places to round the y-ticks to. Defaults to None.</p> <code>None</code> <code>symmetric_missing</code> <code>bool</code> <p>Whether to make the matrix symmetric by replacing the lower triangle with the upper triangle. Defaults to False.</p> <code>False</code> <code>mask_lower_triangle</code> <code>bool</code> <p>Whether to mask the lower triangle. Defaults to False.</p> <code>False</code> <code>mask_upper_triangle</code> <code>bool</code> <p>Whether to mask the upper triangle. Defaults to False.</p> <code>False</code> <code>plot_diagonal_lines</code> <code>bool</code> <p>Whether to plot diagonal lines. Defaults to True.</p> <code>True</code> <code>shift_diagonal_lines</code> <code>int</code> <p>The shift for the diagonal lines. Defaults to 1.</p> <code>1</code> <code>xaxis_ticks_on_top</code> <code>bool</code> <p>Whether to place the x-axis ticks on top. Defaults to True.</p> <code>True</code> <code>title</code> <code>str</code> <p>The title of the plot. Defaults to \"\".</p> <code>''</code> <code>vmin</code> <code>float</code> <p>The minimum value for the color scale. Defaults to 4.5.</p> <code>4.5</code> <code>vmax</code> <code>float</code> <p>The maximum value for the color scale. Defaults to 7.5.</p> <code>7.5</code> <code>k_masking</code> <code>int</code> <p>The k parameter for masking. Defaults to -1.</p> <code>-1</code> <code>green_contour</code> <code>Optional[float]</code> <p>The value for the green contour line. Defaults to 6.0.</p> <code>6.0</code> <code>min_level_contours</code> <code>float</code> <p>The minimum level for the contours. Defaults to 1.</p> <code>1</code> <code>max_level_contours</code> <code>float</code> <p>The maximum level for the contours. Defaults to 15.</p> <code>15</code> <code>delta_levels_contours</code> <code>float</code> <p>The delta between contour levels. Defaults to 0.5.</p> <code>0.5</code> <code>figsize</code> <code>Optional[tuple[float, float]]</code> <p>The size of the figure. Defaults to None.</p> <code>None</code> <code>label_cbar</code> <code>str</code> <p>The label for the colorbar. Defaults to \"Minimum DA ($\\sigma$)\".</p> <code>'Minimum DA (' + '$\\\\sigma$' + ')'</code> <code>colormap</code> <code>str</code> <p>The colormap to use. Defaults to \"coolwarm_r\".</p> <code>'coolwarm_r'</code> <code>style</code> <code>str</code> <p>The style to use for the plot. Defaults to \"ggplot\".</p> <code>'ggplot'</code> <code>output_path</code> <code>str</code> <p>The path to save the plot. Defaults to \"output.pdf\".</p> <code>'output.png'</code> <code>display_plot</code> <code>bool</code> <p>Whether to display the plot. Defaults to True.</p> <code>True</code> <code>latex_fonts</code> <code>bool</code> <p>Whether to use LaTeX fonts. Defaults to True.</p> <code>True</code> <code>vectorize</code> <code>bool</code> <p>Whether to vectorize the plot. Defaults to False.</p> <code>False</code> <code>fill_missing_value_with</code> <code>Optional[str | float]</code> <p>The value to fill missing values with. Can be a number or 'interpolate'. Defaults to None.</p> <code>None</code> <code>dpi</code> <code>int</code> <p>The DPI for the plot. Defaults to 300.</p> <code>300</code> <p>Returns:</p> Type Description <code>tuple[Figure, Axes]</code> <p>tuple[plt.Figure, plt.Axes]: The figure and axes of the plot.</p> Source code in <code>study_da/plot/plot_study.py</code> <pre><code>def plot_heatmap(\n    dataframe_data: pd.DataFrame,\n    horizontal_variable: str,\n    vertical_variable: str,\n    color_variable: str,\n    link: Optional[str] = None,\n    position_qr: Optional[str] = \"top-right\",\n    plot_contours: bool = True,\n    xlabel: Optional[str] = None,\n    ylabel: Optional[str] = None,\n    tick_interval: int = 2,\n    round_xticks: Optional[int] = None,\n    round_yticks: Optional[int] = None,\n    symmetric_missing: bool = False,\n    mask_lower_triangle: bool = False,\n    mask_upper_triangle: bool = False,\n    plot_diagonal_lines: bool = True,\n    shift_diagonal_lines: int = 1,\n    xaxis_ticks_on_top: bool = True,\n    title: str = \"\",\n    vmin: float = 4.5,\n    vmax: float = 7.5,\n    k_masking: int = -1,\n    green_contour: Optional[float] = 6.0,\n    min_level_contours: float = 1,\n    max_level_contours: float = 15,\n    delta_levels_contours: float = 0.5,\n    figsize: Optional[tuple[float, float]] = None,\n    label_cbar: str = \"Minimum DA (\" + r\"$\\sigma$\" + \")\",\n    colormap: str = \"coolwarm_r\",\n    style: str = \"ggplot\",\n    output_path: str = \"output.png\",\n    display_plot: bool = True,\n    latex_fonts: bool = True,\n    vectorize: bool = False,\n    fill_missing_value_with: Optional[str | float] = None,\n    dpi=300,\n) -&gt; tuple[plt.Figure, plt.Axes]:\n    \"\"\"\n    Plots a heatmap from the given dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing the data to plot.\n        horizontal_variable (str): The variable to plot on the horizontal axis.\n        vertical_variable (str): The variable to plot on the vertical axis.\n        color_variable (str): The variable to use for the color scale.\n        link (Optional[str], optional): A link to encode in a QR code. Defaults to None.\n        plot_contours (bool, optional): Whether to plot contours. Defaults to True.\n        xlabel (Optional[str], optional): The label for the x-axis. Defaults to None.\n        ylabel (Optional[str], optional): The label for the y-axis. Defaults to None.\n        tick_interval (int, optional): The interval for the ticks. Defaults to 2.\n        round_xticks (Optional[int], optional): The number of decimal places to round the x-ticks to.\n            Defaults to None.\n        round_yticks (Optional[int], optional): The number of decimal places to round the y-ticks to.\n            Defaults to None.\n        symmetric_missing (bool, optional): Whether to make the matrix symmetric by replacing the\n            lower triangle with the upper triangle. Defaults to False.\n        mask_lower_triangle (bool, optional): Whether to mask the lower triangle. Defaults to False.\n        mask_upper_triangle (bool, optional): Whether to mask the upper triangle. Defaults to False.\n        plot_diagonal_lines (bool, optional): Whether to plot diagonal lines. Defaults to True.\n        shift_diagonal_lines (int, optional): The shift for the diagonal lines. Defaults to 1.\n        xaxis_ticks_on_top (bool, optional): Whether to place the x-axis ticks on top. Defaults to True.\n        title (str, optional): The title of the plot. Defaults to \"\".\n        vmin (float, optional): The minimum value for the color scale. Defaults to 4.5.\n        vmax (float, optional): The maximum value for the color scale. Defaults to 7.5.\n        k_masking (int, optional): The k parameter for masking. Defaults to -1.\n        green_contour (Optional[float], optional): The value for the green contour line. Defaults to 6.0.\n        min_level_contours (float, optional): The minimum level for the contours. Defaults to 1.\n        max_level_contours (float, optional): The maximum level for the contours. Defaults to 15.\n        delta_levels_contours (float, optional): The delta between contour levels. Defaults to 0.5.\n        figsize (Optional[tuple[float, float]], optional): The size of the figure. Defaults to None.\n        label_cbar (str, optional): The label for the colorbar. Defaults to \"Minimum DA ($\\sigma$)\".\n        colormap (str, optional): The colormap to use. Defaults to \"coolwarm_r\".\n        style (str, optional): The style to use for the plot. Defaults to \"ggplot\".\n        output_path (str, optional): The path to save the plot. Defaults to \"output.pdf\".\n        display_plot (bool, optional): Whether to display the plot. Defaults to True.\n        latex_fonts (bool, optional): Whether to use LaTeX fonts. Defaults to True.\n        vectorize (bool, optional): Whether to vectorize the plot. Defaults to False.\n        fill_missing_value_with (Optional[str | float], optional): The value to fill missing values\n            with. Can be a number or 'interpolate'. Defaults to None.\n        dpi (int, optional): The DPI for the plot. Defaults to 300.\n\n    Returns:\n        tuple[plt.Figure, plt.Axes]: The figure and axes of the plot.\n    \"\"\"\n    # Use the requested style\n    _set_style(style, latex_fonts, vectorize)\n\n    # Get the dataframe to plot\n    df_to_plot = dataframe_data.pivot(\n        index=vertical_variable, columns=horizontal_variable, values=color_variable\n    )\n\n    # Get numpy array from dataframe\n    data_array = df_to_plot.to_numpy(dtype=float)\n\n    # Replace NaNs with a value if requested\n    if fill_missing_value_with is not None:\n        if isinstance(fill_missing_value_with, (int, float)):\n            data_array[np.isnan(data_array)] = fill_missing_value_with\n        elif fill_missing_value_with == \"interpolate\":\n            # Interpolate missing values with griddata\n            x = np.arange(data_array.shape[1])\n            y = np.arange(data_array.shape[0])\n            xx, yy = np.meshgrid(x, y)\n            x = xx[~np.isnan(data_array)]\n            y = yy[~np.isnan(data_array)]\n            z = data_array[~np.isnan(data_array)]\n            data_array = griddata((x, y), z, (xx, yy), method=\"cubic\")\n\n    # Mask the lower or upper triangle (checks are done in the function)\n    data_array_masked, mask_main_array = _mask(\n        mask_lower_triangle, mask_upper_triangle, data_array, k_masking\n    )\n\n    # Define colormap and set NaNs to white\n    cmap = matplotlib.colormaps.get_cmap(colormap)\n    cmap.set_bad(\"w\")\n\n    # Build heatmap, with inverted y axis\n    fig, ax = plt.subplots()\n    if figsize is not None:\n        fig.set_size_inches(figsize)\n    im = ax.imshow(data_array_masked, cmap=cmap, vmin=vmin, vmax=vmax)\n    ax.invert_yaxis()\n\n    # Add text annotations\n    ax = _add_text_annotation(df_to_plot, data_array, ax, vmin, vmax)\n\n    # Smooth data for contours\n    mx = _smooth(data_array, symmetric_missing)\n\n    # Plot contours if requested\n    if plot_contours:\n        ax = _add_contours(\n            ax,\n            data_array,\n            mx,\n            green_contour,\n            min_level_contours,\n            max_level_contours,\n            delta_levels_contours,\n            mask_main_array,\n        )\n\n    if plot_diagonal_lines:\n        # Diagonal lines must be plotted after the contour lines, because of bug in matplotlib\n        # Shift might need to be adjusted\n        ax = _add_diagonal_lines(ax, shift=shift_diagonal_lines)\n\n    # Define title and axis labels\n    ax.set_title(\n        title,\n        fontsize=10,\n    )\n\n    # Set axis labels\n    ax = _set_labels(\n        ax,\n        df_to_plot,\n        data_array,\n        horizontal_variable,\n        vertical_variable,\n        xlabel,\n        ylabel,\n        xaxis_ticks_on_top,\n        tick_interval,\n        round_xticks,\n        round_yticks,\n    )\n\n    # Create colorbar\n    cbar = ax.figure.colorbar(im, ax=ax, fraction=0.026, pad=0.04)\n    cbar.ax.set_ylabel(label_cbar, rotation=90, va=\"bottom\", labelpad=15)\n\n    # Remove potential grid\n    plt.grid(visible=None)\n\n    # Add QR code with a link to the topright side (a bit experimental, might need adjustments)\n    if link is not None:\n        fig = add_QR_code(fig, link, position_qr)\n\n    # Save and potentially display the plot\n    if output_path is not None:\n        if output_path.endswith(\".pdf\") and not vectorize:\n            raise ValueError(\"Please set vectorize=True to save as PDF\")\n        elif not output_path.endswith(\".pdf\") and vectorize:\n            raise ValueError(\"Please set vectorize=False to save as PNG or JPG\")\n        plt.savefig(output_path, bbox_inches=\"tight\", dpi=dpi)\n\n    if display_plot:\n        plt.show()\n    return fig, ax\n</code></pre>"},{"location":"reference/study_da/study_da.html","title":"study_da","text":""},{"location":"reference/study_da/study_da.html#study_da.study_da.create","title":"<code>create(path_config_scan='config_scan.yaml', force_overwrite=False, dic_parameter_all_gen=None, dic_parameter_all_gen_naming=None, add_prefix_to_folder_names=False)</code>","text":"<p>Create a study based on the configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>path_config_scan</code> <code>str</code> <p>Path to the configuration file for the scan. Defaults to \"config_scan.yaml\".</p> <code>'config_scan.yaml'</code> <code>force_overwrite</code> <code>bool</code> <p>Flag to force overwrite the study. Defaults to False.</p> <code>False</code> <code>dic_parameter_all_gen</code> <code>Optional[dict[str, dict[str, Any]]]</code> <p>Dictionary of parameters for the scan, if not provided through the scan config. Defaults to None.</p> <code>None</code> <code>dic_parameter_all_gen_naming</code> <code>Optional[dict[str, dict[str, Any]]]</code> <p>Dictionary of parameters for the naming of the scan subfolders, if not provided through the scan config. Defaults to None.</p> <code>None</code> <code>add_prefix_to_folder_names</code> <code>bool</code> <p>Whether to add a prefix to the folder names. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[str, str]</code> <p>tuple[str, str]: The path to the tree file and the name of the main configuration file.</p> Source code in <code>study_da/study_da.py</code> <pre><code>def create(\n    path_config_scan: str = \"config_scan.yaml\",\n    force_overwrite: bool = False,\n    dic_parameter_all_gen: Optional[dict[str, dict[str, Any]]] = None,\n    dic_parameter_all_gen_naming: Optional[dict[str, dict[str, Any]]] = None,\n    add_prefix_to_folder_names: bool = False,\n) -&gt; tuple[str, str]:\n    \"\"\"\n    Create a study based on the configuration file.\n\n    Args:\n        path_config_scan (str, optional): Path to the configuration file for the scan.\n            Defaults to \"config_scan.yaml\".\n        force_overwrite (bool, optional): Flag to force overwrite the study. Defaults to False.\n        dic_parameter_all_gen (Optional[dict[str, dict[str, Any]]], optional): Dictionary of\n            parameters for the scan, if not provided through the scan config. Defaults to None.\n        dic_parameter_all_gen_naming (Optional[dict[str, dict[str, Any]]], optional): Dictionary of\n            parameters for the naming of the scan subfolders, if not provided through the scan\n            config. Defaults to None.\n        add_prefix_to_folder_names (bool, optional): Whether to add a prefix to the folder names.\n            Defaults to False.\n\n    Returns:\n        tuple[str, str]: The path to the tree file and the name of the main configuration file.\n    \"\"\"\n    logging.info(f\"Create study from configuration file: {path_config_scan}\")\n    study = GenerateScan(path_config=path_config_scan)\n    study.create_study(\n        force_overwrite=force_overwrite,\n        dic_parameter_all_gen=dic_parameter_all_gen,\n        dic_parameter_all_gen_naming=dic_parameter_all_gen_naming,\n        add_prefix_to_folder_names=add_prefix_to_folder_names,\n    )\n\n    # Get variables of interest for the submission\n    path_tree = study.path_tree\n    name_main_configuration = study.config[\"dependencies\"][\"main_configuration\"]\n\n    return path_tree, name_main_configuration\n</code></pre>"},{"location":"reference/study_da/study_da.html#study_da.study_da.create_single_job","title":"<code>create_single_job(name_main_configuration, name_executable_generation_1, name_executable_generation_2=None, name_executable_generation_3=None, name_study='single_job_study', force_overwrite=False)</code>","text":"<p>Create a single job study (not a parametric scan) with the specified configuration and executables. Limited to three generations.</p> <p>Parameters:</p> Name Type Description Default <code>name_main_configuration</code> <code>str</code> <p>The name of the main configuration file for the study.</p> required <code>name_executable_generation_1</code> <code>str</code> <p>The name of the executable for the first generation.</p> required <code>name_executable_generation_2</code> <code>Optional[str]</code> <p>The name of the executable for the second generation. Defaults to None.</p> <code>None</code> <code>name_executable_generation_3</code> <code>Optional[str]</code> <p>The name of the executable for the third generation. Defaults to None.</p> <code>None</code> <code>name_study</code> <code>str</code> <p>The name of the study. Defaults to \"single_job_study\".</p> <code>'single_job_study'</code> <code>force_overwrite</code> <code>bool</code> <p>Whether to force overwrite existing files. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The path to the tree file.</p> Source code in <code>study_da/study_da.py</code> <pre><code>def create_single_job(\n    name_main_configuration: str,\n    name_executable_generation_1: str,\n    name_executable_generation_2: Optional[str] = None,\n    name_executable_generation_3: Optional[str] = None,\n    name_study: str = \"single_job_study\",\n    force_overwrite: bool = False,\n) -&gt; str:\n    \"\"\"\n    Create a single job study (not a parametric scan) with the specified configuration and\n    executables. Limited to three generations.\n\n    Args:\n        name_main_configuration (str): The name of the main configuration file for the study.\n        name_executable_generation_1 (str): The name of the executable for the first generation.\n        name_executable_generation_2 (Optional[str], optional): The name of the executable for the\n            second generation. Defaults to None.\n        name_executable_generation_3 (Optional[str], optional): The name of the executable for the\n            third generation. Defaults to None.\n        name_study (str, optional): The name of the study. Defaults to \"single_job_study\".\n        force_overwrite (bool, optional): Whether to force overwrite existing files.\n            Defaults to False.\n\n    Returns:\n        str: The path to the tree file.\n    \"\"\"\n    # Generate the scan dictionnary\n    dic_scan = {\n        \"name\": name_study,\n        \"dependencies\": {\"main_configuration\": name_main_configuration},\n        \"structure\": {\n            \"generation_1\": {\n                \"executable\": name_executable_generation_1,\n            },\n        },\n    }\n\n    if name_executable_generation_2 is not None:\n        dic_scan[\"structure\"][\"generation_2\"] = {\n            \"executable\": name_executable_generation_2,\n        }\n\n    if name_executable_generation_3 is not None:\n        dic_scan[\"structure\"][\"generation_3\"] = {\n            \"executable\": name_executable_generation_3,\n        }\n\n    # Create the study\n    logging.info(f\"Create single job study: {name_study}\")\n    study = GenerateScan(dic_scan=dic_scan)\n    study.create_study(\n        force_overwrite=force_overwrite,\n    )\n\n    return study.path_tree\n</code></pre>"},{"location":"reference/study_da/study_da.html#study_da.study_da.submit","title":"<code>submit(path_tree, path_python_environment='', path_python_environment_container='', path_container_image=None, force_configure=False, dic_config_jobs=None, one_generation_at_a_time=False, keep_submit_until_done=False, wait_time=30, max_try=100, force_submit=False, dic_additional_commands_per_gen=None, dic_dependencies_per_gen=None, dic_copy_back_per_gen=None, name_config='config.yaml')</code>","text":"<p>Submits the jobs to the cluster. Note that copying back large files (e.g. json colliders) can trigger a throttling mechanism in AFS.</p> <p>The following arguments are only used for HTC jobs submission: - dic_additional_commands_per_gen - dic_dependencies_per_gen - dic_copy_back_per_gen - name_config</p> <p>Parameters:</p> Name Type Description Default <code>path_tree</code> <code>str</code> <p>The path to the tree file.</p> required <code>path_python_environment</code> <code>str</code> <p>The path to the python environment. Default to \"\".</p> <code>''</code> <code>path_python_environment_container</code> <code>str</code> <p>The path to the python environment in the container. Default to \"\".</p> <code>''</code> <code>path_container_image</code> <code>Optional[str]</code> <p>The path to the container image. Defaults to None.</p> <code>None</code> <code>force_configure</code> <code>bool</code> <p>Whether to force reconfiguration. Defaults to False.</p> <code>False</code> <code>dic_config_jobs</code> <code>Optional[dict[str, dict[str, Any]]]</code> <p>A dictionary containing the configuration of the jobs. Defaults to None.</p> <code>None</code> <code>one_generation_at_a_time</code> <code>bool</code> <p>Whether to submit one full generation at a time. Defaults to False.</p> <code>False</code> <code>keep_submit_until_done</code> <code>bool</code> <p>Whether to keep submitting jobs until all jobs are finished or failed. Defaults to False.</p> <code>False</code> <code>max_try</code> <code>int</code> <p>The maximum number of tries to submit a job. Defaults to 100.</p> <code>100</code> <code>force_submit</code> <code>bool</code> <p>If True, jobs are resubmitted even though they failed. Defaults to False.</p> <code>False</code> <code>wait_time</code> <code>float</code> <p>The wait time between submissions in minutes. Defaults to 30.</p> <code>30</code> <code>dic_additional_commands_per_gen</code> <code>dict[int, str]</code> <p>Additional commands per generation. Defaults to None.</p> <code>None</code> <code>dic_dependencies_per_gen</code> <code>dict[int, list[str]]</code> <p>Dependencies per generation. Only used when doing a HTC submission. Defaults to None.</p> <code>None</code> <code>dic_copy_back_per_gen</code> <code>Optional[dict[int, dict[str, bool]]]</code> <p>A dictionary containing the files to copy back per generation. Accepted keys are \"parquet\", \"yaml\", \"txt\", \"json\", \"zip\" and \"all\". Defaults to None, corresponding to copying back only \"light\" files, i.e. parquet, yaml and txt.</p> <code>None</code> <code>name_config</code> <code>str</code> <p>The name of the configuration file for the study. Defaults to \"config.yaml\".</p> <code>'config.yaml'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/study_da.py</code> <pre><code>def submit(\n    path_tree: str,\n    path_python_environment: str = \"\",\n    path_python_environment_container: str = \"\",\n    path_container_image: Optional[str] = None,\n    force_configure: bool = False,\n    dic_config_jobs: Optional[dict[str, dict[str, Any]]] = None,\n    one_generation_at_a_time: bool = False,\n    keep_submit_until_done: bool = False,\n    wait_time: float = 30,\n    max_try: int = 100,\n    force_submit: bool = False,\n    dic_additional_commands_per_gen: Optional[dict[int, str]] = None,\n    dic_dependencies_per_gen: Optional[dict[int, list[str]]] = None,\n    dic_copy_back_per_gen: Optional[dict[int, dict[str, bool]]] = None,\n    name_config: str = \"config.yaml\",\n) -&gt; None:\n    \"\"\"\n    Submits the jobs to the cluster. Note that copying back large files (e.g. json colliders)\n    can trigger a throttling mechanism in AFS.\n\n    The following arguments are only used for HTC jobs submission:\n    - dic_additional_commands_per_gen\n    - dic_dependencies_per_gen\n    - dic_copy_back_per_gen\n    - name_config\n\n    Args:\n        path_tree (str): The path to the tree file.\n        path_python_environment (str): The path to the python environment. Default to \"\".\n        path_python_environment_container (str): The path to the python environment in the\n            container. Default to \"\".\n        path_container_image (Optional[str], optional): The path to the container image.\n            Defaults to None.\n        force_configure (bool, optional): Whether to force reconfiguration. Defaults to False.\n        dic_config_jobs (Optional[dict[str, dict[str, Any]]], optional): A dictionary containing\n            the configuration of the jobs. Defaults to None.\n        one_generation_at_a_time (bool, optional): Whether to submit one full generation at a\n            time. Defaults to False.\n        keep_submit_until_done (bool, optional): Whether to keep submitting jobs until all jobs\n            are finished or failed. Defaults to False.\n        max_try (int, optional): The maximum number of tries to submit a job. Defaults to 100.\n        force_submit (bool, optional): If True, jobs are resubmitted even though they failed.\n            Defaults to False.\n        wait_time (float, optional): The wait time between submissions in minutes. Defaults to 30.\n        dic_additional_commands_per_gen (dict[int, str], optional): Additional commands per\n            generation. Defaults to None.\n        dic_dependencies_per_gen (dict[int, list[str]], optional): Dependencies per generation.\n            Only used when doing a HTC submission. Defaults to None.\n        dic_copy_back_per_gen (Optional[dict[int, dict[str, bool]]], optional): A dictionary\n            containing the files to copy back per generation. Accepted keys are \"parquet\",\n            \"yaml\", \"txt\", \"json\", \"zip\" and \"all\". Defaults to None, corresponding to copying\n            back only \"light\" files, i.e. parquet, yaml and txt.\n        name_config (str, optional): The name of the configuration file for the study.\n            Defaults to \"config.yaml\".\n\n    Returns:\n        None\n    \"\"\"\n    # Instantiate the study (does not affect already existing study)\n    study_sub = SubmitScan(\n        path_tree=path_tree,\n        path_python_environment=path_python_environment,\n        path_python_environment_container=path_python_environment_container,\n        path_container_image=path_container_image,\n    )\n\n    # Configure the jobs (will only configure if not already done)\n    study_sub.configure_jobs(force_configure=force_configure, dic_config_jobs=dic_config_jobs)\n\n    # Submit the jobs (only submit the jobs that are not already submitted or finished)\n    if keep_submit_until_done:\n        study_sub.keep_submit_until_done(\n            wait_time=wait_time,\n            max_try=max_try,\n            one_generation_at_a_time=one_generation_at_a_time,\n            dic_additional_commands_per_gen=dic_additional_commands_per_gen,\n            dic_dependencies_per_gen=dic_dependencies_per_gen,\n            dic_copy_back_per_gen=dic_copy_back_per_gen,\n            name_config=name_config,\n            force_submit=force_submit,\n        )\n    else:\n        study_sub.submit(\n            one_generation_at_a_time=one_generation_at_a_time,\n            dic_additional_commands_per_gen=dic_additional_commands_per_gen,\n            dic_dependencies_per_gen=dic_dependencies_per_gen,\n            dic_copy_back_per_gen=dic_copy_back_per_gen,\n            name_config=name_config,\n            force_submit=force_submit,\n        )\n</code></pre>"},{"location":"reference/study_da/assets/index.html","title":"assets","text":""},{"location":"reference/study_da/assets/template_scripts/index.html","title":"template_scripts","text":""},{"location":"reference/study_da/assets/template_scripts/generation_1.html","title":"generation_1","text":"<p>This is a template script for generation 1 of simulation study, in which ones generates a particle distribution and a collider from a MAD-X model.</p>"},{"location":"reference/study_da/assets/template_scripts/generation_2_level_by_nb.html","title":"generation_2_level_by_nb","text":"<p>This is a template script for generation 2 of simulation study, in which ones configures a a Xsuite collider (including luminosity levelling and beam-beam) and tracks a given particle distribution.</p>"},{"location":"reference/study_da/assets/template_scripts/generation_2_level_by_sep.html","title":"generation_2_level_by_sep","text":"<p>This is a template script for generation 2 of simulation study, in which ones configures a a Xsuite collider (including luminosity levelling and beam-beam) and tracks a given particle distribution.</p>"},{"location":"reference/study_da/generate/index.html","title":"generate","text":""},{"location":"reference/study_da/generate/index.html#study_da.generate.MadCollider","title":"<code>MadCollider</code>","text":"<p>MadCollider class is responsible for setting up and managing the collider environment using MAD-X and xsuite.</p> <p>Attributes:</p> Name Type Description <code>sanity_checks</code> <code>bool</code> <p>Flag to enable or disable sanity checks.</p> <code>links</code> <code>str</code> <p>Path to the links configuration.</p> <code>beam_config</code> <code>dict</code> <p>Configuration for the beam.</p> <code>optics</code> <code>str</code> <p>Path to the optics file.</p> <code>enable_imperfections</code> <code>bool</code> <p>Flag to enable or disable imperfections.</p> <code>enable_knob_synthesis</code> <code>bool</code> <p>Flag to enable or disable knob synthesis.</p> <code>rename_coupling_knobs</code> <code>bool</code> <p>Flag to enable or disable renaming of coupling knobs.</p> <code>pars_for_imperfections</code> <code>dict</code> <p>Parameters for imperfections.</p> <code>ver_lhc_run</code> <code>float | None</code> <p>Version of LHC run.</p> <code>ver_hllhc_optics</code> <code>float | None</code> <p>Version of HL-LHC optics.</p> <code>ions</code> <code>bool</code> <p>Flag to indicate if ions are used.</p> <code>phasing</code> <code>dict</code> <p>Phasing configuration.</p> <code>path_collider_file_for_configuration_as_output</code> <code>str</code> <p>Path to save the collider.</p> <code>compress</code> <code>bool</code> <p>Flag to enable or disable compression of collider file.</p> <p>Methods:</p> Name Description <code>ost</code> <p>Property to get the appropriate optics specific tools.</p> <code>prepare_mad_collider</code> <p>Prepares the MAD-X collider environment.</p> <code>build_collider</code> <p>Madx, mad_b4: Madx) -&gt; xt.Multiline: Builds the xsuite collider.</p> <code>activate_RF_and_twiss</code> <p>xt.Multiline) -&gt; None: Activates RF and performs twiss analysis.</p> <code>check_xsuite_lattices</code> <p>xt.Line) -&gt; None: Checks the xsuite lattices.</p> <code>write_collider_to_disk</code> <p>xt.Multiline) -&gt; None: Writes the collider to disk and optionally compresses it.</p> <code>clean_temporary_files</code> <p>Cleans up temporary files created during the process.</p> Source code in <code>study_da/generate/master_classes/mad_collider.py</code> <pre><code>class MadCollider:\n    \"\"\"\n    MadCollider class is responsible for setting up and managing the collider environment using\n    MAD-X and xsuite.\n\n    Attributes:\n        sanity_checks (bool): Flag to enable or disable sanity checks.\n        links (str): Path to the links configuration.\n        beam_config (dict): Configuration for the beam.\n        optics (str): Path to the optics file.\n        enable_imperfections (bool): Flag to enable or disable imperfections.\n        enable_knob_synthesis (bool): Flag to enable or disable knob synthesis.\n        rename_coupling_knobs (bool): Flag to enable or disable renaming of coupling knobs.\n        pars_for_imperfections (dict): Parameters for imperfections.\n        ver_lhc_run (float | None): Version of LHC run.\n        ver_hllhc_optics (float | None): Version of HL-LHC optics.\n        ions (bool): Flag to indicate if ions are used.\n        phasing (dict): Phasing configuration.\n        path_collider_file_for_configuration_as_output (str): Path to save the collider.\n        compress (bool): Flag to enable or disable compression of collider file.\n\n    Methods:\n        ost: Property to get the appropriate optics specific tools.\n        prepare_mad_collider() -&gt; tuple[Madx, Madx]: Prepares the MAD-X collider environment.\n        build_collider(mad_b1b2: Madx, mad_b4: Madx) -&gt; xt.Multiline: Builds the xsuite collider.\n        activate_RF_and_twiss(collider: xt.Multiline) -&gt; None: Activates RF and performs twiss analysis.\n        check_xsuite_lattices(line: xt.Line) -&gt; None: Checks the xsuite lattices.\n        write_collider_to_disk(collider: xt.Multiline) -&gt; None: Writes the collider to disk and\n            optionally compresses it.\n        clean_temporary_files() -&gt; None: Cleans up temporary files created during the process.\n    \"\"\"\n\n    def __init__(self, configuration: dict):\n        \"\"\"\n        Initializes the MadCollider class with the given configuration.\n\n        Args:\n            configuration (dict): A dictionary containing the following keys:\n                - sanity_checks (bool): Flag to enable or disable sanity checks.\n                - links (str): Path to the links configuration.\n                - beam_config (dict): Configuration for the beam.\n                - optics_file (str): Path to the optics file.\n                - enable_imperfections (bool): Flag to enable or disable imperfections.\n                - enable_knob_synthesis (bool): Flag to enable or disable knob synthesis.\n                - rename_coupling_knobs (bool): Flag to enable or disable renaming of coupling\n                    knobs.\n                - pars_for_imperfections (dict): Parameters for imperfections.\n                - ver_lhc_run (float | None): Version of the LHC run, if applicable.\n                - ver_hllhc_optics (float | None): Version of the HL-LHC optics, if applicable.\n                - ions (bool): Flag to indicate if ions are used.\n                - phasing (dict): Configuration for phasing.\n                - path_collider_file_for_configuration_as_output (str): Path to the collider.\n                - compress (bool): Flag to enable or disable compression.\n        \"\"\"\n        # Configuration variables\n        self.sanity_checks: bool = configuration[\"sanity_checks\"]\n        self.links: str = configuration[\"links\"]\n        self.beam_config: dict = configuration[\"beam_config\"]\n        self.optics: str = configuration[\"optics_file\"]\n        self.enable_imperfections: bool = configuration[\"enable_imperfections\"]\n        self.enable_knob_synthesis: bool = configuration[\"enable_knob_synthesis\"]\n        self.rename_coupling_knobs: bool = configuration[\"rename_coupling_knobs\"]\n        self.pars_for_imperfections: dict = configuration[\"pars_for_imperfections\"]\n        self.ver_lhc_run: float | None = configuration[\"ver_lhc_run\"]\n        self.ver_hllhc_optics: float | None = configuration[\"ver_hllhc_optics\"]\n        self.ions: bool = configuration[\"ions\"]\n        self.phasing: dict = configuration[\"phasing\"]\n\n        # Optics specific tools\n        self._ost = None\n\n        # Path to disk and compression\n        self.path_collider_file_for_configuration_as_output = configuration[\n            \"path_collider_file_for_configuration_as_output\"\n        ]\n        self.compress = configuration[\"compress\"]\n\n    @property\n    def ost(self) -&gt; Any:\n        \"\"\"\n        Determines and returns the appropriate optics-specific tools (OST) based on the\n        version of HLLHC optics or LHC run configuration.\n\n        Raises:\n            ValueError: If both `ver_hllhc_optics` and `ver_lhc_run` are defined.\n            ValueError: If no optics-specific tools are available for the given configuration.\n\n        Returns:\n            Any: The appropriate OST module based on the configuration.\n        \"\"\"\n        if self._ost is None:\n            # Check that version is well defined\n            if self.ver_hllhc_optics is not None and self.ver_lhc_run is not None:\n                raise ValueError(\"Only one of ver_hllhc_optics and ver_lhc_run can be defined\")\n\n            # Get the appropriate optics_specific_tools\n            if self.ver_hllhc_optics is not None:\n                match self.ver_hllhc_optics:\n                    case 1.6:\n                        self._ost = ost_hllhc16\n                    case 1.3:\n                        self._ost = ost_hllhc13\n                    case _:\n                        raise ValueError(\"No optics specific tools for this configuration\")\n            elif self.ver_lhc_run == 3.0:\n                self._ost = ost_runIII_ions if self.ions else ost_runIII\n            else:\n                raise ValueError(\"No optics specific tools for the provided configuration\")\n\n        return self._ost\n\n    def prepare_mad_collider(self) -&gt; tuple[Madx, Madx]:\n        # sourcery skip: extract-duplicate-method\n        \"\"\"\n        Prepares the MAD-X collider environment and sequences for beam 1/2 and beam 4.\n\n        This method performs the following steps:\n        1. Creates the MAD-X environment using the provided links.\n        2. Initializes MAD-X instances for beam 1/2 and beam 4 with respective command logs.\n        3. Builds the sequences for both beams using the provided beam configuration.\n        4. Applies the specified optics to the beam 1/2 sequence.\n        5. Optionally performs sanity checks on the beam 1/2 sequence by running TWISS and checking\n            the MAD-X lattices.\n        6. Applies the specified optics to the beam 4 sequence.\n        7. Optionally performs sanity checks on the beam 4 sequence by running TWISS and checking\n            the MAD-X lattices.\n\n        Returns:\n            tuple[Madx, Madx]: A tuple containing the MAD-X instances for beam 1/2 and beam 4.\n        \"\"\"\n        # Make mad environment\n        xm.make_mad_environment(links=self.links)\n\n        # Start mad\n        mad_b1b2 = Madx(command_log=\"mad_collider.log\")\n        mad_b4 = Madx(command_log=\"mad_b4.log\")\n\n        # Build sequences\n        self.ost.build_sequence(mad_b1b2, mylhcbeam=1, beam_config=self.beam_config)\n        self.ost.build_sequence(mad_b4, mylhcbeam=4, beam_config=self.beam_config)\n\n        # Apply optics (only for b1b2, b4 will be generated from b1b2)\n        self.ost.apply_optics(mad_b1b2, optics_file=self.optics)\n\n        if self.sanity_checks:\n            mad_b1b2.use(sequence=\"lhcb1\")\n            mad_b1b2.twiss()\n            self.ost.check_madx_lattices(mad_b1b2)\n            mad_b1b2.use(sequence=\"lhcb2\")\n            mad_b1b2.twiss()\n            self.ost.check_madx_lattices(mad_b1b2)\n\n        # Apply optics (only for b4, just for check)\n        self.ost.apply_optics(mad_b4, optics_file=self.optics)\n        if self.sanity_checks:\n            mad_b4.use(sequence=\"lhcb2\")\n            mad_b4.twiss()\n            # ! Investigate why this is failing for run III\n            try:\n                self.ost.check_madx_lattices(mad_b4)\n            except AssertionError:\n                logging.warning(\"Some sanity checks have failed during the madx lattice check\")\n\n        return mad_b1b2, mad_b4\n\n    def build_collider(self, mad_b1b2: Madx, mad_b4: Madx) -&gt; xt.Multiline:\n        \"\"\"\n        Build an xsuite collider using provided MAD-X sequences and configuration.\n\n        Parameters:\n        mad_b1b2 (Madx): MAD-X instance containing sequences for beam 1 and beam 2.\n        mad_b4 (Madx): MAD-X instance containing sequence for beam 4.\n\n        Returns:\n        xt.Multiline: Constructed xsuite collider.\n\n        Notes:\n        - Converts `ver_lhc_run` and `ver_hllhc_optics` to float if they are not None.\n        - Builds the xsuite collider with the specified sequences and configuration.\n        - Optionally performs sanity checks by computing Twiss parameters for beam 1 and beam 2.\n        \"\"\"\n        # Ensure proper types to avoid assert errors\n        if self.ver_lhc_run is not None:\n            self.ver_lhc_run = float(self.ver_lhc_run)\n        if self.ver_hllhc_optics is not None:\n            self.ver_hllhc_optics = float(self.ver_hllhc_optics)\n\n        # Build xsuite collider\n        collider = xlhc.build_xsuite_collider(\n            sequence_b1=mad_b1b2.sequence.lhcb1,\n            sequence_b2=mad_b1b2.sequence.lhcb2,\n            sequence_b4=mad_b4.sequence.lhcb2,\n            beam_config=self.beam_config,\n            enable_imperfections=self.enable_imperfections,\n            enable_knob_synthesis=self.enable_knob_synthesis,\n            rename_coupling_knobs=self.rename_coupling_knobs,\n            pars_for_imperfections=self.pars_for_imperfections,\n            ver_lhc_run=self.ver_lhc_run,\n            ver_hllhc_optics=self.ver_hllhc_optics,\n        )\n        collider.build_trackers()\n\n        if self.sanity_checks:\n            collider[\"lhcb1\"].twiss(method=\"4d\")\n            collider[\"lhcb2\"].twiss(method=\"4d\")\n\n        return collider\n\n    def activate_RF_and_twiss(self, collider: xt.Multiline) -&gt; None:\n        \"\"\"\n        Activates RF and Twiss parameters for the given collider.\n\n        This method sets the RF knobs for the collider using the values specified\n        in the `phasing` attribute. It also performs sanity checks on the collider\n        lattices if the `sanity_checks` attribute is set to True.\n\n        Args:\n            collider (xt.Multiline): The collider object to configure.\n\n        Returns:\n            None\n        \"\"\"\n        # Define a RF knobs\n        collider.vars[\"vrf400\"] = self.phasing[\"vrf400\"]\n        collider.vars[\"lagrf400.b1\"] = self.phasing[\"lagrf400.b1\"]\n        collider.vars[\"lagrf400.b2\"] = self.phasing[\"lagrf400.b2\"]\n\n        if self.sanity_checks:\n            for my_line in [\"lhcb1\", \"lhcb2\"]:\n                self.check_xsuite_lattices(collider[my_line])\n\n    def check_xsuite_lattices(self, line: xt.Line) -&gt; None:\n        \"\"\"\n        Check the Twiss parameters and tune values for a given xsuite Line object.\n\n        This method computes the Twiss parameters for the provided `line` using the\n        6-dimensional method with a specified matrix stability tolerance. It then\n        prints the Twiss results at all interaction points (IPs) and the horizontal\n        (Qx) and vertical (Qy) tune values.\n\n        Args:\n            line (xt.Line): The xsuite Line object for which to compute and display\n                            the Twiss parameters and tune values.\n\n        Returns:\n            None\n        \"\"\"\n        tw = line.twiss(method=\"6d\", matrix_stability_tol=100)\n        print(f\"--- Now displaying Twiss result at all IPS for line {line}---\")\n        print(tw.rows[\"ip.*\"])\n        # print qx and qy\n        print(f\"--- Now displaying Qx and Qy for line {line}---\")\n        print(tw.qx, tw.qy)\n\n    def write_collider_to_disk(self, collider: xt.Multiline) -&gt; None:\n        \"\"\"\n        Writes the collider object to disk in JSON format and optionally compresses it into a ZIP\n        file.\n\n        Args:\n            collider (xt.Multiline): The collider object to be saved.\n\n        Returns:\n            None\n\n        Raises:\n            OSError: If there is an issue creating the directory or writing the file.\n\n        Notes:\n            - The method ensures that the directory specified in\n                `self.path_collider_file_for_configuration_as_output` exists.\n            - If `self.compress` is True, the JSON file is compressed into a ZIP file to reduce\n                storage usage.\n        \"\"\"\n        # Save collider to json, creating the folder if it does not exist\n        if \"/\" in self.path_collider_file_for_configuration_as_output:\n            os.makedirs(self.path_collider_file_for_configuration_as_output, exist_ok=True)\n        collider.to_json(self.path_collider_file_for_configuration_as_output)\n\n        # Compress the collider file to zip to ease the load on afs\n        if self.compress:\n            compress_and_write(self.path_collider_file_for_configuration_as_output)\n\n    @staticmethod\n    def clean_temporary_files() -&gt; None:\n        \"\"\"\n        Remove all the temporary files created in the process of building the collider.\n\n        This function deletes the following files and directories:\n        - \"mad_collider.log\"\n        - \"mad_b4.log\"\n        - \"temp\" directory\n        - \"errors\"\n        - \"acc-models-lhc\"\n        \"\"\"\n        # Remove all the temporaty files created in the process of building collider\n        os.remove(\"mad_collider.log\")\n        os.remove(\"mad_b4.log\")\n        shutil.rmtree(\"temp\")\n        os.unlink(\"errors\")\n        os.unlink(\"acc-models-lhc\")\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.MadCollider.ost","title":"<code>ost: Any</code>  <code>property</code>","text":"<p>Determines and returns the appropriate optics-specific tools (OST) based on the version of HLLHC optics or LHC run configuration.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both <code>ver_hllhc_optics</code> and <code>ver_lhc_run</code> are defined.</p> <code>ValueError</code> <p>If no optics-specific tools are available for the given configuration.</p> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The appropriate OST module based on the configuration.</p>"},{"location":"reference/study_da/generate/index.html#study_da.generate.MadCollider.__init__","title":"<code>__init__(configuration)</code>","text":"<p>Initializes the MadCollider class with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>configuration</code> <code>dict</code> <p>A dictionary containing the following keys: - sanity_checks (bool): Flag to enable or disable sanity checks. - links (str): Path to the links configuration. - beam_config (dict): Configuration for the beam. - optics_file (str): Path to the optics file. - enable_imperfections (bool): Flag to enable or disable imperfections. - enable_knob_synthesis (bool): Flag to enable or disable knob synthesis. - rename_coupling_knobs (bool): Flag to enable or disable renaming of coupling     knobs. - pars_for_imperfections (dict): Parameters for imperfections. - ver_lhc_run (float | None): Version of the LHC run, if applicable. - ver_hllhc_optics (float | None): Version of the HL-LHC optics, if applicable. - ions (bool): Flag to indicate if ions are used. - phasing (dict): Configuration for phasing. - path_collider_file_for_configuration_as_output (str): Path to the collider. - compress (bool): Flag to enable or disable compression.</p> required Source code in <code>study_da/generate/master_classes/mad_collider.py</code> <pre><code>def __init__(self, configuration: dict):\n    \"\"\"\n    Initializes the MadCollider class with the given configuration.\n\n    Args:\n        configuration (dict): A dictionary containing the following keys:\n            - sanity_checks (bool): Flag to enable or disable sanity checks.\n            - links (str): Path to the links configuration.\n            - beam_config (dict): Configuration for the beam.\n            - optics_file (str): Path to the optics file.\n            - enable_imperfections (bool): Flag to enable or disable imperfections.\n            - enable_knob_synthesis (bool): Flag to enable or disable knob synthesis.\n            - rename_coupling_knobs (bool): Flag to enable or disable renaming of coupling\n                knobs.\n            - pars_for_imperfections (dict): Parameters for imperfections.\n            - ver_lhc_run (float | None): Version of the LHC run, if applicable.\n            - ver_hllhc_optics (float | None): Version of the HL-LHC optics, if applicable.\n            - ions (bool): Flag to indicate if ions are used.\n            - phasing (dict): Configuration for phasing.\n            - path_collider_file_for_configuration_as_output (str): Path to the collider.\n            - compress (bool): Flag to enable or disable compression.\n    \"\"\"\n    # Configuration variables\n    self.sanity_checks: bool = configuration[\"sanity_checks\"]\n    self.links: str = configuration[\"links\"]\n    self.beam_config: dict = configuration[\"beam_config\"]\n    self.optics: str = configuration[\"optics_file\"]\n    self.enable_imperfections: bool = configuration[\"enable_imperfections\"]\n    self.enable_knob_synthesis: bool = configuration[\"enable_knob_synthesis\"]\n    self.rename_coupling_knobs: bool = configuration[\"rename_coupling_knobs\"]\n    self.pars_for_imperfections: dict = configuration[\"pars_for_imperfections\"]\n    self.ver_lhc_run: float | None = configuration[\"ver_lhc_run\"]\n    self.ver_hllhc_optics: float | None = configuration[\"ver_hllhc_optics\"]\n    self.ions: bool = configuration[\"ions\"]\n    self.phasing: dict = configuration[\"phasing\"]\n\n    # Optics specific tools\n    self._ost = None\n\n    # Path to disk and compression\n    self.path_collider_file_for_configuration_as_output = configuration[\n        \"path_collider_file_for_configuration_as_output\"\n    ]\n    self.compress = configuration[\"compress\"]\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.MadCollider.activate_RF_and_twiss","title":"<code>activate_RF_and_twiss(collider)</code>","text":"<p>Activates RF and Twiss parameters for the given collider.</p> <p>This method sets the RF knobs for the collider using the values specified in the <code>phasing</code> attribute. It also performs sanity checks on the collider lattices if the <code>sanity_checks</code> attribute is set to True.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object to configure.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/mad_collider.py</code> <pre><code>def activate_RF_and_twiss(self, collider: xt.Multiline) -&gt; None:\n    \"\"\"\n    Activates RF and Twiss parameters for the given collider.\n\n    This method sets the RF knobs for the collider using the values specified\n    in the `phasing` attribute. It also performs sanity checks on the collider\n    lattices if the `sanity_checks` attribute is set to True.\n\n    Args:\n        collider (xt.Multiline): The collider object to configure.\n\n    Returns:\n        None\n    \"\"\"\n    # Define a RF knobs\n    collider.vars[\"vrf400\"] = self.phasing[\"vrf400\"]\n    collider.vars[\"lagrf400.b1\"] = self.phasing[\"lagrf400.b1\"]\n    collider.vars[\"lagrf400.b2\"] = self.phasing[\"lagrf400.b2\"]\n\n    if self.sanity_checks:\n        for my_line in [\"lhcb1\", \"lhcb2\"]:\n            self.check_xsuite_lattices(collider[my_line])\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.MadCollider.build_collider","title":"<code>build_collider(mad_b1b2, mad_b4)</code>","text":"<p>Build an xsuite collider using provided MAD-X sequences and configuration.</p> <p>Parameters: mad_b1b2 (Madx): MAD-X instance containing sequences for beam 1 and beam 2. mad_b4 (Madx): MAD-X instance containing sequence for beam 4.</p> <p>Returns: xt.Multiline: Constructed xsuite collider.</p> <p>Notes: - Converts <code>ver_lhc_run</code> and <code>ver_hllhc_optics</code> to float if they are not None. - Builds the xsuite collider with the specified sequences and configuration. - Optionally performs sanity checks by computing Twiss parameters for beam 1 and beam 2.</p> Source code in <code>study_da/generate/master_classes/mad_collider.py</code> <pre><code>def build_collider(self, mad_b1b2: Madx, mad_b4: Madx) -&gt; xt.Multiline:\n    \"\"\"\n    Build an xsuite collider using provided MAD-X sequences and configuration.\n\n    Parameters:\n    mad_b1b2 (Madx): MAD-X instance containing sequences for beam 1 and beam 2.\n    mad_b4 (Madx): MAD-X instance containing sequence for beam 4.\n\n    Returns:\n    xt.Multiline: Constructed xsuite collider.\n\n    Notes:\n    - Converts `ver_lhc_run` and `ver_hllhc_optics` to float if they are not None.\n    - Builds the xsuite collider with the specified sequences and configuration.\n    - Optionally performs sanity checks by computing Twiss parameters for beam 1 and beam 2.\n    \"\"\"\n    # Ensure proper types to avoid assert errors\n    if self.ver_lhc_run is not None:\n        self.ver_lhc_run = float(self.ver_lhc_run)\n    if self.ver_hllhc_optics is not None:\n        self.ver_hllhc_optics = float(self.ver_hllhc_optics)\n\n    # Build xsuite collider\n    collider = xlhc.build_xsuite_collider(\n        sequence_b1=mad_b1b2.sequence.lhcb1,\n        sequence_b2=mad_b1b2.sequence.lhcb2,\n        sequence_b4=mad_b4.sequence.lhcb2,\n        beam_config=self.beam_config,\n        enable_imperfections=self.enable_imperfections,\n        enable_knob_synthesis=self.enable_knob_synthesis,\n        rename_coupling_knobs=self.rename_coupling_knobs,\n        pars_for_imperfections=self.pars_for_imperfections,\n        ver_lhc_run=self.ver_lhc_run,\n        ver_hllhc_optics=self.ver_hllhc_optics,\n    )\n    collider.build_trackers()\n\n    if self.sanity_checks:\n        collider[\"lhcb1\"].twiss(method=\"4d\")\n        collider[\"lhcb2\"].twiss(method=\"4d\")\n\n    return collider\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.MadCollider.check_xsuite_lattices","title":"<code>check_xsuite_lattices(line)</code>","text":"<p>Check the Twiss parameters and tune values for a given xsuite Line object.</p> <p>This method computes the Twiss parameters for the provided <code>line</code> using the 6-dimensional method with a specified matrix stability tolerance. It then prints the Twiss results at all interaction points (IPs) and the horizontal (Qx) and vertical (Qy) tune values.</p> <p>Parameters:</p> Name Type Description Default <code>line</code> <code>Line</code> <p>The xsuite Line object for which to compute and display             the Twiss parameters and tune values.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/mad_collider.py</code> <pre><code>def check_xsuite_lattices(self, line: xt.Line) -&gt; None:\n    \"\"\"\n    Check the Twiss parameters and tune values for a given xsuite Line object.\n\n    This method computes the Twiss parameters for the provided `line` using the\n    6-dimensional method with a specified matrix stability tolerance. It then\n    prints the Twiss results at all interaction points (IPs) and the horizontal\n    (Qx) and vertical (Qy) tune values.\n\n    Args:\n        line (xt.Line): The xsuite Line object for which to compute and display\n                        the Twiss parameters and tune values.\n\n    Returns:\n        None\n    \"\"\"\n    tw = line.twiss(method=\"6d\", matrix_stability_tol=100)\n    print(f\"--- Now displaying Twiss result at all IPS for line {line}---\")\n    print(tw.rows[\"ip.*\"])\n    # print qx and qy\n    print(f\"--- Now displaying Qx and Qy for line {line}---\")\n    print(tw.qx, tw.qy)\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.MadCollider.clean_temporary_files","title":"<code>clean_temporary_files()</code>  <code>staticmethod</code>","text":"<p>Remove all the temporary files created in the process of building the collider.</p> <p>This function deletes the following files and directories: - \"mad_collider.log\" - \"mad_b4.log\" - \"temp\" directory - \"errors\" - \"acc-models-lhc\"</p> Source code in <code>study_da/generate/master_classes/mad_collider.py</code> <pre><code>@staticmethod\ndef clean_temporary_files() -&gt; None:\n    \"\"\"\n    Remove all the temporary files created in the process of building the collider.\n\n    This function deletes the following files and directories:\n    - \"mad_collider.log\"\n    - \"mad_b4.log\"\n    - \"temp\" directory\n    - \"errors\"\n    - \"acc-models-lhc\"\n    \"\"\"\n    # Remove all the temporaty files created in the process of building collider\n    os.remove(\"mad_collider.log\")\n    os.remove(\"mad_b4.log\")\n    shutil.rmtree(\"temp\")\n    os.unlink(\"errors\")\n    os.unlink(\"acc-models-lhc\")\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.MadCollider.prepare_mad_collider","title":"<code>prepare_mad_collider()</code>","text":"<p>Prepares the MAD-X collider environment and sequences for beam 1/2 and beam 4.</p> <p>This method performs the following steps: 1. Creates the MAD-X environment using the provided links. 2. Initializes MAD-X instances for beam 1/2 and beam 4 with respective command logs. 3. Builds the sequences for both beams using the provided beam configuration. 4. Applies the specified optics to the beam 1/2 sequence. 5. Optionally performs sanity checks on the beam 1/2 sequence by running TWISS and checking     the MAD-X lattices. 6. Applies the specified optics to the beam 4 sequence. 7. Optionally performs sanity checks on the beam 4 sequence by running TWISS and checking     the MAD-X lattices.</p> <p>Returns:</p> Type Description <code>tuple[Madx, Madx]</code> <p>tuple[Madx, Madx]: A tuple containing the MAD-X instances for beam 1/2 and beam 4.</p> Source code in <code>study_da/generate/master_classes/mad_collider.py</code> <pre><code>def prepare_mad_collider(self) -&gt; tuple[Madx, Madx]:\n    # sourcery skip: extract-duplicate-method\n    \"\"\"\n    Prepares the MAD-X collider environment and sequences for beam 1/2 and beam 4.\n\n    This method performs the following steps:\n    1. Creates the MAD-X environment using the provided links.\n    2. Initializes MAD-X instances for beam 1/2 and beam 4 with respective command logs.\n    3. Builds the sequences for both beams using the provided beam configuration.\n    4. Applies the specified optics to the beam 1/2 sequence.\n    5. Optionally performs sanity checks on the beam 1/2 sequence by running TWISS and checking\n        the MAD-X lattices.\n    6. Applies the specified optics to the beam 4 sequence.\n    7. Optionally performs sanity checks on the beam 4 sequence by running TWISS and checking\n        the MAD-X lattices.\n\n    Returns:\n        tuple[Madx, Madx]: A tuple containing the MAD-X instances for beam 1/2 and beam 4.\n    \"\"\"\n    # Make mad environment\n    xm.make_mad_environment(links=self.links)\n\n    # Start mad\n    mad_b1b2 = Madx(command_log=\"mad_collider.log\")\n    mad_b4 = Madx(command_log=\"mad_b4.log\")\n\n    # Build sequences\n    self.ost.build_sequence(mad_b1b2, mylhcbeam=1, beam_config=self.beam_config)\n    self.ost.build_sequence(mad_b4, mylhcbeam=4, beam_config=self.beam_config)\n\n    # Apply optics (only for b1b2, b4 will be generated from b1b2)\n    self.ost.apply_optics(mad_b1b2, optics_file=self.optics)\n\n    if self.sanity_checks:\n        mad_b1b2.use(sequence=\"lhcb1\")\n        mad_b1b2.twiss()\n        self.ost.check_madx_lattices(mad_b1b2)\n        mad_b1b2.use(sequence=\"lhcb2\")\n        mad_b1b2.twiss()\n        self.ost.check_madx_lattices(mad_b1b2)\n\n    # Apply optics (only for b4, just for check)\n    self.ost.apply_optics(mad_b4, optics_file=self.optics)\n    if self.sanity_checks:\n        mad_b4.use(sequence=\"lhcb2\")\n        mad_b4.twiss()\n        # ! Investigate why this is failing for run III\n        try:\n            self.ost.check_madx_lattices(mad_b4)\n        except AssertionError:\n            logging.warning(\"Some sanity checks have failed during the madx lattice check\")\n\n    return mad_b1b2, mad_b4\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.MadCollider.write_collider_to_disk","title":"<code>write_collider_to_disk(collider)</code>","text":"<p>Writes the collider object to disk in JSON format and optionally compresses it into a ZIP file.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object to be saved.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If there is an issue creating the directory or writing the file.</p> Notes <ul> <li>The method ensures that the directory specified in     <code>self.path_collider_file_for_configuration_as_output</code> exists.</li> <li>If <code>self.compress</code> is True, the JSON file is compressed into a ZIP file to reduce     storage usage.</li> </ul> Source code in <code>study_da/generate/master_classes/mad_collider.py</code> <pre><code>def write_collider_to_disk(self, collider: xt.Multiline) -&gt; None:\n    \"\"\"\n    Writes the collider object to disk in JSON format and optionally compresses it into a ZIP\n    file.\n\n    Args:\n        collider (xt.Multiline): The collider object to be saved.\n\n    Returns:\n        None\n\n    Raises:\n        OSError: If there is an issue creating the directory or writing the file.\n\n    Notes:\n        - The method ensures that the directory specified in\n            `self.path_collider_file_for_configuration_as_output` exists.\n        - If `self.compress` is True, the JSON file is compressed into a ZIP file to reduce\n            storage usage.\n    \"\"\"\n    # Save collider to json, creating the folder if it does not exist\n    if \"/\" in self.path_collider_file_for_configuration_as_output:\n        os.makedirs(self.path_collider_file_for_configuration_as_output, exist_ok=True)\n    collider.to_json(self.path_collider_file_for_configuration_as_output)\n\n    # Compress the collider file to zip to ease the load on afs\n    if self.compress:\n        compress_and_write(self.path_collider_file_for_configuration_as_output)\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.ParticlesDistribution","title":"<code>ParticlesDistribution</code>","text":"<p>ParticlesDistribution class to generate and manage particle distributions.</p> <p>Attributes:</p> Name Type Description <code>r_min</code> <code>int</code> <p>Minimum radial distance.</p> <code>r_max</code> <code>int</code> <p>Maximum radial distance.</p> <code>n_r</code> <code>int</code> <p>Number of radial points.</p> <code>n_angles</code> <code>int</code> <p>Number of angular points.</p> <code>n_split</code> <code>int</code> <p>Number of splits for parallelization.</p> <code>path_distribution_folder_output</code> <code>str</code> <p>Path to the folder where distributions will be saved.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>dict): Initializes the ParticlesDistribution with the given configuration.</p> <code>get_radial_list</code> <p>float | None = None, upper_crop: float | None = None) -&gt; np.ndarray: Generates a list of radial distances, optionally cropped.</p> <code>get_angular_list</code> <p>Generates a list of angular values.</p> <code>return_distribution_as_list</code> <p>bool = True, lower_crop: float | None = None, upper_crop: float | None) -&gt; list[np.ndarray]: Returns the particle distribution as a list of numpy arrays, optionally split for parallelization.</p> <code>write_particle_distribution_to_disk</code> <p>list[np.ndarray]) -&gt; list[str]: Writes the particle distribution to disk in Parquet format and returns the list of file paths.</p> Source code in <code>study_da/generate/master_classes/particles_distribution.py</code> <pre><code>class ParticlesDistribution:\n    \"\"\"\n    ParticlesDistribution class to generate and manage particle distributions.\n\n    Attributes:\n        r_min (int): Minimum radial distance.\n        r_max (int): Maximum radial distance.\n        n_r (int): Number of radial points.\n        n_angles (int): Number of angular points.\n        n_split (int): Number of splits for parallelization.\n        path_distribution_folder_output (str): Path to the folder where distributions will be saved.\n\n    Methods:\n        __init__(configuration: dict):\n            Initializes the ParticlesDistribution with the given configuration.\n\n        get_radial_list(lower_crop: float | None = None, upper_crop: float | None = None)\n            -&gt; np.ndarray:\n            Generates a list of radial distances, optionally cropped.\n\n        get_angular_list() -&gt; np.ndarray:\n            Generates a list of angular values.\n\n        return_distribution_as_list(split: bool = True, lower_crop: float | None = None,\n            upper_crop: float | None) -&gt; list[np.ndarray]:\n            Returns the particle distribution as a list of numpy arrays, optionally split for\n            parallelization.\n\n        write_particle_distribution_to_disk(ll_particles: list[np.ndarray]) -&gt; list[str]:\n            Writes the particle distribution to disk in Parquet format and returns the list of file\n            paths.\n    \"\"\"\n\n    def __init__(self, configuration: dict):\n        \"\"\"\n        Initialize the particle distribution with the given configuration.\n\n        Args:\n            configuration (dict): A dictionary containing the configuration parameters.\n                - r_min (int): Minimum radius value.\n                - r_max (int): Maximum radius value.\n                - n_r (int): Number of radius points.\n                - n_angles (int): Number of angle points.\n                - n_split (int): Number of splits for parallelization.\n                - path_distribution_folder_output (str): Path to the folder where the distribution will be\n                    saved.\n        \"\"\"\n        # Variables used to define the distribution\n        self.r_min: int = configuration[\"r_min\"]\n        self.r_max: int = configuration[\"r_max\"]\n        self.n_r: int = configuration[\"n_r\"]\n        self.n_angles: int = configuration[\"n_angles\"]\n\n        # Variables to split the distribution for parallelization\n        self.n_split: int = configuration[\"n_split\"]\n\n        # Variable to write the distribution to disk\n        self.path_distribution_folder_output: str = configuration[\"path_distribution_folder_output\"]\n\n    def get_radial_list(\n        self, lower_crop: float | None = None, upper_crop: float | None = None\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Generate a list of radial distances within specified bounds.\n\n        Args:\n            lower_crop (float | None): The lower bound to crop the radial distances.\n                If None, no lower cropping is applied. Defaults to None.\n            upper_crop (float | None): The upper bound to crop the radial distances.\n                If None, no upper cropping is applied. Defaults to None.\n\n        Returns:\n            np.ndarray: An array of radial distances within the specified bounds.\n        \"\"\"\n        radial_list = np.linspace(self.r_min, self.r_max, self.n_r, endpoint=False)\n        if upper_crop:\n            radial_list = radial_list[radial_list &lt;= 7.5]\n        if lower_crop:\n            radial_list = radial_list[radial_list &gt;= 2.5]\n        return radial_list\n\n    def get_angular_list(self) -&gt; np.ndarray:\n        \"\"\"\n        Generate a list of angular values.\n\n        This method creates a list of angular values ranging from 0 to 90 degrees,\n        excluding the first and last values. The number of angles generated is\n        determined by the instance variable `self.n_angles`.\n\n        Returns:\n            numpy.ndarray: An array of angular values.\n        \"\"\"\n        return np.linspace(0, 90, self.n_angles + 2)[1:-1]\n\n    def return_distribution_as_list(\n        self, split: bool = True, lower_crop: float | None = None, upper_crop: float | None = None\n    ) -&gt; list[np.ndarray]:\n        \"\"\"\n        Returns the particle distribution as a list of numpy arrays.\n\n        This method generates a particle distribution by creating a Cartesian product\n        of radial and angular lists. The resulting distribution can be optionally split\n        into multiple parts for parallel computation.\n\n        Args:\n            split (bool): If True, the distribution is split into multiple parts.\n                Defaults to True.\n            lower_crop (float | None): The lower bound for cropping the radial list.\n                If None, no lower cropping is applied. Defaults to None.\n            upper_crop (float | None): The upper bound for cropping the radial list.\n                If None, no upper cropping is applied. Defaults to None.\n\n        Returns:\n            list[np.ndarray]: A list of numpy arrays representing the particle distribution.\n                If `split` is True, the list contains multiple arrays for parallel computation.\n                Otherwise, the list contains a single array.\n        \"\"\"\n        # Get radial list and angular list\n        radial_list = self.get_radial_list(lower_crop=lower_crop, upper_crop=upper_crop)\n        angular_list = self.get_angular_list()\n\n        # Define particle distribution as a cartesian product of the radial and angular lists\n        l_particles = np.array(\n            [\n                (particle_id, ii[1], ii[0])\n                for particle_id, ii in enumerate(itertools.product(angular_list, radial_list))\n            ]\n        )\n\n        # Potentially split the distribution to parallelize the computation\n        if split:\n            return list(np.array_split(l_particles, self.n_split))\n\n        return [l_particles]\n\n    def write_particle_distribution_to_disk(\n        self, ll_particles: list[list[np.ndarray]]\n    ) -&gt; list[str]:\n        \"\"\"\n        Writes a list of particle distributions to disk in Parquet format.\n\n        Args:\n            ll_particles (list[list[np.ndarray]]): A list of particle distributions,\n                where each distribution is a list containing particle data.\n\n        Returns:\n            list[str]: A list of file paths where the particle distributions\n            have been saved.\n\n        The method creates a directory specified by `self.path_distribution_folder_output`\n        if it does not already exist. Each particle distribution is saved as a\n        Parquet file in this directory. The files are named sequentially using\n        a zero-padded index (e.g., '00.parquet', '01.parquet', etc.).\n        \"\"\"\n        # Define folder to store the distributions\n        os.makedirs(self.path_distribution_folder_output, exist_ok=True)\n\n        # Write the distribution to disk\n        l_path_files = []\n        for idx_chunk, l_particles in enumerate(ll_particles):\n            path_file = f\"{self.path_distribution_folder_output}/{idx_chunk:02}.parquet\"\n            pd.DataFrame(\n                l_particles,\n                columns=[\n                    \"particle_id\",\n                    \"normalized amplitude in xy-plane\",\n                    \"angle in xy-plane [deg]\",\n                ],\n            ).to_parquet(path_file)\n            l_path_files.append(path_file)\n\n        return l_path_files\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.ParticlesDistribution.__init__","title":"<code>__init__(configuration)</code>","text":"<p>Initialize the particle distribution with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>configuration</code> <code>dict</code> <p>A dictionary containing the configuration parameters. - r_min (int): Minimum radius value. - r_max (int): Maximum radius value. - n_r (int): Number of radius points. - n_angles (int): Number of angle points. - n_split (int): Number of splits for parallelization. - path_distribution_folder_output (str): Path to the folder where the distribution will be     saved.</p> required Source code in <code>study_da/generate/master_classes/particles_distribution.py</code> <pre><code>def __init__(self, configuration: dict):\n    \"\"\"\n    Initialize the particle distribution with the given configuration.\n\n    Args:\n        configuration (dict): A dictionary containing the configuration parameters.\n            - r_min (int): Minimum radius value.\n            - r_max (int): Maximum radius value.\n            - n_r (int): Number of radius points.\n            - n_angles (int): Number of angle points.\n            - n_split (int): Number of splits for parallelization.\n            - path_distribution_folder_output (str): Path to the folder where the distribution will be\n                saved.\n    \"\"\"\n    # Variables used to define the distribution\n    self.r_min: int = configuration[\"r_min\"]\n    self.r_max: int = configuration[\"r_max\"]\n    self.n_r: int = configuration[\"n_r\"]\n    self.n_angles: int = configuration[\"n_angles\"]\n\n    # Variables to split the distribution for parallelization\n    self.n_split: int = configuration[\"n_split\"]\n\n    # Variable to write the distribution to disk\n    self.path_distribution_folder_output: str = configuration[\"path_distribution_folder_output\"]\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.ParticlesDistribution.get_angular_list","title":"<code>get_angular_list()</code>","text":"<p>Generate a list of angular values.</p> <p>This method creates a list of angular values ranging from 0 to 90 degrees, excluding the first and last values. The number of angles generated is determined by the instance variable <code>self.n_angles</code>.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: An array of angular values.</p> Source code in <code>study_da/generate/master_classes/particles_distribution.py</code> <pre><code>def get_angular_list(self) -&gt; np.ndarray:\n    \"\"\"\n    Generate a list of angular values.\n\n    This method creates a list of angular values ranging from 0 to 90 degrees,\n    excluding the first and last values. The number of angles generated is\n    determined by the instance variable `self.n_angles`.\n\n    Returns:\n        numpy.ndarray: An array of angular values.\n    \"\"\"\n    return np.linspace(0, 90, self.n_angles + 2)[1:-1]\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.ParticlesDistribution.get_radial_list","title":"<code>get_radial_list(lower_crop=None, upper_crop=None)</code>","text":"<p>Generate a list of radial distances within specified bounds.</p> <p>Parameters:</p> Name Type Description Default <code>lower_crop</code> <code>float | None</code> <p>The lower bound to crop the radial distances. If None, no lower cropping is applied. Defaults to None.</p> <code>None</code> <code>upper_crop</code> <code>float | None</code> <p>The upper bound to crop the radial distances. If None, no upper cropping is applied. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: An array of radial distances within the specified bounds.</p> Source code in <code>study_da/generate/master_classes/particles_distribution.py</code> <pre><code>def get_radial_list(\n    self, lower_crop: float | None = None, upper_crop: float | None = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Generate a list of radial distances within specified bounds.\n\n    Args:\n        lower_crop (float | None): The lower bound to crop the radial distances.\n            If None, no lower cropping is applied. Defaults to None.\n        upper_crop (float | None): The upper bound to crop the radial distances.\n            If None, no upper cropping is applied. Defaults to None.\n\n    Returns:\n        np.ndarray: An array of radial distances within the specified bounds.\n    \"\"\"\n    radial_list = np.linspace(self.r_min, self.r_max, self.n_r, endpoint=False)\n    if upper_crop:\n        radial_list = radial_list[radial_list &lt;= 7.5]\n    if lower_crop:\n        radial_list = radial_list[radial_list &gt;= 2.5]\n    return radial_list\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.ParticlesDistribution.return_distribution_as_list","title":"<code>return_distribution_as_list(split=True, lower_crop=None, upper_crop=None)</code>","text":"<p>Returns the particle distribution as a list of numpy arrays.</p> <p>This method generates a particle distribution by creating a Cartesian product of radial and angular lists. The resulting distribution can be optionally split into multiple parts for parallel computation.</p> <p>Parameters:</p> Name Type Description Default <code>split</code> <code>bool</code> <p>If True, the distribution is split into multiple parts. Defaults to True.</p> <code>True</code> <code>lower_crop</code> <code>float | None</code> <p>The lower bound for cropping the radial list. If None, no lower cropping is applied. Defaults to None.</p> <code>None</code> <code>upper_crop</code> <code>float | None</code> <p>The upper bound for cropping the radial list. If None, no upper cropping is applied. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[ndarray]</code> <p>list[np.ndarray]: A list of numpy arrays representing the particle distribution. If <code>split</code> is True, the list contains multiple arrays for parallel computation. Otherwise, the list contains a single array.</p> Source code in <code>study_da/generate/master_classes/particles_distribution.py</code> <pre><code>def return_distribution_as_list(\n    self, split: bool = True, lower_crop: float | None = None, upper_crop: float | None = None\n) -&gt; list[np.ndarray]:\n    \"\"\"\n    Returns the particle distribution as a list of numpy arrays.\n\n    This method generates a particle distribution by creating a Cartesian product\n    of radial and angular lists. The resulting distribution can be optionally split\n    into multiple parts for parallel computation.\n\n    Args:\n        split (bool): If True, the distribution is split into multiple parts.\n            Defaults to True.\n        lower_crop (float | None): The lower bound for cropping the radial list.\n            If None, no lower cropping is applied. Defaults to None.\n        upper_crop (float | None): The upper bound for cropping the radial list.\n            If None, no upper cropping is applied. Defaults to None.\n\n    Returns:\n        list[np.ndarray]: A list of numpy arrays representing the particle distribution.\n            If `split` is True, the list contains multiple arrays for parallel computation.\n            Otherwise, the list contains a single array.\n    \"\"\"\n    # Get radial list and angular list\n    radial_list = self.get_radial_list(lower_crop=lower_crop, upper_crop=upper_crop)\n    angular_list = self.get_angular_list()\n\n    # Define particle distribution as a cartesian product of the radial and angular lists\n    l_particles = np.array(\n        [\n            (particle_id, ii[1], ii[0])\n            for particle_id, ii in enumerate(itertools.product(angular_list, radial_list))\n        ]\n    )\n\n    # Potentially split the distribution to parallelize the computation\n    if split:\n        return list(np.array_split(l_particles, self.n_split))\n\n    return [l_particles]\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.ParticlesDistribution.write_particle_distribution_to_disk","title":"<code>write_particle_distribution_to_disk(ll_particles)</code>","text":"<p>Writes a list of particle distributions to disk in Parquet format.</p> <p>Parameters:</p> Name Type Description Default <code>ll_particles</code> <code>list[list[ndarray]]</code> <p>A list of particle distributions, where each distribution is a list containing particle data.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of file paths where the particle distributions</p> <code>list[str]</code> <p>have been saved.</p> <p>The method creates a directory specified by <code>self.path_distribution_folder_output</code> if it does not already exist. Each particle distribution is saved as a Parquet file in this directory. The files are named sequentially using a zero-padded index (e.g., '00.parquet', '01.parquet', etc.).</p> Source code in <code>study_da/generate/master_classes/particles_distribution.py</code> <pre><code>def write_particle_distribution_to_disk(\n    self, ll_particles: list[list[np.ndarray]]\n) -&gt; list[str]:\n    \"\"\"\n    Writes a list of particle distributions to disk in Parquet format.\n\n    Args:\n        ll_particles (list[list[np.ndarray]]): A list of particle distributions,\n            where each distribution is a list containing particle data.\n\n    Returns:\n        list[str]: A list of file paths where the particle distributions\n        have been saved.\n\n    The method creates a directory specified by `self.path_distribution_folder_output`\n    if it does not already exist. Each particle distribution is saved as a\n    Parquet file in this directory. The files are named sequentially using\n    a zero-padded index (e.g., '00.parquet', '01.parquet', etc.).\n    \"\"\"\n    # Define folder to store the distributions\n    os.makedirs(self.path_distribution_folder_output, exist_ok=True)\n\n    # Write the distribution to disk\n    l_path_files = []\n    for idx_chunk, l_particles in enumerate(ll_particles):\n        path_file = f\"{self.path_distribution_folder_output}/{idx_chunk:02}.parquet\"\n        pd.DataFrame(\n            l_particles,\n            columns=[\n                \"particle_id\",\n                \"normalized amplitude in xy-plane\",\n                \"angle in xy-plane [deg]\",\n            ],\n        ).to_parquet(path_file)\n        l_path_files.append(path_file)\n\n    return l_path_files\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider","title":"<code>XsuiteCollider</code>","text":"<p>XsuiteCollider is a class designed to handle the configuration and manipulation of a collider using the Xsuite library. It provides methods to load, configure, and tune the collider, as well as to perform luminosity leveling and beam-beam interaction setup.</p> <p>Attributes:</p> Name Type Description <code>path_collider_file_for_configuration_as_input</code> <code>str</code> <p>Path to the collider file to load.</p> <code>config_beambeam</code> <code>dict</code> <p>Configuration for beam-beam interactions.</p> <code>config_knobs_and_tuning</code> <code>dict</code> <p>Configuration for knobs and tuning.</p> <code>config_lumi_leveling</code> <code>dict</code> <p>Configuration for luminosity leveling.</p> <code>config_lumi_leveling_ip1_5</code> <code>dict or None</code> <p>Configuration for luminosity leveling at IP1 and IP5.</p> <code>config_collider</code> <code>dict</code> <p>Configuration for the collider.</p> <code>ver_hllhc_optics</code> <code>float</code> <p>Version of the HL-LHC optics.</p> <code>ver_lhc_run</code> <code>float</code> <p>Version of the LHC run.</p> <code>ions</code> <code>bool</code> <p>Flag indicating if ions are used.</p> <code>_dict_orbit_correction</code> <code>dict or None</code> <p>Dictionary for orbit correction.</p> <code>_crab</code> <code>bool or None</code> <p>Flag indicating if crab cavities are used.</p> <code>save_output_collider</code> <code>bool</code> <p>Flag indicating if the final collider should be saved.</p> <code>path_collider_file_for_tracking_as_output</code> <code>str</code> <p>Path to save the final collider.</p> <p>Methods:</p> Name Description <code>dict_orbit_correction</code> <p>Property to get the dictionary for orbit correction.</p> <code>load_collider</code> <p>Loads the collider from a file.</p> <code>install_beam_beam_wrapper</code> <p>Installs beam-beam lenses in the collider.</p> <code>set_knobs</code> <p>Sets the knobs for the collider.</p> <code>match_tune_and_chroma</code> <p>Matches the tune and chromaticity of the collider.</p> <code>set_filling_and_bunch_tracked</code> <p>Sets the filling scheme and tracks the bunch.</p> <code>compute_collision_from_scheme</code> <p>Computes the number of collisions from the filling scheme.</p> <code>crab</code> <p>Property to get the crab cavities status.</p> <code>level_all_by_separation</code> <p>Levels all IPs by separation.</p> <code>level_ip1_5_by_bunch_intensity</code> <p>Levels IP1 and IP5 by bunch intensity.</p> <code>level_ip2_8_by_separation</code> <p>Levels IP2 and IP8 by separation.</p> <code>add_linear_coupling</code> <p>Adds linear coupling to the collider.</p> <code>assert_tune_chroma_coupling</code> <p>Asserts the tune, chromaticity, and coupling of the collider.</p> <code>configure_beam_beam</code> <p>Configures the beam-beam interactions.</p> <code>record_final_luminosity</code> <p>Records the final luminosity of the collider.</p> <code>write_collider_to_disk</code> <p>Writes the collider configuration to disk.</p> <code>update_configuration_knob</code> <p>Updates a specific knob in the collider.</p> <code>return_fingerprint</code> <p>Returns a fingerprint of the collider's configuration.</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>class XsuiteCollider:\n    \"\"\"\n    XsuiteCollider is a class designed to handle the configuration and manipulation of a collider\n    using the Xsuite library. It provides methods to load, configure, and tune the collider,\n    as well as to perform luminosity leveling and beam-beam interaction setup.\n\n    Attributes:\n        path_collider_file_for_configuration_as_input (str): Path to the collider file to load.\n        config_beambeam (dict): Configuration for beam-beam interactions.\n        config_knobs_and_tuning (dict): Configuration for knobs and tuning.\n        config_lumi_leveling (dict): Configuration for luminosity leveling.\n        config_lumi_leveling_ip1_5 (dict or None): Configuration for luminosity leveling at IP1 and\n            IP5.\n        config_collider (dict): Configuration for the collider.\n        ver_hllhc_optics (float): Version of the HL-LHC optics.\n        ver_lhc_run (float): Version of the LHC run.\n        ions (bool): Flag indicating if ions are used.\n        _dict_orbit_correction (dict or None): Dictionary for orbit correction.\n        _crab (bool or None): Flag indicating if crab cavities are used.\n        save_output_collider (bool): Flag indicating if the final collider should be saved.\n        path_collider_file_for_tracking_as_output (str): Path to save the final collider.\n\n    Methods:\n        dict_orbit_correction: Property to get the dictionary for orbit correction.\n        load_collider: Loads the collider from a file.\n        install_beam_beam_wrapper: Installs beam-beam lenses in the collider.\n        set_knobs: Sets the knobs for the collider.\n        match_tune_and_chroma: Matches the tune and chromaticity of the collider.\n        set_filling_and_bunch_tracked: Sets the filling scheme and tracks the bunch.\n        compute_collision_from_scheme: Computes the number of collisions from the filling scheme.\n        crab: Property to get the crab cavities status.\n        level_all_by_separation: Levels all IPs by separation.\n        level_ip1_5_by_bunch_intensity: Levels IP1 and IP5 by bunch intensity.\n        level_ip2_8_by_separation: Levels IP2 and IP8 by separation.\n        add_linear_coupling: Adds linear coupling to the collider.\n        assert_tune_chroma_coupling: Asserts the tune, chromaticity, and coupling of the collider.\n        configure_beam_beam: Configures the beam-beam interactions.\n        record_final_luminosity: Records the final luminosity of the collider.\n        write_collider_to_disk: Writes the collider configuration to disk.\n        update_configuration_knob: Updates a specific knob in the collider.\n        return_fingerprint: Returns a fingerprint of the collider's configuration.\n    \"\"\"\n\n    def __init__(\n        self,\n        configuration: dict,\n        path_collider_file_for_configuration_as_input: str,\n        ver_hllhc_optics: float,\n        ver_lhc_run: float,\n        ions: bool,\n    ):\n        \"\"\"\n        Initialize the XsuiteCollider class with the given configuration and parameters.\n\n        Args:\n            configuration (dict): A dictionary containing various configuration settings.\n                - config_beambeam (dict): Configuration for beam-beam interactions.\n                - config_knobs_and_tuning (dict): Configuration for knobs and tuning.\n                - config_lumi_leveling (dict): Configuration for luminosity leveling.\n                - save_output_collider (bool): Flag to save the final collider to disk.\n                - path_collider_file_for_tracking_as_output (str): Path to save the final collider.\n                - config_lumi_leveling_ip1_5 (optional): Configuration for luminosity leveling at\n                    IP1 and IP5.\n            path_collider_file_for_configuration_as_input (str): Path to the collider file.\n            ver_hllhc_optics (float): Version of the HL-LHC optics.\n            ver_lhc_run (float): Version of the LHC run.\n            ions (bool): Flag indicating if ions are used.\n        \"\"\"\n        # Collider file path\n        self.path_collider_file_for_configuration_as_input = (\n            path_collider_file_for_configuration_as_input\n        )\n\n        # Configuration variables\n        self.config_beambeam: dict[str, Any] = configuration[\"config_beambeam\"]\n        self.config_knobs_and_tuning: dict[str, Any] = configuration[\"config_knobs_and_tuning\"]\n        self.config_lumi_leveling: dict[str, Any] = configuration[\"config_lumi_leveling\"]\n\n        # self.config_lumi_leveling_ip1_5 will be None if not present in the configuration\n        self.config_lumi_leveling_ip1_5: dict[str, Any] = configuration.get(\n            \"config_lumi_leveling_ip1_5\"\n        )\n\n        # Collider configuration\n        self.config_collider: dict[str, Any] = configuration\n\n        # Optics version (needed to select the appropriate optics specific functions)\n        self.ver_hllhc_optics: float = ver_hllhc_optics\n        self.ver_lhc_run: float = ver_lhc_run\n        self.ions: bool = ions\n        self._dict_orbit_correction: dict | None = None\n\n        # Crab cavities\n        self._crab: bool | None = None\n\n        # Save collider to disk\n        self.save_output_collider = configuration[\"save_output_collider\"]\n        self.path_collider_file_for_tracking_as_output = configuration[\n            \"path_collider_file_for_tracking_as_output\"\n        ]\n        self.compress = configuration[\"compress\"]\n\n    @property\n    def dict_orbit_correction(self) -&gt; dict:\n        \"\"\"\n        Generates and returns a dictionary containing orbit correction parameters.\n\n        This method checks if the orbit correction dictionary has already been generated.\n        If not, it determines the appropriate set of orbit correction parameters based on\n        the version of HLLHC optics or LHC run provided.\n\n        Returns:\n            dict: A dictionary containing orbit correction parameters.\n\n        Raises:\n            ValueError: If both `ver_hllhc_optics` and `ver_lhc_run` are defined.\n            ValueError: If no optics specific tools are available for the provided configuration.\n        \"\"\"\n        if self._dict_orbit_correction is None:\n            # Check that version is well defined\n            if self.ver_hllhc_optics is not None and self.ver_lhc_run is not None:\n                raise ValueError(\"Only one of ver_hllhc_optics and ver_lhc_run can be defined\")\n\n            # Get the appropriate optics_specific_tools\n            if self.ver_hllhc_optics is not None:\n                match self.ver_hllhc_optics:\n                    case 1.6:\n                        self._dict_orbit_correction = gen_corr_hllhc16()\n                    case 1.3:\n                        self._dict_orbit_correction = gen_corr_hllhc13()\n                    case _:\n                        raise ValueError(\"No optics specific tools for this configuration\")\n            elif self.ver_lhc_run == 3.0:\n                self._dict_orbit_correction = (\n                    gen_corr_runIII_ions() if self.ions else gen_corr_runIII()\n                )\n            else:\n                raise ValueError(\"No optics specific tools for the provided configuration\")\n\n        return self._dict_orbit_correction\n\n    @staticmethod\n    def _load_collider(path_collider) -&gt; xt.Multiline:\n        \"\"\"\n        Load a collider configuration from a file using an external path.\n\n        If the file path ends with \".zip\", the file is uncompressed locally\n        and the collider configuration is loaded from the uncompressed file.\n        Otherwise, the collider configuration is loaded directly from the file.\n\n        Returns:\n            xt.Multiline: The loaded collider configuration.\n        \"\"\"\n\n        # Correct collider file path if it is a zip file\n        if os.path.exists(f\"{path_collider}.zip\") and not path_collider.endswith(\".zip\"):\n            path_collider += \".zip\"\n\n        # Load as a json if not zip\n        if not path_collider.endswith(\".zip\"):\n            return xt.Multiline.from_json(path_collider)\n\n        # Uncompress file locally\n        logging.info(f\"Unzipping {path_collider}\")\n        with ZipFile(path_collider, \"r\") as zip_ref:\n            zip_ref.extractall()\n        final_path = os.path.basename(path_collider).replace(\".zip\", \"\")\n        return xt.Multiline.from_json(final_path)\n\n    def load_collider(self) -&gt; xt.Multiline:\n        \"\"\"\n        Load a collider configuration from a file.\n\n        If the file path ends with \".zip\", the file is uncompressed locally\n        and the collider configuration is loaded from the uncompressed file.\n        Otherwise, the collider configuration is loaded directly from the file.\n\n        Returns:\n            xt.Multiline: The loaded collider configuration.\n        \"\"\"\n        return self._load_collider(self.path_collider_file_for_configuration_as_input)\n\n    def install_beam_beam_wrapper(self, collider: xt.Multiline) -&gt; None:\n        \"\"\"\n        This method installs beam-beam interactions in the collider with the specified\n        parameters. The beam-beam lenses are initially inactive and not configured.\n\n        Args:\n            collider (xt.Multiline): The collider object where the beam-beam interactions\n                will be installed.\n\n        Returns:\n            None\n        \"\"\"\n        # Install beam-beam lenses (inactive and not configured)\n        collider.install_beambeam_interactions(\n            clockwise_line=\"lhcb1\",\n            anticlockwise_line=\"lhcb2\",\n            ip_names=[\"ip1\", \"ip2\", \"ip5\", \"ip8\"],\n            delay_at_ips_slots=[0, 891, 0, 2670],\n            num_long_range_encounters_per_side=self.config_beambeam[\n                \"num_long_range_encounters_per_side\"\n            ],\n            num_slices_head_on=self.config_beambeam[\"num_slices_head_on\"],\n            harmonic_number=35640,\n            bunch_spacing_buckets=self.config_beambeam[\"bunch_spacing_buckets\"],\n            sigmaz=self.config_beambeam[\"sigma_z\"],\n        )\n\n    def set_knobs(self, collider: xt.Multiline) -&gt; None:\n        \"\"\"\n        Set all knobs for the collider, including crossing angles, dispersion correction,\n        RF, crab cavities, experimental magnets, etc.\n\n        Args:\n            collider (xt.Multiline): The collider object to which the knob settings will be applied.\n\n        Returns:\n            None\n        \"\"\"\n        # Set all knobs (crossing angles, dispersion correction, rf, crab cavities,\n        # experimental magnets, etc.)\n        for kk, vv in self.config_knobs_and_tuning[\"knob_settings\"].items():\n            collider.vars[kk] = vv\n\n        # Crab fix (if needed)\n        if self.ver_hllhc_optics is not None and self.ver_hllhc_optics == 1.3:\n            apply_crab_fix(collider, self.config_knobs_and_tuning)\n\n    def match_tune_and_chroma(\n        self, collider: xt.Multiline, match_linear_coupling_to_zero: bool = True\n    ) -&gt; None:\n        \"\"\"\n        This method adjusts the tune and chromaticity of the specified collider lines\n        (\"lhcb1\" and \"lhcb2\") to the target values defined in the configuration. It also\n        optionally matches the linear coupling to zero.\n\n        Args:\n            collider (xt.Multiline): The collider object containing the lines to be tuned.\n            match_linear_coupling_to_zero (bool, optional): If True, linear coupling will be\n                matched to zero. Defaults to True.\n\n        Returns:\n            None\n        \"\"\"\n        for line_name in [\"lhcb1\", \"lhcb2\"]:\n            knob_names = self.config_knobs_and_tuning[\"knob_names\"][line_name]\n\n            targets = {\n                \"qx\": self.config_knobs_and_tuning[\"qx\"][line_name],\n                \"qy\": self.config_knobs_and_tuning[\"qy\"][line_name],\n                \"dqx\": self.config_knobs_and_tuning[\"dqx\"][line_name],\n                \"dqy\": self.config_knobs_and_tuning[\"dqy\"][line_name],\n            }\n\n            xm.machine_tuning(\n                line=collider[line_name],\n                enable_closed_orbit_correction=True,\n                enable_linear_coupling_correction=match_linear_coupling_to_zero,\n                enable_tune_correction=True,\n                enable_chromaticity_correction=True,\n                knob_names=knob_names,\n                targets=targets,\n                line_co_ref=collider[f\"{line_name}_co_ref\"],\n                co_corr_config=self.dict_orbit_correction[line_name],\n            )\n\n    def set_filling_and_bunch_tracked(self, ask_worst_bunch: bool = False) -&gt; None:\n        \"\"\"\n        Sets the filling scheme and determines the bunch to be tracked for beam-beam interactions.\n\n        This method performs the following steps:\n        1. Retrieves the filling scheme path from the configuration.\n        2. Checks if the filling scheme path needs to be obtained from the template schemes.\n        3. Loads and verifies the filling scheme, potentially converting it if necessary.\n        4. Updates the configuration with the correct filling scheme path.\n        5. Determines the number of long-range encounters to consider.\n        6. If the bunch number for beam 1 is not provided, it identifies the bunch with the largest\n        number of long-range interactions.\n           - If `ask_worst_bunch` is True, prompts the user to confirm or provide a bunch number.\n           - Otherwise, automatically selects the worst bunch.\n        7. If the bunch number for beam 2 is not provided, it automatically selects the worst bunch.\n\n        Args:\n            ask_worst_bunch (bool): If True, prompts the user to confirm or provide the bunch number\n                for beam 1. Defaults to False.\n\n        Returns:\n            None\n        \"\"\"\n        # Get the filling scheme path\n        filling_scheme_path = self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"]\n\n        # Check if the filling scheme path must be obtained from the template schemes\n        scheme_folder = (\n            pathlib.Path(__file__).parent.parent.parent.resolve().joinpath(\"assets/filling_schemes\")\n        )\n        if filling_scheme_path in os.listdir(scheme_folder):\n            filling_scheme_path = str(scheme_folder.joinpath(filling_scheme_path))\n            self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"] = filling_scheme_path\n\n        # Load and check filling scheme, potentially convert it\n        filling_scheme_path = load_and_check_filling_scheme(filling_scheme_path)\n\n        # Correct filling scheme in config, as it might have been converted\n        self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"] = filling_scheme_path\n\n        # Get number of LR to consider\n        n_LR = self.config_beambeam[\"num_long_range_encounters_per_side\"][\"ip1\"]\n\n        # If the bunch number is None, the bunch with the largest number of long-range interactions is used\n        if self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] is None:\n            # Case the bunch number has not been provided\n            worst_bunch_b1 = get_worst_bunch(\n                filling_scheme_path, number_of_LR_to_consider=n_LR, beam=\"beam_1\"\n            )\n            if ask_worst_bunch:\n                while self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] is None:\n                    bool_inp = input(\n                        \"The bunch number for beam 1 has not been provided. Do you want to use the\"\n                        \" bunch with the largest number of long-range interactions? It is the bunch\"\n                        \" number \" + str(worst_bunch_b1) + \" (y/n): \"\n                    )\n                    if bool_inp == \"y\":\n                        self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] = (\n                            worst_bunch_b1\n                        )\n                    elif bool_inp == \"n\":\n                        self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] = int(\n                            input(\"Please enter the bunch number for beam 1: \")\n                        )\n            else:\n                self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] = worst_bunch_b1\n\n        if self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b2\"] is None:\n            worst_bunch_b2 = get_worst_bunch(\n                filling_scheme_path, number_of_LR_to_consider=n_LR, beam=\"beam_2\"\n            )\n            # For beam 2, just select the worst bunch by default\n            self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b2\"] = worst_bunch_b2\n\n    def compute_collision_from_scheme(self) -&gt; tuple[int, int, int]:\n        \"\"\"\n        This method reads a filling scheme from a JSON file specified in the configuration, converts\n        the filling scheme into boolean arrays for two beams, and calculates the number of\n        collisions at IP1 &amp; IP5, IP2, and IP8 by performing convolutions on the arrays.\n\n        Returns:\n            tuple[int, int, int]: A tuple containing the number of collisions at IP1 &amp; IP5, IP2, and\n                IP8 respectively.\n\n        Raises:\n            ValueError: If the filling scheme file is not in JSON format.\n            AssertionError: If the length of the beam arrays is not 3564.\n        \"\"\"\n        # Get the filling scheme path (in json or csv format)\n        filling_scheme_path = self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"]\n\n        # Load the filling scheme\n        if not filling_scheme_path.endswith(\".json\"):\n            raise ValueError(\n                f\"Unknown filling scheme file format: {filling_scheme_path}. It you provided a csv\"\n                \" file, it should have been automatically convert when running the script\"\n                \" 001_make_folders.py. Something went wrong.\"\n            )\n\n        with open(filling_scheme_path, \"r\") as fid:\n            filling_scheme = json.load(fid)\n\n        # Extract booleans beam arrays\n        array_b1 = np.array(filling_scheme[\"beam1\"])\n        array_b2 = np.array(filling_scheme[\"beam2\"])\n\n        # Assert that the arrays have the required length, and do the convolution\n        assert len(array_b1) == len(array_b2) == 3564\n        n_collisions_ip1_and_5 = array_b1 @ array_b2\n        n_collisions_ip2 = np.roll(array_b1, 891) @ array_b2\n        n_collisions_ip8 = np.roll(array_b1, 2670) @ array_b2\n\n        return int(n_collisions_ip1_and_5), int(n_collisions_ip2), int(n_collisions_ip8)\n\n    @property\n    def crab(self) -&gt; bool:\n        \"\"\"\n        This method checks the configuration settings for the presence and value of the\n        \"on_crab1\" knob. If the knob is present and its value is non-zero, it sets the\n        `_crab` attribute to True, indicating that crab cavities are active. Otherwise,\n        it sets `_crab` to False.\n\n        Returns:\n            bool: True if crab cavities are active, False otherwise.\n        \"\"\"\n        if self._crab is None:\n            # Get crab cavities\n            self._crab = False\n            if \"on_crab1\" in self.config_knobs_and_tuning[\"knob_settings\"]:\n                crab_val = float(self.config_knobs_and_tuning[\"knob_settings\"][\"on_crab1\"])\n                if abs(crab_val) &gt; 0:\n                    self._crab = True\n        return self._crab\n\n    def level_all_by_separation(\n        self,\n        n_collisions_ip1_and_5: int,\n        n_collisions_ip2: int,\n        n_collisions_ip8: int,\n        collider: xt.Multiline,\n    ) -&gt; None:\n        \"\"\"\n        This method updates the number of colliding bunches for IP1, IP2, IP5, and IP8 in the\n        configuration file and performs luminosity leveling using the provided collider object.\n        It also updates the separation knobs for the collider based on the new configuration.\n\n        Args:\n            n_collisions_ip1_and_5 (int): Number of collisions at interaction points 1 and 5.\n            n_collisions_ip2 (int): Number of collisions at interaction point 2.\n            n_collisions_ip8 (int): Number of collisions at interaction point 8.\n            collider (xt.Multiline): The collider object to be used for luminosity leveling.\n\n        Returns:\n            None\n        \"\"\"\n        # Update the number of bunches in the configuration file\n        l_n_collisions = [\n            n_collisions_ip1_and_5,\n            n_collisions_ip2,\n            n_collisions_ip1_and_5,\n            n_collisions_ip8,\n        ]\n        for ip, n_collisions in zip([\"ip1\", \"ip2\", \"ip5\", \"ip8\"], l_n_collisions):\n            if ip in self.config_lumi_leveling:\n                self.config_lumi_leveling[ip][\"num_colliding_bunches\"] = n_collisions\n            else:\n                logging.warning(f\"IP {ip} is not in the configuration\")\n\n        # ! Crabs are not handled in the following function\n        xm.lhc.luminosity_leveling(  # type: ignore\n            collider,\n            config_lumi_leveling=self.config_lumi_leveling,\n            config_beambeam=self.config_beambeam,\n        )\n\n        # Update configuration\n        if \"ip1\" in self.config_lumi_leveling:\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip1\"], \"on_sep1\")\n        if \"ip2\" in self.config_lumi_leveling:\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2\")\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2h\")\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2v\")\n        if \"ip5\" in self.config_lumi_leveling:\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip5\"], \"on_sep5\")\n        if \"ip8\" in self.config_lumi_leveling:\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8\")\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8h\")\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8v\")\n\n    def level_ip1_5_by_bunch_intensity(\n        self,\n        collider: xt.Multiline,\n        n_collisions_ip1_and_5: int,\n    ) -&gt; None:\n        \"\"\"\n        This method modifies the bunch intensity to achieve the desired luminosity\n        levels in IP 1 and 5. It updates the configuration with the new intensity values.\n\n        Args:\n            collider (xt.Multiline): The collider object containing the beam and lattice\n                configuration.\n            n_collisions_ip1_and_5 (int):\n                The number of collisions in IP 1 and 5.\n\n        Returns:\n            None\n        \"\"\"\n        # Initial intensity\n        bunch_intensity = self.config_beambeam[\"num_particles_per_bunch\"]\n\n        # First level luminosity in IP 1/5 changing the intensity\n        if (\n            self.config_lumi_leveling_ip1_5 is not None\n            and not self.config_lumi_leveling_ip1_5[\"skip_leveling\"]\n        ):\n            logging.info(\"Leveling luminosity in IP 1/5 varying the intensity\")\n            # Update the number of bunches in the configuration file\n            self.config_lumi_leveling_ip1_5[\"num_colliding_bunches\"] = n_collisions_ip1_and_5\n\n            # Do the levelling\n            bunch_intensity = luminosity_leveling_ip1_5(\n                collider,\n                self.config_lumi_leveling_ip1_5,\n                self.config_beambeam,\n                crab=self.crab,\n                cross_section=self.config_beambeam[\"cross_section\"],\n            )\n\n        # Update the configuration\n        self.config_beambeam[\"final_num_particles_per_bunch\"] = float(bunch_intensity)\n\n    def level_ip2_8_by_separation(\n        self,\n        n_collisions_ip2: int,\n        n_collisions_ip8: int,\n        collider: xt.Multiline,\n    ) -&gt; None:\n        \"\"\"\n        This method updates the number of colliding bunches for IP2 and IP8 in the configuration\n        file, performs luminosity leveling for the specified collider, and updates the separation\n        knobs for both interaction points.\n\n        Args:\n            n_collisions_ip2 (int): The number of collisions at interaction point 2 (IP2).\n            n_collisions_ip8 (int): The number of collisions at interaction point 8 (IP8).\n            collider (xt.Multiline): The collider object for which the luminosity leveling is to be\n                performed.\n\n        Returns:\n            None\n        \"\"\"\n        # Update the number of bunches in the configuration file\n        if \"ip2\" in self.config_lumi_leveling:\n            self.config_lumi_leveling[\"ip2\"][\"num_colliding_bunches\"] = n_collisions_ip2\n        if \"ip8\" in self.config_lumi_leveling:\n            self.config_lumi_leveling[\"ip8\"][\"num_colliding_bunches\"] = n_collisions_ip8\n\n        # Ensure the the num particles per bunch corresponds to the final one\n        temp_num_particles_per_bunch = self.config_beambeam[\"num_particles_per_bunch\"]\n        if \"final_num_particles_per_bunch\" in self.config_beambeam:\n            self.config_beambeam[\"num_particles_per_bunch\"] = self.config_beambeam[\n                \"final_num_particles_per_bunch\"\n            ]\n        # Do levelling in IP2 and IP8\n        xm.lhc.luminosity_leveling(  # type: ignore\n            collider,\n            config_lumi_leveling=self.config_lumi_leveling,\n            config_beambeam=self.config_beambeam,\n        )\n\n        # Update configuration\n        if \"ip2\" in self.config_lumi_leveling:\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2\")\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2h\")\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2v\")\n        if \"ip8\" in self.config_lumi_leveling:\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8\")\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8h\")\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8v\")\n\n        # Set back the num particles per bunch to its initial value\n        self.config_beambeam[\"num_particles_per_bunch\"] = temp_num_particles_per_bunch\n\n    def add_linear_coupling(self, collider: xt.Multiline) -&gt; None:\n        \"\"\"\n        Adds linear coupling to the collider based on the version of the LHC run or HL-LHC optics.\n\n        This method adjusts the collider variables to introduce linear coupling. The specific\n        adjustments depend on the version of the LHC run or HL-LHC optics being used.\n\n        Args:\n            collider (xt.Multiline): The collider object to which linear coupling will be added.\n\n        Returns:\n            None\n\n        Raises:\n            ValueError: If the version of the optics or run is unknown.\n\n        Notes:\n            - For LHC Run 3.0, the `cmrs.b1_sq` and `cmrs.b2_sq` variables are adjusted.\n            - For HL-LHC optics versions 1.6, 1.5, 1.4, and 1.3, the `c_minus_re_b1` and\n            `c_minus_re_b2` variables are adjusted.\n        \"\"\"\n        # Add linear coupling as the target in the tuning of the base collider was 0\n        # (not possible to set it the target to 0.001 for now)\n        if self.ver_lhc_run == 3.0:\n            collider.vars[\"cmrs.b1_sq\"] += self.config_knobs_and_tuning[\"delta_cmr\"]\n            collider.vars[\"cmrs.b2_sq\"] += self.config_knobs_and_tuning[\"delta_cmr\"]\n        elif self.ver_hllhc_optics in [1.6, 1.5, 1.4, 1.3]:\n            collider.vars[\"c_minus_re_b1\"] += self.config_knobs_and_tuning[\"delta_cmr\"]\n            collider.vars[\"c_minus_re_b2\"] += self.config_knobs_and_tuning[\"delta_cmr\"]\n        else:\n            raise ValueError(\n                f\"Unknown version of the optics/run: {self.ver_hllhc_optics}, {self.ver_lhc_run}.\"\n            )\n\n    def assert_tune_chroma_coupling(self, collider: xt.Multiline) -&gt; None:\n        \"\"\"\n        Asserts that the tune, chromaticity, and linear coupling of the collider\n        match the expected values specified in the configuration.\n\n        Args:\n            collider (xt.Multiline): The collider object containing the lines to be checked.\n\n        Returns:\n            None\n\n        Raises:\n            AssertionError: If any of the tune, chromaticity, or linear coupling values do not match\n                the expected values within the specified tolerances.\n\n        Notes:\n            The function checks the following parameters for each line (\"lhcb1\" and \"lhcb2\"):\n            - Horizontal tune (qx)\n            - Vertical tune (qy)\n            - Horizontal chromaticity (dqx)\n            - Vertical chromaticity (dqy)\n            - Linear coupling (c_minus)\n\n        The expected values are retrieved from the `self.config_knobs_and_tuning` dictionary.\n        \"\"\"\n        for line_name in [\"lhcb1\", \"lhcb2\"]:\n            tw = collider[line_name].twiss()\n            assert np.isclose(tw.qx, self.config_knobs_and_tuning[\"qx\"][line_name], atol=1e-4), (\n                f\"tune_x is not correct for {line_name}. Expected\"\n                f\" {self.config_knobs_and_tuning['qx'][line_name]}, got {tw.qx}\"\n            )\n            assert np.isclose(tw.qy, self.config_knobs_and_tuning[\"qy\"][line_name], atol=1e-4), (\n                f\"tune_y is not correct for {line_name}. Expected\"\n                f\" {self.config_knobs_and_tuning['qy'][line_name]}, got {tw.qy}\"\n            )\n            assert np.isclose(\n                tw.dqx,\n                self.config_knobs_and_tuning[\"dqx\"][line_name],\n                rtol=1e-2,\n            ), (\n                f\"chromaticity_x is not correct for {line_name}. Expected\"\n                f\" {self.config_knobs_and_tuning['dqx'][line_name]}, got {tw.dqx}\"\n            )\n            assert np.isclose(\n                tw.dqy,\n                self.config_knobs_and_tuning[\"dqy\"][line_name],\n                rtol=1e-2,\n            ), (\n                f\"chromaticity_y is not correct for {line_name}. Expected\"\n                f\" {self.config_knobs_and_tuning['dqy'][line_name]}, got {tw.dqy}\"\n            )\n\n            assert np.isclose(\n                tw.c_minus,\n                self.config_knobs_and_tuning[\"delta_cmr\"],\n                atol=5e-3,\n            ), (\n                f\"linear coupling is not correct for {line_name}. Expected\"\n                f\" {self.config_knobs_and_tuning['delta_cmr']}, got {tw.c_minus}\"\n            )\n\n    def record_beta_functions(self, collider: xt.Multiline) -&gt; None:\n        \"\"\"\n        Records the beta functions at the IPs in the collider.\n\n        Args:\n            collider (xt.Multiline): The collider object to record the beta functions.\n\n        Returns:\n            None\n        \"\"\"\n        # Record beta functions at the IPs\n        for ip in [\"ip1\", \"ip2\", \"ip5\", \"ip8\"]:\n            tw = collider.lhcb1.twiss()\n            self.config_collider[f\"beta_x_{ip}\"] = float(np.round(float(tw[\"betx\", ip]), 5))\n            self.config_collider[f\"beta_y_{ip}\"] = float(np.round(float(tw[\"bety\", ip]), 5))\n\n    def configure_beam_beam(self, collider: xt.Multiline) -&gt; None:\n        \"\"\"\n        Configures the beam-beam interactions for the collider.\n\n        This method sets up the beam-beam interactions by configuring the number of particles per\n        bunch, the horizontal emittance (nemitt_x), and the vertical emittance (nemitt_y) based on\n        the provided configuration. Additionally, it configures the filling scheme mask and bunch\n        numbers if a filling pattern is specified in the configuration.\n\n        Args:\n            collider (xt.Multiline): The collider object to configure.\n\n        Returns:\n            None\n        \"\"\"\n        collider.configure_beambeam_interactions(\n            num_particles=self.config_beambeam[\"num_particles_per_bunch\"],\n            nemitt_x=self.config_beambeam[\"nemitt_x\"],\n            nemitt_y=self.config_beambeam[\"nemitt_y\"],\n        )\n\n        # Configure filling scheme mask and bunch numbers\n        if \"mask_with_filling_pattern\" in self.config_beambeam and (\n            \"pattern_fname\" in self.config_beambeam[\"mask_with_filling_pattern\"]\n            and self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"] is not None\n        ):\n            fname = self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"]\n            with open(fname, \"r\") as fid:\n                filling = json.load(fid)\n            filling_pattern_cw = filling[\"beam1\"]\n            filling_pattern_acw = filling[\"beam2\"]\n\n            # Initialize bunch numbers with empty values\n            i_bunch_cw = None\n            i_bunch_acw = None\n\n            # Only track bunch number if a filling pattern has been provided\n            if \"i_bunch_b1\" in self.config_beambeam[\"mask_with_filling_pattern\"]:\n                i_bunch_cw = self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"]\n            if \"i_bunch_b2\" in self.config_beambeam[\"mask_with_filling_pattern\"]:\n                i_bunch_acw = self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b2\"]\n\n            # Note that a bunch number must be provided if a filling pattern is provided\n            # Apply filling pattern\n            collider.apply_filling_pattern(\n                filling_pattern_cw=filling_pattern_cw,\n                filling_pattern_acw=filling_pattern_acw,\n                i_bunch_cw=i_bunch_cw,\n                i_bunch_acw=i_bunch_acw,\n            )\n\n    def record_final_luminosity(self, collider: xt.Multiline, l_n_collisions: list[int]) -&gt; None:\n        \"\"\"\n        Records the final luminosity and pile-up for specified interaction points (IPs)\n        in the collider, both with and without beam-beam effects.\n\n        Args:\n            collider : (xt.Multiline): The collider object configured.\n            l_n_collisions (list[int]): A list containing the number of colliding bunches for each\n                IP.\n\n        Returns:\n            None\n        \"\"\"\n        # Define IPs in which the luminosity will be computed\n        l_ip = [\"ip1\", \"ip2\", \"ip5\", \"ip8\"]\n\n        # Ensure that the final number of particles per bunch is defined, even\n        # if the leveling has been done by separation\n        if \"final_num_particles_per_bunch\" not in self.config_beambeam:\n            self.config_beambeam[\"final_num_particles_per_bunch\"] = self.config_beambeam[\n                \"num_particles_per_bunch\"\n            ]\n\n        def _twiss_and_compute_lumi(collider, l_n_collisions):\n            # Loop over each IP and record the luminosity\n            twiss_b1 = collider[\"lhcb1\"].twiss()\n            twiss_b2 = collider[\"lhcb2\"].twiss()\n            l_lumi = []\n            l_PU = []\n            for n_col, ip in zip(l_n_collisions, l_ip):\n                L = xt.lumi.luminosity_from_twiss(  # type: ignore\n                    n_colliding_bunches=n_col,\n                    num_particles_per_bunch=self.config_beambeam[\"final_num_particles_per_bunch\"],\n                    ip_name=ip,\n                    nemitt_x=self.config_beambeam[\"nemitt_x\"],\n                    nemitt_y=self.config_beambeam[\"nemitt_y\"],\n                    sigma_z=self.config_beambeam[\"sigma_z\"],\n                    twiss_b1=twiss_b1,\n                    twiss_b2=twiss_b2,\n                    crab=self.crab,\n                )\n                PU = compute_PU(\n                    L,\n                    n_col,\n                    twiss_b1[\"T_rev0\"],\n                    cross_section=self.config_beambeam[\"cross_section\"],\n                )\n\n                l_lumi.append(L)\n                l_PU.append(PU)\n\n            return l_lumi, l_PU\n\n        # Get the final luminosity in all IPs, without beam-beam\n        collider.vars[\"beambeam_scale\"] = 0\n        l_lumi, l_PU = _twiss_and_compute_lumi(collider, l_n_collisions)\n\n        # Update configuration\n        for ip, L, PU in zip(l_ip, l_lumi, l_PU):\n            self.config_beambeam[f\"luminosity_{ip}_without_beam_beam\"] = float(L)\n            self.config_beambeam[f\"Pile-up_{ip}_without_beam_beam\"] = float(PU)\n\n        # Get the final luminosity in all IPs, with beam-beam\n        collider.vars[\"beambeam_scale\"] = 1\n        l_lumi, l_PU = _twiss_and_compute_lumi(collider, l_n_collisions)\n\n        # Update configuration\n        for ip, L, PU in zip(l_ip, l_lumi, l_PU):\n            self.config_beambeam[f\"luminosity_{ip}_with_beam_beam\"] = float(L)\n            self.config_beambeam[f\"Pile-up_{ip}_with_beam_beam\"] = float(PU)\n\n    def write_collider_to_disk(self, collider, full_configuration) -&gt; None:\n        \"\"\"\n        Writes the collider object to disk in JSON format if the save_output_collider flag is set.\n\n        Args:\n            collider (Collider): The collider object to be saved.\n            full_configuration (dict): The full configuration dictionary to be deep-copied into the\n                collider's metadata.\n\n        Returns:\n            None\n        \"\"\"\n        if self.save_output_collider:\n            logging.info(\"Saving collider as json\")\n            if (\n                hasattr(collider, \"metadata\")\n                and collider.metadata is not None\n                and isinstance(collider.metadata, dict)\n            ):\n                collider.metadata.update(copy.deepcopy(full_configuration))\n            else:\n                collider.metadata = copy.deepcopy(full_configuration)\n            collider.to_json(self.path_collider_file_for_tracking_as_output)\n\n            # Compress the collider file to zip to ease the load on afs\n            if self.compress:\n                compress_and_write(self.path_collider_file_for_tracking_as_output)\n\n    @staticmethod\n    def update_configuration_knob(\n        collider: xt.Multiline, dictionnary: dict, knob_name: str\n    ) -&gt; None:\n        \"\"\"\n        Updates the given dictionary with the final value of a specified knob from the collider.\n\n        Args:\n            collider (xt.Multiline): The collider object containing various variables.\n            dictionnary (dict): The dictionary to be updated with the knob's final value.\n            knob_name (str): The name of the knob whose value is to be retrieved and stored.\n\n        Returns:\n            None\n        \"\"\"\n        if knob_name in collider.vars.keys():\n            dictionnary[f\"final_{knob_name}\"] = float(collider.vars[knob_name]._value)\n        else:\n            logging.warning(f\"Knob {knob_name} not found in the collider\")\n\n    @staticmethod\n    def return_fingerprint(collider, line_name=\"lhcb1\") -&gt; str:\n        \"\"\"\n        Generate a detailed fingerprint of the specified collider line. Useful to compare two\n        colliders.\n\n        Args:\n            collider (xt.Multiline): The collider object containing the line data.\n            line_name (str): The name of the line to analyze within the collider. Default to \"lhcb1\".\n\n        Returns:\n            str:\n                A formatted string containing detailed information about the collider line, including:\n                - Installed element types\n                - Tunes and chromaticity\n                - Synchrotron tune and slip factor\n                - Twiss parameters and phases at interaction points (IPs)\n                - Dispersion and crab dispersion at IPs\n                - Amplitude detuning coefficients\n                - Non-linear chromaticity\n                - Tunes and momentum compaction vs delta\n        \"\"\"\n        line = collider[line_name]\n\n        tw = line.twiss()\n        tt = line.get_table()\n\n        det = line.get_amplitude_detuning_coefficients(a0_sigmas=0.1, a1_sigmas=0.2, a2_sigmas=0.3)\n\n        det_table = xt.Table(\n            {\n                \"name\": np.array(list(det.keys())),\n                \"value\": np.array(list(det.values())),\n            }\n        )\n\n        nl_chrom = line.get_non_linear_chromaticity(\n            delta0_range=(-2e-4, 2e-4), num_delta=5, fit_order=3\n        )\n\n        out = \"\"\n\n        out += f\"Line: {line_name}\\n\"\n        out += \"\\n\"\n\n        out += \"Installed element types:\\n\"\n        out += repr([nn for nn in sorted(list(set(tt.element_type))) if len(nn) &gt; 0]) + \"\\n\"\n        out += \"\\n\"\n\n        out += f'Tunes:        Qx  = {tw[\"qx\"]:.5f}       Qy = {tw[\"qy\"]:.5f}\\n'\n        out += f\"\"\"Chromaticity: Q'x = {tw[\"dqx\"]:.2f}     Q'y = \"\"\" + f'{tw[\"dqy\"]:.2f}\\n'\n        out += f'c_minus:      {tw[\"c_minus\"]:.5e}\\n'\n        out += \"\\n\"\n\n        out += f'Synchrotron tune: {tw[\"qs\"]:5e}\\n'\n        out += f'Slip factor:      {tw[\"slip_factor\"]:.5e}\\n'\n        out += \"\\n\"\n\n        out += \"Twiss parameters and phases at IPs:\\n\"\n        out += (\n            tw.rows[\"ip.*\"]\n            .cols[\"name s betx bety alfx alfy mux muy\"]\n            .show(output=str, max_col_width=int(1e6), digits=8)\n        )\n        out += \"\\n\\n\"\n\n        out += \"Dispersion at IPs:\\n\"\n        out += (\n            tw.rows[\"ip.*\"]\n            .cols[\"name s dx dy dpx dpy\"]\n            .show(output=str, max_col_width=int(1e6), digits=8)\n        )\n        out += \"\\n\\n\"\n\n        out += \"Crab dispersion at IPs:\\n\"\n        out += (\n            tw.rows[\"ip.*\"]\n            .cols[\"name s dx_zeta dy_zeta dpx_zeta dpy_zeta\"]\n            .show(output=str, max_col_width=int(1e6), digits=8)\n        )\n        out += \"\\n\\n\"\n\n        out += \"Amplitude detuning coefficients:\\n\"\n        out += det_table.show(output=str, max_col_width=int(1e6), digits=6)\n        out += \"\\n\\n\"\n\n        out += \"Non-linear chromaticity:\\n\"\n        out += f'dnqx = {list(nl_chrom[\"dnqx\"])}\\n'\n        out += f'dnqy = {list(nl_chrom[\"dnqy\"])}\\n'\n        out += \"\\n\\n\"\n\n        out += \"Tunes and momentum compaction vs delta:\\n\"\n        out += nl_chrom.show(output=str, max_col_width=int(1e6), digits=6)\n        out += \"\\n\\n\"\n\n        return out\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.crab","title":"<code>crab: bool</code>  <code>property</code>","text":"<p>This method checks the configuration settings for the presence and value of the \"on_crab1\" knob. If the knob is present and its value is non-zero, it sets the <code>_crab</code> attribute to True, indicating that crab cavities are active. Otherwise, it sets <code>_crab</code> to False.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if crab cavities are active, False otherwise.</p>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.dict_orbit_correction","title":"<code>dict_orbit_correction: dict</code>  <code>property</code>","text":"<p>Generates and returns a dictionary containing orbit correction parameters.</p> <p>This method checks if the orbit correction dictionary has already been generated. If not, it determines the appropriate set of orbit correction parameters based on the version of HLLHC optics or LHC run provided.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing orbit correction parameters.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both <code>ver_hllhc_optics</code> and <code>ver_lhc_run</code> are defined.</p> <code>ValueError</code> <p>If no optics specific tools are available for the provided configuration.</p>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.__init__","title":"<code>__init__(configuration, path_collider_file_for_configuration_as_input, ver_hllhc_optics, ver_lhc_run, ions)</code>","text":"<p>Initialize the XsuiteCollider class with the given configuration and parameters.</p> <p>Parameters:</p> Name Type Description Default <code>configuration</code> <code>dict</code> <p>A dictionary containing various configuration settings. - config_beambeam (dict): Configuration for beam-beam interactions. - config_knobs_and_tuning (dict): Configuration for knobs and tuning. - config_lumi_leveling (dict): Configuration for luminosity leveling. - save_output_collider (bool): Flag to save the final collider to disk. - path_collider_file_for_tracking_as_output (str): Path to save the final collider. - config_lumi_leveling_ip1_5 (optional): Configuration for luminosity leveling at     IP1 and IP5.</p> required <code>path_collider_file_for_configuration_as_input</code> <code>str</code> <p>Path to the collider file.</p> required <code>ver_hllhc_optics</code> <code>float</code> <p>Version of the HL-LHC optics.</p> required <code>ver_lhc_run</code> <code>float</code> <p>Version of the LHC run.</p> required <code>ions</code> <code>bool</code> <p>Flag indicating if ions are used.</p> required Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def __init__(\n    self,\n    configuration: dict,\n    path_collider_file_for_configuration_as_input: str,\n    ver_hllhc_optics: float,\n    ver_lhc_run: float,\n    ions: bool,\n):\n    \"\"\"\n    Initialize the XsuiteCollider class with the given configuration and parameters.\n\n    Args:\n        configuration (dict): A dictionary containing various configuration settings.\n            - config_beambeam (dict): Configuration for beam-beam interactions.\n            - config_knobs_and_tuning (dict): Configuration for knobs and tuning.\n            - config_lumi_leveling (dict): Configuration for luminosity leveling.\n            - save_output_collider (bool): Flag to save the final collider to disk.\n            - path_collider_file_for_tracking_as_output (str): Path to save the final collider.\n            - config_lumi_leveling_ip1_5 (optional): Configuration for luminosity leveling at\n                IP1 and IP5.\n        path_collider_file_for_configuration_as_input (str): Path to the collider file.\n        ver_hllhc_optics (float): Version of the HL-LHC optics.\n        ver_lhc_run (float): Version of the LHC run.\n        ions (bool): Flag indicating if ions are used.\n    \"\"\"\n    # Collider file path\n    self.path_collider_file_for_configuration_as_input = (\n        path_collider_file_for_configuration_as_input\n    )\n\n    # Configuration variables\n    self.config_beambeam: dict[str, Any] = configuration[\"config_beambeam\"]\n    self.config_knobs_and_tuning: dict[str, Any] = configuration[\"config_knobs_and_tuning\"]\n    self.config_lumi_leveling: dict[str, Any] = configuration[\"config_lumi_leveling\"]\n\n    # self.config_lumi_leveling_ip1_5 will be None if not present in the configuration\n    self.config_lumi_leveling_ip1_5: dict[str, Any] = configuration.get(\n        \"config_lumi_leveling_ip1_5\"\n    )\n\n    # Collider configuration\n    self.config_collider: dict[str, Any] = configuration\n\n    # Optics version (needed to select the appropriate optics specific functions)\n    self.ver_hllhc_optics: float = ver_hllhc_optics\n    self.ver_lhc_run: float = ver_lhc_run\n    self.ions: bool = ions\n    self._dict_orbit_correction: dict | None = None\n\n    # Crab cavities\n    self._crab: bool | None = None\n\n    # Save collider to disk\n    self.save_output_collider = configuration[\"save_output_collider\"]\n    self.path_collider_file_for_tracking_as_output = configuration[\n        \"path_collider_file_for_tracking_as_output\"\n    ]\n    self.compress = configuration[\"compress\"]\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.add_linear_coupling","title":"<code>add_linear_coupling(collider)</code>","text":"<p>Adds linear coupling to the collider based on the version of the LHC run or HL-LHC optics.</p> <p>This method adjusts the collider variables to introduce linear coupling. The specific adjustments depend on the version of the LHC run or HL-LHC optics being used.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object to which linear coupling will be added.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the version of the optics or run is unknown.</p> Notes <ul> <li>For LHC Run 3.0, the <code>cmrs.b1_sq</code> and <code>cmrs.b2_sq</code> variables are adjusted.</li> <li>For HL-LHC optics versions 1.6, 1.5, 1.4, and 1.3, the <code>c_minus_re_b1</code> and <code>c_minus_re_b2</code> variables are adjusted.</li> </ul> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def add_linear_coupling(self, collider: xt.Multiline) -&gt; None:\n    \"\"\"\n    Adds linear coupling to the collider based on the version of the LHC run or HL-LHC optics.\n\n    This method adjusts the collider variables to introduce linear coupling. The specific\n    adjustments depend on the version of the LHC run or HL-LHC optics being used.\n\n    Args:\n        collider (xt.Multiline): The collider object to which linear coupling will be added.\n\n    Returns:\n        None\n\n    Raises:\n        ValueError: If the version of the optics or run is unknown.\n\n    Notes:\n        - For LHC Run 3.0, the `cmrs.b1_sq` and `cmrs.b2_sq` variables are adjusted.\n        - For HL-LHC optics versions 1.6, 1.5, 1.4, and 1.3, the `c_minus_re_b1` and\n        `c_minus_re_b2` variables are adjusted.\n    \"\"\"\n    # Add linear coupling as the target in the tuning of the base collider was 0\n    # (not possible to set it the target to 0.001 for now)\n    if self.ver_lhc_run == 3.0:\n        collider.vars[\"cmrs.b1_sq\"] += self.config_knobs_and_tuning[\"delta_cmr\"]\n        collider.vars[\"cmrs.b2_sq\"] += self.config_knobs_and_tuning[\"delta_cmr\"]\n    elif self.ver_hllhc_optics in [1.6, 1.5, 1.4, 1.3]:\n        collider.vars[\"c_minus_re_b1\"] += self.config_knobs_and_tuning[\"delta_cmr\"]\n        collider.vars[\"c_minus_re_b2\"] += self.config_knobs_and_tuning[\"delta_cmr\"]\n    else:\n        raise ValueError(\n            f\"Unknown version of the optics/run: {self.ver_hllhc_optics}, {self.ver_lhc_run}.\"\n        )\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.assert_tune_chroma_coupling","title":"<code>assert_tune_chroma_coupling(collider)</code>","text":"<p>Asserts that the tune, chromaticity, and linear coupling of the collider match the expected values specified in the configuration.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object containing the lines to be checked.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If any of the tune, chromaticity, or linear coupling values do not match the expected values within the specified tolerances.</p> Notes <p>The function checks the following parameters for each line (\"lhcb1\" and \"lhcb2\"): - Horizontal tune (qx) - Vertical tune (qy) - Horizontal chromaticity (dqx) - Vertical chromaticity (dqy) - Linear coupling (c_minus)</p> <p>The expected values are retrieved from the <code>self.config_knobs_and_tuning</code> dictionary.</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def assert_tune_chroma_coupling(self, collider: xt.Multiline) -&gt; None:\n    \"\"\"\n    Asserts that the tune, chromaticity, and linear coupling of the collider\n    match the expected values specified in the configuration.\n\n    Args:\n        collider (xt.Multiline): The collider object containing the lines to be checked.\n\n    Returns:\n        None\n\n    Raises:\n        AssertionError: If any of the tune, chromaticity, or linear coupling values do not match\n            the expected values within the specified tolerances.\n\n    Notes:\n        The function checks the following parameters for each line (\"lhcb1\" and \"lhcb2\"):\n        - Horizontal tune (qx)\n        - Vertical tune (qy)\n        - Horizontal chromaticity (dqx)\n        - Vertical chromaticity (dqy)\n        - Linear coupling (c_minus)\n\n    The expected values are retrieved from the `self.config_knobs_and_tuning` dictionary.\n    \"\"\"\n    for line_name in [\"lhcb1\", \"lhcb2\"]:\n        tw = collider[line_name].twiss()\n        assert np.isclose(tw.qx, self.config_knobs_and_tuning[\"qx\"][line_name], atol=1e-4), (\n            f\"tune_x is not correct for {line_name}. Expected\"\n            f\" {self.config_knobs_and_tuning['qx'][line_name]}, got {tw.qx}\"\n        )\n        assert np.isclose(tw.qy, self.config_knobs_and_tuning[\"qy\"][line_name], atol=1e-4), (\n            f\"tune_y is not correct for {line_name}. Expected\"\n            f\" {self.config_knobs_and_tuning['qy'][line_name]}, got {tw.qy}\"\n        )\n        assert np.isclose(\n            tw.dqx,\n            self.config_knobs_and_tuning[\"dqx\"][line_name],\n            rtol=1e-2,\n        ), (\n            f\"chromaticity_x is not correct for {line_name}. Expected\"\n            f\" {self.config_knobs_and_tuning['dqx'][line_name]}, got {tw.dqx}\"\n        )\n        assert np.isclose(\n            tw.dqy,\n            self.config_knobs_and_tuning[\"dqy\"][line_name],\n            rtol=1e-2,\n        ), (\n            f\"chromaticity_y is not correct for {line_name}. Expected\"\n            f\" {self.config_knobs_and_tuning['dqy'][line_name]}, got {tw.dqy}\"\n        )\n\n        assert np.isclose(\n            tw.c_minus,\n            self.config_knobs_and_tuning[\"delta_cmr\"],\n            atol=5e-3,\n        ), (\n            f\"linear coupling is not correct for {line_name}. Expected\"\n            f\" {self.config_knobs_and_tuning['delta_cmr']}, got {tw.c_minus}\"\n        )\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.compute_collision_from_scheme","title":"<code>compute_collision_from_scheme()</code>","text":"<p>This method reads a filling scheme from a JSON file specified in the configuration, converts the filling scheme into boolean arrays for two beams, and calculates the number of collisions at IP1 &amp; IP5, IP2, and IP8 by performing convolutions on the arrays.</p> <p>Returns:</p> Type Description <code>tuple[int, int, int]</code> <p>tuple[int, int, int]: A tuple containing the number of collisions at IP1 &amp; IP5, IP2, and IP8 respectively.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the filling scheme file is not in JSON format.</p> <code>AssertionError</code> <p>If the length of the beam arrays is not 3564.</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def compute_collision_from_scheme(self) -&gt; tuple[int, int, int]:\n    \"\"\"\n    This method reads a filling scheme from a JSON file specified in the configuration, converts\n    the filling scheme into boolean arrays for two beams, and calculates the number of\n    collisions at IP1 &amp; IP5, IP2, and IP8 by performing convolutions on the arrays.\n\n    Returns:\n        tuple[int, int, int]: A tuple containing the number of collisions at IP1 &amp; IP5, IP2, and\n            IP8 respectively.\n\n    Raises:\n        ValueError: If the filling scheme file is not in JSON format.\n        AssertionError: If the length of the beam arrays is not 3564.\n    \"\"\"\n    # Get the filling scheme path (in json or csv format)\n    filling_scheme_path = self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"]\n\n    # Load the filling scheme\n    if not filling_scheme_path.endswith(\".json\"):\n        raise ValueError(\n            f\"Unknown filling scheme file format: {filling_scheme_path}. It you provided a csv\"\n            \" file, it should have been automatically convert when running the script\"\n            \" 001_make_folders.py. Something went wrong.\"\n        )\n\n    with open(filling_scheme_path, \"r\") as fid:\n        filling_scheme = json.load(fid)\n\n    # Extract booleans beam arrays\n    array_b1 = np.array(filling_scheme[\"beam1\"])\n    array_b2 = np.array(filling_scheme[\"beam2\"])\n\n    # Assert that the arrays have the required length, and do the convolution\n    assert len(array_b1) == len(array_b2) == 3564\n    n_collisions_ip1_and_5 = array_b1 @ array_b2\n    n_collisions_ip2 = np.roll(array_b1, 891) @ array_b2\n    n_collisions_ip8 = np.roll(array_b1, 2670) @ array_b2\n\n    return int(n_collisions_ip1_and_5), int(n_collisions_ip2), int(n_collisions_ip8)\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.configure_beam_beam","title":"<code>configure_beam_beam(collider)</code>","text":"<p>Configures the beam-beam interactions for the collider.</p> <p>This method sets up the beam-beam interactions by configuring the number of particles per bunch, the horizontal emittance (nemitt_x), and the vertical emittance (nemitt_y) based on the provided configuration. Additionally, it configures the filling scheme mask and bunch numbers if a filling pattern is specified in the configuration.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object to configure.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def configure_beam_beam(self, collider: xt.Multiline) -&gt; None:\n    \"\"\"\n    Configures the beam-beam interactions for the collider.\n\n    This method sets up the beam-beam interactions by configuring the number of particles per\n    bunch, the horizontal emittance (nemitt_x), and the vertical emittance (nemitt_y) based on\n    the provided configuration. Additionally, it configures the filling scheme mask and bunch\n    numbers if a filling pattern is specified in the configuration.\n\n    Args:\n        collider (xt.Multiline): The collider object to configure.\n\n    Returns:\n        None\n    \"\"\"\n    collider.configure_beambeam_interactions(\n        num_particles=self.config_beambeam[\"num_particles_per_bunch\"],\n        nemitt_x=self.config_beambeam[\"nemitt_x\"],\n        nemitt_y=self.config_beambeam[\"nemitt_y\"],\n    )\n\n    # Configure filling scheme mask and bunch numbers\n    if \"mask_with_filling_pattern\" in self.config_beambeam and (\n        \"pattern_fname\" in self.config_beambeam[\"mask_with_filling_pattern\"]\n        and self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"] is not None\n    ):\n        fname = self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"]\n        with open(fname, \"r\") as fid:\n            filling = json.load(fid)\n        filling_pattern_cw = filling[\"beam1\"]\n        filling_pattern_acw = filling[\"beam2\"]\n\n        # Initialize bunch numbers with empty values\n        i_bunch_cw = None\n        i_bunch_acw = None\n\n        # Only track bunch number if a filling pattern has been provided\n        if \"i_bunch_b1\" in self.config_beambeam[\"mask_with_filling_pattern\"]:\n            i_bunch_cw = self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"]\n        if \"i_bunch_b2\" in self.config_beambeam[\"mask_with_filling_pattern\"]:\n            i_bunch_acw = self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b2\"]\n\n        # Note that a bunch number must be provided if a filling pattern is provided\n        # Apply filling pattern\n        collider.apply_filling_pattern(\n            filling_pattern_cw=filling_pattern_cw,\n            filling_pattern_acw=filling_pattern_acw,\n            i_bunch_cw=i_bunch_cw,\n            i_bunch_acw=i_bunch_acw,\n        )\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.install_beam_beam_wrapper","title":"<code>install_beam_beam_wrapper(collider)</code>","text":"<p>This method installs beam-beam interactions in the collider with the specified parameters. The beam-beam lenses are initially inactive and not configured.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object where the beam-beam interactions will be installed.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def install_beam_beam_wrapper(self, collider: xt.Multiline) -&gt; None:\n    \"\"\"\n    This method installs beam-beam interactions in the collider with the specified\n    parameters. The beam-beam lenses are initially inactive and not configured.\n\n    Args:\n        collider (xt.Multiline): The collider object where the beam-beam interactions\n            will be installed.\n\n    Returns:\n        None\n    \"\"\"\n    # Install beam-beam lenses (inactive and not configured)\n    collider.install_beambeam_interactions(\n        clockwise_line=\"lhcb1\",\n        anticlockwise_line=\"lhcb2\",\n        ip_names=[\"ip1\", \"ip2\", \"ip5\", \"ip8\"],\n        delay_at_ips_slots=[0, 891, 0, 2670],\n        num_long_range_encounters_per_side=self.config_beambeam[\n            \"num_long_range_encounters_per_side\"\n        ],\n        num_slices_head_on=self.config_beambeam[\"num_slices_head_on\"],\n        harmonic_number=35640,\n        bunch_spacing_buckets=self.config_beambeam[\"bunch_spacing_buckets\"],\n        sigmaz=self.config_beambeam[\"sigma_z\"],\n    )\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.level_all_by_separation","title":"<code>level_all_by_separation(n_collisions_ip1_and_5, n_collisions_ip2, n_collisions_ip8, collider)</code>","text":"<p>This method updates the number of colliding bunches for IP1, IP2, IP5, and IP8 in the configuration file and performs luminosity leveling using the provided collider object. It also updates the separation knobs for the collider based on the new configuration.</p> <p>Parameters:</p> Name Type Description Default <code>n_collisions_ip1_and_5</code> <code>int</code> <p>Number of collisions at interaction points 1 and 5.</p> required <code>n_collisions_ip2</code> <code>int</code> <p>Number of collisions at interaction point 2.</p> required <code>n_collisions_ip8</code> <code>int</code> <p>Number of collisions at interaction point 8.</p> required <code>collider</code> <code>Multiline</code> <p>The collider object to be used for luminosity leveling.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def level_all_by_separation(\n    self,\n    n_collisions_ip1_and_5: int,\n    n_collisions_ip2: int,\n    n_collisions_ip8: int,\n    collider: xt.Multiline,\n) -&gt; None:\n    \"\"\"\n    This method updates the number of colliding bunches for IP1, IP2, IP5, and IP8 in the\n    configuration file and performs luminosity leveling using the provided collider object.\n    It also updates the separation knobs for the collider based on the new configuration.\n\n    Args:\n        n_collisions_ip1_and_5 (int): Number of collisions at interaction points 1 and 5.\n        n_collisions_ip2 (int): Number of collisions at interaction point 2.\n        n_collisions_ip8 (int): Number of collisions at interaction point 8.\n        collider (xt.Multiline): The collider object to be used for luminosity leveling.\n\n    Returns:\n        None\n    \"\"\"\n    # Update the number of bunches in the configuration file\n    l_n_collisions = [\n        n_collisions_ip1_and_5,\n        n_collisions_ip2,\n        n_collisions_ip1_and_5,\n        n_collisions_ip8,\n    ]\n    for ip, n_collisions in zip([\"ip1\", \"ip2\", \"ip5\", \"ip8\"], l_n_collisions):\n        if ip in self.config_lumi_leveling:\n            self.config_lumi_leveling[ip][\"num_colliding_bunches\"] = n_collisions\n        else:\n            logging.warning(f\"IP {ip} is not in the configuration\")\n\n    # ! Crabs are not handled in the following function\n    xm.lhc.luminosity_leveling(  # type: ignore\n        collider,\n        config_lumi_leveling=self.config_lumi_leveling,\n        config_beambeam=self.config_beambeam,\n    )\n\n    # Update configuration\n    if \"ip1\" in self.config_lumi_leveling:\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip1\"], \"on_sep1\")\n    if \"ip2\" in self.config_lumi_leveling:\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2\")\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2h\")\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2v\")\n    if \"ip5\" in self.config_lumi_leveling:\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip5\"], \"on_sep5\")\n    if \"ip8\" in self.config_lumi_leveling:\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8\")\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8h\")\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8v\")\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.level_ip1_5_by_bunch_intensity","title":"<code>level_ip1_5_by_bunch_intensity(collider, n_collisions_ip1_and_5)</code>","text":"<p>This method modifies the bunch intensity to achieve the desired luminosity levels in IP 1 and 5. It updates the configuration with the new intensity values.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object containing the beam and lattice configuration.</p> required <code>n_collisions_ip1_and_5</code> <code>int</code> <p>The number of collisions in IP 1 and 5.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def level_ip1_5_by_bunch_intensity(\n    self,\n    collider: xt.Multiline,\n    n_collisions_ip1_and_5: int,\n) -&gt; None:\n    \"\"\"\n    This method modifies the bunch intensity to achieve the desired luminosity\n    levels in IP 1 and 5. It updates the configuration with the new intensity values.\n\n    Args:\n        collider (xt.Multiline): The collider object containing the beam and lattice\n            configuration.\n        n_collisions_ip1_and_5 (int):\n            The number of collisions in IP 1 and 5.\n\n    Returns:\n        None\n    \"\"\"\n    # Initial intensity\n    bunch_intensity = self.config_beambeam[\"num_particles_per_bunch\"]\n\n    # First level luminosity in IP 1/5 changing the intensity\n    if (\n        self.config_lumi_leveling_ip1_5 is not None\n        and not self.config_lumi_leveling_ip1_5[\"skip_leveling\"]\n    ):\n        logging.info(\"Leveling luminosity in IP 1/5 varying the intensity\")\n        # Update the number of bunches in the configuration file\n        self.config_lumi_leveling_ip1_5[\"num_colliding_bunches\"] = n_collisions_ip1_and_5\n\n        # Do the levelling\n        bunch_intensity = luminosity_leveling_ip1_5(\n            collider,\n            self.config_lumi_leveling_ip1_5,\n            self.config_beambeam,\n            crab=self.crab,\n            cross_section=self.config_beambeam[\"cross_section\"],\n        )\n\n    # Update the configuration\n    self.config_beambeam[\"final_num_particles_per_bunch\"] = float(bunch_intensity)\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.level_ip2_8_by_separation","title":"<code>level_ip2_8_by_separation(n_collisions_ip2, n_collisions_ip8, collider)</code>","text":"<p>This method updates the number of colliding bunches for IP2 and IP8 in the configuration file, performs luminosity leveling for the specified collider, and updates the separation knobs for both interaction points.</p> <p>Parameters:</p> Name Type Description Default <code>n_collisions_ip2</code> <code>int</code> <p>The number of collisions at interaction point 2 (IP2).</p> required <code>n_collisions_ip8</code> <code>int</code> <p>The number of collisions at interaction point 8 (IP8).</p> required <code>collider</code> <code>Multiline</code> <p>The collider object for which the luminosity leveling is to be performed.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def level_ip2_8_by_separation(\n    self,\n    n_collisions_ip2: int,\n    n_collisions_ip8: int,\n    collider: xt.Multiline,\n) -&gt; None:\n    \"\"\"\n    This method updates the number of colliding bunches for IP2 and IP8 in the configuration\n    file, performs luminosity leveling for the specified collider, and updates the separation\n    knobs for both interaction points.\n\n    Args:\n        n_collisions_ip2 (int): The number of collisions at interaction point 2 (IP2).\n        n_collisions_ip8 (int): The number of collisions at interaction point 8 (IP8).\n        collider (xt.Multiline): The collider object for which the luminosity leveling is to be\n            performed.\n\n    Returns:\n        None\n    \"\"\"\n    # Update the number of bunches in the configuration file\n    if \"ip2\" in self.config_lumi_leveling:\n        self.config_lumi_leveling[\"ip2\"][\"num_colliding_bunches\"] = n_collisions_ip2\n    if \"ip8\" in self.config_lumi_leveling:\n        self.config_lumi_leveling[\"ip8\"][\"num_colliding_bunches\"] = n_collisions_ip8\n\n    # Ensure the the num particles per bunch corresponds to the final one\n    temp_num_particles_per_bunch = self.config_beambeam[\"num_particles_per_bunch\"]\n    if \"final_num_particles_per_bunch\" in self.config_beambeam:\n        self.config_beambeam[\"num_particles_per_bunch\"] = self.config_beambeam[\n            \"final_num_particles_per_bunch\"\n        ]\n    # Do levelling in IP2 and IP8\n    xm.lhc.luminosity_leveling(  # type: ignore\n        collider,\n        config_lumi_leveling=self.config_lumi_leveling,\n        config_beambeam=self.config_beambeam,\n    )\n\n    # Update configuration\n    if \"ip2\" in self.config_lumi_leveling:\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2\")\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2h\")\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2v\")\n    if \"ip8\" in self.config_lumi_leveling:\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8\")\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8h\")\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8v\")\n\n    # Set back the num particles per bunch to its initial value\n    self.config_beambeam[\"num_particles_per_bunch\"] = temp_num_particles_per_bunch\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.load_collider","title":"<code>load_collider()</code>","text":"<p>Load a collider configuration from a file.</p> <p>If the file path ends with \".zip\", the file is uncompressed locally and the collider configuration is loaded from the uncompressed file. Otherwise, the collider configuration is loaded directly from the file.</p> <p>Returns:</p> Type Description <code>Multiline</code> <p>xt.Multiline: The loaded collider configuration.</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def load_collider(self) -&gt; xt.Multiline:\n    \"\"\"\n    Load a collider configuration from a file.\n\n    If the file path ends with \".zip\", the file is uncompressed locally\n    and the collider configuration is loaded from the uncompressed file.\n    Otherwise, the collider configuration is loaded directly from the file.\n\n    Returns:\n        xt.Multiline: The loaded collider configuration.\n    \"\"\"\n    return self._load_collider(self.path_collider_file_for_configuration_as_input)\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.match_tune_and_chroma","title":"<code>match_tune_and_chroma(collider, match_linear_coupling_to_zero=True)</code>","text":"<p>This method adjusts the tune and chromaticity of the specified collider lines (\"lhcb1\" and \"lhcb2\") to the target values defined in the configuration. It also optionally matches the linear coupling to zero.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object containing the lines to be tuned.</p> required <code>match_linear_coupling_to_zero</code> <code>bool</code> <p>If True, linear coupling will be matched to zero. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def match_tune_and_chroma(\n    self, collider: xt.Multiline, match_linear_coupling_to_zero: bool = True\n) -&gt; None:\n    \"\"\"\n    This method adjusts the tune and chromaticity of the specified collider lines\n    (\"lhcb1\" and \"lhcb2\") to the target values defined in the configuration. It also\n    optionally matches the linear coupling to zero.\n\n    Args:\n        collider (xt.Multiline): The collider object containing the lines to be tuned.\n        match_linear_coupling_to_zero (bool, optional): If True, linear coupling will be\n            matched to zero. Defaults to True.\n\n    Returns:\n        None\n    \"\"\"\n    for line_name in [\"lhcb1\", \"lhcb2\"]:\n        knob_names = self.config_knobs_and_tuning[\"knob_names\"][line_name]\n\n        targets = {\n            \"qx\": self.config_knobs_and_tuning[\"qx\"][line_name],\n            \"qy\": self.config_knobs_and_tuning[\"qy\"][line_name],\n            \"dqx\": self.config_knobs_and_tuning[\"dqx\"][line_name],\n            \"dqy\": self.config_knobs_and_tuning[\"dqy\"][line_name],\n        }\n\n        xm.machine_tuning(\n            line=collider[line_name],\n            enable_closed_orbit_correction=True,\n            enable_linear_coupling_correction=match_linear_coupling_to_zero,\n            enable_tune_correction=True,\n            enable_chromaticity_correction=True,\n            knob_names=knob_names,\n            targets=targets,\n            line_co_ref=collider[f\"{line_name}_co_ref\"],\n            co_corr_config=self.dict_orbit_correction[line_name],\n        )\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.record_beta_functions","title":"<code>record_beta_functions(collider)</code>","text":"<p>Records the beta functions at the IPs in the collider.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object to record the beta functions.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def record_beta_functions(self, collider: xt.Multiline) -&gt; None:\n    \"\"\"\n    Records the beta functions at the IPs in the collider.\n\n    Args:\n        collider (xt.Multiline): The collider object to record the beta functions.\n\n    Returns:\n        None\n    \"\"\"\n    # Record beta functions at the IPs\n    for ip in [\"ip1\", \"ip2\", \"ip5\", \"ip8\"]:\n        tw = collider.lhcb1.twiss()\n        self.config_collider[f\"beta_x_{ip}\"] = float(np.round(float(tw[\"betx\", ip]), 5))\n        self.config_collider[f\"beta_y_{ip}\"] = float(np.round(float(tw[\"bety\", ip]), 5))\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.record_final_luminosity","title":"<code>record_final_luminosity(collider, l_n_collisions)</code>","text":"<p>Records the final luminosity and pile-up for specified interaction points (IPs) in the collider, both with and without beam-beam effects.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <p>(xt.Multiline): The collider object configured.</p> required <code>l_n_collisions</code> <code>list[int]</code> <p>A list containing the number of colliding bunches for each IP.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def record_final_luminosity(self, collider: xt.Multiline, l_n_collisions: list[int]) -&gt; None:\n    \"\"\"\n    Records the final luminosity and pile-up for specified interaction points (IPs)\n    in the collider, both with and without beam-beam effects.\n\n    Args:\n        collider : (xt.Multiline): The collider object configured.\n        l_n_collisions (list[int]): A list containing the number of colliding bunches for each\n            IP.\n\n    Returns:\n        None\n    \"\"\"\n    # Define IPs in which the luminosity will be computed\n    l_ip = [\"ip1\", \"ip2\", \"ip5\", \"ip8\"]\n\n    # Ensure that the final number of particles per bunch is defined, even\n    # if the leveling has been done by separation\n    if \"final_num_particles_per_bunch\" not in self.config_beambeam:\n        self.config_beambeam[\"final_num_particles_per_bunch\"] = self.config_beambeam[\n            \"num_particles_per_bunch\"\n        ]\n\n    def _twiss_and_compute_lumi(collider, l_n_collisions):\n        # Loop over each IP and record the luminosity\n        twiss_b1 = collider[\"lhcb1\"].twiss()\n        twiss_b2 = collider[\"lhcb2\"].twiss()\n        l_lumi = []\n        l_PU = []\n        for n_col, ip in zip(l_n_collisions, l_ip):\n            L = xt.lumi.luminosity_from_twiss(  # type: ignore\n                n_colliding_bunches=n_col,\n                num_particles_per_bunch=self.config_beambeam[\"final_num_particles_per_bunch\"],\n                ip_name=ip,\n                nemitt_x=self.config_beambeam[\"nemitt_x\"],\n                nemitt_y=self.config_beambeam[\"nemitt_y\"],\n                sigma_z=self.config_beambeam[\"sigma_z\"],\n                twiss_b1=twiss_b1,\n                twiss_b2=twiss_b2,\n                crab=self.crab,\n            )\n            PU = compute_PU(\n                L,\n                n_col,\n                twiss_b1[\"T_rev0\"],\n                cross_section=self.config_beambeam[\"cross_section\"],\n            )\n\n            l_lumi.append(L)\n            l_PU.append(PU)\n\n        return l_lumi, l_PU\n\n    # Get the final luminosity in all IPs, without beam-beam\n    collider.vars[\"beambeam_scale\"] = 0\n    l_lumi, l_PU = _twiss_and_compute_lumi(collider, l_n_collisions)\n\n    # Update configuration\n    for ip, L, PU in zip(l_ip, l_lumi, l_PU):\n        self.config_beambeam[f\"luminosity_{ip}_without_beam_beam\"] = float(L)\n        self.config_beambeam[f\"Pile-up_{ip}_without_beam_beam\"] = float(PU)\n\n    # Get the final luminosity in all IPs, with beam-beam\n    collider.vars[\"beambeam_scale\"] = 1\n    l_lumi, l_PU = _twiss_and_compute_lumi(collider, l_n_collisions)\n\n    # Update configuration\n    for ip, L, PU in zip(l_ip, l_lumi, l_PU):\n        self.config_beambeam[f\"luminosity_{ip}_with_beam_beam\"] = float(L)\n        self.config_beambeam[f\"Pile-up_{ip}_with_beam_beam\"] = float(PU)\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.return_fingerprint","title":"<code>return_fingerprint(collider, line_name='lhcb1')</code>  <code>staticmethod</code>","text":"<p>Generate a detailed fingerprint of the specified collider line. Useful to compare two colliders.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object containing the line data.</p> required <code>line_name</code> <code>str</code> <p>The name of the line to analyze within the collider. Default to \"lhcb1\".</p> <code>'lhcb1'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A formatted string containing detailed information about the collider line, including: - Installed element types - Tunes and chromaticity - Synchrotron tune and slip factor - Twiss parameters and phases at interaction points (IPs) - Dispersion and crab dispersion at IPs - Amplitude detuning coefficients - Non-linear chromaticity - Tunes and momentum compaction vs delta</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>@staticmethod\ndef return_fingerprint(collider, line_name=\"lhcb1\") -&gt; str:\n    \"\"\"\n    Generate a detailed fingerprint of the specified collider line. Useful to compare two\n    colliders.\n\n    Args:\n        collider (xt.Multiline): The collider object containing the line data.\n        line_name (str): The name of the line to analyze within the collider. Default to \"lhcb1\".\n\n    Returns:\n        str:\n            A formatted string containing detailed information about the collider line, including:\n            - Installed element types\n            - Tunes and chromaticity\n            - Synchrotron tune and slip factor\n            - Twiss parameters and phases at interaction points (IPs)\n            - Dispersion and crab dispersion at IPs\n            - Amplitude detuning coefficients\n            - Non-linear chromaticity\n            - Tunes and momentum compaction vs delta\n    \"\"\"\n    line = collider[line_name]\n\n    tw = line.twiss()\n    tt = line.get_table()\n\n    det = line.get_amplitude_detuning_coefficients(a0_sigmas=0.1, a1_sigmas=0.2, a2_sigmas=0.3)\n\n    det_table = xt.Table(\n        {\n            \"name\": np.array(list(det.keys())),\n            \"value\": np.array(list(det.values())),\n        }\n    )\n\n    nl_chrom = line.get_non_linear_chromaticity(\n        delta0_range=(-2e-4, 2e-4), num_delta=5, fit_order=3\n    )\n\n    out = \"\"\n\n    out += f\"Line: {line_name}\\n\"\n    out += \"\\n\"\n\n    out += \"Installed element types:\\n\"\n    out += repr([nn for nn in sorted(list(set(tt.element_type))) if len(nn) &gt; 0]) + \"\\n\"\n    out += \"\\n\"\n\n    out += f'Tunes:        Qx  = {tw[\"qx\"]:.5f}       Qy = {tw[\"qy\"]:.5f}\\n'\n    out += f\"\"\"Chromaticity: Q'x = {tw[\"dqx\"]:.2f}     Q'y = \"\"\" + f'{tw[\"dqy\"]:.2f}\\n'\n    out += f'c_minus:      {tw[\"c_minus\"]:.5e}\\n'\n    out += \"\\n\"\n\n    out += f'Synchrotron tune: {tw[\"qs\"]:5e}\\n'\n    out += f'Slip factor:      {tw[\"slip_factor\"]:.5e}\\n'\n    out += \"\\n\"\n\n    out += \"Twiss parameters and phases at IPs:\\n\"\n    out += (\n        tw.rows[\"ip.*\"]\n        .cols[\"name s betx bety alfx alfy mux muy\"]\n        .show(output=str, max_col_width=int(1e6), digits=8)\n    )\n    out += \"\\n\\n\"\n\n    out += \"Dispersion at IPs:\\n\"\n    out += (\n        tw.rows[\"ip.*\"]\n        .cols[\"name s dx dy dpx dpy\"]\n        .show(output=str, max_col_width=int(1e6), digits=8)\n    )\n    out += \"\\n\\n\"\n\n    out += \"Crab dispersion at IPs:\\n\"\n    out += (\n        tw.rows[\"ip.*\"]\n        .cols[\"name s dx_zeta dy_zeta dpx_zeta dpy_zeta\"]\n        .show(output=str, max_col_width=int(1e6), digits=8)\n    )\n    out += \"\\n\\n\"\n\n    out += \"Amplitude detuning coefficients:\\n\"\n    out += det_table.show(output=str, max_col_width=int(1e6), digits=6)\n    out += \"\\n\\n\"\n\n    out += \"Non-linear chromaticity:\\n\"\n    out += f'dnqx = {list(nl_chrom[\"dnqx\"])}\\n'\n    out += f'dnqy = {list(nl_chrom[\"dnqy\"])}\\n'\n    out += \"\\n\\n\"\n\n    out += \"Tunes and momentum compaction vs delta:\\n\"\n    out += nl_chrom.show(output=str, max_col_width=int(1e6), digits=6)\n    out += \"\\n\\n\"\n\n    return out\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.set_filling_and_bunch_tracked","title":"<code>set_filling_and_bunch_tracked(ask_worst_bunch=False)</code>","text":"<p>Sets the filling scheme and determines the bunch to be tracked for beam-beam interactions.</p> <p>This method performs the following steps: 1. Retrieves the filling scheme path from the configuration. 2. Checks if the filling scheme path needs to be obtained from the template schemes. 3. Loads and verifies the filling scheme, potentially converting it if necessary. 4. Updates the configuration with the correct filling scheme path. 5. Determines the number of long-range encounters to consider. 6. If the bunch number for beam 1 is not provided, it identifies the bunch with the largest number of long-range interactions.    - If <code>ask_worst_bunch</code> is True, prompts the user to confirm or provide a bunch number.    - Otherwise, automatically selects the worst bunch. 7. If the bunch number for beam 2 is not provided, it automatically selects the worst bunch.</p> <p>Parameters:</p> Name Type Description Default <code>ask_worst_bunch</code> <code>bool</code> <p>If True, prompts the user to confirm or provide the bunch number for beam 1. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def set_filling_and_bunch_tracked(self, ask_worst_bunch: bool = False) -&gt; None:\n    \"\"\"\n    Sets the filling scheme and determines the bunch to be tracked for beam-beam interactions.\n\n    This method performs the following steps:\n    1. Retrieves the filling scheme path from the configuration.\n    2. Checks if the filling scheme path needs to be obtained from the template schemes.\n    3. Loads and verifies the filling scheme, potentially converting it if necessary.\n    4. Updates the configuration with the correct filling scheme path.\n    5. Determines the number of long-range encounters to consider.\n    6. If the bunch number for beam 1 is not provided, it identifies the bunch with the largest\n    number of long-range interactions.\n       - If `ask_worst_bunch` is True, prompts the user to confirm or provide a bunch number.\n       - Otherwise, automatically selects the worst bunch.\n    7. If the bunch number for beam 2 is not provided, it automatically selects the worst bunch.\n\n    Args:\n        ask_worst_bunch (bool): If True, prompts the user to confirm or provide the bunch number\n            for beam 1. Defaults to False.\n\n    Returns:\n        None\n    \"\"\"\n    # Get the filling scheme path\n    filling_scheme_path = self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"]\n\n    # Check if the filling scheme path must be obtained from the template schemes\n    scheme_folder = (\n        pathlib.Path(__file__).parent.parent.parent.resolve().joinpath(\"assets/filling_schemes\")\n    )\n    if filling_scheme_path in os.listdir(scheme_folder):\n        filling_scheme_path = str(scheme_folder.joinpath(filling_scheme_path))\n        self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"] = filling_scheme_path\n\n    # Load and check filling scheme, potentially convert it\n    filling_scheme_path = load_and_check_filling_scheme(filling_scheme_path)\n\n    # Correct filling scheme in config, as it might have been converted\n    self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"] = filling_scheme_path\n\n    # Get number of LR to consider\n    n_LR = self.config_beambeam[\"num_long_range_encounters_per_side\"][\"ip1\"]\n\n    # If the bunch number is None, the bunch with the largest number of long-range interactions is used\n    if self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] is None:\n        # Case the bunch number has not been provided\n        worst_bunch_b1 = get_worst_bunch(\n            filling_scheme_path, number_of_LR_to_consider=n_LR, beam=\"beam_1\"\n        )\n        if ask_worst_bunch:\n            while self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] is None:\n                bool_inp = input(\n                    \"The bunch number for beam 1 has not been provided. Do you want to use the\"\n                    \" bunch with the largest number of long-range interactions? It is the bunch\"\n                    \" number \" + str(worst_bunch_b1) + \" (y/n): \"\n                )\n                if bool_inp == \"y\":\n                    self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] = (\n                        worst_bunch_b1\n                    )\n                elif bool_inp == \"n\":\n                    self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] = int(\n                        input(\"Please enter the bunch number for beam 1: \")\n                    )\n        else:\n            self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] = worst_bunch_b1\n\n    if self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b2\"] is None:\n        worst_bunch_b2 = get_worst_bunch(\n            filling_scheme_path, number_of_LR_to_consider=n_LR, beam=\"beam_2\"\n        )\n        # For beam 2, just select the worst bunch by default\n        self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b2\"] = worst_bunch_b2\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.set_knobs","title":"<code>set_knobs(collider)</code>","text":"<p>Set all knobs for the collider, including crossing angles, dispersion correction, RF, crab cavities, experimental magnets, etc.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object to which the knob settings will be applied.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def set_knobs(self, collider: xt.Multiline) -&gt; None:\n    \"\"\"\n    Set all knobs for the collider, including crossing angles, dispersion correction,\n    RF, crab cavities, experimental magnets, etc.\n\n    Args:\n        collider (xt.Multiline): The collider object to which the knob settings will be applied.\n\n    Returns:\n        None\n    \"\"\"\n    # Set all knobs (crossing angles, dispersion correction, rf, crab cavities,\n    # experimental magnets, etc.)\n    for kk, vv in self.config_knobs_and_tuning[\"knob_settings\"].items():\n        collider.vars[kk] = vv\n\n    # Crab fix (if needed)\n    if self.ver_hllhc_optics is not None and self.ver_hllhc_optics == 1.3:\n        apply_crab_fix(collider, self.config_knobs_and_tuning)\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.update_configuration_knob","title":"<code>update_configuration_knob(collider, dictionnary, knob_name)</code>  <code>staticmethod</code>","text":"<p>Updates the given dictionary with the final value of a specified knob from the collider.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object containing various variables.</p> required <code>dictionnary</code> <code>dict</code> <p>The dictionary to be updated with the knob's final value.</p> required <code>knob_name</code> <code>str</code> <p>The name of the knob whose value is to be retrieved and stored.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>@staticmethod\ndef update_configuration_knob(\n    collider: xt.Multiline, dictionnary: dict, knob_name: str\n) -&gt; None:\n    \"\"\"\n    Updates the given dictionary with the final value of a specified knob from the collider.\n\n    Args:\n        collider (xt.Multiline): The collider object containing various variables.\n        dictionnary (dict): The dictionary to be updated with the knob's final value.\n        knob_name (str): The name of the knob whose value is to be retrieved and stored.\n\n    Returns:\n        None\n    \"\"\"\n    if knob_name in collider.vars.keys():\n        dictionnary[f\"final_{knob_name}\"] = float(collider.vars[knob_name]._value)\n    else:\n        logging.warning(f\"Knob {knob_name} not found in the collider\")\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.write_collider_to_disk","title":"<code>write_collider_to_disk(collider, full_configuration)</code>","text":"<p>Writes the collider object to disk in JSON format if the save_output_collider flag is set.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Collider</code> <p>The collider object to be saved.</p> required <code>full_configuration</code> <code>dict</code> <p>The full configuration dictionary to be deep-copied into the collider's metadata.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def write_collider_to_disk(self, collider, full_configuration) -&gt; None:\n    \"\"\"\n    Writes the collider object to disk in JSON format if the save_output_collider flag is set.\n\n    Args:\n        collider (Collider): The collider object to be saved.\n        full_configuration (dict): The full configuration dictionary to be deep-copied into the\n            collider's metadata.\n\n    Returns:\n        None\n    \"\"\"\n    if self.save_output_collider:\n        logging.info(\"Saving collider as json\")\n        if (\n            hasattr(collider, \"metadata\")\n            and collider.metadata is not None\n            and isinstance(collider.metadata, dict)\n        ):\n            collider.metadata.update(copy.deepcopy(full_configuration))\n        else:\n            collider.metadata = copy.deepcopy(full_configuration)\n        collider.to_json(self.path_collider_file_for_tracking_as_output)\n\n        # Compress the collider file to zip to ease the load on afs\n        if self.compress:\n            compress_and_write(self.path_collider_file_for_tracking_as_output)\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteTracking","title":"<code>XsuiteTracking</code>","text":"<p>XsuiteTracking class for managing particle tracking simulations.</p> <p>Attributes:</p> Name Type Description <code>context_str</code> <code>str</code> <p>The context for the simulation (e.g., \"cupy\", \"opencl\", \"cpu\").</p> <code>device_number</code> <code>int</code> <p>The device number for GPU contexts.</p> <code>_context</code> <code>Context</code> <p>The context object for the simulation.</p> <code>beam</code> <code>str</code> <p>The beam configuration.</p> <code>distribution_file</code> <code>str</code> <p>The file path to the particle data.</p> <code>delta_max</code> <code>float</code> <p>The maximum delta value for particles.</p> <code>n_turns</code> <code>int</code> <p>The number of turns for the simulation.</p> <code>nemitt_x</code> <code>float</code> <p>The normalized emittance in the x direction.</p> <code>nemitt_y</code> <code>float</code> <p>The normalized emittance in the y direction.</p> <p>Methods:</p> Name Description <code>context</code> <p>Get the context object for the simulation.</p> <code>prepare_particle_distribution_for_tracking</code> <p>Prepare the particle distribution for tracking.</p> <code>track</code> <p>Track the particles in the collider.</p> Source code in <code>study_da/generate/master_classes/xsuite_tracking.py</code> <pre><code>class XsuiteTracking:\n    \"\"\"\n    XsuiteTracking class for managing particle tracking simulations.\n\n    Attributes:\n        context_str (str): The context for the simulation (e.g., \"cupy\", \"opencl\", \"cpu\").\n        device_number (int): The device number for GPU contexts.\n        _context (xo.Context): The context object for the simulation.\n        beam (str): The beam configuration.\n        distribution_file (str): The file path to the particle data.\n        delta_max (float): The maximum delta value for particles.\n        n_turns (int): The number of turns for the simulation.\n        nemitt_x (float): The normalized emittance in the x direction.\n        nemitt_y (float): The normalized emittance in the y direction.\n\n    Methods:\n        context: Get the context object for the simulation.\n        prepare_particle_distribution_for_tracking: Prepare the particle distribution for tracking.\n        track: Track the particles in the collider.\n    \"\"\"\n\n    def __init__(self, configuration: dict, nemitt_x: float, nemitt_y: float) -&gt; None:\n        \"\"\"\n        Initialize the tracking configuration.\n\n        Args:\n            configuration (dict): A dictionary containing the configuration parameters.\n                Expected keys:\n                - \"context\": str, context string for the simulation.\n                - \"device_number\": int, device number for the simulation.\n                - \"beam\": str, beam type for the simulation.\n                - \"distribution_file\": str, path to the particle file.\n                - \"delta_max\": float, maximum delta value for the simulation.\n                - \"n_turns\": int, number of turns for the simulation.\n            nemitt_x (float): Normalized emittance in the x-plane.\n            nemitt_y (float): Normalized emittance in the y-plane.\n        \"\"\"\n        # Context parameters\n        self.context_str: str = configuration[\"context\"]\n        self.device_number: int = configuration[\"device_number\"]\n        self._context = None\n\n        # Simulation parameters\n        self.beam: str = configuration[\"beam\"]\n        self.distribution_file: str = configuration[\"distribution_file\"]\n        self.path_distribution_folder_input: str = configuration[\"path_distribution_folder_input\"]\n        self.particle_path: str = f\"{self.path_distribution_folder_input}/{self.distribution_file}\"\n        self.delta_max: float = configuration[\"delta_max\"]\n        self.n_turns: int = configuration[\"n_turns\"]\n\n        # Beambeam parameters\n        self.nemitt_x: float = nemitt_x\n        self.nemitt_y: float = nemitt_y\n\n    @property\n    def context(self) -&gt; Any:\n        \"\"\"\n        Returns the context for the current instance. If the context is not already set,\n        it initializes the context based on the `context_str` attribute. The context can\n        be one of the following:\n\n        - \"cupy\": Uses `xo.ContextCupy`. If `device_number` is specified, it initializes\n            the context with the given device number.\n        - \"opencl\": Uses `xo.ContextPyopencl`.\n        - \"cpu\": Uses `xo.ContextCpu`.\n        - Any other value: Logs a warning and defaults to `xo.ContextCpu`.\n\n        If `device_number` is specified but the context is not \"cupy\", a warning is logged\n        indicating that the device number will be ignored.\n\n        Returns:\n            Any: The initialized context.\n        \"\"\"\n        if self._context is None:\n            if self.device_number is not None and self.context_str not in [\"cupy\"]:\n                logging.warning(\"Device number will be ignored since context is not cupy\")\n            match self.context_str:\n                case \"cupy\":\n                    if self.device_number is not None:\n                        self._context = xo.ContextCupy(device=self.device_number)\n                    else:\n                        self._context = xo.ContextCupy()\n                case \"opencl\":\n                    self._context = xo.ContextPyopencl()\n                case \"cpu\":\n                    self._context = xo.ContextCpu()\n                case _:\n                    logging.warning(\"Context not recognized, using cpu\")\n                    self._context = xo.ContextCpu()\n        return self._context\n\n    # ? I removed type hints for the output as I get an unclear linting error\n    # TODO: Check the proper type hints for the output\n    def prepare_particle_distribution_for_tracking(self, collider: xt.Multiline) -&gt; tuple:\n        \"\"\"\n        Prepare a particle distribution for tracking in the collider.\n\n        This method reads particle data from a parquet file, processes the data to\n        generate normalized amplitudes and angles, and then builds particles for\n        tracking in the collider. If the context is set to use GPU, the collider\n        trackers are reset and rebuilt accordingly.\n\n        Args:\n            collider (xt.Multiline): The collider object containing the beam and\n                tracking information.\n\n        Returns:\n            tuple: A tuple containing:\n                - xp.Particles: The particles ready for tracking.\n                - np.ndarray: Array of particle IDs.\n                - np.ndarray: Array of normalized amplitudes in the xy-plane.\n                - np.ndarray: Array of angles in the xy-plane in radians.\n        \"\"\"\n        # Reset the tracker to go to GPU if needed\n        if self.context_str in [\"cupy\", \"opencl\"]:\n            collider.discard_trackers()\n            collider.build_trackers(_context=self.context)\n\n        particle_df = pd.read_parquet(self.particle_path)\n\n        r_vect = particle_df[\"normalized amplitude in xy-plane\"].values\n        theta_vect = particle_df[\"angle in xy-plane [deg]\"].values * np.pi / 180  # type: ignore # [rad]\n\n        A1_in_sigma = r_vect * np.cos(theta_vect)\n        A2_in_sigma = r_vect * np.sin(theta_vect)\n\n        particles = collider[self.beam].build_particles(\n            x_norm=A1_in_sigma,\n            y_norm=A2_in_sigma,\n            delta=self.delta_max,\n            scale_with_transverse_norm_emitt=(\n                self.nemitt_x,\n                self.nemitt_y,\n            ),\n            _context=self.context,\n        )\n\n        particle_id = particle_df.particle_id.values\n        return particles, particle_id, r_vect, theta_vect\n\n    def track(self, collider: xt.Multiline, particles: xp.Particles) -&gt; dict:\n        \"\"\"\n        Tracks particles through a collider for a specified number of turns and logs the elapsed time.\n\n        Args:\n            collider (xt.Multiline): The collider object containing the beamline to be tracked.\n            particles (xp.Particles): The particles to be tracked.\n\n        Returns:\n            dict: A dictionary representation of the tracked particles.\n        \"\"\"\n        # Optimize line for tracking\n        collider[self.beam].optimize_for_tracking()\n\n        # Track\n        num_turns = self.n_turns\n        a = time.time()\n        collider[self.beam].track(particles, turn_by_turn_monitor=False, num_turns=num_turns)\n        b = time.time()\n\n        logging.info(f\"Elapsed time: {b-a} s\")\n        logging.info(\n            f\"Elapsed time per particle per turn: {(b-a)/particles._capacity/num_turns*1e6} us\"\n        )\n\n        return particles.to_dict()\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteTracking.context","title":"<code>context: Any</code>  <code>property</code>","text":"<p>Returns the context for the current instance. If the context is not already set, it initializes the context based on the <code>context_str</code> attribute. The context can be one of the following:</p> <ul> <li>\"cupy\": Uses <code>xo.ContextCupy</code>. If <code>device_number</code> is specified, it initializes     the context with the given device number.</li> <li>\"opencl\": Uses <code>xo.ContextPyopencl</code>.</li> <li>\"cpu\": Uses <code>xo.ContextCpu</code>.</li> <li>Any other value: Logs a warning and defaults to <code>xo.ContextCpu</code>.</li> </ul> <p>If <code>device_number</code> is specified but the context is not \"cupy\", a warning is logged indicating that the device number will be ignored.</p> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The initialized context.</p>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteTracking.__init__","title":"<code>__init__(configuration, nemitt_x, nemitt_y)</code>","text":"<p>Initialize the tracking configuration.</p> <p>Parameters:</p> Name Type Description Default <code>configuration</code> <code>dict</code> <p>A dictionary containing the configuration parameters. Expected keys: - \"context\": str, context string for the simulation. - \"device_number\": int, device number for the simulation. - \"beam\": str, beam type for the simulation. - \"distribution_file\": str, path to the particle file. - \"delta_max\": float, maximum delta value for the simulation. - \"n_turns\": int, number of turns for the simulation.</p> required <code>nemitt_x</code> <code>float</code> <p>Normalized emittance in the x-plane.</p> required <code>nemitt_y</code> <code>float</code> <p>Normalized emittance in the y-plane.</p> required Source code in <code>study_da/generate/master_classes/xsuite_tracking.py</code> <pre><code>def __init__(self, configuration: dict, nemitt_x: float, nemitt_y: float) -&gt; None:\n    \"\"\"\n    Initialize the tracking configuration.\n\n    Args:\n        configuration (dict): A dictionary containing the configuration parameters.\n            Expected keys:\n            - \"context\": str, context string for the simulation.\n            - \"device_number\": int, device number for the simulation.\n            - \"beam\": str, beam type for the simulation.\n            - \"distribution_file\": str, path to the particle file.\n            - \"delta_max\": float, maximum delta value for the simulation.\n            - \"n_turns\": int, number of turns for the simulation.\n        nemitt_x (float): Normalized emittance in the x-plane.\n        nemitt_y (float): Normalized emittance in the y-plane.\n    \"\"\"\n    # Context parameters\n    self.context_str: str = configuration[\"context\"]\n    self.device_number: int = configuration[\"device_number\"]\n    self._context = None\n\n    # Simulation parameters\n    self.beam: str = configuration[\"beam\"]\n    self.distribution_file: str = configuration[\"distribution_file\"]\n    self.path_distribution_folder_input: str = configuration[\"path_distribution_folder_input\"]\n    self.particle_path: str = f\"{self.path_distribution_folder_input}/{self.distribution_file}\"\n    self.delta_max: float = configuration[\"delta_max\"]\n    self.n_turns: int = configuration[\"n_turns\"]\n\n    # Beambeam parameters\n    self.nemitt_x: float = nemitt_x\n    self.nemitt_y: float = nemitt_y\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteTracking.prepare_particle_distribution_for_tracking","title":"<code>prepare_particle_distribution_for_tracking(collider)</code>","text":"<p>Prepare a particle distribution for tracking in the collider.</p> <p>This method reads particle data from a parquet file, processes the data to generate normalized amplitudes and angles, and then builds particles for tracking in the collider. If the context is set to use GPU, the collider trackers are reset and rebuilt accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object containing the beam and tracking information.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing: - xp.Particles: The particles ready for tracking. - np.ndarray: Array of particle IDs. - np.ndarray: Array of normalized amplitudes in the xy-plane. - np.ndarray: Array of angles in the xy-plane in radians.</p> Source code in <code>study_da/generate/master_classes/xsuite_tracking.py</code> <pre><code>def prepare_particle_distribution_for_tracking(self, collider: xt.Multiline) -&gt; tuple:\n    \"\"\"\n    Prepare a particle distribution for tracking in the collider.\n\n    This method reads particle data from a parquet file, processes the data to\n    generate normalized amplitudes and angles, and then builds particles for\n    tracking in the collider. If the context is set to use GPU, the collider\n    trackers are reset and rebuilt accordingly.\n\n    Args:\n        collider (xt.Multiline): The collider object containing the beam and\n            tracking information.\n\n    Returns:\n        tuple: A tuple containing:\n            - xp.Particles: The particles ready for tracking.\n            - np.ndarray: Array of particle IDs.\n            - np.ndarray: Array of normalized amplitudes in the xy-plane.\n            - np.ndarray: Array of angles in the xy-plane in radians.\n    \"\"\"\n    # Reset the tracker to go to GPU if needed\n    if self.context_str in [\"cupy\", \"opencl\"]:\n        collider.discard_trackers()\n        collider.build_trackers(_context=self.context)\n\n    particle_df = pd.read_parquet(self.particle_path)\n\n    r_vect = particle_df[\"normalized amplitude in xy-plane\"].values\n    theta_vect = particle_df[\"angle in xy-plane [deg]\"].values * np.pi / 180  # type: ignore # [rad]\n\n    A1_in_sigma = r_vect * np.cos(theta_vect)\n    A2_in_sigma = r_vect * np.sin(theta_vect)\n\n    particles = collider[self.beam].build_particles(\n        x_norm=A1_in_sigma,\n        y_norm=A2_in_sigma,\n        delta=self.delta_max,\n        scale_with_transverse_norm_emitt=(\n            self.nemitt_x,\n            self.nemitt_y,\n        ),\n        _context=self.context,\n    )\n\n    particle_id = particle_df.particle_id.values\n    return particles, particle_id, r_vect, theta_vect\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteTracking.track","title":"<code>track(collider, particles)</code>","text":"<p>Tracks particles through a collider for a specified number of turns and logs the elapsed time.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object containing the beamline to be tracked.</p> required <code>particles</code> <code>Particles</code> <p>The particles to be tracked.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the tracked particles.</p> Source code in <code>study_da/generate/master_classes/xsuite_tracking.py</code> <pre><code>def track(self, collider: xt.Multiline, particles: xp.Particles) -&gt; dict:\n    \"\"\"\n    Tracks particles through a collider for a specified number of turns and logs the elapsed time.\n\n    Args:\n        collider (xt.Multiline): The collider object containing the beamline to be tracked.\n        particles (xp.Particles): The particles to be tracked.\n\n    Returns:\n        dict: A dictionary representation of the tracked particles.\n    \"\"\"\n    # Optimize line for tracking\n    collider[self.beam].optimize_for_tracking()\n\n    # Track\n    num_turns = self.n_turns\n    a = time.time()\n    collider[self.beam].track(particles, turn_by_turn_monitor=False, num_turns=num_turns)\n    b = time.time()\n\n    logging.info(f\"Elapsed time: {b-a} s\")\n    logging.info(\n        f\"Elapsed time per particle per turn: {(b-a)/particles._capacity/num_turns*1e6} us\"\n    )\n\n    return particles.to_dict()\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.find_item_in_dic","title":"<code>find_item_in_dic(obj, key)</code>","text":"<p>Find an item in a nested dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>dict</code> <p>The nested dictionary.</p> required <code>key</code> <code>str</code> <p>The key to find in the nested dictionary.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The value corresponding to the key in the nested dictionary.</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def find_item_in_dic(obj: dict, key: str) -&gt; Any:\n    \"\"\"Find an item in a nested dictionary.\n\n    Args:\n        obj (dict): The nested dictionary.\n        key (str): The key to find in the nested dictionary.\n\n    Returns:\n        Any: The value corresponding to the key in the nested dictionary.\n\n    \"\"\"\n    if key in obj:\n        return obj[key]\n    for v in obj.values():\n        if isinstance(v, dict):\n            item = find_item_in_dic(v, key)\n            if item is not None:\n                return item\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.load_dic_from_path","title":"<code>load_dic_from_path(path, ryaml=None)</code>","text":"<p>Load a dictionary from a yaml file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the yaml file.</p> required <code>ryaml</code> <code>YAML</code> <p>The yaml reader.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[dict, YAML]</code> <p>tuple[dict, ruamel.yaml.YAML]: The dictionary and the yaml reader.</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def load_dic_from_path(\n    path: str, ryaml: ruamel.yaml.YAML | None = None\n) -&gt; tuple[dict, ruamel.yaml.YAML]:\n    \"\"\"Load a dictionary from a yaml file.\n\n    Args:\n        path (str): The path to the yaml file.\n        ryaml (ruamel.yaml.YAML): The yaml reader.\n\n    Returns:\n        tuple[dict, ruamel.yaml.YAML]: The dictionary and the yaml reader.\n\n    \"\"\"\n\n    if ryaml is None:\n        # Initialize yaml reader\n        ryaml = ruamel.yaml.YAML()\n\n    # Load dic\n    with open(path, \"r\") as fid:\n        dic = ryaml.load(fid)\n\n    return dic, ryaml\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.nested_get","title":"<code>nested_get(dic, keys)</code>","text":"<p>Get the value from a nested dictionary using a list of keys.</p> <p>Parameters:</p> Name Type Description Default <code>dic</code> <code>dict</code> <p>The nested dictionary.</p> required <code>keys</code> <code>list</code> <p>The list of keys to traverse the nested dictionary.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The value corresponding to the keys in the nested dictionary.</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def nested_get(dic: dict, keys: list) -&gt; Any:\n    # Adapted from https://stackoverflow.com/questions/14692690/access-nested-dictionary-items-via-a-list-of-keys\n    \"\"\"Get the value from a nested dictionary using a list of keys.\n\n    Args:\n        dic (dict): The nested dictionary.\n        keys (list): The list of keys to traverse the nested dictionary.\n\n    Returns:\n        Any: The value corresponding to the keys in the nested dictionary.\n\n    \"\"\"\n    for key in keys:\n        dic = dic[key]\n    return dic\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.nested_set","title":"<code>nested_set(dic, keys, value)</code>","text":"<p>Set a value in a nested dictionary using a list of keys.</p> <p>Parameters:</p> Name Type Description Default <code>dic</code> <code>dict</code> <p>The nested dictionary.</p> required <code>keys</code> <code>list</code> <p>The list of keys to traverse the nested dictionary.</p> required <code>value</code> <code>Any</code> <p>The value to set in the nested dictionary.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def nested_set(dic: dict, keys: list, value: Any) -&gt; None:\n    \"\"\"Set a value in a nested dictionary using a list of keys.\n\n    Args:\n        dic (dict): The nested dictionary.\n        keys (list): The list of keys to traverse the nested dictionary.\n        value (Any): The value to set in the nested dictionary.\n\n    Returns:\n        None\n\n    \"\"\"\n    for key in keys[:-1]:\n        dic = dic.setdefault(key, {})\n    dic[keys[-1]] = value\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.set_item_in_dic","title":"<code>set_item_in_dic(obj, key, value, found=False)</code>","text":"<p>Set an item in a nested dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>dict</code> <p>The nested dictionary.</p> required <code>key</code> <code>str</code> <p>The key to set in the nested dictionary.</p> required <code>value</code> <code>Any</code> <p>The value to set in the nested dictionary.</p> required <code>found</code> <code>bool</code> <p>Whether the key has been found in the nested dictionary.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def set_item_in_dic(obj: dict, key: str, value: Any, found: bool = False) -&gt; None:\n    \"\"\"Set an item in a nested dictionary.\n\n    Args:\n        obj (dict): The nested dictionary.\n        key (str): The key to set in the nested dictionary.\n        value (Any): The value to set in the nested dictionary.\n        found (bool): Whether the key has been found in the nested dictionary.\n\n    Returns:\n        None\n\n    \"\"\"\n    if key in obj:\n        if found:\n            raise ValueError(f\"Key {key} found more than once in the nested dictionary.\")\n\n        obj[key] = value\n        found = True\n    for v in obj.values():\n        if isinstance(v, dict):\n            set_item_in_dic(v, key, value, found)\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.write_dic_to_path","title":"<code>write_dic_to_path(dic, path, ryaml=None)</code>","text":"<p>Write a dictionary to a yaml file.</p> <p>Parameters:</p> Name Type Description Default <code>dic</code> <code>dict</code> <p>The dictionary to write.</p> required <code>path</code> <code>str</code> <p>The path to the yaml file.</p> required <code>ryaml</code> <code>YAML</code> <p>The yaml reader.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def write_dic_to_path(dic: dict, path: str, ryaml: ruamel.yaml.YAML | None = None) -&gt; None:\n    \"\"\"Write a dictionary to a yaml file.\n\n    Args:\n        dic (dict): The dictionary to write.\n        path (str): The path to the yaml file.\n        ryaml (ruamel.yaml.YAML): The yaml reader.\n\n    Returns:\n        None\n\n    \"\"\"\n\n    if ryaml is None:\n        # Initialize yaml reader\n        ryaml = ruamel.yaml.YAML()\n\n    # Write dic\n    with open(path, \"w\") as fid:\n        ryaml.dump(dic, fid)\n        # Force os to write to disk now, to avoid race conditions\n        fid.flush()\n        os.fsync(fid.fileno())\n</code></pre>"},{"location":"reference/study_da/generate/generate_scan.html","title":"generate_scan","text":"<p>This class is used to generate a study (along with the corresponding tree) from a parameter file, and potentially a set of template files.</p> <p>This class makes use of \"eval\", which is known to be a security risk. ast.literal_eval can't be used here because variables from a specified namespace are being passed to the eval function. The use of eval is justified here because the input is controlled by the user only through the configuration file. If the user wants to write an unsafe expression, it's their responsibility.</p>"},{"location":"reference/study_da/generate/generate_scan.html#study_da.generate.generate_scan.GenerateScan","title":"<code>GenerateScan</code>","text":"<p>A class to generate a study (along with the corresponding tree) from a parameter file, and potentially a set of template files.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>dict</code> <p>The configuration dictionary.</p> <code>ryaml</code> <code>YAML</code> <p>The YAML parser.</p> <code>dic_common_parameters</code> <code>dict</code> <p>Dictionary of common parameters across generations.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initializes the generation scan with a configuration file or dictionary.</p> <code>render</code> <p>Renders the study file using a template.</p> <code>write</code> <p>Writes the study file to disk.</p> <code>generate_render_write</code> <p>Generates, renders, and writes the study file.</p> <code>get_dic_parametric_scans</code> <p>Retrieves dictionaries of parametric scan values.</p> <code>parse_parameter_space</code> <p>Parses the parameter space for a given parameter.</p> <code>browse_and_collect_parameter_space</code> <p>Browses and collects the parameter space for a given generation.</p> <code>postprocess_parameter_lists</code> <p>Postprocesses the parameter lists.</p> <code>create_scans</code> <p>Creates study files for parametric scans.</p> <code>complete_tree</code> <p>Completes the tree structure of the study dictionary.</p> <code>write_tree</code> <p>Writes the study tree structure to a YAML file.</p> <code>create_study_for_current_gen</code> <p>Creates study files for the current generation.</p> <code>create_study</code> <p>Creates study files for the entire study.</p> <code>eval_conditions</code> <p>Evaluates the conditions to filter out some parameter values.</p> <code>filter_for_concomitant_parameters</code> <p>Filters the conditions for concomitant parameters.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>class GenerateScan:\n    \"\"\"\n    A class to generate a study (along with the corresponding tree) from a parameter file,\n    and potentially a set of template files.\n\n    Attributes:\n        config (dict): The configuration dictionary.\n        ryaml (yaml.YAML): The YAML parser.\n        dic_common_parameters (dict): Dictionary of common parameters across generations.\n\n    Methods:\n        __init__(): Initializes the generation scan with a configuration file or dictionary.\n        render(): Renders the study file using a template.\n        write(): Writes the study file to disk.\n        generate_render_write(): Generates, renders, and writes the study file.\n        get_dic_parametric_scans(): Retrieves dictionaries of parametric scan values.\n        parse_parameter_space(): Parses the parameter space for a given parameter.\n        browse_and_collect_parameter_space(): Browses and collects the parameter space for a given\n            generation.\n        postprocess_parameter_lists(): Postprocesses the parameter lists.\n        create_scans(): Creates study files for parametric scans.\n        complete_tree(): Completes the tree structure of the study dictionary.\n        write_tree(): Writes the study tree structure to a YAML file.\n        create_study_for_current_gen(): Creates study files for the current generation.\n        create_study(): Creates study files for the entire study.\n        eval_conditions(): Evaluates the conditions to filter out some parameter values.\n        filter_for_concomitant_parameters(): Filters the conditions for concomitant parameters.\n    \"\"\"\n\n    def __init__(\n        self, path_config: Optional[str] = None, dic_scan: Optional[dict[str, Any]] = None\n    ):  # sourcery skip: remove-redundant-if\n        \"\"\"\n        Initialize the generation scan with a configuration file or dictionary.\n\n        Args:\n            path_config (Optional[str]): Path to the configuration file for the scan.\n                Default is None.\n            dic_scan (Optional[dict[str, Any]]): Dictionary containing the scan configuration.\n                Default is None.\n\n        Raises:\n            ValueError: If neither or both of `path_config` and `dic_scan` are provided.\n        \"\"\"\n        # Load the study configuration from file or dictionary\n        if dic_scan is None and path_config is None:\n            raise ValueError(\n                \"Either a path to the configuration file or a dictionary must be provided.\"\n            )\n        elif dic_scan is not None and path_config is not None:\n            raise ValueError(\"Only one of the configuration file or dictionary must be provided.\")\n        elif path_config is not None:\n            self.config, self.ryaml = load_dic_from_path(path_config)\n        elif dic_scan is not None:\n            self.config = dic_scan\n            self.ryaml = yaml.YAML()\n        else:\n            raise ValueError(\"An unexpected error occurred.\")\n\n        # Parameters common across all generations (e.g. for parallelization)\n        self.dic_common_parameters: dict[str, Any] = {}\n\n        # Path to the tree file\n        self.path_tree = self.config[\"name\"] + \"/\" + \"tree.yaml\"\n\n    def render(\n        self,\n        str_parameters: str,\n        template_path: str,\n        path_main_configuration: str,\n        study_path: Optional[str] = None,\n        str_dependencies: Optional[dict[str, str]] = None,\n    ) -&gt; str:\n        \"\"\"\n        Renders the study file using a template.\n\n        Args:\n            str_parameters (str): The string representation of parameters to declare/mutate.\n            template_path (str): The path to the template file.\n            path_main_configuration (str): The path to the main configuration file.\n            study_path (str, optional): The path to the root of the study. Defaults to None.\n            dependencies (dict[str, str], optional): The dictionary of dependencies. Defaults to {}.\n\n        Returns:\n            str: The rendered study file.\n        \"\"\"\n\n        # Handle mutable default argument\n        if str_dependencies is None:\n            dependencies = \"\"\n        if study_path is None:\n            study_path = \"\"\n\n        # Generate generations from template\n        directory_path = os.path.dirname(template_path)\n        template_name = os.path.basename(template_path)\n        environment = Environment(\n            loader=FileSystemLoader(directory_path),\n            variable_start_string=\"{}  ###---\",\n            variable_end_string=\"---###\",\n        )\n        template = environment.get_template(template_name)\n\n        # Better not to render the dependencies path this way, as it becomes too cumbersome to\n        # handle the paths when using clusters\n\n        return template.render(\n            parameters=str_parameters,\n            main_configuration=path_main_configuration,\n            path_root_study=study_path,\n            # dependencies = str_dependencies,\n        )\n\n    def write(self, study_str: str, file_path: str, format_with_black: bool = True):\n        \"\"\"\n        Writes the study file to disk.\n\n        Args:\n            study_str (str): The study file string.\n            file_path (str): The path to write the study file.\n            format_with_black (bool, optional): Whether to format the output file with black.\n                Defaults to True.\n        \"\"\"\n\n        # Format the string with black\n        if format_with_black:\n            study_str = format_str(study_str, mode=FileMode())\n\n        # Make folder if it doesn't exist\n        folder = os.path.dirname(file_path)\n        if folder != \"\":\n            os.makedirs(folder, exist_ok=True)\n\n        with open(file_path, mode=\"w\", encoding=\"utf-8\") as file:\n            file.write(study_str)\n\n    def generate_render_write(\n        self,\n        gen_name: str,\n        job_directory_path: str,\n        template_path: str,\n        depth_gen: int,\n        dic_mutated_parameters: dict[str, Any] = {},\n    ) -&gt; list[str]:  # sourcery skip: default-mutable-arg\n        \"\"\"\n        Generates, renders, and writes the study file.\n\n        Args:\n            gen_name (str): The name of the generation.\n            study_path (str): The path to the job folder.\n            template_path (str): The path to the template folder.\n            depth_gen (int): The depth of the generation in the tree.\n            dic_mutated_parameters (dict[str, Any], optional): The dictionary of mutated parameters.\n                Defaults to {}.\n\n        Returns:\n            tuple[str, list[str]]: The study file string and the list of study paths.\n        \"\"\"\n\n        directory_path_gen = f\"{job_directory_path}\"\n        if not directory_path_gen.endswith(\"/\"):\n            directory_path_gen += \"/\"\n        file_path_gen = f\"{directory_path_gen}{gen_name}.py\"\n        logging.info(f'Now rendering generation \"{file_path_gen}\"')\n\n        # Generate the string of parameters\n        str_parameters = \"{\"\n        for key, value in dic_mutated_parameters.items():\n            if isinstance(value, str):\n                str_parameters += f\"'{key}' : '{value}', \"\n            else:\n                str_parameters += f\"'{key}' : {value}, \"\n        str_parameters += \"}\"\n\n        # Adapt the dict of dependencies to the current generation\n        dic_dependencies = self.config[\"dependencies\"] if \"dependencies\" in self.config else {}\n\n        # Unpacking list of dependencies\n        dic_dependencies = {\n            **{\n                key: value for key, value in dic_dependencies.items() if not isinstance(value, list)\n            },\n            **{\n                f\"{key}_{str(i).zfill(len(str(len(value))))}\": i_value\n                for key, value in dic_dependencies.items()\n                if isinstance(value, list)\n                for i, i_value in enumerate(value)\n            },\n        }\n        self.config[\"dependencies\"] = dic_dependencies\n\n        # Initial dependencies are always copied at the root of the study (hence value.split(\"/\")[-1])\n        dic_dependencies = {\n            key: \"../\" * depth_gen + value.split(\"/\")[-1] for key, value in dic_dependencies.items()\n        }\n\n        # Always load configuration from above generation, and remove the path from dependencies\n        path_main_configuration = \"../\" + dic_dependencies.pop(\"main_configuration\").split(\"/\")[-1]\n\n        # Create the str for the dependencies\n        str_dependencies = \"{\"\n        for key, value in dic_dependencies.items():\n            str_dependencies += f\"'{key}' : '{value}', \"\n        str_dependencies += \"}\"\n\n        # Render and write the study file\n        study_str = self.render(\n            str_parameters,\n            template_path=template_path,\n            path_main_configuration=path_main_configuration,\n            study_path=os.path.abspath(self.config[\"name\"]),\n            str_dependencies=str_dependencies,\n        )\n\n        self.write(study_str, file_path_gen)\n        return [directory_path_gen]\n\n    def get_dic_parametric_scans(\n        self, generation: str\n    ) -&gt; tuple[dict[str, Any], dict[str, Any], np.ndarray | None]:\n        \"\"\"\n        Retrieves dictionaries of parametric scan values.\n\n        Args:\n            generation: The generation name.\n\n        Returns:\n            tuple[dict[str, Any], dict[str, Any], np.ndarray|None]: The dictionaries of parametric\n                scan values, another dictionnary with better naming for the tree creation, and an\n                array of conditions to filter out some parameter values.\n        \"\"\"\n\n        if generation == \"base\":\n            raise ValueError(\"Generation 'base' should not have scans.\")\n\n        # Remember common parameters as they might be used across generations\n        if \"common_parameters\" in self.config[\"structure\"][generation]:\n            self.dic_common_parameters[generation] = {}\n            for parameter in self.config[\"structure\"][generation][\"common_parameters\"]:\n                self.dic_common_parameters[generation][parameter] = self.config[\"structure\"][\n                    generation\n                ][\"common_parameters\"][parameter]\n\n        # Check that the generation has scans\n        if (\n            \"scans\" not in self.config[\"structure\"][generation]\n            or self.config[\"structure\"][generation][\"scans\"] is None\n        ):\n            dic_parameter_lists = {\"\": [generation]}\n            dic_parameter_lists_for_naming = {\"\": [generation]}\n            array_conditions = None\n            ll_concomitant_parameters = []\n        else:\n            # Browse and collect the parameter space for the generation\n            (\n                dic_parameter_lists,\n                dic_parameter_lists_for_naming,\n                dic_subvariables,\n                ll_concomitant_parameters,\n                l_conditions,\n            ) = self.browse_and_collect_parameter_space(generation)\n\n            # Get the dimension corresponding to each parameter\n            dic_dimension_indices = {\n                parameter: idx for idx, parameter in enumerate(dic_parameter_lists)\n            }\n\n            # Generate array of conditions to filter out some of the values later\n            # Is an array of True values if no conditions are present\n            array_conditions = self.eval_conditions(l_conditions, dic_parameter_lists)\n\n            # Filter for concomitant parameters\n            array_conditions = self.filter_for_concomitant_parameters(\n                array_conditions, ll_concomitant_parameters, dic_dimension_indices\n            )\n\n            # Postprocess the parameter lists and update the dictionaries\n            dic_parameter_lists, dic_parameter_lists_for_naming = self.postprocess_parameter_lists(\n                dic_parameter_lists, dic_parameter_lists_for_naming, dic_subvariables\n            )\n\n        return (\n            dic_parameter_lists,\n            dic_parameter_lists_for_naming,\n            array_conditions,\n        )\n\n    def parse_parameter_space(\n        self,\n        parameter: str,\n        dic_curr_parameter: dict[str, Any],\n        dic_parameter_lists: dict[str, Any],\n        dic_parameter_lists_for_naming: dict[str, Any],\n    ) -&gt; tuple[dict[str, Any], dict[str, Any]]:\n        \"\"\"\n        Parses the parameter space for a given parameter.\n\n        Args:\n            parameter (str): The parameter name.\n            dic_curr_parameter (dict[str, Any]): The dictionary of current parameter values.\n            dic_parameter_lists (dict[str, Any]): The dictionary of parameter lists.\n            dic_parameter_lists_for_naming (dict[str, Any]): The dictionary of parameter lists for naming.\n\n        Returns:\n            tuple[dict[str, Any], dict[str, Any]]: The updated dictionaries of parameter lists.\n        \"\"\"\n\n        if \"linspace\" in dic_curr_parameter:\n            parameter_list = linspace(dic_curr_parameter[\"linspace\"])\n            dic_parameter_lists_for_naming[parameter] = parameter_list\n        elif \"logspace\" in dic_curr_parameter:\n            parameter_list = logspace(dic_curr_parameter[\"logspace\"])\n            dic_parameter_lists_for_naming[parameter] = parameter_list\n        elif \"path_list\" in dic_curr_parameter:\n            l_values_path_list = dic_curr_parameter[\"path_list\"]\n            parameter_list = list_values_path(l_values_path_list, self.dic_common_parameters)\n            dic_parameter_lists_for_naming[parameter] = [\n                f\"{n:02d}\" for n, path in enumerate(parameter_list)\n            ]\n        elif \"list\" in dic_curr_parameter:\n            parameter_list = dic_curr_parameter[\"list\"]\n            dic_parameter_lists_for_naming[parameter] = parameter_list\n        elif \"expression\" in dic_curr_parameter:\n            parameter_list = np.round(\n                eval(dic_curr_parameter[\"expression\"], copy.deepcopy(dic_parameter_lists)),\n                8,\n            )\n            dic_parameter_lists_for_naming[parameter] = parameter_list\n        else:\n            raise ValueError(f\"Scanning method for parameter {parameter} is not recognized.\")\n\n        dic_parameter_lists[parameter] = np.array(parameter_list)\n        return dic_parameter_lists, dic_parameter_lists_for_naming\n\n    def browse_and_collect_parameter_space(\n        self,\n        generation: str,\n    ) -&gt; tuple[\n        dict[str, Any],\n        dict[str, Any],\n        dict[str, Any],\n        list[list[str]],\n        list[str],\n    ]:\n        \"\"\"\n        Browses and collects the parameter space for a given generation.\n\n        Args:\n            generation (str): The generation name.\n\n        Returns:\n            tuple[dict[str, Any], dict[str, Any], dict[str, Any], list[list[str]]]: The updated\n                dictionaries of parameter lists.\n        \"\"\"\n\n        l_conditions = []\n        ll_concomitant_parameters = []\n        dic_subvariables = {}\n        dic_parameter_lists = {}\n        dic_parameter_lists_for_naming = {}\n        for parameter in self.config[\"structure\"][generation][\"scans\"]:\n            dic_curr_parameter = self.config[\"structure\"][generation][\"scans\"][parameter]\n\n            # Parse the parameter space\n            dic_parameter_lists, dic_parameter_lists_for_naming = self.parse_parameter_space(\n                parameter, dic_curr_parameter, dic_parameter_lists, dic_parameter_lists_for_naming\n            )\n\n            # Store potential subvariables\n            if \"subvariables\" in dic_curr_parameter:\n                dic_subvariables[parameter] = dic_curr_parameter[\"subvariables\"]\n\n            # Save the condition if it exists\n            if \"condition\" in dic_curr_parameter:\n                l_conditions.append(dic_curr_parameter[\"condition\"])\n\n            # Save the concomitant parameters if they exist\n            if \"concomitant\" in dic_curr_parameter:\n                if not isinstance(dic_curr_parameter[\"concomitant\"], list):\n                    dic_curr_parameter[\"concomitant\"] = [dic_curr_parameter[\"concomitant\"]]\n                for concomitant_parameter in dic_curr_parameter[\"concomitant\"]:\n                    # Assert that the parameters list have the same size\n                    assert len(dic_parameter_lists[parameter]) == len(\n                        dic_parameter_lists[concomitant_parameter]\n                    ), (\n                        f\"Parameters {parameter} and {concomitant_parameter} must have the \"\n                        \"same size.\"\n                    )\n                # Add to the list for filtering later\n                ll_concomitant_parameters.append([parameter] + dic_curr_parameter[\"concomitant\"])\n\n        return (\n            dic_parameter_lists,\n            dic_parameter_lists_for_naming,\n            dic_subvariables,\n            ll_concomitant_parameters,\n            l_conditions,\n        )\n\n    def postprocess_parameter_lists(\n        self,\n        dic_parameter_lists: dict[str, Any],\n        dic_parameter_lists_for_naming: dict[str, Any],\n        dic_subvariables: dict[str, Any],\n    ) -&gt; tuple[dict[str, Any], dict[str, Any]]:\n        \"\"\"\n        Post-processes parameter lists by ensuring values are not numpy types and handling nested\n        parameters.\n\n        Args:\n            dic_parameter_lists (dict[str, Any]): Dictionary containing parameter lists.\n            dic_parameter_lists_for_naming (dict[str, Any]): Dictionary containing parameter lists\n                for naming.\n            dic_subvariables (dict[str, Any]): Dictionary containing subvariables for nested\n                parameters.\n\n        Returns:\n            tuple[dict[str, Any], dict[str, Any]]: Updated dictionaries of parameter lists and\n                parameter lists for naming.\n        \"\"\"\n        for parameter, parameter_list in dic_parameter_lists.items():\n            parameter_list_for_naming = dic_parameter_lists_for_naming[parameter]\n\n            # Ensure that all values are not numpy types (to avoid serialization issues)\n            parameter_list = [x.item() if isinstance(x, np.generic) else x for x in parameter_list]\n\n            # Handle nested parameters\n            parameter_list_updated = (\n                convert_for_subvariables(dic_subvariables[parameter], parameter_list)\n                if parameter in dic_subvariables\n                else parameter_list\n            )\n            # Update the dictionaries\n            dic_parameter_lists[parameter] = parameter_list_updated\n            dic_parameter_lists_for_naming[parameter] = parameter_list_for_naming\n\n        return dic_parameter_lists, dic_parameter_lists_for_naming\n\n    def create_scans(\n        self,\n        generation: str,\n        generation_path: str,\n        template_path: str,\n        depth_gen: int,\n        dic_parameter_lists: Optional[dict[str, Any]] = None,\n        dic_parameter_lists_for_naming: Optional[dict[str, Any]] = None,\n        add_prefix_to_folder_names: bool = False,\n    ) -&gt; list[str]:\n        \"\"\"\n        Creates study files for parametric scans.\n        If a dictionary of parameter lists is provided, the scan will be done on the parameter\n        lists (no cartesian product). Otherwise, the scan will be done on the cartesian product of\n        the parameters defined in the scan configuration file.\n\n        Args:\n            generation (str): The generation name.\n            generation_path (str): The (relative) path to the generation folder.\n            template_path (str): The path to the template folder.\n            depth_gen (int): The depth of the generation in the tree.\n            dic_parameter_lists (Optional[dict[str, Any]]): The dictionary of parameter lists.\n                Defaults to None.\n            dic_parameter_lists_for_naming (Optional[dict[str, Any]]): The dictionary of parameter\n                lists for naming. Defaults to None.\n            add_prefix_to_folder_names (bool): Whether to add a prefix to the folder names. Defaults\n                to False.\n\n        Returns:\n            tuple[list[str], list[str]]: The list of study file strings and the list of study paths.\n        \"\"\"\n        if dic_parameter_lists is None:\n            # Get dictionnary of parametric values being scanned\n            dic_parameter_lists, dic_parameter_lists_for_naming, array_conditions = (\n                self.get_dic_parametric_scans(generation)\n            )\n            apply_cartesian_product = True\n        else:\n            if dic_parameter_lists_for_naming is None:\n                dic_parameter_lists_for_naming = copy.deepcopy(dic_parameter_lists)\n            array_conditions = None\n            apply_cartesian_product = False\n\n        # Generate render write for the parameters parameters\n        l_study_path = []\n        if apply_cartesian_product:\n            logging.info(\n                f\"Now generation cartesian product of all parameters for generation: {generation}\"\n            )\n            array_param_values = itertools.product(*dic_parameter_lists.values())\n            array_param_values_for_naming = itertools.product(\n                *dic_parameter_lists_for_naming.values()\n            )\n            array_idx = itertools.product(*[range(len(x)) for x in dic_parameter_lists.values()])\n        else:\n            logging.info(f\"Now generation parameters for generation: {generation}\")\n            array_param_values = [list(x) for x in zip(*dic_parameter_lists.values())]\n            array_param_values_for_naming = [\n                list(x) for x in zip(*dic_parameter_lists_for_naming.values())\n            ]\n            array_idx = range(len(array_param_values))\n\n        # Loop over the parameters\n        to_disk_len = np.sum(array_conditions) if array_conditions is not None else 1\n        to_disk_idx = 0\n        for idx, (l_values, l_values_for_naming, l_idx) in enumerate(\n            zip(array_param_values, array_param_values_for_naming, array_idx)\n        ):\n            # Check the idx to keep if conditions are present\n            if array_conditions is not None and not array_conditions[l_idx]:\n                continue\n\n            # Create the path for the study\n            dic_mutated_parameters = dict(zip(dic_parameter_lists.keys(), l_values))\n            dic_mutated_parameters_for_naming = dict(\n                zip(dic_parameter_lists.keys(), l_values_for_naming)\n            )\n\n            # Handle prefix\n            prefix_path = \"\"\n            if add_prefix_to_folder_names:\n                prefix_path = f\"ID_{str(to_disk_idx).zfill(len(str(to_disk_len)))}_\"\n                to_disk_idx += 1\n\n            # Handle suffix\n            suffix_path = \"_\".join(\n                [\n                    f\"{parameter}_{value}\"\n                    for parameter, value in dic_mutated_parameters_for_naming.items()\n                ]\n            )\n            suffix_path = suffix_path.removeprefix(\"_\")\n\n            # Create final path\n            path = generation_path + prefix_path + suffix_path + \"/\"\n\n            # Add common parameters\n            if generation in self.dic_common_parameters:\n                dic_mutated_parameters |= self.dic_common_parameters[generation]\n\n            # Remove \"\" from mutated parameters, if it's in the dictionary\n            # as it's only used when no scan is done\n            if \"\" in dic_mutated_parameters:\n                dic_mutated_parameters.pop(\"\")\n\n            # Generate the study for current generation\n            self.generate_render_write(\n                generation,\n                path,\n                template_path,\n                depth_gen,\n                dic_mutated_parameters=dic_mutated_parameters,\n            )\n\n            # Append the list of study paths to build the tree later on\n            l_study_path.append(path)\n\n        if not l_study_path:\n            logging.warning(\n                f\"No study paths were created for generation {generation}.\"\n                \"Please check the conditions.\"\n            )\n\n        return l_study_path\n\n    def complete_tree(\n        self, dictionary_tree: dict, l_study_path_next_gen: list[str], gen: str\n    ) -&gt; dict:\n        \"\"\"\n        Completes the tree structure of the study dictionary.\n\n        Args:\n            dictionary_tree (dict): The dictionary representing the study tree structure.\n            l_study_path_next_gen (list[str]): The list of study paths for the next gen.\n            gen (str): The generation name.\n\n        Returns:\n            dict: The updated dictionary representing the study tree structure.\n        \"\"\"\n        logging.info(f\"Completing the tree structure for generation: {gen}\")\n        for path_next in l_study_path_next_gen:\n            nested_set(\n                dictionary_tree,\n                path_next.split(\"/\")[1:-1] + [gen],\n                {\"file\": f\"{path_next}{gen}.py\"},\n            )\n\n        return dictionary_tree\n\n    def write_tree(self, dictionary_tree: dict):\n        \"\"\"\n        Writes the study tree structure to a YAML file.\n\n        Args:\n            dictionary_tree (dict): The dictionary representing the study tree structure.\n        \"\"\"\n        logging.info(\"Writing the tree structure to a YAML file.\")\n        ryaml = yaml.YAML()\n        with open(self.path_tree, \"w\") as yaml_file:\n            ryaml.indent(sequence=4, offset=2)\n            ryaml.dump(dictionary_tree, yaml_file)\n\n    def create_study_for_current_gen(\n        self,\n        generation: str,\n        generation_path: str,\n        depth_gen: int,\n        dic_parameter_lists: Optional[dict[str, Any]] = None,\n        dic_parameter_lists_for_naming: Optional[dict[str, Any]] = None,\n        add_prefix_to_folder_names: bool = False,\n    ) -&gt; list[str]:\n        \"\"\"\n        Creates study files for the current generation.\n\n        Args:\n            generation (str): The name of the current generation.\n            directory_path (str): The (relative) path to the directory folder for the current\n                generation.\n            depth_gen (int): The depth of the generation in the tree.\n            dic_parameter_lists (Optional[dict[str, Any]]): The dictionary of parameter lists.\n                Defaults to None.\n            dic_parameter_lists_for_naming (Optional[dict[str, Any]]): The dictionary of parameter\n                lists for naming. Defaults to None.\n            add_prefix_to_folder_names (bool): Whether to add a prefix to the folder names. Defaults\n                to False.\n\n        Returns:\n            tuple[list[str], list[str]]: The list of study file strings and the list of study paths.\n        \"\"\"\n        executable_path = self.config[\"structure\"][generation][\"executable\"]\n        path_local_template = (\n            f\"{os.path.dirname(inspect.getfile(GenerateScan))}/../assets/template_scripts/\"\n        )\n\n        # Check if the executable path corresponds to a file\n        if not os.path.isfile(executable_path):\n            # Check if the executable path corresponds to a file in the template folder\n            executable_path_template = f\"{path_local_template}{executable_path}\"\n            if not os.path.isfile(executable_path_template):\n                raise FileNotFoundError(\n                    f\"Executable file {executable_path} not found locally nor in the study-da \"\n                    \"template folder.\"\n                )\n            else:\n                executable_path = executable_path_template\n\n        # Ensure that the values in dic_parameter_lists can be dumped with ryaml\n        if dic_parameter_lists is not None:\n            # Recursively convert all numpy types to standard types\n            clean_dic(dic_parameter_lists)\n            logging.info(\"An external dictionary of parameters was provided.\")\n        else:\n            logging.info(\"Creating the dictionnary of parameters from the configuration file.\")\n\n        return self.create_scans(\n            generation,\n            generation_path,\n            executable_path,\n            depth_gen,\n            dic_parameter_lists,\n            dic_parameter_lists_for_naming,\n            add_prefix_to_folder_names,\n        )\n\n    def browse_and_creat_study(\n        self,\n        dic_parameter_all_gen: Optional[dict[str, dict[str, Any]]],\n        dic_parameter_all_gen_naming: Optional[dict[str, dict[str, Any]]],\n        add_prefix_to_folder_names: bool,\n    ) -&gt; dict:\n        l_study_path = [self.config[\"name\"] + \"/\"]\n        dictionary_tree = {}\n\n        # Browse through the generations\n        l_generations = list(self.config[\"structure\"].keys())\n        for idx, generation in enumerate(l_generations):\n            l_study_path_all_next_generation = []\n            logging.info(f\"Taking care of generation: {generation}\")\n            for study_path in l_study_path:\n                if dic_parameter_all_gen is None or generation not in dic_parameter_all_gen:\n                    dic_parameter_current_gen = None\n                    dic_parameter_naming_current_gen = None\n                else:\n                    dic_parameter_current_gen = dic_parameter_all_gen[generation]\n                    if (\n                        dic_parameter_all_gen_naming is not None\n                        and generation in dic_parameter_all_gen_naming\n                    ):\n                        dic_parameter_naming_current_gen = dic_parameter_all_gen_naming[generation]\n                    else:\n                        dic_parameter_naming_current_gen = None\n\n                # Get list of paths for the children of the current study\n                l_study_path_next_generation = self.create_study_for_current_gen(\n                    generation,\n                    study_path,\n                    idx + 1,\n                    dic_parameter_current_gen,\n                    dic_parameter_naming_current_gen,\n                    add_prefix_to_folder_names,\n                )\n                # Update tree\n                dictionary_tree = self.complete_tree(\n                    dictionary_tree, l_study_path_next_generation, generation\n                )\n                # Complete list of paths for the children of all studies (of the current generation)\n                l_study_path_all_next_generation.extend(l_study_path_next_generation)\n\n            # Update study path for next later\n            l_study_path = l_study_path_all_next_generation\n\n        return dictionary_tree\n\n    def create_study(\n        self,\n        tree_file: bool = True,\n        force_overwrite: bool = False,\n        dic_parameter_all_gen: Optional[dict[str, dict[str, Any]]] = None,\n        dic_parameter_all_gen_naming: Optional[dict[str, dict[str, Any]]] = None,\n        add_prefix_to_folder_names: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Creates study files for the entire study.\n\n        Args:\n            tree_file (bool, optional): Whether to write the study tree structure to a YAML file.\n                Defaults to True.\n            force_overwrite (bool, optional): Whether to overwrite existing study files.\n                Defaults to False.\n            dic_parameter_all_gen (Optional[dict[str, dict[str, Any]]]): The dictionary of parameter\n                lists for all generations. Defaults to None.\n            dic_parameter_all_gen_naming (Optional[dict[str, dict[str, Any]]]): The dictionary of\n                parameter lists for all generations for naming. Defaults to None.\n            add_prefix_to_folder_names (bool): Whether to add a prefix to the folder names. Defaults\n                to False.\n\n        Returns:\n            list[str]: The list of study file strings.\n        \"\"\"\n\n        # Raise an error if dic_parameter_all_gen_naming is not None while dic_parameter_all_gen is None\n        if dic_parameter_all_gen is None and dic_parameter_all_gen_naming is not None:\n            raise ValueError(\n                \"If dic_parameter_all_gen_naming is defined, dic_parameter_all_gen must be defined.\"\n            )\n\n        # Remove existing study if force_overwrite\n        if os.path.exists(self.config[\"name\"]):\n            if not force_overwrite:\n                logging.info(\n                    f\"Study {self.config['name']} already exists. Set force_overwrite to True to \"\n                    \"overwrite. Continuing without overwriting.\"\n                )\n                return\n            shutil.rmtree(self.config[\"name\"])\n\n        # Browse through the generations and create the study\n        dictionary_tree = self.browse_and_creat_study(\n            dic_parameter_all_gen,\n            dic_parameter_all_gen_naming,\n            add_prefix_to_folder_names,\n        )\n\n        # Add dependencies to root of the study\n        if \"dependencies\" in self.config:\n            for dependency, path in self.config[\"dependencies\"].items():\n                # Check if the dependency exists as a file\n                if not os.path.isfile(path):\n                    # Check if the dependency exists as a file in the template folder\n                    path_template = f\"{os.path.dirname(inspect.getfile(GenerateScan))}/../assets/configurations/{path}\"\n                    if not os.path.isfile(path_template):\n                        raise FileNotFoundError(\n                            f\"Dependency file {path} not found locally nor in the study-da \"\n                            \"template folder.\"\n                        )\n                    else:\n                        path = path_template\n                shutil.copy2(path, self.config[\"name\"])\n\n        if tree_file:\n            self.write_tree(dictionary_tree)\n\n    @staticmethod\n    def eval_conditions(l_condition: list[str], dic_parameter_lists: dict[str, Any]) -&gt; np.ndarray:\n        \"\"\"\n        Evaluates the conditions to filter out some parameter values.\n\n        Args:\n            l_condition (list[str]): The list of conditions.\n            dic_parameter_lists (dict[str: Any]): The dictionary of parameter lists.\n\n        Returns:\n            np.ndarray: The array of conditions.\n        \"\"\"\n        # Initialize the array of parameters as a meshgrid of all parameters\n        l_parameters = list(dic_parameter_lists.values())\n        meshgrid = np.meshgrid(*l_parameters, indexing=\"ij\")\n\n        # Associate the parameters to their names\n        dic_param_mesh = dict(zip(dic_parameter_lists.keys(), meshgrid))\n\n        # Evaluate the conditions and take the intersection of all conditions\n        array_conditions = np.ones_like(meshgrid[0], dtype=bool)\n        for condition in l_condition:\n            array_conditions = array_conditions &amp; eval(condition, dic_param_mesh)\n\n        return array_conditions\n\n    @staticmethod\n    def filter_for_concomitant_parameters(\n        array_conditions: np.ndarray,\n        ll_concomitant_parameters: list[list[str]],\n        dic_dimension_indices: dict[str, int],\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Filters the conditions for concomitant parameters.\n\n        Args:\n            array_conditions (np.ndarray): The array of conditions.\n            ll_concomitant_parameters (list[list[str]]): The list of concomitant parameters.\n            dic_dimension_indices (dict[str, int]): The dictionary of dimension indices.\n\n        Returns:\n            np.ndarray: The filtered array of conditions.\n        \"\"\"\n\n        # Return the array of conditions if no concomitant parameters\n        if not ll_concomitant_parameters:\n            return array_conditions\n\n        # Get the indices of the concomitant parameters\n        ll_idx_concomitant_parameters = [\n            [dic_dimension_indices[parameter] for parameter in concomitant_parameters]\n            for concomitant_parameters in ll_concomitant_parameters\n        ]\n\n        # Browse all the values of array_conditions\n        for idx, _ in np.ndenumerate(array_conditions):\n            # Check if the value is on the diagonal of the concomitant parameters\n            for l_idx_concomitant_parameter in ll_idx_concomitant_parameters:\n                if any(\n                    idx[i] != idx[j]\n                    for i, j in itertools.combinations(l_idx_concomitant_parameter, 2)\n                ):\n                    array_conditions[idx] = False\n                    break\n\n        return array_conditions\n</code></pre>"},{"location":"reference/study_da/generate/generate_scan.html#study_da.generate.generate_scan.GenerateScan.__init__","title":"<code>__init__(path_config=None, dic_scan=None)</code>","text":"<p>Initialize the generation scan with a configuration file or dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>path_config</code> <code>Optional[str]</code> <p>Path to the configuration file for the scan. Default is None.</p> <code>None</code> <code>dic_scan</code> <code>Optional[dict[str, Any]]</code> <p>Dictionary containing the scan configuration. Default is None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither or both of <code>path_config</code> and <code>dic_scan</code> are provided.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def __init__(\n    self, path_config: Optional[str] = None, dic_scan: Optional[dict[str, Any]] = None\n):  # sourcery skip: remove-redundant-if\n    \"\"\"\n    Initialize the generation scan with a configuration file or dictionary.\n\n    Args:\n        path_config (Optional[str]): Path to the configuration file for the scan.\n            Default is None.\n        dic_scan (Optional[dict[str, Any]]): Dictionary containing the scan configuration.\n            Default is None.\n\n    Raises:\n        ValueError: If neither or both of `path_config` and `dic_scan` are provided.\n    \"\"\"\n    # Load the study configuration from file or dictionary\n    if dic_scan is None and path_config is None:\n        raise ValueError(\n            \"Either a path to the configuration file or a dictionary must be provided.\"\n        )\n    elif dic_scan is not None and path_config is not None:\n        raise ValueError(\"Only one of the configuration file or dictionary must be provided.\")\n    elif path_config is not None:\n        self.config, self.ryaml = load_dic_from_path(path_config)\n    elif dic_scan is not None:\n        self.config = dic_scan\n        self.ryaml = yaml.YAML()\n    else:\n        raise ValueError(\"An unexpected error occurred.\")\n\n    # Parameters common across all generations (e.g. for parallelization)\n    self.dic_common_parameters: dict[str, Any] = {}\n\n    # Path to the tree file\n    self.path_tree = self.config[\"name\"] + \"/\" + \"tree.yaml\"\n</code></pre>"},{"location":"reference/study_da/generate/generate_scan.html#study_da.generate.generate_scan.GenerateScan.browse_and_collect_parameter_space","title":"<code>browse_and_collect_parameter_space(generation)</code>","text":"<p>Browses and collects the parameter space for a given generation.</p> <p>Parameters:</p> Name Type Description Default <code>generation</code> <code>str</code> <p>The generation name.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Any], dict[str, Any], dict[str, Any], list[list[str]], list[str]]</code> <p>tuple[dict[str, Any], dict[str, Any], dict[str, Any], list[list[str]]]: The updated dictionaries of parameter lists.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def browse_and_collect_parameter_space(\n    self,\n    generation: str,\n) -&gt; tuple[\n    dict[str, Any],\n    dict[str, Any],\n    dict[str, Any],\n    list[list[str]],\n    list[str],\n]:\n    \"\"\"\n    Browses and collects the parameter space for a given generation.\n\n    Args:\n        generation (str): The generation name.\n\n    Returns:\n        tuple[dict[str, Any], dict[str, Any], dict[str, Any], list[list[str]]]: The updated\n            dictionaries of parameter lists.\n    \"\"\"\n\n    l_conditions = []\n    ll_concomitant_parameters = []\n    dic_subvariables = {}\n    dic_parameter_lists = {}\n    dic_parameter_lists_for_naming = {}\n    for parameter in self.config[\"structure\"][generation][\"scans\"]:\n        dic_curr_parameter = self.config[\"structure\"][generation][\"scans\"][parameter]\n\n        # Parse the parameter space\n        dic_parameter_lists, dic_parameter_lists_for_naming = self.parse_parameter_space(\n            parameter, dic_curr_parameter, dic_parameter_lists, dic_parameter_lists_for_naming\n        )\n\n        # Store potential subvariables\n        if \"subvariables\" in dic_curr_parameter:\n            dic_subvariables[parameter] = dic_curr_parameter[\"subvariables\"]\n\n        # Save the condition if it exists\n        if \"condition\" in dic_curr_parameter:\n            l_conditions.append(dic_curr_parameter[\"condition\"])\n\n        # Save the concomitant parameters if they exist\n        if \"concomitant\" in dic_curr_parameter:\n            if not isinstance(dic_curr_parameter[\"concomitant\"], list):\n                dic_curr_parameter[\"concomitant\"] = [dic_curr_parameter[\"concomitant\"]]\n            for concomitant_parameter in dic_curr_parameter[\"concomitant\"]:\n                # Assert that the parameters list have the same size\n                assert len(dic_parameter_lists[parameter]) == len(\n                    dic_parameter_lists[concomitant_parameter]\n                ), (\n                    f\"Parameters {parameter} and {concomitant_parameter} must have the \"\n                    \"same size.\"\n                )\n            # Add to the list for filtering later\n            ll_concomitant_parameters.append([parameter] + dic_curr_parameter[\"concomitant\"])\n\n    return (\n        dic_parameter_lists,\n        dic_parameter_lists_for_naming,\n        dic_subvariables,\n        ll_concomitant_parameters,\n        l_conditions,\n    )\n</code></pre>"},{"location":"reference/study_da/generate/generate_scan.html#study_da.generate.generate_scan.GenerateScan.complete_tree","title":"<code>complete_tree(dictionary_tree, l_study_path_next_gen, gen)</code>","text":"<p>Completes the tree structure of the study dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary_tree</code> <code>dict</code> <p>The dictionary representing the study tree structure.</p> required <code>l_study_path_next_gen</code> <code>list[str]</code> <p>The list of study paths for the next gen.</p> required <code>gen</code> <code>str</code> <p>The generation name.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The updated dictionary representing the study tree structure.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def complete_tree(\n    self, dictionary_tree: dict, l_study_path_next_gen: list[str], gen: str\n) -&gt; dict:\n    \"\"\"\n    Completes the tree structure of the study dictionary.\n\n    Args:\n        dictionary_tree (dict): The dictionary representing the study tree structure.\n        l_study_path_next_gen (list[str]): The list of study paths for the next gen.\n        gen (str): The generation name.\n\n    Returns:\n        dict: The updated dictionary representing the study tree structure.\n    \"\"\"\n    logging.info(f\"Completing the tree structure for generation: {gen}\")\n    for path_next in l_study_path_next_gen:\n        nested_set(\n            dictionary_tree,\n            path_next.split(\"/\")[1:-1] + [gen],\n            {\"file\": f\"{path_next}{gen}.py\"},\n        )\n\n    return dictionary_tree\n</code></pre>"},{"location":"reference/study_da/generate/generate_scan.html#study_da.generate.generate_scan.GenerateScan.create_scans","title":"<code>create_scans(generation, generation_path, template_path, depth_gen, dic_parameter_lists=None, dic_parameter_lists_for_naming=None, add_prefix_to_folder_names=False)</code>","text":"<p>Creates study files for parametric scans. If a dictionary of parameter lists is provided, the scan will be done on the parameter lists (no cartesian product). Otherwise, the scan will be done on the cartesian product of the parameters defined in the scan configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>generation</code> <code>str</code> <p>The generation name.</p> required <code>generation_path</code> <code>str</code> <p>The (relative) path to the generation folder.</p> required <code>template_path</code> <code>str</code> <p>The path to the template folder.</p> required <code>depth_gen</code> <code>int</code> <p>The depth of the generation in the tree.</p> required <code>dic_parameter_lists</code> <code>Optional[dict[str, Any]]</code> <p>The dictionary of parameter lists. Defaults to None.</p> <code>None</code> <code>dic_parameter_lists_for_naming</code> <code>Optional[dict[str, Any]]</code> <p>The dictionary of parameter lists for naming. Defaults to None.</p> <code>None</code> <code>add_prefix_to_folder_names</code> <code>bool</code> <p>Whether to add a prefix to the folder names. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>tuple[list[str], list[str]]: The list of study file strings and the list of study paths.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def create_scans(\n    self,\n    generation: str,\n    generation_path: str,\n    template_path: str,\n    depth_gen: int,\n    dic_parameter_lists: Optional[dict[str, Any]] = None,\n    dic_parameter_lists_for_naming: Optional[dict[str, Any]] = None,\n    add_prefix_to_folder_names: bool = False,\n) -&gt; list[str]:\n    \"\"\"\n    Creates study files for parametric scans.\n    If a dictionary of parameter lists is provided, the scan will be done on the parameter\n    lists (no cartesian product). Otherwise, the scan will be done on the cartesian product of\n    the parameters defined in the scan configuration file.\n\n    Args:\n        generation (str): The generation name.\n        generation_path (str): The (relative) path to the generation folder.\n        template_path (str): The path to the template folder.\n        depth_gen (int): The depth of the generation in the tree.\n        dic_parameter_lists (Optional[dict[str, Any]]): The dictionary of parameter lists.\n            Defaults to None.\n        dic_parameter_lists_for_naming (Optional[dict[str, Any]]): The dictionary of parameter\n            lists for naming. Defaults to None.\n        add_prefix_to_folder_names (bool): Whether to add a prefix to the folder names. Defaults\n            to False.\n\n    Returns:\n        tuple[list[str], list[str]]: The list of study file strings and the list of study paths.\n    \"\"\"\n    if dic_parameter_lists is None:\n        # Get dictionnary of parametric values being scanned\n        dic_parameter_lists, dic_parameter_lists_for_naming, array_conditions = (\n            self.get_dic_parametric_scans(generation)\n        )\n        apply_cartesian_product = True\n    else:\n        if dic_parameter_lists_for_naming is None:\n            dic_parameter_lists_for_naming = copy.deepcopy(dic_parameter_lists)\n        array_conditions = None\n        apply_cartesian_product = False\n\n    # Generate render write for the parameters parameters\n    l_study_path = []\n    if apply_cartesian_product:\n        logging.info(\n            f\"Now generation cartesian product of all parameters for generation: {generation}\"\n        )\n        array_param_values = itertools.product(*dic_parameter_lists.values())\n        array_param_values_for_naming = itertools.product(\n            *dic_parameter_lists_for_naming.values()\n        )\n        array_idx = itertools.product(*[range(len(x)) for x in dic_parameter_lists.values()])\n    else:\n        logging.info(f\"Now generation parameters for generation: {generation}\")\n        array_param_values = [list(x) for x in zip(*dic_parameter_lists.values())]\n        array_param_values_for_naming = [\n            list(x) for x in zip(*dic_parameter_lists_for_naming.values())\n        ]\n        array_idx = range(len(array_param_values))\n\n    # Loop over the parameters\n    to_disk_len = np.sum(array_conditions) if array_conditions is not None else 1\n    to_disk_idx = 0\n    for idx, (l_values, l_values_for_naming, l_idx) in enumerate(\n        zip(array_param_values, array_param_values_for_naming, array_idx)\n    ):\n        # Check the idx to keep if conditions are present\n        if array_conditions is not None and not array_conditions[l_idx]:\n            continue\n\n        # Create the path for the study\n        dic_mutated_parameters = dict(zip(dic_parameter_lists.keys(), l_values))\n        dic_mutated_parameters_for_naming = dict(\n            zip(dic_parameter_lists.keys(), l_values_for_naming)\n        )\n\n        # Handle prefix\n        prefix_path = \"\"\n        if add_prefix_to_folder_names:\n            prefix_path = f\"ID_{str(to_disk_idx).zfill(len(str(to_disk_len)))}_\"\n            to_disk_idx += 1\n\n        # Handle suffix\n        suffix_path = \"_\".join(\n            [\n                f\"{parameter}_{value}\"\n                for parameter, value in dic_mutated_parameters_for_naming.items()\n            ]\n        )\n        suffix_path = suffix_path.removeprefix(\"_\")\n\n        # Create final path\n        path = generation_path + prefix_path + suffix_path + \"/\"\n\n        # Add common parameters\n        if generation in self.dic_common_parameters:\n            dic_mutated_parameters |= self.dic_common_parameters[generation]\n\n        # Remove \"\" from mutated parameters, if it's in the dictionary\n        # as it's only used when no scan is done\n        if \"\" in dic_mutated_parameters:\n            dic_mutated_parameters.pop(\"\")\n\n        # Generate the study for current generation\n        self.generate_render_write(\n            generation,\n            path,\n            template_path,\n            depth_gen,\n            dic_mutated_parameters=dic_mutated_parameters,\n        )\n\n        # Append the list of study paths to build the tree later on\n        l_study_path.append(path)\n\n    if not l_study_path:\n        logging.warning(\n            f\"No study paths were created for generation {generation}.\"\n            \"Please check the conditions.\"\n        )\n\n    return l_study_path\n</code></pre>"},{"location":"reference/study_da/generate/generate_scan.html#study_da.generate.generate_scan.GenerateScan.create_study","title":"<code>create_study(tree_file=True, force_overwrite=False, dic_parameter_all_gen=None, dic_parameter_all_gen_naming=None, add_prefix_to_folder_names=False)</code>","text":"<p>Creates study files for the entire study.</p> <p>Parameters:</p> Name Type Description Default <code>tree_file</code> <code>bool</code> <p>Whether to write the study tree structure to a YAML file. Defaults to True.</p> <code>True</code> <code>force_overwrite</code> <code>bool</code> <p>Whether to overwrite existing study files. Defaults to False.</p> <code>False</code> <code>dic_parameter_all_gen</code> <code>Optional[dict[str, dict[str, Any]]]</code> <p>The dictionary of parameter lists for all generations. Defaults to None.</p> <code>None</code> <code>dic_parameter_all_gen_naming</code> <code>Optional[dict[str, dict[str, Any]]]</code> <p>The dictionary of parameter lists for all generations for naming. Defaults to None.</p> <code>None</code> <code>add_prefix_to_folder_names</code> <code>bool</code> <p>Whether to add a prefix to the folder names. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>list[str]: The list of study file strings.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def create_study(\n    self,\n    tree_file: bool = True,\n    force_overwrite: bool = False,\n    dic_parameter_all_gen: Optional[dict[str, dict[str, Any]]] = None,\n    dic_parameter_all_gen_naming: Optional[dict[str, dict[str, Any]]] = None,\n    add_prefix_to_folder_names: bool = False,\n) -&gt; None:\n    \"\"\"\n    Creates study files for the entire study.\n\n    Args:\n        tree_file (bool, optional): Whether to write the study tree structure to a YAML file.\n            Defaults to True.\n        force_overwrite (bool, optional): Whether to overwrite existing study files.\n            Defaults to False.\n        dic_parameter_all_gen (Optional[dict[str, dict[str, Any]]]): The dictionary of parameter\n            lists for all generations. Defaults to None.\n        dic_parameter_all_gen_naming (Optional[dict[str, dict[str, Any]]]): The dictionary of\n            parameter lists for all generations for naming. Defaults to None.\n        add_prefix_to_folder_names (bool): Whether to add a prefix to the folder names. Defaults\n            to False.\n\n    Returns:\n        list[str]: The list of study file strings.\n    \"\"\"\n\n    # Raise an error if dic_parameter_all_gen_naming is not None while dic_parameter_all_gen is None\n    if dic_parameter_all_gen is None and dic_parameter_all_gen_naming is not None:\n        raise ValueError(\n            \"If dic_parameter_all_gen_naming is defined, dic_parameter_all_gen must be defined.\"\n        )\n\n    # Remove existing study if force_overwrite\n    if os.path.exists(self.config[\"name\"]):\n        if not force_overwrite:\n            logging.info(\n                f\"Study {self.config['name']} already exists. Set force_overwrite to True to \"\n                \"overwrite. Continuing without overwriting.\"\n            )\n            return\n        shutil.rmtree(self.config[\"name\"])\n\n    # Browse through the generations and create the study\n    dictionary_tree = self.browse_and_creat_study(\n        dic_parameter_all_gen,\n        dic_parameter_all_gen_naming,\n        add_prefix_to_folder_names,\n    )\n\n    # Add dependencies to root of the study\n    if \"dependencies\" in self.config:\n        for dependency, path in self.config[\"dependencies\"].items():\n            # Check if the dependency exists as a file\n            if not os.path.isfile(path):\n                # Check if the dependency exists as a file in the template folder\n                path_template = f\"{os.path.dirname(inspect.getfile(GenerateScan))}/../assets/configurations/{path}\"\n                if not os.path.isfile(path_template):\n                    raise FileNotFoundError(\n                        f\"Dependency file {path} not found locally nor in the study-da \"\n                        \"template folder.\"\n                    )\n                else:\n                    path = path_template\n            shutil.copy2(path, self.config[\"name\"])\n\n    if tree_file:\n        self.write_tree(dictionary_tree)\n</code></pre>"},{"location":"reference/study_da/generate/generate_scan.html#study_da.generate.generate_scan.GenerateScan.create_study_for_current_gen","title":"<code>create_study_for_current_gen(generation, generation_path, depth_gen, dic_parameter_lists=None, dic_parameter_lists_for_naming=None, add_prefix_to_folder_names=False)</code>","text":"<p>Creates study files for the current generation.</p> <p>Parameters:</p> Name Type Description Default <code>generation</code> <code>str</code> <p>The name of the current generation.</p> required <code>directory_path</code> <code>str</code> <p>The (relative) path to the directory folder for the current generation.</p> required <code>depth_gen</code> <code>int</code> <p>The depth of the generation in the tree.</p> required <code>dic_parameter_lists</code> <code>Optional[dict[str, Any]]</code> <p>The dictionary of parameter lists. Defaults to None.</p> <code>None</code> <code>dic_parameter_lists_for_naming</code> <code>Optional[dict[str, Any]]</code> <p>The dictionary of parameter lists for naming. Defaults to None.</p> <code>None</code> <code>add_prefix_to_folder_names</code> <code>bool</code> <p>Whether to add a prefix to the folder names. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>tuple[list[str], list[str]]: The list of study file strings and the list of study paths.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def create_study_for_current_gen(\n    self,\n    generation: str,\n    generation_path: str,\n    depth_gen: int,\n    dic_parameter_lists: Optional[dict[str, Any]] = None,\n    dic_parameter_lists_for_naming: Optional[dict[str, Any]] = None,\n    add_prefix_to_folder_names: bool = False,\n) -&gt; list[str]:\n    \"\"\"\n    Creates study files for the current generation.\n\n    Args:\n        generation (str): The name of the current generation.\n        directory_path (str): The (relative) path to the directory folder for the current\n            generation.\n        depth_gen (int): The depth of the generation in the tree.\n        dic_parameter_lists (Optional[dict[str, Any]]): The dictionary of parameter lists.\n            Defaults to None.\n        dic_parameter_lists_for_naming (Optional[dict[str, Any]]): The dictionary of parameter\n            lists for naming. Defaults to None.\n        add_prefix_to_folder_names (bool): Whether to add a prefix to the folder names. Defaults\n            to False.\n\n    Returns:\n        tuple[list[str], list[str]]: The list of study file strings and the list of study paths.\n    \"\"\"\n    executable_path = self.config[\"structure\"][generation][\"executable\"]\n    path_local_template = (\n        f\"{os.path.dirname(inspect.getfile(GenerateScan))}/../assets/template_scripts/\"\n    )\n\n    # Check if the executable path corresponds to a file\n    if not os.path.isfile(executable_path):\n        # Check if the executable path corresponds to a file in the template folder\n        executable_path_template = f\"{path_local_template}{executable_path}\"\n        if not os.path.isfile(executable_path_template):\n            raise FileNotFoundError(\n                f\"Executable file {executable_path} not found locally nor in the study-da \"\n                \"template folder.\"\n            )\n        else:\n            executable_path = executable_path_template\n\n    # Ensure that the values in dic_parameter_lists can be dumped with ryaml\n    if dic_parameter_lists is not None:\n        # Recursively convert all numpy types to standard types\n        clean_dic(dic_parameter_lists)\n        logging.info(\"An external dictionary of parameters was provided.\")\n    else:\n        logging.info(\"Creating the dictionnary of parameters from the configuration file.\")\n\n    return self.create_scans(\n        generation,\n        generation_path,\n        executable_path,\n        depth_gen,\n        dic_parameter_lists,\n        dic_parameter_lists_for_naming,\n        add_prefix_to_folder_names,\n    )\n</code></pre>"},{"location":"reference/study_da/generate/generate_scan.html#study_da.generate.generate_scan.GenerateScan.eval_conditions","title":"<code>eval_conditions(l_condition, dic_parameter_lists)</code>  <code>staticmethod</code>","text":"<p>Evaluates the conditions to filter out some parameter values.</p> <p>Parameters:</p> Name Type Description Default <code>l_condition</code> <code>list[str]</code> <p>The list of conditions.</p> required <code>dic_parameter_lists</code> <code>dict[str</code> <p>Any]): The dictionary of parameter lists.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The array of conditions.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>@staticmethod\ndef eval_conditions(l_condition: list[str], dic_parameter_lists: dict[str, Any]) -&gt; np.ndarray:\n    \"\"\"\n    Evaluates the conditions to filter out some parameter values.\n\n    Args:\n        l_condition (list[str]): The list of conditions.\n        dic_parameter_lists (dict[str: Any]): The dictionary of parameter lists.\n\n    Returns:\n        np.ndarray: The array of conditions.\n    \"\"\"\n    # Initialize the array of parameters as a meshgrid of all parameters\n    l_parameters = list(dic_parameter_lists.values())\n    meshgrid = np.meshgrid(*l_parameters, indexing=\"ij\")\n\n    # Associate the parameters to their names\n    dic_param_mesh = dict(zip(dic_parameter_lists.keys(), meshgrid))\n\n    # Evaluate the conditions and take the intersection of all conditions\n    array_conditions = np.ones_like(meshgrid[0], dtype=bool)\n    for condition in l_condition:\n        array_conditions = array_conditions &amp; eval(condition, dic_param_mesh)\n\n    return array_conditions\n</code></pre>"},{"location":"reference/study_da/generate/generate_scan.html#study_da.generate.generate_scan.GenerateScan.filter_for_concomitant_parameters","title":"<code>filter_for_concomitant_parameters(array_conditions, ll_concomitant_parameters, dic_dimension_indices)</code>  <code>staticmethod</code>","text":"<p>Filters the conditions for concomitant parameters.</p> <p>Parameters:</p> Name Type Description Default <code>array_conditions</code> <code>ndarray</code> <p>The array of conditions.</p> required <code>ll_concomitant_parameters</code> <code>list[list[str]]</code> <p>The list of concomitant parameters.</p> required <code>dic_dimension_indices</code> <code>dict[str, int]</code> <p>The dictionary of dimension indices.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The filtered array of conditions.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>@staticmethod\ndef filter_for_concomitant_parameters(\n    array_conditions: np.ndarray,\n    ll_concomitant_parameters: list[list[str]],\n    dic_dimension_indices: dict[str, int],\n) -&gt; np.ndarray:\n    \"\"\"\n    Filters the conditions for concomitant parameters.\n\n    Args:\n        array_conditions (np.ndarray): The array of conditions.\n        ll_concomitant_parameters (list[list[str]]): The list of concomitant parameters.\n        dic_dimension_indices (dict[str, int]): The dictionary of dimension indices.\n\n    Returns:\n        np.ndarray: The filtered array of conditions.\n    \"\"\"\n\n    # Return the array of conditions if no concomitant parameters\n    if not ll_concomitant_parameters:\n        return array_conditions\n\n    # Get the indices of the concomitant parameters\n    ll_idx_concomitant_parameters = [\n        [dic_dimension_indices[parameter] for parameter in concomitant_parameters]\n        for concomitant_parameters in ll_concomitant_parameters\n    ]\n\n    # Browse all the values of array_conditions\n    for idx, _ in np.ndenumerate(array_conditions):\n        # Check if the value is on the diagonal of the concomitant parameters\n        for l_idx_concomitant_parameter in ll_idx_concomitant_parameters:\n            if any(\n                idx[i] != idx[j]\n                for i, j in itertools.combinations(l_idx_concomitant_parameter, 2)\n            ):\n                array_conditions[idx] = False\n                break\n\n    return array_conditions\n</code></pre>"},{"location":"reference/study_da/generate/generate_scan.html#study_da.generate.generate_scan.GenerateScan.generate_render_write","title":"<code>generate_render_write(gen_name, job_directory_path, template_path, depth_gen, dic_mutated_parameters={})</code>","text":"<p>Generates, renders, and writes the study file.</p> <p>Parameters:</p> Name Type Description Default <code>gen_name</code> <code>str</code> <p>The name of the generation.</p> required <code>study_path</code> <code>str</code> <p>The path to the job folder.</p> required <code>template_path</code> <code>str</code> <p>The path to the template folder.</p> required <code>depth_gen</code> <code>int</code> <p>The depth of the generation in the tree.</p> required <code>dic_mutated_parameters</code> <code>dict[str, Any]</code> <p>The dictionary of mutated parameters. Defaults to {}.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>tuple[str, list[str]]: The study file string and the list of study paths.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def generate_render_write(\n    self,\n    gen_name: str,\n    job_directory_path: str,\n    template_path: str,\n    depth_gen: int,\n    dic_mutated_parameters: dict[str, Any] = {},\n) -&gt; list[str]:  # sourcery skip: default-mutable-arg\n    \"\"\"\n    Generates, renders, and writes the study file.\n\n    Args:\n        gen_name (str): The name of the generation.\n        study_path (str): The path to the job folder.\n        template_path (str): The path to the template folder.\n        depth_gen (int): The depth of the generation in the tree.\n        dic_mutated_parameters (dict[str, Any], optional): The dictionary of mutated parameters.\n            Defaults to {}.\n\n    Returns:\n        tuple[str, list[str]]: The study file string and the list of study paths.\n    \"\"\"\n\n    directory_path_gen = f\"{job_directory_path}\"\n    if not directory_path_gen.endswith(\"/\"):\n        directory_path_gen += \"/\"\n    file_path_gen = f\"{directory_path_gen}{gen_name}.py\"\n    logging.info(f'Now rendering generation \"{file_path_gen}\"')\n\n    # Generate the string of parameters\n    str_parameters = \"{\"\n    for key, value in dic_mutated_parameters.items():\n        if isinstance(value, str):\n            str_parameters += f\"'{key}' : '{value}', \"\n        else:\n            str_parameters += f\"'{key}' : {value}, \"\n    str_parameters += \"}\"\n\n    # Adapt the dict of dependencies to the current generation\n    dic_dependencies = self.config[\"dependencies\"] if \"dependencies\" in self.config else {}\n\n    # Unpacking list of dependencies\n    dic_dependencies = {\n        **{\n            key: value for key, value in dic_dependencies.items() if not isinstance(value, list)\n        },\n        **{\n            f\"{key}_{str(i).zfill(len(str(len(value))))}\": i_value\n            for key, value in dic_dependencies.items()\n            if isinstance(value, list)\n            for i, i_value in enumerate(value)\n        },\n    }\n    self.config[\"dependencies\"] = dic_dependencies\n\n    # Initial dependencies are always copied at the root of the study (hence value.split(\"/\")[-1])\n    dic_dependencies = {\n        key: \"../\" * depth_gen + value.split(\"/\")[-1] for key, value in dic_dependencies.items()\n    }\n\n    # Always load configuration from above generation, and remove the path from dependencies\n    path_main_configuration = \"../\" + dic_dependencies.pop(\"main_configuration\").split(\"/\")[-1]\n\n    # Create the str for the dependencies\n    str_dependencies = \"{\"\n    for key, value in dic_dependencies.items():\n        str_dependencies += f\"'{key}' : '{value}', \"\n    str_dependencies += \"}\"\n\n    # Render and write the study file\n    study_str = self.render(\n        str_parameters,\n        template_path=template_path,\n        path_main_configuration=path_main_configuration,\n        study_path=os.path.abspath(self.config[\"name\"]),\n        str_dependencies=str_dependencies,\n    )\n\n    self.write(study_str, file_path_gen)\n    return [directory_path_gen]\n</code></pre>"},{"location":"reference/study_da/generate/generate_scan.html#study_da.generate.generate_scan.GenerateScan.get_dic_parametric_scans","title":"<code>get_dic_parametric_scans(generation)</code>","text":"<p>Retrieves dictionaries of parametric scan values.</p> <p>Parameters:</p> Name Type Description Default <code>generation</code> <code>str</code> <p>The generation name.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Any], dict[str, Any], ndarray | None]</code> <p>tuple[dict[str, Any], dict[str, Any], np.ndarray|None]: The dictionaries of parametric scan values, another dictionnary with better naming for the tree creation, and an array of conditions to filter out some parameter values.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def get_dic_parametric_scans(\n    self, generation: str\n) -&gt; tuple[dict[str, Any], dict[str, Any], np.ndarray | None]:\n    \"\"\"\n    Retrieves dictionaries of parametric scan values.\n\n    Args:\n        generation: The generation name.\n\n    Returns:\n        tuple[dict[str, Any], dict[str, Any], np.ndarray|None]: The dictionaries of parametric\n            scan values, another dictionnary with better naming for the tree creation, and an\n            array of conditions to filter out some parameter values.\n    \"\"\"\n\n    if generation == \"base\":\n        raise ValueError(\"Generation 'base' should not have scans.\")\n\n    # Remember common parameters as they might be used across generations\n    if \"common_parameters\" in self.config[\"structure\"][generation]:\n        self.dic_common_parameters[generation] = {}\n        for parameter in self.config[\"structure\"][generation][\"common_parameters\"]:\n            self.dic_common_parameters[generation][parameter] = self.config[\"structure\"][\n                generation\n            ][\"common_parameters\"][parameter]\n\n    # Check that the generation has scans\n    if (\n        \"scans\" not in self.config[\"structure\"][generation]\n        or self.config[\"structure\"][generation][\"scans\"] is None\n    ):\n        dic_parameter_lists = {\"\": [generation]}\n        dic_parameter_lists_for_naming = {\"\": [generation]}\n        array_conditions = None\n        ll_concomitant_parameters = []\n    else:\n        # Browse and collect the parameter space for the generation\n        (\n            dic_parameter_lists,\n            dic_parameter_lists_for_naming,\n            dic_subvariables,\n            ll_concomitant_parameters,\n            l_conditions,\n        ) = self.browse_and_collect_parameter_space(generation)\n\n        # Get the dimension corresponding to each parameter\n        dic_dimension_indices = {\n            parameter: idx for idx, parameter in enumerate(dic_parameter_lists)\n        }\n\n        # Generate array of conditions to filter out some of the values later\n        # Is an array of True values if no conditions are present\n        array_conditions = self.eval_conditions(l_conditions, dic_parameter_lists)\n\n        # Filter for concomitant parameters\n        array_conditions = self.filter_for_concomitant_parameters(\n            array_conditions, ll_concomitant_parameters, dic_dimension_indices\n        )\n\n        # Postprocess the parameter lists and update the dictionaries\n        dic_parameter_lists, dic_parameter_lists_for_naming = self.postprocess_parameter_lists(\n            dic_parameter_lists, dic_parameter_lists_for_naming, dic_subvariables\n        )\n\n    return (\n        dic_parameter_lists,\n        dic_parameter_lists_for_naming,\n        array_conditions,\n    )\n</code></pre>"},{"location":"reference/study_da/generate/generate_scan.html#study_da.generate.generate_scan.GenerateScan.parse_parameter_space","title":"<code>parse_parameter_space(parameter, dic_curr_parameter, dic_parameter_lists, dic_parameter_lists_for_naming)</code>","text":"<p>Parses the parameter space for a given parameter.</p> <p>Parameters:</p> Name Type Description Default <code>parameter</code> <code>str</code> <p>The parameter name.</p> required <code>dic_curr_parameter</code> <code>dict[str, Any]</code> <p>The dictionary of current parameter values.</p> required <code>dic_parameter_lists</code> <code>dict[str, Any]</code> <p>The dictionary of parameter lists.</p> required <code>dic_parameter_lists_for_naming</code> <code>dict[str, Any]</code> <p>The dictionary of parameter lists for naming.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Any], dict[str, Any]]</code> <p>tuple[dict[str, Any], dict[str, Any]]: The updated dictionaries of parameter lists.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def parse_parameter_space(\n    self,\n    parameter: str,\n    dic_curr_parameter: dict[str, Any],\n    dic_parameter_lists: dict[str, Any],\n    dic_parameter_lists_for_naming: dict[str, Any],\n) -&gt; tuple[dict[str, Any], dict[str, Any]]:\n    \"\"\"\n    Parses the parameter space for a given parameter.\n\n    Args:\n        parameter (str): The parameter name.\n        dic_curr_parameter (dict[str, Any]): The dictionary of current parameter values.\n        dic_parameter_lists (dict[str, Any]): The dictionary of parameter lists.\n        dic_parameter_lists_for_naming (dict[str, Any]): The dictionary of parameter lists for naming.\n\n    Returns:\n        tuple[dict[str, Any], dict[str, Any]]: The updated dictionaries of parameter lists.\n    \"\"\"\n\n    if \"linspace\" in dic_curr_parameter:\n        parameter_list = linspace(dic_curr_parameter[\"linspace\"])\n        dic_parameter_lists_for_naming[parameter] = parameter_list\n    elif \"logspace\" in dic_curr_parameter:\n        parameter_list = logspace(dic_curr_parameter[\"logspace\"])\n        dic_parameter_lists_for_naming[parameter] = parameter_list\n    elif \"path_list\" in dic_curr_parameter:\n        l_values_path_list = dic_curr_parameter[\"path_list\"]\n        parameter_list = list_values_path(l_values_path_list, self.dic_common_parameters)\n        dic_parameter_lists_for_naming[parameter] = [\n            f\"{n:02d}\" for n, path in enumerate(parameter_list)\n        ]\n    elif \"list\" in dic_curr_parameter:\n        parameter_list = dic_curr_parameter[\"list\"]\n        dic_parameter_lists_for_naming[parameter] = parameter_list\n    elif \"expression\" in dic_curr_parameter:\n        parameter_list = np.round(\n            eval(dic_curr_parameter[\"expression\"], copy.deepcopy(dic_parameter_lists)),\n            8,\n        )\n        dic_parameter_lists_for_naming[parameter] = parameter_list\n    else:\n        raise ValueError(f\"Scanning method for parameter {parameter} is not recognized.\")\n\n    dic_parameter_lists[parameter] = np.array(parameter_list)\n    return dic_parameter_lists, dic_parameter_lists_for_naming\n</code></pre>"},{"location":"reference/study_da/generate/generate_scan.html#study_da.generate.generate_scan.GenerateScan.postprocess_parameter_lists","title":"<code>postprocess_parameter_lists(dic_parameter_lists, dic_parameter_lists_for_naming, dic_subvariables)</code>","text":"<p>Post-processes parameter lists by ensuring values are not numpy types and handling nested parameters.</p> <p>Parameters:</p> Name Type Description Default <code>dic_parameter_lists</code> <code>dict[str, Any]</code> <p>Dictionary containing parameter lists.</p> required <code>dic_parameter_lists_for_naming</code> <code>dict[str, Any]</code> <p>Dictionary containing parameter lists for naming.</p> required <code>dic_subvariables</code> <code>dict[str, Any]</code> <p>Dictionary containing subvariables for nested parameters.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Any], dict[str, Any]]</code> <p>tuple[dict[str, Any], dict[str, Any]]: Updated dictionaries of parameter lists and parameter lists for naming.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def postprocess_parameter_lists(\n    self,\n    dic_parameter_lists: dict[str, Any],\n    dic_parameter_lists_for_naming: dict[str, Any],\n    dic_subvariables: dict[str, Any],\n) -&gt; tuple[dict[str, Any], dict[str, Any]]:\n    \"\"\"\n    Post-processes parameter lists by ensuring values are not numpy types and handling nested\n    parameters.\n\n    Args:\n        dic_parameter_lists (dict[str, Any]): Dictionary containing parameter lists.\n        dic_parameter_lists_for_naming (dict[str, Any]): Dictionary containing parameter lists\n            for naming.\n        dic_subvariables (dict[str, Any]): Dictionary containing subvariables for nested\n            parameters.\n\n    Returns:\n        tuple[dict[str, Any], dict[str, Any]]: Updated dictionaries of parameter lists and\n            parameter lists for naming.\n    \"\"\"\n    for parameter, parameter_list in dic_parameter_lists.items():\n        parameter_list_for_naming = dic_parameter_lists_for_naming[parameter]\n\n        # Ensure that all values are not numpy types (to avoid serialization issues)\n        parameter_list = [x.item() if isinstance(x, np.generic) else x for x in parameter_list]\n\n        # Handle nested parameters\n        parameter_list_updated = (\n            convert_for_subvariables(dic_subvariables[parameter], parameter_list)\n            if parameter in dic_subvariables\n            else parameter_list\n        )\n        # Update the dictionaries\n        dic_parameter_lists[parameter] = parameter_list_updated\n        dic_parameter_lists_for_naming[parameter] = parameter_list_for_naming\n\n    return dic_parameter_lists, dic_parameter_lists_for_naming\n</code></pre>"},{"location":"reference/study_da/generate/generate_scan.html#study_da.generate.generate_scan.GenerateScan.render","title":"<code>render(str_parameters, template_path, path_main_configuration, study_path=None, str_dependencies=None)</code>","text":"<p>Renders the study file using a template.</p> <p>Parameters:</p> Name Type Description Default <code>str_parameters</code> <code>str</code> <p>The string representation of parameters to declare/mutate.</p> required <code>template_path</code> <code>str</code> <p>The path to the template file.</p> required <code>path_main_configuration</code> <code>str</code> <p>The path to the main configuration file.</p> required <code>study_path</code> <code>str</code> <p>The path to the root of the study. Defaults to None.</p> <code>None</code> <code>dependencies</code> <code>dict[str, str]</code> <p>The dictionary of dependencies. Defaults to {}.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The rendered study file.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def render(\n    self,\n    str_parameters: str,\n    template_path: str,\n    path_main_configuration: str,\n    study_path: Optional[str] = None,\n    str_dependencies: Optional[dict[str, str]] = None,\n) -&gt; str:\n    \"\"\"\n    Renders the study file using a template.\n\n    Args:\n        str_parameters (str): The string representation of parameters to declare/mutate.\n        template_path (str): The path to the template file.\n        path_main_configuration (str): The path to the main configuration file.\n        study_path (str, optional): The path to the root of the study. Defaults to None.\n        dependencies (dict[str, str], optional): The dictionary of dependencies. Defaults to {}.\n\n    Returns:\n        str: The rendered study file.\n    \"\"\"\n\n    # Handle mutable default argument\n    if str_dependencies is None:\n        dependencies = \"\"\n    if study_path is None:\n        study_path = \"\"\n\n    # Generate generations from template\n    directory_path = os.path.dirname(template_path)\n    template_name = os.path.basename(template_path)\n    environment = Environment(\n        loader=FileSystemLoader(directory_path),\n        variable_start_string=\"{}  ###---\",\n        variable_end_string=\"---###\",\n    )\n    template = environment.get_template(template_name)\n\n    # Better not to render the dependencies path this way, as it becomes too cumbersome to\n    # handle the paths when using clusters\n\n    return template.render(\n        parameters=str_parameters,\n        main_configuration=path_main_configuration,\n        path_root_study=study_path,\n        # dependencies = str_dependencies,\n    )\n</code></pre>"},{"location":"reference/study_da/generate/generate_scan.html#study_da.generate.generate_scan.GenerateScan.write","title":"<code>write(study_str, file_path, format_with_black=True)</code>","text":"<p>Writes the study file to disk.</p> <p>Parameters:</p> Name Type Description Default <code>study_str</code> <code>str</code> <p>The study file string.</p> required <code>file_path</code> <code>str</code> <p>The path to write the study file.</p> required <code>format_with_black</code> <code>bool</code> <p>Whether to format the output file with black. Defaults to True.</p> <code>True</code> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def write(self, study_str: str, file_path: str, format_with_black: bool = True):\n    \"\"\"\n    Writes the study file to disk.\n\n    Args:\n        study_str (str): The study file string.\n        file_path (str): The path to write the study file.\n        format_with_black (bool, optional): Whether to format the output file with black.\n            Defaults to True.\n    \"\"\"\n\n    # Format the string with black\n    if format_with_black:\n        study_str = format_str(study_str, mode=FileMode())\n\n    # Make folder if it doesn't exist\n    folder = os.path.dirname(file_path)\n    if folder != \"\":\n        os.makedirs(folder, exist_ok=True)\n\n    with open(file_path, mode=\"w\", encoding=\"utf-8\") as file:\n        file.write(study_str)\n</code></pre>"},{"location":"reference/study_da/generate/generate_scan.html#study_da.generate.generate_scan.GenerateScan.write_tree","title":"<code>write_tree(dictionary_tree)</code>","text":"<p>Writes the study tree structure to a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary_tree</code> <code>dict</code> <p>The dictionary representing the study tree structure.</p> required Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def write_tree(self, dictionary_tree: dict):\n    \"\"\"\n    Writes the study tree structure to a YAML file.\n\n    Args:\n        dictionary_tree (dict): The dictionary representing the study tree structure.\n    \"\"\"\n    logging.info(\"Writing the tree structure to a YAML file.\")\n    ryaml = yaml.YAML()\n    with open(self.path_tree, \"w\") as yaml_file:\n        ryaml.indent(sequence=4, offset=2)\n        ryaml.dump(dictionary_tree, yaml_file)\n</code></pre>"},{"location":"reference/study_da/generate/parameter_space.html","title":"parameter_space","text":"<p>This module provides functions to generate parameter spaces for studies.</p> <p>Functions:</p> Name Description <code>convert_for_subvariables</code> <p>list[str], parameter_list: list) -&gt; list: Convert the parameter list to a list of dictionaries with subvariables as keys.</p> <code>linspace</code> <p>list) -&gt; np.ndarray: Generate a list of evenly spaced values over a specified interval.</p> <code>logspace</code> <p>list) -&gt; np.ndarray: Generate a list of values that are evenly spaced on a log scale.</p> <code>list_values_path</code> <p>list[str], dic_common_parameters: dict[str, Any]) -&gt; list[str]: Generate a list of path names from an initial path name.</p>"},{"location":"reference/study_da/generate/parameter_space.html#study_da.generate.parameter_space.convert_for_subvariables","title":"<code>convert_for_subvariables(l_subvariables, parameter_list)</code>","text":"<p>Convert the parameter list to a list of dictionaries with subvariables as keys.</p> <p>Parameters:</p> Name Type Description Default <code>l_subvariables</code> <code>list[str]</code> <p>List of subvariables.</p> required <code>parameter_list</code> <code>list</code> <p>List with the parameter values.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>List of dictionaries with subvariables as keys.</p> Source code in <code>study_da/generate/parameter_space.py</code> <pre><code>def convert_for_subvariables(l_subvariables: list[str], parameter_list: list) -&gt; list:\n    \"\"\"Convert the parameter list to a list of dictionaries with subvariables as keys.\n\n    Args:\n        l_subvariables (list[str]): List of subvariables.\n        parameter_list (list): List with the parameter values.\n\n    Returns:\n        list: List of dictionaries with subvariables as keys.\n    \"\"\"\n    return [{subvar: value for subvar in l_subvariables} for value in parameter_list]\n</code></pre>"},{"location":"reference/study_da/generate/parameter_space.html#study_da.generate.parameter_space.linspace","title":"<code>linspace(l_values_linspace)</code>","text":"<p>Generate a list of evenly spaced values over a specified interval.</p> <p>Parameters:</p> Name Type Description Default <code>l_values_linspace</code> <code>list</code> <p>List with the values for the linspace function.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: List of evenly spaced values.</p> Source code in <code>study_da/generate/parameter_space.py</code> <pre><code>def linspace(l_values_linspace: list) -&gt; np.ndarray:\n    \"\"\"Generate a list of evenly spaced values over a specified interval.\n\n    Args:\n        l_values_linspace (list): List with the values for the linspace function.\n\n    Returns:\n        np.ndarray: List of evenly spaced values.\"\"\"\n\n    # Check that all values in the list are floats or integers\n    if not all(isinstance(value, (float, int)) for value in l_values_linspace):\n        raise ValueError(\n            \"All values in the list for the linspace function must be floats or integers.\"\n        )\n    return np.round(\n        np.linspace(\n            l_values_linspace[0],\n            l_values_linspace[1],\n            l_values_linspace[2],\n            endpoint=True,\n        ),\n        8,\n    )\n</code></pre>"},{"location":"reference/study_da/generate/parameter_space.html#study_da.generate.parameter_space.list_values_path","title":"<code>list_values_path(l_values_path_list, dic_common_parameters)</code>","text":"<p>Generate a list of path names from an inital path name.</p> <p>Parameters:</p> Name Type Description Default <code>l_values_path_list</code> <code>list</code> <p>List with the initial path names and number of paths.</p> required <code>dic_common_parameters</code> <code>dict</code> <p>Dictionary with the parameters common to the whole study.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list[str]</code> <p>List of final path values from the initial paths.</p> Source code in <code>study_da/generate/parameter_space.py</code> <pre><code>def list_values_path(\n    l_values_path_list: list[str], dic_common_parameters: dict[str, Any]\n) -&gt; list[str]:\n    \"\"\"Generate a list of path names from an inital path name.\n\n    Args:\n        l_values_path_list (list): List with the initial path names and number of paths.\n        dic_common_parameters (dict): Dictionary with the parameters common to the whole study.\n\n    Returns:\n        list: List of final path values from the initial paths.\n    \"\"\"\n    # Check that all values in the list are strings\n    if not all(isinstance(value, str) for value in l_values_path_list):\n        raise ValueError(\n            \"All values in the list for the list_values_path function must be strings.\"\n        )\n    n_path_arg = l_values_path_list[1]\n    n_path = find_item_in_dic(dic_common_parameters, n_path_arg)\n    if n_path is None:\n        raise ValueError(f\"Parameter {n_path_arg} is not defined in the scan configuration.\")\n    return [l_values_path_list[0].replace(\"____\", f\"{n:02d}\") for n in range(n_path)]\n</code></pre>"},{"location":"reference/study_da/generate/parameter_space.html#study_da.generate.parameter_space.logspace","title":"<code>logspace(l_values_logspace)</code>","text":"<p>Generate a list of values that are evenly spaced on a log scale.</p> <p>Parameters:</p> Name Type Description Default <code>l_values_logspace</code> <code>list</code> <p>List with the values for the logspace function.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: List of values that are evenly spaced on a log scale.</p> Source code in <code>study_da/generate/parameter_space.py</code> <pre><code>def logspace(l_values_logspace: list) -&gt; np.ndarray:\n    \"\"\"Generate a list of values that are evenly spaced on a log scale.\n\n    Args:\n        l_values_logspace (list): List with the values for the logspace function.\n\n    Returns:\n        np.ndarray: List of values that are evenly spaced on a log scale.\n    \"\"\"\n\n    # Check that all values in the list are floats or integers\n    if not all(isinstance(value, (float, int)) for value in l_values_logspace):\n        raise ValueError(\n            \"All values in the list for the logspace function must be floats or integers.\"\n        )\n    return np.round(\n        np.logspace(\n            l_values_logspace[0],\n            l_values_logspace[1],\n            l_values_logspace[2],\n            endpoint=True,\n        ),\n        8,\n    )\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/index.html","title":"master_classes","text":""},{"location":"reference/study_da/generate/master_classes/mad_collider.html","title":"mad_collider","text":"<p>This class is used to build a Xsuite collider from a madx sequence and optics.</p>"},{"location":"reference/study_da/generate/master_classes/mad_collider.html#study_da.generate.master_classes.mad_collider.MadCollider","title":"<code>MadCollider</code>","text":"<p>MadCollider class is responsible for setting up and managing the collider environment using MAD-X and xsuite.</p> <p>Attributes:</p> Name Type Description <code>sanity_checks</code> <code>bool</code> <p>Flag to enable or disable sanity checks.</p> <code>links</code> <code>str</code> <p>Path to the links configuration.</p> <code>beam_config</code> <code>dict</code> <p>Configuration for the beam.</p> <code>optics</code> <code>str</code> <p>Path to the optics file.</p> <code>enable_imperfections</code> <code>bool</code> <p>Flag to enable or disable imperfections.</p> <code>enable_knob_synthesis</code> <code>bool</code> <p>Flag to enable or disable knob synthesis.</p> <code>rename_coupling_knobs</code> <code>bool</code> <p>Flag to enable or disable renaming of coupling knobs.</p> <code>pars_for_imperfections</code> <code>dict</code> <p>Parameters for imperfections.</p> <code>ver_lhc_run</code> <code>float | None</code> <p>Version of LHC run.</p> <code>ver_hllhc_optics</code> <code>float | None</code> <p>Version of HL-LHC optics.</p> <code>ions</code> <code>bool</code> <p>Flag to indicate if ions are used.</p> <code>phasing</code> <code>dict</code> <p>Phasing configuration.</p> <code>path_collider_file_for_configuration_as_output</code> <code>str</code> <p>Path to save the collider.</p> <code>compress</code> <code>bool</code> <p>Flag to enable or disable compression of collider file.</p> <p>Methods:</p> Name Description <code>ost</code> <p>Property to get the appropriate optics specific tools.</p> <code>prepare_mad_collider</code> <p>Prepares the MAD-X collider environment.</p> <code>build_collider</code> <p>Madx, mad_b4: Madx) -&gt; xt.Multiline: Builds the xsuite collider.</p> <code>activate_RF_and_twiss</code> <p>xt.Multiline) -&gt; None: Activates RF and performs twiss analysis.</p> <code>check_xsuite_lattices</code> <p>xt.Line) -&gt; None: Checks the xsuite lattices.</p> <code>write_collider_to_disk</code> <p>xt.Multiline) -&gt; None: Writes the collider to disk and optionally compresses it.</p> <code>clean_temporary_files</code> <p>Cleans up temporary files created during the process.</p> Source code in <code>study_da/generate/master_classes/mad_collider.py</code> <pre><code>class MadCollider:\n    \"\"\"\n    MadCollider class is responsible for setting up and managing the collider environment using\n    MAD-X and xsuite.\n\n    Attributes:\n        sanity_checks (bool): Flag to enable or disable sanity checks.\n        links (str): Path to the links configuration.\n        beam_config (dict): Configuration for the beam.\n        optics (str): Path to the optics file.\n        enable_imperfections (bool): Flag to enable or disable imperfections.\n        enable_knob_synthesis (bool): Flag to enable or disable knob synthesis.\n        rename_coupling_knobs (bool): Flag to enable or disable renaming of coupling knobs.\n        pars_for_imperfections (dict): Parameters for imperfections.\n        ver_lhc_run (float | None): Version of LHC run.\n        ver_hllhc_optics (float | None): Version of HL-LHC optics.\n        ions (bool): Flag to indicate if ions are used.\n        phasing (dict): Phasing configuration.\n        path_collider_file_for_configuration_as_output (str): Path to save the collider.\n        compress (bool): Flag to enable or disable compression of collider file.\n\n    Methods:\n        ost: Property to get the appropriate optics specific tools.\n        prepare_mad_collider() -&gt; tuple[Madx, Madx]: Prepares the MAD-X collider environment.\n        build_collider(mad_b1b2: Madx, mad_b4: Madx) -&gt; xt.Multiline: Builds the xsuite collider.\n        activate_RF_and_twiss(collider: xt.Multiline) -&gt; None: Activates RF and performs twiss analysis.\n        check_xsuite_lattices(line: xt.Line) -&gt; None: Checks the xsuite lattices.\n        write_collider_to_disk(collider: xt.Multiline) -&gt; None: Writes the collider to disk and\n            optionally compresses it.\n        clean_temporary_files() -&gt; None: Cleans up temporary files created during the process.\n    \"\"\"\n\n    def __init__(self, configuration: dict):\n        \"\"\"\n        Initializes the MadCollider class with the given configuration.\n\n        Args:\n            configuration (dict): A dictionary containing the following keys:\n                - sanity_checks (bool): Flag to enable or disable sanity checks.\n                - links (str): Path to the links configuration.\n                - beam_config (dict): Configuration for the beam.\n                - optics_file (str): Path to the optics file.\n                - enable_imperfections (bool): Flag to enable or disable imperfections.\n                - enable_knob_synthesis (bool): Flag to enable or disable knob synthesis.\n                - rename_coupling_knobs (bool): Flag to enable or disable renaming of coupling\n                    knobs.\n                - pars_for_imperfections (dict): Parameters for imperfections.\n                - ver_lhc_run (float | None): Version of the LHC run, if applicable.\n                - ver_hllhc_optics (float | None): Version of the HL-LHC optics, if applicable.\n                - ions (bool): Flag to indicate if ions are used.\n                - phasing (dict): Configuration for phasing.\n                - path_collider_file_for_configuration_as_output (str): Path to the collider.\n                - compress (bool): Flag to enable or disable compression.\n        \"\"\"\n        # Configuration variables\n        self.sanity_checks: bool = configuration[\"sanity_checks\"]\n        self.links: str = configuration[\"links\"]\n        self.beam_config: dict = configuration[\"beam_config\"]\n        self.optics: str = configuration[\"optics_file\"]\n        self.enable_imperfections: bool = configuration[\"enable_imperfections\"]\n        self.enable_knob_synthesis: bool = configuration[\"enable_knob_synthesis\"]\n        self.rename_coupling_knobs: bool = configuration[\"rename_coupling_knobs\"]\n        self.pars_for_imperfections: dict = configuration[\"pars_for_imperfections\"]\n        self.ver_lhc_run: float | None = configuration[\"ver_lhc_run\"]\n        self.ver_hllhc_optics: float | None = configuration[\"ver_hllhc_optics\"]\n        self.ions: bool = configuration[\"ions\"]\n        self.phasing: dict = configuration[\"phasing\"]\n\n        # Optics specific tools\n        self._ost = None\n\n        # Path to disk and compression\n        self.path_collider_file_for_configuration_as_output = configuration[\n            \"path_collider_file_for_configuration_as_output\"\n        ]\n        self.compress = configuration[\"compress\"]\n\n    @property\n    def ost(self) -&gt; Any:\n        \"\"\"\n        Determines and returns the appropriate optics-specific tools (OST) based on the\n        version of HLLHC optics or LHC run configuration.\n\n        Raises:\n            ValueError: If both `ver_hllhc_optics` and `ver_lhc_run` are defined.\n            ValueError: If no optics-specific tools are available for the given configuration.\n\n        Returns:\n            Any: The appropriate OST module based on the configuration.\n        \"\"\"\n        if self._ost is None:\n            # Check that version is well defined\n            if self.ver_hllhc_optics is not None and self.ver_lhc_run is not None:\n                raise ValueError(\"Only one of ver_hllhc_optics and ver_lhc_run can be defined\")\n\n            # Get the appropriate optics_specific_tools\n            if self.ver_hllhc_optics is not None:\n                match self.ver_hllhc_optics:\n                    case 1.6:\n                        self._ost = ost_hllhc16\n                    case 1.3:\n                        self._ost = ost_hllhc13\n                    case _:\n                        raise ValueError(\"No optics specific tools for this configuration\")\n            elif self.ver_lhc_run == 3.0:\n                self._ost = ost_runIII_ions if self.ions else ost_runIII\n            else:\n                raise ValueError(\"No optics specific tools for the provided configuration\")\n\n        return self._ost\n\n    def prepare_mad_collider(self) -&gt; tuple[Madx, Madx]:\n        # sourcery skip: extract-duplicate-method\n        \"\"\"\n        Prepares the MAD-X collider environment and sequences for beam 1/2 and beam 4.\n\n        This method performs the following steps:\n        1. Creates the MAD-X environment using the provided links.\n        2. Initializes MAD-X instances for beam 1/2 and beam 4 with respective command logs.\n        3. Builds the sequences for both beams using the provided beam configuration.\n        4. Applies the specified optics to the beam 1/2 sequence.\n        5. Optionally performs sanity checks on the beam 1/2 sequence by running TWISS and checking\n            the MAD-X lattices.\n        6. Applies the specified optics to the beam 4 sequence.\n        7. Optionally performs sanity checks on the beam 4 sequence by running TWISS and checking\n            the MAD-X lattices.\n\n        Returns:\n            tuple[Madx, Madx]: A tuple containing the MAD-X instances for beam 1/2 and beam 4.\n        \"\"\"\n        # Make mad environment\n        xm.make_mad_environment(links=self.links)\n\n        # Start mad\n        mad_b1b2 = Madx(command_log=\"mad_collider.log\")\n        mad_b4 = Madx(command_log=\"mad_b4.log\")\n\n        # Build sequences\n        self.ost.build_sequence(mad_b1b2, mylhcbeam=1, beam_config=self.beam_config)\n        self.ost.build_sequence(mad_b4, mylhcbeam=4, beam_config=self.beam_config)\n\n        # Apply optics (only for b1b2, b4 will be generated from b1b2)\n        self.ost.apply_optics(mad_b1b2, optics_file=self.optics)\n\n        if self.sanity_checks:\n            mad_b1b2.use(sequence=\"lhcb1\")\n            mad_b1b2.twiss()\n            self.ost.check_madx_lattices(mad_b1b2)\n            mad_b1b2.use(sequence=\"lhcb2\")\n            mad_b1b2.twiss()\n            self.ost.check_madx_lattices(mad_b1b2)\n\n        # Apply optics (only for b4, just for check)\n        self.ost.apply_optics(mad_b4, optics_file=self.optics)\n        if self.sanity_checks:\n            mad_b4.use(sequence=\"lhcb2\")\n            mad_b4.twiss()\n            # ! Investigate why this is failing for run III\n            try:\n                self.ost.check_madx_lattices(mad_b4)\n            except AssertionError:\n                logging.warning(\"Some sanity checks have failed during the madx lattice check\")\n\n        return mad_b1b2, mad_b4\n\n    def build_collider(self, mad_b1b2: Madx, mad_b4: Madx) -&gt; xt.Multiline:\n        \"\"\"\n        Build an xsuite collider using provided MAD-X sequences and configuration.\n\n        Parameters:\n        mad_b1b2 (Madx): MAD-X instance containing sequences for beam 1 and beam 2.\n        mad_b4 (Madx): MAD-X instance containing sequence for beam 4.\n\n        Returns:\n        xt.Multiline: Constructed xsuite collider.\n\n        Notes:\n        - Converts `ver_lhc_run` and `ver_hllhc_optics` to float if they are not None.\n        - Builds the xsuite collider with the specified sequences and configuration.\n        - Optionally performs sanity checks by computing Twiss parameters for beam 1 and beam 2.\n        \"\"\"\n        # Ensure proper types to avoid assert errors\n        if self.ver_lhc_run is not None:\n            self.ver_lhc_run = float(self.ver_lhc_run)\n        if self.ver_hllhc_optics is not None:\n            self.ver_hllhc_optics = float(self.ver_hllhc_optics)\n\n        # Build xsuite collider\n        collider = xlhc.build_xsuite_collider(\n            sequence_b1=mad_b1b2.sequence.lhcb1,\n            sequence_b2=mad_b1b2.sequence.lhcb2,\n            sequence_b4=mad_b4.sequence.lhcb2,\n            beam_config=self.beam_config,\n            enable_imperfections=self.enable_imperfections,\n            enable_knob_synthesis=self.enable_knob_synthesis,\n            rename_coupling_knobs=self.rename_coupling_knobs,\n            pars_for_imperfections=self.pars_for_imperfections,\n            ver_lhc_run=self.ver_lhc_run,\n            ver_hllhc_optics=self.ver_hllhc_optics,\n        )\n        collider.build_trackers()\n\n        if self.sanity_checks:\n            collider[\"lhcb1\"].twiss(method=\"4d\")\n            collider[\"lhcb2\"].twiss(method=\"4d\")\n\n        return collider\n\n    def activate_RF_and_twiss(self, collider: xt.Multiline) -&gt; None:\n        \"\"\"\n        Activates RF and Twiss parameters for the given collider.\n\n        This method sets the RF knobs for the collider using the values specified\n        in the `phasing` attribute. It also performs sanity checks on the collider\n        lattices if the `sanity_checks` attribute is set to True.\n\n        Args:\n            collider (xt.Multiline): The collider object to configure.\n\n        Returns:\n            None\n        \"\"\"\n        # Define a RF knobs\n        collider.vars[\"vrf400\"] = self.phasing[\"vrf400\"]\n        collider.vars[\"lagrf400.b1\"] = self.phasing[\"lagrf400.b1\"]\n        collider.vars[\"lagrf400.b2\"] = self.phasing[\"lagrf400.b2\"]\n\n        if self.sanity_checks:\n            for my_line in [\"lhcb1\", \"lhcb2\"]:\n                self.check_xsuite_lattices(collider[my_line])\n\n    def check_xsuite_lattices(self, line: xt.Line) -&gt; None:\n        \"\"\"\n        Check the Twiss parameters and tune values for a given xsuite Line object.\n\n        This method computes the Twiss parameters for the provided `line` using the\n        6-dimensional method with a specified matrix stability tolerance. It then\n        prints the Twiss results at all interaction points (IPs) and the horizontal\n        (Qx) and vertical (Qy) tune values.\n\n        Args:\n            line (xt.Line): The xsuite Line object for which to compute and display\n                            the Twiss parameters and tune values.\n\n        Returns:\n            None\n        \"\"\"\n        tw = line.twiss(method=\"6d\", matrix_stability_tol=100)\n        print(f\"--- Now displaying Twiss result at all IPS for line {line}---\")\n        print(tw.rows[\"ip.*\"])\n        # print qx and qy\n        print(f\"--- Now displaying Qx and Qy for line {line}---\")\n        print(tw.qx, tw.qy)\n\n    def write_collider_to_disk(self, collider: xt.Multiline) -&gt; None:\n        \"\"\"\n        Writes the collider object to disk in JSON format and optionally compresses it into a ZIP\n        file.\n\n        Args:\n            collider (xt.Multiline): The collider object to be saved.\n\n        Returns:\n            None\n\n        Raises:\n            OSError: If there is an issue creating the directory or writing the file.\n\n        Notes:\n            - The method ensures that the directory specified in\n                `self.path_collider_file_for_configuration_as_output` exists.\n            - If `self.compress` is True, the JSON file is compressed into a ZIP file to reduce\n                storage usage.\n        \"\"\"\n        # Save collider to json, creating the folder if it does not exist\n        if \"/\" in self.path_collider_file_for_configuration_as_output:\n            os.makedirs(self.path_collider_file_for_configuration_as_output, exist_ok=True)\n        collider.to_json(self.path_collider_file_for_configuration_as_output)\n\n        # Compress the collider file to zip to ease the load on afs\n        if self.compress:\n            compress_and_write(self.path_collider_file_for_configuration_as_output)\n\n    @staticmethod\n    def clean_temporary_files() -&gt; None:\n        \"\"\"\n        Remove all the temporary files created in the process of building the collider.\n\n        This function deletes the following files and directories:\n        - \"mad_collider.log\"\n        - \"mad_b4.log\"\n        - \"temp\" directory\n        - \"errors\"\n        - \"acc-models-lhc\"\n        \"\"\"\n        # Remove all the temporaty files created in the process of building collider\n        os.remove(\"mad_collider.log\")\n        os.remove(\"mad_b4.log\")\n        shutil.rmtree(\"temp\")\n        os.unlink(\"errors\")\n        os.unlink(\"acc-models-lhc\")\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/mad_collider.html#study_da.generate.master_classes.mad_collider.MadCollider.ost","title":"<code>ost: Any</code>  <code>property</code>","text":"<p>Determines and returns the appropriate optics-specific tools (OST) based on the version of HLLHC optics or LHC run configuration.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both <code>ver_hllhc_optics</code> and <code>ver_lhc_run</code> are defined.</p> <code>ValueError</code> <p>If no optics-specific tools are available for the given configuration.</p> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The appropriate OST module based on the configuration.</p>"},{"location":"reference/study_da/generate/master_classes/mad_collider.html#study_da.generate.master_classes.mad_collider.MadCollider.__init__","title":"<code>__init__(configuration)</code>","text":"<p>Initializes the MadCollider class with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>configuration</code> <code>dict</code> <p>A dictionary containing the following keys: - sanity_checks (bool): Flag to enable or disable sanity checks. - links (str): Path to the links configuration. - beam_config (dict): Configuration for the beam. - optics_file (str): Path to the optics file. - enable_imperfections (bool): Flag to enable or disable imperfections. - enable_knob_synthesis (bool): Flag to enable or disable knob synthesis. - rename_coupling_knobs (bool): Flag to enable or disable renaming of coupling     knobs. - pars_for_imperfections (dict): Parameters for imperfections. - ver_lhc_run (float | None): Version of the LHC run, if applicable. - ver_hllhc_optics (float | None): Version of the HL-LHC optics, if applicable. - ions (bool): Flag to indicate if ions are used. - phasing (dict): Configuration for phasing. - path_collider_file_for_configuration_as_output (str): Path to the collider. - compress (bool): Flag to enable or disable compression.</p> required Source code in <code>study_da/generate/master_classes/mad_collider.py</code> <pre><code>def __init__(self, configuration: dict):\n    \"\"\"\n    Initializes the MadCollider class with the given configuration.\n\n    Args:\n        configuration (dict): A dictionary containing the following keys:\n            - sanity_checks (bool): Flag to enable or disable sanity checks.\n            - links (str): Path to the links configuration.\n            - beam_config (dict): Configuration for the beam.\n            - optics_file (str): Path to the optics file.\n            - enable_imperfections (bool): Flag to enable or disable imperfections.\n            - enable_knob_synthesis (bool): Flag to enable or disable knob synthesis.\n            - rename_coupling_knobs (bool): Flag to enable or disable renaming of coupling\n                knobs.\n            - pars_for_imperfections (dict): Parameters for imperfections.\n            - ver_lhc_run (float | None): Version of the LHC run, if applicable.\n            - ver_hllhc_optics (float | None): Version of the HL-LHC optics, if applicable.\n            - ions (bool): Flag to indicate if ions are used.\n            - phasing (dict): Configuration for phasing.\n            - path_collider_file_for_configuration_as_output (str): Path to the collider.\n            - compress (bool): Flag to enable or disable compression.\n    \"\"\"\n    # Configuration variables\n    self.sanity_checks: bool = configuration[\"sanity_checks\"]\n    self.links: str = configuration[\"links\"]\n    self.beam_config: dict = configuration[\"beam_config\"]\n    self.optics: str = configuration[\"optics_file\"]\n    self.enable_imperfections: bool = configuration[\"enable_imperfections\"]\n    self.enable_knob_synthesis: bool = configuration[\"enable_knob_synthesis\"]\n    self.rename_coupling_knobs: bool = configuration[\"rename_coupling_knobs\"]\n    self.pars_for_imperfections: dict = configuration[\"pars_for_imperfections\"]\n    self.ver_lhc_run: float | None = configuration[\"ver_lhc_run\"]\n    self.ver_hllhc_optics: float | None = configuration[\"ver_hllhc_optics\"]\n    self.ions: bool = configuration[\"ions\"]\n    self.phasing: dict = configuration[\"phasing\"]\n\n    # Optics specific tools\n    self._ost = None\n\n    # Path to disk and compression\n    self.path_collider_file_for_configuration_as_output = configuration[\n        \"path_collider_file_for_configuration_as_output\"\n    ]\n    self.compress = configuration[\"compress\"]\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/mad_collider.html#study_da.generate.master_classes.mad_collider.MadCollider.activate_RF_and_twiss","title":"<code>activate_RF_and_twiss(collider)</code>","text":"<p>Activates RF and Twiss parameters for the given collider.</p> <p>This method sets the RF knobs for the collider using the values specified in the <code>phasing</code> attribute. It also performs sanity checks on the collider lattices if the <code>sanity_checks</code> attribute is set to True.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object to configure.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/mad_collider.py</code> <pre><code>def activate_RF_and_twiss(self, collider: xt.Multiline) -&gt; None:\n    \"\"\"\n    Activates RF and Twiss parameters for the given collider.\n\n    This method sets the RF knobs for the collider using the values specified\n    in the `phasing` attribute. It also performs sanity checks on the collider\n    lattices if the `sanity_checks` attribute is set to True.\n\n    Args:\n        collider (xt.Multiline): The collider object to configure.\n\n    Returns:\n        None\n    \"\"\"\n    # Define a RF knobs\n    collider.vars[\"vrf400\"] = self.phasing[\"vrf400\"]\n    collider.vars[\"lagrf400.b1\"] = self.phasing[\"lagrf400.b1\"]\n    collider.vars[\"lagrf400.b2\"] = self.phasing[\"lagrf400.b2\"]\n\n    if self.sanity_checks:\n        for my_line in [\"lhcb1\", \"lhcb2\"]:\n            self.check_xsuite_lattices(collider[my_line])\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/mad_collider.html#study_da.generate.master_classes.mad_collider.MadCollider.build_collider","title":"<code>build_collider(mad_b1b2, mad_b4)</code>","text":"<p>Build an xsuite collider using provided MAD-X sequences and configuration.</p> <p>Parameters: mad_b1b2 (Madx): MAD-X instance containing sequences for beam 1 and beam 2. mad_b4 (Madx): MAD-X instance containing sequence for beam 4.</p> <p>Returns: xt.Multiline: Constructed xsuite collider.</p> <p>Notes: - Converts <code>ver_lhc_run</code> and <code>ver_hllhc_optics</code> to float if they are not None. - Builds the xsuite collider with the specified sequences and configuration. - Optionally performs sanity checks by computing Twiss parameters for beam 1 and beam 2.</p> Source code in <code>study_da/generate/master_classes/mad_collider.py</code> <pre><code>def build_collider(self, mad_b1b2: Madx, mad_b4: Madx) -&gt; xt.Multiline:\n    \"\"\"\n    Build an xsuite collider using provided MAD-X sequences and configuration.\n\n    Parameters:\n    mad_b1b2 (Madx): MAD-X instance containing sequences for beam 1 and beam 2.\n    mad_b4 (Madx): MAD-X instance containing sequence for beam 4.\n\n    Returns:\n    xt.Multiline: Constructed xsuite collider.\n\n    Notes:\n    - Converts `ver_lhc_run` and `ver_hllhc_optics` to float if they are not None.\n    - Builds the xsuite collider with the specified sequences and configuration.\n    - Optionally performs sanity checks by computing Twiss parameters for beam 1 and beam 2.\n    \"\"\"\n    # Ensure proper types to avoid assert errors\n    if self.ver_lhc_run is not None:\n        self.ver_lhc_run = float(self.ver_lhc_run)\n    if self.ver_hllhc_optics is not None:\n        self.ver_hllhc_optics = float(self.ver_hllhc_optics)\n\n    # Build xsuite collider\n    collider = xlhc.build_xsuite_collider(\n        sequence_b1=mad_b1b2.sequence.lhcb1,\n        sequence_b2=mad_b1b2.sequence.lhcb2,\n        sequence_b4=mad_b4.sequence.lhcb2,\n        beam_config=self.beam_config,\n        enable_imperfections=self.enable_imperfections,\n        enable_knob_synthesis=self.enable_knob_synthesis,\n        rename_coupling_knobs=self.rename_coupling_knobs,\n        pars_for_imperfections=self.pars_for_imperfections,\n        ver_lhc_run=self.ver_lhc_run,\n        ver_hllhc_optics=self.ver_hllhc_optics,\n    )\n    collider.build_trackers()\n\n    if self.sanity_checks:\n        collider[\"lhcb1\"].twiss(method=\"4d\")\n        collider[\"lhcb2\"].twiss(method=\"4d\")\n\n    return collider\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/mad_collider.html#study_da.generate.master_classes.mad_collider.MadCollider.check_xsuite_lattices","title":"<code>check_xsuite_lattices(line)</code>","text":"<p>Check the Twiss parameters and tune values for a given xsuite Line object.</p> <p>This method computes the Twiss parameters for the provided <code>line</code> using the 6-dimensional method with a specified matrix stability tolerance. It then prints the Twiss results at all interaction points (IPs) and the horizontal (Qx) and vertical (Qy) tune values.</p> <p>Parameters:</p> Name Type Description Default <code>line</code> <code>Line</code> <p>The xsuite Line object for which to compute and display             the Twiss parameters and tune values.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/mad_collider.py</code> <pre><code>def check_xsuite_lattices(self, line: xt.Line) -&gt; None:\n    \"\"\"\n    Check the Twiss parameters and tune values for a given xsuite Line object.\n\n    This method computes the Twiss parameters for the provided `line` using the\n    6-dimensional method with a specified matrix stability tolerance. It then\n    prints the Twiss results at all interaction points (IPs) and the horizontal\n    (Qx) and vertical (Qy) tune values.\n\n    Args:\n        line (xt.Line): The xsuite Line object for which to compute and display\n                        the Twiss parameters and tune values.\n\n    Returns:\n        None\n    \"\"\"\n    tw = line.twiss(method=\"6d\", matrix_stability_tol=100)\n    print(f\"--- Now displaying Twiss result at all IPS for line {line}---\")\n    print(tw.rows[\"ip.*\"])\n    # print qx and qy\n    print(f\"--- Now displaying Qx and Qy for line {line}---\")\n    print(tw.qx, tw.qy)\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/mad_collider.html#study_da.generate.master_classes.mad_collider.MadCollider.clean_temporary_files","title":"<code>clean_temporary_files()</code>  <code>staticmethod</code>","text":"<p>Remove all the temporary files created in the process of building the collider.</p> <p>This function deletes the following files and directories: - \"mad_collider.log\" - \"mad_b4.log\" - \"temp\" directory - \"errors\" - \"acc-models-lhc\"</p> Source code in <code>study_da/generate/master_classes/mad_collider.py</code> <pre><code>@staticmethod\ndef clean_temporary_files() -&gt; None:\n    \"\"\"\n    Remove all the temporary files created in the process of building the collider.\n\n    This function deletes the following files and directories:\n    - \"mad_collider.log\"\n    - \"mad_b4.log\"\n    - \"temp\" directory\n    - \"errors\"\n    - \"acc-models-lhc\"\n    \"\"\"\n    # Remove all the temporaty files created in the process of building collider\n    os.remove(\"mad_collider.log\")\n    os.remove(\"mad_b4.log\")\n    shutil.rmtree(\"temp\")\n    os.unlink(\"errors\")\n    os.unlink(\"acc-models-lhc\")\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/mad_collider.html#study_da.generate.master_classes.mad_collider.MadCollider.prepare_mad_collider","title":"<code>prepare_mad_collider()</code>","text":"<p>Prepares the MAD-X collider environment and sequences for beam 1/2 and beam 4.</p> <p>This method performs the following steps: 1. Creates the MAD-X environment using the provided links. 2. Initializes MAD-X instances for beam 1/2 and beam 4 with respective command logs. 3. Builds the sequences for both beams using the provided beam configuration. 4. Applies the specified optics to the beam 1/2 sequence. 5. Optionally performs sanity checks on the beam 1/2 sequence by running TWISS and checking     the MAD-X lattices. 6. Applies the specified optics to the beam 4 sequence. 7. Optionally performs sanity checks on the beam 4 sequence by running TWISS and checking     the MAD-X lattices.</p> <p>Returns:</p> Type Description <code>tuple[Madx, Madx]</code> <p>tuple[Madx, Madx]: A tuple containing the MAD-X instances for beam 1/2 and beam 4.</p> Source code in <code>study_da/generate/master_classes/mad_collider.py</code> <pre><code>def prepare_mad_collider(self) -&gt; tuple[Madx, Madx]:\n    # sourcery skip: extract-duplicate-method\n    \"\"\"\n    Prepares the MAD-X collider environment and sequences for beam 1/2 and beam 4.\n\n    This method performs the following steps:\n    1. Creates the MAD-X environment using the provided links.\n    2. Initializes MAD-X instances for beam 1/2 and beam 4 with respective command logs.\n    3. Builds the sequences for both beams using the provided beam configuration.\n    4. Applies the specified optics to the beam 1/2 sequence.\n    5. Optionally performs sanity checks on the beam 1/2 sequence by running TWISS and checking\n        the MAD-X lattices.\n    6. Applies the specified optics to the beam 4 sequence.\n    7. Optionally performs sanity checks on the beam 4 sequence by running TWISS and checking\n        the MAD-X lattices.\n\n    Returns:\n        tuple[Madx, Madx]: A tuple containing the MAD-X instances for beam 1/2 and beam 4.\n    \"\"\"\n    # Make mad environment\n    xm.make_mad_environment(links=self.links)\n\n    # Start mad\n    mad_b1b2 = Madx(command_log=\"mad_collider.log\")\n    mad_b4 = Madx(command_log=\"mad_b4.log\")\n\n    # Build sequences\n    self.ost.build_sequence(mad_b1b2, mylhcbeam=1, beam_config=self.beam_config)\n    self.ost.build_sequence(mad_b4, mylhcbeam=4, beam_config=self.beam_config)\n\n    # Apply optics (only for b1b2, b4 will be generated from b1b2)\n    self.ost.apply_optics(mad_b1b2, optics_file=self.optics)\n\n    if self.sanity_checks:\n        mad_b1b2.use(sequence=\"lhcb1\")\n        mad_b1b2.twiss()\n        self.ost.check_madx_lattices(mad_b1b2)\n        mad_b1b2.use(sequence=\"lhcb2\")\n        mad_b1b2.twiss()\n        self.ost.check_madx_lattices(mad_b1b2)\n\n    # Apply optics (only for b4, just for check)\n    self.ost.apply_optics(mad_b4, optics_file=self.optics)\n    if self.sanity_checks:\n        mad_b4.use(sequence=\"lhcb2\")\n        mad_b4.twiss()\n        # ! Investigate why this is failing for run III\n        try:\n            self.ost.check_madx_lattices(mad_b4)\n        except AssertionError:\n            logging.warning(\"Some sanity checks have failed during the madx lattice check\")\n\n    return mad_b1b2, mad_b4\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/mad_collider.html#study_da.generate.master_classes.mad_collider.MadCollider.write_collider_to_disk","title":"<code>write_collider_to_disk(collider)</code>","text":"<p>Writes the collider object to disk in JSON format and optionally compresses it into a ZIP file.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object to be saved.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If there is an issue creating the directory or writing the file.</p> Notes <ul> <li>The method ensures that the directory specified in     <code>self.path_collider_file_for_configuration_as_output</code> exists.</li> <li>If <code>self.compress</code> is True, the JSON file is compressed into a ZIP file to reduce     storage usage.</li> </ul> Source code in <code>study_da/generate/master_classes/mad_collider.py</code> <pre><code>def write_collider_to_disk(self, collider: xt.Multiline) -&gt; None:\n    \"\"\"\n    Writes the collider object to disk in JSON format and optionally compresses it into a ZIP\n    file.\n\n    Args:\n        collider (xt.Multiline): The collider object to be saved.\n\n    Returns:\n        None\n\n    Raises:\n        OSError: If there is an issue creating the directory or writing the file.\n\n    Notes:\n        - The method ensures that the directory specified in\n            `self.path_collider_file_for_configuration_as_output` exists.\n        - If `self.compress` is True, the JSON file is compressed into a ZIP file to reduce\n            storage usage.\n    \"\"\"\n    # Save collider to json, creating the folder if it does not exist\n    if \"/\" in self.path_collider_file_for_configuration_as_output:\n        os.makedirs(self.path_collider_file_for_configuration_as_output, exist_ok=True)\n    collider.to_json(self.path_collider_file_for_configuration_as_output)\n\n    # Compress the collider file to zip to ease the load on afs\n    if self.compress:\n        compress_and_write(self.path_collider_file_for_configuration_as_output)\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/particles_distribution.html","title":"particles_distribution","text":"<p>This class is used to define and write to disk the particles distribution.</p>"},{"location":"reference/study_da/generate/master_classes/particles_distribution.html#study_da.generate.master_classes.particles_distribution.ParticlesDistribution","title":"<code>ParticlesDistribution</code>","text":"<p>ParticlesDistribution class to generate and manage particle distributions.</p> <p>Attributes:</p> Name Type Description <code>r_min</code> <code>int</code> <p>Minimum radial distance.</p> <code>r_max</code> <code>int</code> <p>Maximum radial distance.</p> <code>n_r</code> <code>int</code> <p>Number of radial points.</p> <code>n_angles</code> <code>int</code> <p>Number of angular points.</p> <code>n_split</code> <code>int</code> <p>Number of splits for parallelization.</p> <code>path_distribution_folder_output</code> <code>str</code> <p>Path to the folder where distributions will be saved.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>dict): Initializes the ParticlesDistribution with the given configuration.</p> <code>get_radial_list</code> <p>float | None = None, upper_crop: float | None = None) -&gt; np.ndarray: Generates a list of radial distances, optionally cropped.</p> <code>get_angular_list</code> <p>Generates a list of angular values.</p> <code>return_distribution_as_list</code> <p>bool = True, lower_crop: float | None = None, upper_crop: float | None) -&gt; list[np.ndarray]: Returns the particle distribution as a list of numpy arrays, optionally split for parallelization.</p> <code>write_particle_distribution_to_disk</code> <p>list[np.ndarray]) -&gt; list[str]: Writes the particle distribution to disk in Parquet format and returns the list of file paths.</p> Source code in <code>study_da/generate/master_classes/particles_distribution.py</code> <pre><code>class ParticlesDistribution:\n    \"\"\"\n    ParticlesDistribution class to generate and manage particle distributions.\n\n    Attributes:\n        r_min (int): Minimum radial distance.\n        r_max (int): Maximum radial distance.\n        n_r (int): Number of radial points.\n        n_angles (int): Number of angular points.\n        n_split (int): Number of splits for parallelization.\n        path_distribution_folder_output (str): Path to the folder where distributions will be saved.\n\n    Methods:\n        __init__(configuration: dict):\n            Initializes the ParticlesDistribution with the given configuration.\n\n        get_radial_list(lower_crop: float | None = None, upper_crop: float | None = None)\n            -&gt; np.ndarray:\n            Generates a list of radial distances, optionally cropped.\n\n        get_angular_list() -&gt; np.ndarray:\n            Generates a list of angular values.\n\n        return_distribution_as_list(split: bool = True, lower_crop: float | None = None,\n            upper_crop: float | None) -&gt; list[np.ndarray]:\n            Returns the particle distribution as a list of numpy arrays, optionally split for\n            parallelization.\n\n        write_particle_distribution_to_disk(ll_particles: list[np.ndarray]) -&gt; list[str]:\n            Writes the particle distribution to disk in Parquet format and returns the list of file\n            paths.\n    \"\"\"\n\n    def __init__(self, configuration: dict):\n        \"\"\"\n        Initialize the particle distribution with the given configuration.\n\n        Args:\n            configuration (dict): A dictionary containing the configuration parameters.\n                - r_min (int): Minimum radius value.\n                - r_max (int): Maximum radius value.\n                - n_r (int): Number of radius points.\n                - n_angles (int): Number of angle points.\n                - n_split (int): Number of splits for parallelization.\n                - path_distribution_folder_output (str): Path to the folder where the distribution will be\n                    saved.\n        \"\"\"\n        # Variables used to define the distribution\n        self.r_min: int = configuration[\"r_min\"]\n        self.r_max: int = configuration[\"r_max\"]\n        self.n_r: int = configuration[\"n_r\"]\n        self.n_angles: int = configuration[\"n_angles\"]\n\n        # Variables to split the distribution for parallelization\n        self.n_split: int = configuration[\"n_split\"]\n\n        # Variable to write the distribution to disk\n        self.path_distribution_folder_output: str = configuration[\"path_distribution_folder_output\"]\n\n    def get_radial_list(\n        self, lower_crop: float | None = None, upper_crop: float | None = None\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Generate a list of radial distances within specified bounds.\n\n        Args:\n            lower_crop (float | None): The lower bound to crop the radial distances.\n                If None, no lower cropping is applied. Defaults to None.\n            upper_crop (float | None): The upper bound to crop the radial distances.\n                If None, no upper cropping is applied. Defaults to None.\n\n        Returns:\n            np.ndarray: An array of radial distances within the specified bounds.\n        \"\"\"\n        radial_list = np.linspace(self.r_min, self.r_max, self.n_r, endpoint=False)\n        if upper_crop:\n            radial_list = radial_list[radial_list &lt;= 7.5]\n        if lower_crop:\n            radial_list = radial_list[radial_list &gt;= 2.5]\n        return radial_list\n\n    def get_angular_list(self) -&gt; np.ndarray:\n        \"\"\"\n        Generate a list of angular values.\n\n        This method creates a list of angular values ranging from 0 to 90 degrees,\n        excluding the first and last values. The number of angles generated is\n        determined by the instance variable `self.n_angles`.\n\n        Returns:\n            numpy.ndarray: An array of angular values.\n        \"\"\"\n        return np.linspace(0, 90, self.n_angles + 2)[1:-1]\n\n    def return_distribution_as_list(\n        self, split: bool = True, lower_crop: float | None = None, upper_crop: float | None = None\n    ) -&gt; list[np.ndarray]:\n        \"\"\"\n        Returns the particle distribution as a list of numpy arrays.\n\n        This method generates a particle distribution by creating a Cartesian product\n        of radial and angular lists. The resulting distribution can be optionally split\n        into multiple parts for parallel computation.\n\n        Args:\n            split (bool): If True, the distribution is split into multiple parts.\n                Defaults to True.\n            lower_crop (float | None): The lower bound for cropping the radial list.\n                If None, no lower cropping is applied. Defaults to None.\n            upper_crop (float | None): The upper bound for cropping the radial list.\n                If None, no upper cropping is applied. Defaults to None.\n\n        Returns:\n            list[np.ndarray]: A list of numpy arrays representing the particle distribution.\n                If `split` is True, the list contains multiple arrays for parallel computation.\n                Otherwise, the list contains a single array.\n        \"\"\"\n        # Get radial list and angular list\n        radial_list = self.get_radial_list(lower_crop=lower_crop, upper_crop=upper_crop)\n        angular_list = self.get_angular_list()\n\n        # Define particle distribution as a cartesian product of the radial and angular lists\n        l_particles = np.array(\n            [\n                (particle_id, ii[1], ii[0])\n                for particle_id, ii in enumerate(itertools.product(angular_list, radial_list))\n            ]\n        )\n\n        # Potentially split the distribution to parallelize the computation\n        if split:\n            return list(np.array_split(l_particles, self.n_split))\n\n        return [l_particles]\n\n    def write_particle_distribution_to_disk(\n        self, ll_particles: list[list[np.ndarray]]\n    ) -&gt; list[str]:\n        \"\"\"\n        Writes a list of particle distributions to disk in Parquet format.\n\n        Args:\n            ll_particles (list[list[np.ndarray]]): A list of particle distributions,\n                where each distribution is a list containing particle data.\n\n        Returns:\n            list[str]: A list of file paths where the particle distributions\n            have been saved.\n\n        The method creates a directory specified by `self.path_distribution_folder_output`\n        if it does not already exist. Each particle distribution is saved as a\n        Parquet file in this directory. The files are named sequentially using\n        a zero-padded index (e.g., '00.parquet', '01.parquet', etc.).\n        \"\"\"\n        # Define folder to store the distributions\n        os.makedirs(self.path_distribution_folder_output, exist_ok=True)\n\n        # Write the distribution to disk\n        l_path_files = []\n        for idx_chunk, l_particles in enumerate(ll_particles):\n            path_file = f\"{self.path_distribution_folder_output}/{idx_chunk:02}.parquet\"\n            pd.DataFrame(\n                l_particles,\n                columns=[\n                    \"particle_id\",\n                    \"normalized amplitude in xy-plane\",\n                    \"angle in xy-plane [deg]\",\n                ],\n            ).to_parquet(path_file)\n            l_path_files.append(path_file)\n\n        return l_path_files\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/particles_distribution.html#study_da.generate.master_classes.particles_distribution.ParticlesDistribution.__init__","title":"<code>__init__(configuration)</code>","text":"<p>Initialize the particle distribution with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>configuration</code> <code>dict</code> <p>A dictionary containing the configuration parameters. - r_min (int): Minimum radius value. - r_max (int): Maximum radius value. - n_r (int): Number of radius points. - n_angles (int): Number of angle points. - n_split (int): Number of splits for parallelization. - path_distribution_folder_output (str): Path to the folder where the distribution will be     saved.</p> required Source code in <code>study_da/generate/master_classes/particles_distribution.py</code> <pre><code>def __init__(self, configuration: dict):\n    \"\"\"\n    Initialize the particle distribution with the given configuration.\n\n    Args:\n        configuration (dict): A dictionary containing the configuration parameters.\n            - r_min (int): Minimum radius value.\n            - r_max (int): Maximum radius value.\n            - n_r (int): Number of radius points.\n            - n_angles (int): Number of angle points.\n            - n_split (int): Number of splits for parallelization.\n            - path_distribution_folder_output (str): Path to the folder where the distribution will be\n                saved.\n    \"\"\"\n    # Variables used to define the distribution\n    self.r_min: int = configuration[\"r_min\"]\n    self.r_max: int = configuration[\"r_max\"]\n    self.n_r: int = configuration[\"n_r\"]\n    self.n_angles: int = configuration[\"n_angles\"]\n\n    # Variables to split the distribution for parallelization\n    self.n_split: int = configuration[\"n_split\"]\n\n    # Variable to write the distribution to disk\n    self.path_distribution_folder_output: str = configuration[\"path_distribution_folder_output\"]\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/particles_distribution.html#study_da.generate.master_classes.particles_distribution.ParticlesDistribution.get_angular_list","title":"<code>get_angular_list()</code>","text":"<p>Generate a list of angular values.</p> <p>This method creates a list of angular values ranging from 0 to 90 degrees, excluding the first and last values. The number of angles generated is determined by the instance variable <code>self.n_angles</code>.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: An array of angular values.</p> Source code in <code>study_da/generate/master_classes/particles_distribution.py</code> <pre><code>def get_angular_list(self) -&gt; np.ndarray:\n    \"\"\"\n    Generate a list of angular values.\n\n    This method creates a list of angular values ranging from 0 to 90 degrees,\n    excluding the first and last values. The number of angles generated is\n    determined by the instance variable `self.n_angles`.\n\n    Returns:\n        numpy.ndarray: An array of angular values.\n    \"\"\"\n    return np.linspace(0, 90, self.n_angles + 2)[1:-1]\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/particles_distribution.html#study_da.generate.master_classes.particles_distribution.ParticlesDistribution.get_radial_list","title":"<code>get_radial_list(lower_crop=None, upper_crop=None)</code>","text":"<p>Generate a list of radial distances within specified bounds.</p> <p>Parameters:</p> Name Type Description Default <code>lower_crop</code> <code>float | None</code> <p>The lower bound to crop the radial distances. If None, no lower cropping is applied. Defaults to None.</p> <code>None</code> <code>upper_crop</code> <code>float | None</code> <p>The upper bound to crop the radial distances. If None, no upper cropping is applied. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: An array of radial distances within the specified bounds.</p> Source code in <code>study_da/generate/master_classes/particles_distribution.py</code> <pre><code>def get_radial_list(\n    self, lower_crop: float | None = None, upper_crop: float | None = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Generate a list of radial distances within specified bounds.\n\n    Args:\n        lower_crop (float | None): The lower bound to crop the radial distances.\n            If None, no lower cropping is applied. Defaults to None.\n        upper_crop (float | None): The upper bound to crop the radial distances.\n            If None, no upper cropping is applied. Defaults to None.\n\n    Returns:\n        np.ndarray: An array of radial distances within the specified bounds.\n    \"\"\"\n    radial_list = np.linspace(self.r_min, self.r_max, self.n_r, endpoint=False)\n    if upper_crop:\n        radial_list = radial_list[radial_list &lt;= 7.5]\n    if lower_crop:\n        radial_list = radial_list[radial_list &gt;= 2.5]\n    return radial_list\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/particles_distribution.html#study_da.generate.master_classes.particles_distribution.ParticlesDistribution.return_distribution_as_list","title":"<code>return_distribution_as_list(split=True, lower_crop=None, upper_crop=None)</code>","text":"<p>Returns the particle distribution as a list of numpy arrays.</p> <p>This method generates a particle distribution by creating a Cartesian product of radial and angular lists. The resulting distribution can be optionally split into multiple parts for parallel computation.</p> <p>Parameters:</p> Name Type Description Default <code>split</code> <code>bool</code> <p>If True, the distribution is split into multiple parts. Defaults to True.</p> <code>True</code> <code>lower_crop</code> <code>float | None</code> <p>The lower bound for cropping the radial list. If None, no lower cropping is applied. Defaults to None.</p> <code>None</code> <code>upper_crop</code> <code>float | None</code> <p>The upper bound for cropping the radial list. If None, no upper cropping is applied. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[ndarray]</code> <p>list[np.ndarray]: A list of numpy arrays representing the particle distribution. If <code>split</code> is True, the list contains multiple arrays for parallel computation. Otherwise, the list contains a single array.</p> Source code in <code>study_da/generate/master_classes/particles_distribution.py</code> <pre><code>def return_distribution_as_list(\n    self, split: bool = True, lower_crop: float | None = None, upper_crop: float | None = None\n) -&gt; list[np.ndarray]:\n    \"\"\"\n    Returns the particle distribution as a list of numpy arrays.\n\n    This method generates a particle distribution by creating a Cartesian product\n    of radial and angular lists. The resulting distribution can be optionally split\n    into multiple parts for parallel computation.\n\n    Args:\n        split (bool): If True, the distribution is split into multiple parts.\n            Defaults to True.\n        lower_crop (float | None): The lower bound for cropping the radial list.\n            If None, no lower cropping is applied. Defaults to None.\n        upper_crop (float | None): The upper bound for cropping the radial list.\n            If None, no upper cropping is applied. Defaults to None.\n\n    Returns:\n        list[np.ndarray]: A list of numpy arrays representing the particle distribution.\n            If `split` is True, the list contains multiple arrays for parallel computation.\n            Otherwise, the list contains a single array.\n    \"\"\"\n    # Get radial list and angular list\n    radial_list = self.get_radial_list(lower_crop=lower_crop, upper_crop=upper_crop)\n    angular_list = self.get_angular_list()\n\n    # Define particle distribution as a cartesian product of the radial and angular lists\n    l_particles = np.array(\n        [\n            (particle_id, ii[1], ii[0])\n            for particle_id, ii in enumerate(itertools.product(angular_list, radial_list))\n        ]\n    )\n\n    # Potentially split the distribution to parallelize the computation\n    if split:\n        return list(np.array_split(l_particles, self.n_split))\n\n    return [l_particles]\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/particles_distribution.html#study_da.generate.master_classes.particles_distribution.ParticlesDistribution.write_particle_distribution_to_disk","title":"<code>write_particle_distribution_to_disk(ll_particles)</code>","text":"<p>Writes a list of particle distributions to disk in Parquet format.</p> <p>Parameters:</p> Name Type Description Default <code>ll_particles</code> <code>list[list[ndarray]]</code> <p>A list of particle distributions, where each distribution is a list containing particle data.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of file paths where the particle distributions</p> <code>list[str]</code> <p>have been saved.</p> <p>The method creates a directory specified by <code>self.path_distribution_folder_output</code> if it does not already exist. Each particle distribution is saved as a Parquet file in this directory. The files are named sequentially using a zero-padded index (e.g., '00.parquet', '01.parquet', etc.).</p> Source code in <code>study_da/generate/master_classes/particles_distribution.py</code> <pre><code>def write_particle_distribution_to_disk(\n    self, ll_particles: list[list[np.ndarray]]\n) -&gt; list[str]:\n    \"\"\"\n    Writes a list of particle distributions to disk in Parquet format.\n\n    Args:\n        ll_particles (list[list[np.ndarray]]): A list of particle distributions,\n            where each distribution is a list containing particle data.\n\n    Returns:\n        list[str]: A list of file paths where the particle distributions\n        have been saved.\n\n    The method creates a directory specified by `self.path_distribution_folder_output`\n    if it does not already exist. Each particle distribution is saved as a\n    Parquet file in this directory. The files are named sequentially using\n    a zero-padded index (e.g., '00.parquet', '01.parquet', etc.).\n    \"\"\"\n    # Define folder to store the distributions\n    os.makedirs(self.path_distribution_folder_output, exist_ok=True)\n\n    # Write the distribution to disk\n    l_path_files = []\n    for idx_chunk, l_particles in enumerate(ll_particles):\n        path_file = f\"{self.path_distribution_folder_output}/{idx_chunk:02}.parquet\"\n        pd.DataFrame(\n            l_particles,\n            columns=[\n                \"particle_id\",\n                \"normalized amplitude in xy-plane\",\n                \"angle in xy-plane [deg]\",\n            ],\n        ).to_parquet(path_file)\n        l_path_files.append(path_file)\n\n    return l_path_files\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/scheme_utils.html","title":"scheme_utils","text":"<p>This class is used to inspect and compute some properties of the filling scheme.</p>"},{"location":"reference/study_da/generate/master_classes/scheme_utils.html#study_da.generate.master_classes.scheme_utils.get_worst_bunch","title":"<code>get_worst_bunch(filling_scheme_path, number_of_LR_to_consider=26, beam='beam_1')</code>","text":""},{"location":"reference/study_da/generate/master_classes/scheme_utils.html#study_da.generate.master_classes.scheme_utils.get_worst_bunch--adapted-from-httpsgithubcompycompletefillingpatternsblob5f28d1a99e9a2ef7cc5c171d0cab6679946309e8fillingpatternsbbfunctionspyl233","title":"Adapted from https://github.com/PyCOMPLETE/FillingPatterns/blob/5f28d1a99e9a2ef7cc5c171d0cab6679946309e8/fillingpatterns/bbFunctions.py#L233","text":"<p>Given a filling scheme, containing two arrays of booleans representing the trains of bunches for the two beams, this function returns the worst bunch for each beam, according to their collision schedule.</p> <p>Parameters:</p> Name Type Description Default <code>filling_scheme_path</code> <code>str</code> <p>Path to the filling scheme file.</p> required <code>number_of_LR_to_consider</code> <code>int</code> <p>Number of long range collisions to consider. Defaults to 26.</p> <code>26</code> <code>beam</code> <code>str</code> <p>Beam for which to compute the worst bunch. Defaults to \"beam_1\".</p> <code>'beam_1'</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The worst bunch for the specified beam.</p> Source code in <code>study_da/generate/master_classes/scheme_utils.py</code> <pre><code>def get_worst_bunch(\n    filling_scheme_path: str, number_of_LR_to_consider: int = 26, beam=\"beam_1\"\n) -&gt; int:\n    \"\"\"\n    # Adapted from https://github.com/PyCOMPLETE/FillingPatterns/blob/5f28d1a99e9a2ef7cc5c171d0cab6679946309e8/fillingpatterns/bbFunctions.py#L233\n    Given a filling scheme, containing two arrays of booleans representing the trains of bunches for\n    the two beams, this function returns the worst bunch for each beam, according to their collision\n    schedule.\n\n    Args:\n        filling_scheme_path (str): Path to the filling scheme file.\n        number_of_LR_to_consider (int): Number of long range collisions to consider. Defaults to 26.\n        beam (str): Beam for which to compute the worst bunch. Defaults to \"beam_1\".\n\n    Returns:\n        int: The worst bunch for the specified beam.\n\n    \"\"\"\n\n    if not filling_scheme_path.endswith(\".json\"):\n        raise ValueError(\"Only json filling schemes are supported\")\n\n    with open(filling_scheme_path, \"r\") as fid:\n        filling_scheme = json.load(fid)\n    # Extract booleans beam arrays\n    array_b1 = np.array(filling_scheme[\"beam1\"])\n    array_b2 = np.array(filling_scheme[\"beam2\"])\n\n    # Get bunches index\n    B1_bunches_index = np.flatnonzero(array_b1)\n    B2_bunches_index = np.flatnonzero(array_b2)\n\n    # Compute the number of long range collisions per bunch\n    l_long_range_per_bunch = _compute_LR_per_bunch(\n        array_b1, array_b2, B1_bunches_index, B2_bunches_index, number_of_LR_to_consider, beam=beam\n    )\n\n    # Get the worst bunch for both beams\n    if beam == \"beam_1\":\n        worst_bunch = B1_bunches_index[np.argmax(l_long_range_per_bunch)]\n    elif beam == \"beam_2\":\n        worst_bunch = B2_bunches_index[np.argmax(l_long_range_per_bunch)]\n    else:\n        raise ValueError(\"beam must be either 'beam_1' or 'beam_2\")\n\n    # Need to explicitly convert to int for json serialization\n    return int(worst_bunch)\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/scheme_utils.html#study_da.generate.master_classes.scheme_utils.load_and_check_filling_scheme","title":"<code>load_and_check_filling_scheme(filling_scheme_path)</code>","text":"<p>Load and check the filling scheme from a JSON file. Convert the filling scheme to the correct format if needed.</p> <p>Parameters:</p> Name Type Description Default <code>filling_scheme_path</code> <code>str</code> <p>Path to the filling scheme file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the converted filling scheme file.</p> Source code in <code>study_da/generate/master_classes/scheme_utils.py</code> <pre><code>def load_and_check_filling_scheme(filling_scheme_path: str) -&gt; str:\n    \"\"\"Load and check the filling scheme from a JSON file. Convert the filling scheme to the correct\n    format if needed.\n\n    Args:\n        filling_scheme_path (str): Path to the filling scheme file.\n\n    Returns:\n        str: Path to the converted filling scheme file.\n    \"\"\"\n    if not filling_scheme_path.endswith(\".json\"):\n        raise ValueError(\"Filling scheme must be in json format\")\n\n    # Check that the converted filling scheme doesn't already exist\n    filling_scheme_path_converted = filling_scheme_path.replace(\".json\", \"_converted.json\")\n    if os.path.exists(filling_scheme_path_converted):\n        return filling_scheme_path_converted\n\n    with open(filling_scheme_path, \"r\") as fid:\n        d_filling_scheme = json.load(fid)\n\n    if \"beam1\" in d_filling_scheme.keys() and \"beam2\" in d_filling_scheme.keys():\n        # If the filling scheme not already in the correct format, convert\n        if \"schemebeam1\" in d_filling_scheme.keys() or \"schemebeam2\" in d_filling_scheme.keys():\n            d_filling_scheme[\"beam1\"] = d_filling_scheme[\"schemebeam1\"]\n            d_filling_scheme[\"beam2\"] = d_filling_scheme[\"schemebeam2\"]\n            # Delete all the other keys\n            d_filling_scheme = {\n                k: v for k, v in d_filling_scheme.items() if k in [\"beam1\", \"beam2\"]\n            }\n            # Dump the dictionary back to the file\n            with open(filling_scheme_path_converted, \"w\") as fid:\n                json.dump(d_filling_scheme, fid)\n\n            # Else, do nothing\n\n    else:\n        # One can potentially use b1_array, b2_array to scan the bunches later\n        b1_array, b2_array = reformat_filling_scheme_from_lpc(\n            filling_scheme_path, filling_scheme_path_converted\n        )\n        filling_scheme_path = filling_scheme_path_converted\n\n    return filling_scheme_path\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/scheme_utils.html#study_da.generate.master_classes.scheme_utils.reformat_filling_scheme_from_lpc","title":"<code>reformat_filling_scheme_from_lpc(filling_scheme_path, filling_scheme_path_converted)</code>","text":"<p>This function is used to convert the filling scheme from the LPC to the format used in the xtrack library. The filling scheme from the LPC is a list of bunches for each beam, where each bunch is represented by a 1 in the list. The function converts this list to a list of indices of the filled bunches. The function also returns the indices of the filled bunches for each beam.</p> <p>Parameters:</p> Name Type Description Default <code>filling_scheme_path</code> <code>str</code> <p>Path to the filling scheme file.</p> required <code>filling_scheme_path_converted</code> <code>str</code> <p>Path to the converted filling scheme file.</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>tuple[np.ndarray, np.ndarray]: Indices of the filled bunches for each beam.</p> Source code in <code>study_da/generate/master_classes/scheme_utils.py</code> <pre><code>def reformat_filling_scheme_from_lpc(\n    filling_scheme_path: str, filling_scheme_path_converted: str\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    This function is used to convert the filling scheme from the LPC to the format used in the\n    xtrack library. The filling scheme from the LPC is a list of bunches for each beam, where each\n    bunch is represented by a 1 in the list. The function converts this list to a list of indices\n    of the filled bunches. The function also returns the indices of the filled bunches for each beam.\n\n    Args:\n        filling_scheme_path (str): Path to the filling scheme file.\n        filling_scheme_path_converted (str): Path to the converted filling scheme file.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]: Indices of the filled bunches for each beam.\n    \"\"\"\n\n    # Load the filling scheme directly if json\n    with open(filling_scheme_path, \"r\") as fid:\n        data = json.load(fid)\n\n    # Take the first fill number\n    fill_number = list(data[\"fills\"].keys())[0]\n\n    # Do the conversion (Matteo's code)\n    B1 = np.zeros(3564)\n    B2 = np.zeros(3564)\n    l_lines = data[\"fills\"][f\"{fill_number}\"][\"csv\"].split(\"\\n\")\n    for idx, line in enumerate(l_lines):\n        # First time one encounters a line with 'Slot' in it, start indexing\n        if \"Slot\" in line:\n            # B1 is initially empty\n            if np.sum(B1) == 0:\n                for line_2 in l_lines[idx + 1 :]:\n                    l_line = line_2.split(\",\")\n                    if len(l_line) &gt; 1:\n                        slot = l_line[1]\n                        B1[int(slot)] = 1\n                    else:\n                        break\n\n            elif np.sum(B2) == 0:\n                for line_2 in l_lines[idx + 1 :]:\n                    l_line = line_2.split(\",\")\n                    if len(l_line) &gt; 1:\n                        slot = l_line[1]\n                        B2[int(slot)] = 1\n                    else:\n                        break\n            else:\n                break\n\n    data_json = {\"beam1\": [int(ii) for ii in B1], \"beam2\": [int(ii) for ii in B2]}\n\n    with open(filling_scheme_path_converted, \"w\") as file_bool:\n        json.dump(data_json, file_bool)\n    return B1, B2\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/utils.html","title":"utils","text":"<p>This module provides utility functions for file compression.</p> <p>Functions:</p> Name Description <code>compress_and_write</code> <p>str) -&gt; str: Compresses a file using ZIP compression and writes it to disk, then removes the original uncompressed file.</p> Imports <p>os: Provides a way of using operating system dependent functionality like reading or writing to     the file system. zipfile: Provides tools to create, read, write, append, and list a ZIP file.</p>"},{"location":"reference/study_da/generate/master_classes/utils.html#study_da.generate.master_classes.utils.compress_and_write","title":"<code>compress_and_write(path_to_file)</code>","text":"<p>Compress a file and write it to disk.</p> <p>Parameters:</p> Name Type Description Default <code>path_to_file</code> <code>str</code> <p>Path to the file to compress.</p> required <p>Returns:     path_to_output (str): Path to the output file.</p> Source code in <code>study_da/generate/master_classes/utils.py</code> <pre><code>def compress_and_write(path_to_file: str) -&gt; str:\n    \"\"\"Compress a file and write it to disk.\n\n    Args:\n        path_to_file (str): Path to the file to compress.\n    Returns:\n        path_to_output (str): Path to the output file.\n\n    \"\"\"\n    with ZipFile(\n        f\"{path_to_file}.zip\",\n        \"w\",\n        ZIP_DEFLATED,\n        compresslevel=9,\n    ) as zipf:\n        zipf.write(path_to_file)\n\n    # Remove the uncompressed file\n    os.remove(path_to_file)\n\n    return f\"{path_to_file}.zip\"\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html","title":"xsuite_collider","text":"<p>This class is used to build a Xsuite collider from a madx sequence and optics.</p>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider","title":"<code>XsuiteCollider</code>","text":"<p>XsuiteCollider is a class designed to handle the configuration and manipulation of a collider using the Xsuite library. It provides methods to load, configure, and tune the collider, as well as to perform luminosity leveling and beam-beam interaction setup.</p> <p>Attributes:</p> Name Type Description <code>path_collider_file_for_configuration_as_input</code> <code>str</code> <p>Path to the collider file to load.</p> <code>config_beambeam</code> <code>dict</code> <p>Configuration for beam-beam interactions.</p> <code>config_knobs_and_tuning</code> <code>dict</code> <p>Configuration for knobs and tuning.</p> <code>config_lumi_leveling</code> <code>dict</code> <p>Configuration for luminosity leveling.</p> <code>config_lumi_leveling_ip1_5</code> <code>dict or None</code> <p>Configuration for luminosity leveling at IP1 and IP5.</p> <code>config_collider</code> <code>dict</code> <p>Configuration for the collider.</p> <code>ver_hllhc_optics</code> <code>float</code> <p>Version of the HL-LHC optics.</p> <code>ver_lhc_run</code> <code>float</code> <p>Version of the LHC run.</p> <code>ions</code> <code>bool</code> <p>Flag indicating if ions are used.</p> <code>_dict_orbit_correction</code> <code>dict or None</code> <p>Dictionary for orbit correction.</p> <code>_crab</code> <code>bool or None</code> <p>Flag indicating if crab cavities are used.</p> <code>save_output_collider</code> <code>bool</code> <p>Flag indicating if the final collider should be saved.</p> <code>path_collider_file_for_tracking_as_output</code> <code>str</code> <p>Path to save the final collider.</p> <p>Methods:</p> Name Description <code>dict_orbit_correction</code> <p>Property to get the dictionary for orbit correction.</p> <code>load_collider</code> <p>Loads the collider from a file.</p> <code>install_beam_beam_wrapper</code> <p>Installs beam-beam lenses in the collider.</p> <code>set_knobs</code> <p>Sets the knobs for the collider.</p> <code>match_tune_and_chroma</code> <p>Matches the tune and chromaticity of the collider.</p> <code>set_filling_and_bunch_tracked</code> <p>Sets the filling scheme and tracks the bunch.</p> <code>compute_collision_from_scheme</code> <p>Computes the number of collisions from the filling scheme.</p> <code>crab</code> <p>Property to get the crab cavities status.</p> <code>level_all_by_separation</code> <p>Levels all IPs by separation.</p> <code>level_ip1_5_by_bunch_intensity</code> <p>Levels IP1 and IP5 by bunch intensity.</p> <code>level_ip2_8_by_separation</code> <p>Levels IP2 and IP8 by separation.</p> <code>add_linear_coupling</code> <p>Adds linear coupling to the collider.</p> <code>assert_tune_chroma_coupling</code> <p>Asserts the tune, chromaticity, and coupling of the collider.</p> <code>configure_beam_beam</code> <p>Configures the beam-beam interactions.</p> <code>record_final_luminosity</code> <p>Records the final luminosity of the collider.</p> <code>write_collider_to_disk</code> <p>Writes the collider configuration to disk.</p> <code>update_configuration_knob</code> <p>Updates a specific knob in the collider.</p> <code>return_fingerprint</code> <p>Returns a fingerprint of the collider's configuration.</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>class XsuiteCollider:\n    \"\"\"\n    XsuiteCollider is a class designed to handle the configuration and manipulation of a collider\n    using the Xsuite library. It provides methods to load, configure, and tune the collider,\n    as well as to perform luminosity leveling and beam-beam interaction setup.\n\n    Attributes:\n        path_collider_file_for_configuration_as_input (str): Path to the collider file to load.\n        config_beambeam (dict): Configuration for beam-beam interactions.\n        config_knobs_and_tuning (dict): Configuration for knobs and tuning.\n        config_lumi_leveling (dict): Configuration for luminosity leveling.\n        config_lumi_leveling_ip1_5 (dict or None): Configuration for luminosity leveling at IP1 and\n            IP5.\n        config_collider (dict): Configuration for the collider.\n        ver_hllhc_optics (float): Version of the HL-LHC optics.\n        ver_lhc_run (float): Version of the LHC run.\n        ions (bool): Flag indicating if ions are used.\n        _dict_orbit_correction (dict or None): Dictionary for orbit correction.\n        _crab (bool or None): Flag indicating if crab cavities are used.\n        save_output_collider (bool): Flag indicating if the final collider should be saved.\n        path_collider_file_for_tracking_as_output (str): Path to save the final collider.\n\n    Methods:\n        dict_orbit_correction: Property to get the dictionary for orbit correction.\n        load_collider: Loads the collider from a file.\n        install_beam_beam_wrapper: Installs beam-beam lenses in the collider.\n        set_knobs: Sets the knobs for the collider.\n        match_tune_and_chroma: Matches the tune and chromaticity of the collider.\n        set_filling_and_bunch_tracked: Sets the filling scheme and tracks the bunch.\n        compute_collision_from_scheme: Computes the number of collisions from the filling scheme.\n        crab: Property to get the crab cavities status.\n        level_all_by_separation: Levels all IPs by separation.\n        level_ip1_5_by_bunch_intensity: Levels IP1 and IP5 by bunch intensity.\n        level_ip2_8_by_separation: Levels IP2 and IP8 by separation.\n        add_linear_coupling: Adds linear coupling to the collider.\n        assert_tune_chroma_coupling: Asserts the tune, chromaticity, and coupling of the collider.\n        configure_beam_beam: Configures the beam-beam interactions.\n        record_final_luminosity: Records the final luminosity of the collider.\n        write_collider_to_disk: Writes the collider configuration to disk.\n        update_configuration_knob: Updates a specific knob in the collider.\n        return_fingerprint: Returns a fingerprint of the collider's configuration.\n    \"\"\"\n\n    def __init__(\n        self,\n        configuration: dict,\n        path_collider_file_for_configuration_as_input: str,\n        ver_hllhc_optics: float,\n        ver_lhc_run: float,\n        ions: bool,\n    ):\n        \"\"\"\n        Initialize the XsuiteCollider class with the given configuration and parameters.\n\n        Args:\n            configuration (dict): A dictionary containing various configuration settings.\n                - config_beambeam (dict): Configuration for beam-beam interactions.\n                - config_knobs_and_tuning (dict): Configuration for knobs and tuning.\n                - config_lumi_leveling (dict): Configuration for luminosity leveling.\n                - save_output_collider (bool): Flag to save the final collider to disk.\n                - path_collider_file_for_tracking_as_output (str): Path to save the final collider.\n                - config_lumi_leveling_ip1_5 (optional): Configuration for luminosity leveling at\n                    IP1 and IP5.\n            path_collider_file_for_configuration_as_input (str): Path to the collider file.\n            ver_hllhc_optics (float): Version of the HL-LHC optics.\n            ver_lhc_run (float): Version of the LHC run.\n            ions (bool): Flag indicating if ions are used.\n        \"\"\"\n        # Collider file path\n        self.path_collider_file_for_configuration_as_input = (\n            path_collider_file_for_configuration_as_input\n        )\n\n        # Configuration variables\n        self.config_beambeam: dict[str, Any] = configuration[\"config_beambeam\"]\n        self.config_knobs_and_tuning: dict[str, Any] = configuration[\"config_knobs_and_tuning\"]\n        self.config_lumi_leveling: dict[str, Any] = configuration[\"config_lumi_leveling\"]\n\n        # self.config_lumi_leveling_ip1_5 will be None if not present in the configuration\n        self.config_lumi_leveling_ip1_5: dict[str, Any] = configuration.get(\n            \"config_lumi_leveling_ip1_5\"\n        )\n\n        # Collider configuration\n        self.config_collider: dict[str, Any] = configuration\n\n        # Optics version (needed to select the appropriate optics specific functions)\n        self.ver_hllhc_optics: float = ver_hllhc_optics\n        self.ver_lhc_run: float = ver_lhc_run\n        self.ions: bool = ions\n        self._dict_orbit_correction: dict | None = None\n\n        # Crab cavities\n        self._crab: bool | None = None\n\n        # Save collider to disk\n        self.save_output_collider = configuration[\"save_output_collider\"]\n        self.path_collider_file_for_tracking_as_output = configuration[\n            \"path_collider_file_for_tracking_as_output\"\n        ]\n        self.compress = configuration[\"compress\"]\n\n    @property\n    def dict_orbit_correction(self) -&gt; dict:\n        \"\"\"\n        Generates and returns a dictionary containing orbit correction parameters.\n\n        This method checks if the orbit correction dictionary has already been generated.\n        If not, it determines the appropriate set of orbit correction parameters based on\n        the version of HLLHC optics or LHC run provided.\n\n        Returns:\n            dict: A dictionary containing orbit correction parameters.\n\n        Raises:\n            ValueError: If both `ver_hllhc_optics` and `ver_lhc_run` are defined.\n            ValueError: If no optics specific tools are available for the provided configuration.\n        \"\"\"\n        if self._dict_orbit_correction is None:\n            # Check that version is well defined\n            if self.ver_hllhc_optics is not None and self.ver_lhc_run is not None:\n                raise ValueError(\"Only one of ver_hllhc_optics and ver_lhc_run can be defined\")\n\n            # Get the appropriate optics_specific_tools\n            if self.ver_hllhc_optics is not None:\n                match self.ver_hllhc_optics:\n                    case 1.6:\n                        self._dict_orbit_correction = gen_corr_hllhc16()\n                    case 1.3:\n                        self._dict_orbit_correction = gen_corr_hllhc13()\n                    case _:\n                        raise ValueError(\"No optics specific tools for this configuration\")\n            elif self.ver_lhc_run == 3.0:\n                self._dict_orbit_correction = (\n                    gen_corr_runIII_ions() if self.ions else gen_corr_runIII()\n                )\n            else:\n                raise ValueError(\"No optics specific tools for the provided configuration\")\n\n        return self._dict_orbit_correction\n\n    @staticmethod\n    def _load_collider(path_collider) -&gt; xt.Multiline:\n        \"\"\"\n        Load a collider configuration from a file using an external path.\n\n        If the file path ends with \".zip\", the file is uncompressed locally\n        and the collider configuration is loaded from the uncompressed file.\n        Otherwise, the collider configuration is loaded directly from the file.\n\n        Returns:\n            xt.Multiline: The loaded collider configuration.\n        \"\"\"\n\n        # Correct collider file path if it is a zip file\n        if os.path.exists(f\"{path_collider}.zip\") and not path_collider.endswith(\".zip\"):\n            path_collider += \".zip\"\n\n        # Load as a json if not zip\n        if not path_collider.endswith(\".zip\"):\n            return xt.Multiline.from_json(path_collider)\n\n        # Uncompress file locally\n        logging.info(f\"Unzipping {path_collider}\")\n        with ZipFile(path_collider, \"r\") as zip_ref:\n            zip_ref.extractall()\n        final_path = os.path.basename(path_collider).replace(\".zip\", \"\")\n        return xt.Multiline.from_json(final_path)\n\n    def load_collider(self) -&gt; xt.Multiline:\n        \"\"\"\n        Load a collider configuration from a file.\n\n        If the file path ends with \".zip\", the file is uncompressed locally\n        and the collider configuration is loaded from the uncompressed file.\n        Otherwise, the collider configuration is loaded directly from the file.\n\n        Returns:\n            xt.Multiline: The loaded collider configuration.\n        \"\"\"\n        return self._load_collider(self.path_collider_file_for_configuration_as_input)\n\n    def install_beam_beam_wrapper(self, collider: xt.Multiline) -&gt; None:\n        \"\"\"\n        This method installs beam-beam interactions in the collider with the specified\n        parameters. The beam-beam lenses are initially inactive and not configured.\n\n        Args:\n            collider (xt.Multiline): The collider object where the beam-beam interactions\n                will be installed.\n\n        Returns:\n            None\n        \"\"\"\n        # Install beam-beam lenses (inactive and not configured)\n        collider.install_beambeam_interactions(\n            clockwise_line=\"lhcb1\",\n            anticlockwise_line=\"lhcb2\",\n            ip_names=[\"ip1\", \"ip2\", \"ip5\", \"ip8\"],\n            delay_at_ips_slots=[0, 891, 0, 2670],\n            num_long_range_encounters_per_side=self.config_beambeam[\n                \"num_long_range_encounters_per_side\"\n            ],\n            num_slices_head_on=self.config_beambeam[\"num_slices_head_on\"],\n            harmonic_number=35640,\n            bunch_spacing_buckets=self.config_beambeam[\"bunch_spacing_buckets\"],\n            sigmaz=self.config_beambeam[\"sigma_z\"],\n        )\n\n    def set_knobs(self, collider: xt.Multiline) -&gt; None:\n        \"\"\"\n        Set all knobs for the collider, including crossing angles, dispersion correction,\n        RF, crab cavities, experimental magnets, etc.\n\n        Args:\n            collider (xt.Multiline): The collider object to which the knob settings will be applied.\n\n        Returns:\n            None\n        \"\"\"\n        # Set all knobs (crossing angles, dispersion correction, rf, crab cavities,\n        # experimental magnets, etc.)\n        for kk, vv in self.config_knobs_and_tuning[\"knob_settings\"].items():\n            collider.vars[kk] = vv\n\n        # Crab fix (if needed)\n        if self.ver_hllhc_optics is not None and self.ver_hllhc_optics == 1.3:\n            apply_crab_fix(collider, self.config_knobs_and_tuning)\n\n    def match_tune_and_chroma(\n        self, collider: xt.Multiline, match_linear_coupling_to_zero: bool = True\n    ) -&gt; None:\n        \"\"\"\n        This method adjusts the tune and chromaticity of the specified collider lines\n        (\"lhcb1\" and \"lhcb2\") to the target values defined in the configuration. It also\n        optionally matches the linear coupling to zero.\n\n        Args:\n            collider (xt.Multiline): The collider object containing the lines to be tuned.\n            match_linear_coupling_to_zero (bool, optional): If True, linear coupling will be\n                matched to zero. Defaults to True.\n\n        Returns:\n            None\n        \"\"\"\n        for line_name in [\"lhcb1\", \"lhcb2\"]:\n            knob_names = self.config_knobs_and_tuning[\"knob_names\"][line_name]\n\n            targets = {\n                \"qx\": self.config_knobs_and_tuning[\"qx\"][line_name],\n                \"qy\": self.config_knobs_and_tuning[\"qy\"][line_name],\n                \"dqx\": self.config_knobs_and_tuning[\"dqx\"][line_name],\n                \"dqy\": self.config_knobs_and_tuning[\"dqy\"][line_name],\n            }\n\n            xm.machine_tuning(\n                line=collider[line_name],\n                enable_closed_orbit_correction=True,\n                enable_linear_coupling_correction=match_linear_coupling_to_zero,\n                enable_tune_correction=True,\n                enable_chromaticity_correction=True,\n                knob_names=knob_names,\n                targets=targets,\n                line_co_ref=collider[f\"{line_name}_co_ref\"],\n                co_corr_config=self.dict_orbit_correction[line_name],\n            )\n\n    def set_filling_and_bunch_tracked(self, ask_worst_bunch: bool = False) -&gt; None:\n        \"\"\"\n        Sets the filling scheme and determines the bunch to be tracked for beam-beam interactions.\n\n        This method performs the following steps:\n        1. Retrieves the filling scheme path from the configuration.\n        2. Checks if the filling scheme path needs to be obtained from the template schemes.\n        3. Loads and verifies the filling scheme, potentially converting it if necessary.\n        4. Updates the configuration with the correct filling scheme path.\n        5. Determines the number of long-range encounters to consider.\n        6. If the bunch number for beam 1 is not provided, it identifies the bunch with the largest\n        number of long-range interactions.\n           - If `ask_worst_bunch` is True, prompts the user to confirm or provide a bunch number.\n           - Otherwise, automatically selects the worst bunch.\n        7. If the bunch number for beam 2 is not provided, it automatically selects the worst bunch.\n\n        Args:\n            ask_worst_bunch (bool): If True, prompts the user to confirm or provide the bunch number\n                for beam 1. Defaults to False.\n\n        Returns:\n            None\n        \"\"\"\n        # Get the filling scheme path\n        filling_scheme_path = self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"]\n\n        # Check if the filling scheme path must be obtained from the template schemes\n        scheme_folder = (\n            pathlib.Path(__file__).parent.parent.parent.resolve().joinpath(\"assets/filling_schemes\")\n        )\n        if filling_scheme_path in os.listdir(scheme_folder):\n            filling_scheme_path = str(scheme_folder.joinpath(filling_scheme_path))\n            self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"] = filling_scheme_path\n\n        # Load and check filling scheme, potentially convert it\n        filling_scheme_path = load_and_check_filling_scheme(filling_scheme_path)\n\n        # Correct filling scheme in config, as it might have been converted\n        self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"] = filling_scheme_path\n\n        # Get number of LR to consider\n        n_LR = self.config_beambeam[\"num_long_range_encounters_per_side\"][\"ip1\"]\n\n        # If the bunch number is None, the bunch with the largest number of long-range interactions is used\n        if self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] is None:\n            # Case the bunch number has not been provided\n            worst_bunch_b1 = get_worst_bunch(\n                filling_scheme_path, number_of_LR_to_consider=n_LR, beam=\"beam_1\"\n            )\n            if ask_worst_bunch:\n                while self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] is None:\n                    bool_inp = input(\n                        \"The bunch number for beam 1 has not been provided. Do you want to use the\"\n                        \" bunch with the largest number of long-range interactions? It is the bunch\"\n                        \" number \" + str(worst_bunch_b1) + \" (y/n): \"\n                    )\n                    if bool_inp == \"y\":\n                        self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] = (\n                            worst_bunch_b1\n                        )\n                    elif bool_inp == \"n\":\n                        self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] = int(\n                            input(\"Please enter the bunch number for beam 1: \")\n                        )\n            else:\n                self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] = worst_bunch_b1\n\n        if self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b2\"] is None:\n            worst_bunch_b2 = get_worst_bunch(\n                filling_scheme_path, number_of_LR_to_consider=n_LR, beam=\"beam_2\"\n            )\n            # For beam 2, just select the worst bunch by default\n            self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b2\"] = worst_bunch_b2\n\n    def compute_collision_from_scheme(self) -&gt; tuple[int, int, int]:\n        \"\"\"\n        This method reads a filling scheme from a JSON file specified in the configuration, converts\n        the filling scheme into boolean arrays for two beams, and calculates the number of\n        collisions at IP1 &amp; IP5, IP2, and IP8 by performing convolutions on the arrays.\n\n        Returns:\n            tuple[int, int, int]: A tuple containing the number of collisions at IP1 &amp; IP5, IP2, and\n                IP8 respectively.\n\n        Raises:\n            ValueError: If the filling scheme file is not in JSON format.\n            AssertionError: If the length of the beam arrays is not 3564.\n        \"\"\"\n        # Get the filling scheme path (in json or csv format)\n        filling_scheme_path = self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"]\n\n        # Load the filling scheme\n        if not filling_scheme_path.endswith(\".json\"):\n            raise ValueError(\n                f\"Unknown filling scheme file format: {filling_scheme_path}. It you provided a csv\"\n                \" file, it should have been automatically convert when running the script\"\n                \" 001_make_folders.py. Something went wrong.\"\n            )\n\n        with open(filling_scheme_path, \"r\") as fid:\n            filling_scheme = json.load(fid)\n\n        # Extract booleans beam arrays\n        array_b1 = np.array(filling_scheme[\"beam1\"])\n        array_b2 = np.array(filling_scheme[\"beam2\"])\n\n        # Assert that the arrays have the required length, and do the convolution\n        assert len(array_b1) == len(array_b2) == 3564\n        n_collisions_ip1_and_5 = array_b1 @ array_b2\n        n_collisions_ip2 = np.roll(array_b1, 891) @ array_b2\n        n_collisions_ip8 = np.roll(array_b1, 2670) @ array_b2\n\n        return int(n_collisions_ip1_and_5), int(n_collisions_ip2), int(n_collisions_ip8)\n\n    @property\n    def crab(self) -&gt; bool:\n        \"\"\"\n        This method checks the configuration settings for the presence and value of the\n        \"on_crab1\" knob. If the knob is present and its value is non-zero, it sets the\n        `_crab` attribute to True, indicating that crab cavities are active. Otherwise,\n        it sets `_crab` to False.\n\n        Returns:\n            bool: True if crab cavities are active, False otherwise.\n        \"\"\"\n        if self._crab is None:\n            # Get crab cavities\n            self._crab = False\n            if \"on_crab1\" in self.config_knobs_and_tuning[\"knob_settings\"]:\n                crab_val = float(self.config_knobs_and_tuning[\"knob_settings\"][\"on_crab1\"])\n                if abs(crab_val) &gt; 0:\n                    self._crab = True\n        return self._crab\n\n    def level_all_by_separation(\n        self,\n        n_collisions_ip1_and_5: int,\n        n_collisions_ip2: int,\n        n_collisions_ip8: int,\n        collider: xt.Multiline,\n    ) -&gt; None:\n        \"\"\"\n        This method updates the number of colliding bunches for IP1, IP2, IP5, and IP8 in the\n        configuration file and performs luminosity leveling using the provided collider object.\n        It also updates the separation knobs for the collider based on the new configuration.\n\n        Args:\n            n_collisions_ip1_and_5 (int): Number of collisions at interaction points 1 and 5.\n            n_collisions_ip2 (int): Number of collisions at interaction point 2.\n            n_collisions_ip8 (int): Number of collisions at interaction point 8.\n            collider (xt.Multiline): The collider object to be used for luminosity leveling.\n\n        Returns:\n            None\n        \"\"\"\n        # Update the number of bunches in the configuration file\n        l_n_collisions = [\n            n_collisions_ip1_and_5,\n            n_collisions_ip2,\n            n_collisions_ip1_and_5,\n            n_collisions_ip8,\n        ]\n        for ip, n_collisions in zip([\"ip1\", \"ip2\", \"ip5\", \"ip8\"], l_n_collisions):\n            if ip in self.config_lumi_leveling:\n                self.config_lumi_leveling[ip][\"num_colliding_bunches\"] = n_collisions\n            else:\n                logging.warning(f\"IP {ip} is not in the configuration\")\n\n        # ! Crabs are not handled in the following function\n        xm.lhc.luminosity_leveling(  # type: ignore\n            collider,\n            config_lumi_leveling=self.config_lumi_leveling,\n            config_beambeam=self.config_beambeam,\n        )\n\n        # Update configuration\n        if \"ip1\" in self.config_lumi_leveling:\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip1\"], \"on_sep1\")\n        if \"ip2\" in self.config_lumi_leveling:\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2\")\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2h\")\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2v\")\n        if \"ip5\" in self.config_lumi_leveling:\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip5\"], \"on_sep5\")\n        if \"ip8\" in self.config_lumi_leveling:\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8\")\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8h\")\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8v\")\n\n    def level_ip1_5_by_bunch_intensity(\n        self,\n        collider: xt.Multiline,\n        n_collisions_ip1_and_5: int,\n    ) -&gt; None:\n        \"\"\"\n        This method modifies the bunch intensity to achieve the desired luminosity\n        levels in IP 1 and 5. It updates the configuration with the new intensity values.\n\n        Args:\n            collider (xt.Multiline): The collider object containing the beam and lattice\n                configuration.\n            n_collisions_ip1_and_5 (int):\n                The number of collisions in IP 1 and 5.\n\n        Returns:\n            None\n        \"\"\"\n        # Initial intensity\n        bunch_intensity = self.config_beambeam[\"num_particles_per_bunch\"]\n\n        # First level luminosity in IP 1/5 changing the intensity\n        if (\n            self.config_lumi_leveling_ip1_5 is not None\n            and not self.config_lumi_leveling_ip1_5[\"skip_leveling\"]\n        ):\n            logging.info(\"Leveling luminosity in IP 1/5 varying the intensity\")\n            # Update the number of bunches in the configuration file\n            self.config_lumi_leveling_ip1_5[\"num_colliding_bunches\"] = n_collisions_ip1_and_5\n\n            # Do the levelling\n            bunch_intensity = luminosity_leveling_ip1_5(\n                collider,\n                self.config_lumi_leveling_ip1_5,\n                self.config_beambeam,\n                crab=self.crab,\n                cross_section=self.config_beambeam[\"cross_section\"],\n            )\n\n        # Update the configuration\n        self.config_beambeam[\"final_num_particles_per_bunch\"] = float(bunch_intensity)\n\n    def level_ip2_8_by_separation(\n        self,\n        n_collisions_ip2: int,\n        n_collisions_ip8: int,\n        collider: xt.Multiline,\n    ) -&gt; None:\n        \"\"\"\n        This method updates the number of colliding bunches for IP2 and IP8 in the configuration\n        file, performs luminosity leveling for the specified collider, and updates the separation\n        knobs for both interaction points.\n\n        Args:\n            n_collisions_ip2 (int): The number of collisions at interaction point 2 (IP2).\n            n_collisions_ip8 (int): The number of collisions at interaction point 8 (IP8).\n            collider (xt.Multiline): The collider object for which the luminosity leveling is to be\n                performed.\n\n        Returns:\n            None\n        \"\"\"\n        # Update the number of bunches in the configuration file\n        if \"ip2\" in self.config_lumi_leveling:\n            self.config_lumi_leveling[\"ip2\"][\"num_colliding_bunches\"] = n_collisions_ip2\n        if \"ip8\" in self.config_lumi_leveling:\n            self.config_lumi_leveling[\"ip8\"][\"num_colliding_bunches\"] = n_collisions_ip8\n\n        # Ensure the the num particles per bunch corresponds to the final one\n        temp_num_particles_per_bunch = self.config_beambeam[\"num_particles_per_bunch\"]\n        if \"final_num_particles_per_bunch\" in self.config_beambeam:\n            self.config_beambeam[\"num_particles_per_bunch\"] = self.config_beambeam[\n                \"final_num_particles_per_bunch\"\n            ]\n        # Do levelling in IP2 and IP8\n        xm.lhc.luminosity_leveling(  # type: ignore\n            collider,\n            config_lumi_leveling=self.config_lumi_leveling,\n            config_beambeam=self.config_beambeam,\n        )\n\n        # Update configuration\n        if \"ip2\" in self.config_lumi_leveling:\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2\")\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2h\")\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2v\")\n        if \"ip8\" in self.config_lumi_leveling:\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8\")\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8h\")\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8v\")\n\n        # Set back the num particles per bunch to its initial value\n        self.config_beambeam[\"num_particles_per_bunch\"] = temp_num_particles_per_bunch\n\n    def add_linear_coupling(self, collider: xt.Multiline) -&gt; None:\n        \"\"\"\n        Adds linear coupling to the collider based on the version of the LHC run or HL-LHC optics.\n\n        This method adjusts the collider variables to introduce linear coupling. The specific\n        adjustments depend on the version of the LHC run or HL-LHC optics being used.\n\n        Args:\n            collider (xt.Multiline): The collider object to which linear coupling will be added.\n\n        Returns:\n            None\n\n        Raises:\n            ValueError: If the version of the optics or run is unknown.\n\n        Notes:\n            - For LHC Run 3.0, the `cmrs.b1_sq` and `cmrs.b2_sq` variables are adjusted.\n            - For HL-LHC optics versions 1.6, 1.5, 1.4, and 1.3, the `c_minus_re_b1` and\n            `c_minus_re_b2` variables are adjusted.\n        \"\"\"\n        # Add linear coupling as the target in the tuning of the base collider was 0\n        # (not possible to set it the target to 0.001 for now)\n        if self.ver_lhc_run == 3.0:\n            collider.vars[\"cmrs.b1_sq\"] += self.config_knobs_and_tuning[\"delta_cmr\"]\n            collider.vars[\"cmrs.b2_sq\"] += self.config_knobs_and_tuning[\"delta_cmr\"]\n        elif self.ver_hllhc_optics in [1.6, 1.5, 1.4, 1.3]:\n            collider.vars[\"c_minus_re_b1\"] += self.config_knobs_and_tuning[\"delta_cmr\"]\n            collider.vars[\"c_minus_re_b2\"] += self.config_knobs_and_tuning[\"delta_cmr\"]\n        else:\n            raise ValueError(\n                f\"Unknown version of the optics/run: {self.ver_hllhc_optics}, {self.ver_lhc_run}.\"\n            )\n\n    def assert_tune_chroma_coupling(self, collider: xt.Multiline) -&gt; None:\n        \"\"\"\n        Asserts that the tune, chromaticity, and linear coupling of the collider\n        match the expected values specified in the configuration.\n\n        Args:\n            collider (xt.Multiline): The collider object containing the lines to be checked.\n\n        Returns:\n            None\n\n        Raises:\n            AssertionError: If any of the tune, chromaticity, or linear coupling values do not match\n                the expected values within the specified tolerances.\n\n        Notes:\n            The function checks the following parameters for each line (\"lhcb1\" and \"lhcb2\"):\n            - Horizontal tune (qx)\n            - Vertical tune (qy)\n            - Horizontal chromaticity (dqx)\n            - Vertical chromaticity (dqy)\n            - Linear coupling (c_minus)\n\n        The expected values are retrieved from the `self.config_knobs_and_tuning` dictionary.\n        \"\"\"\n        for line_name in [\"lhcb1\", \"lhcb2\"]:\n            tw = collider[line_name].twiss()\n            assert np.isclose(tw.qx, self.config_knobs_and_tuning[\"qx\"][line_name], atol=1e-4), (\n                f\"tune_x is not correct for {line_name}. Expected\"\n                f\" {self.config_knobs_and_tuning['qx'][line_name]}, got {tw.qx}\"\n            )\n            assert np.isclose(tw.qy, self.config_knobs_and_tuning[\"qy\"][line_name], atol=1e-4), (\n                f\"tune_y is not correct for {line_name}. Expected\"\n                f\" {self.config_knobs_and_tuning['qy'][line_name]}, got {tw.qy}\"\n            )\n            assert np.isclose(\n                tw.dqx,\n                self.config_knobs_and_tuning[\"dqx\"][line_name],\n                rtol=1e-2,\n            ), (\n                f\"chromaticity_x is not correct for {line_name}. Expected\"\n                f\" {self.config_knobs_and_tuning['dqx'][line_name]}, got {tw.dqx}\"\n            )\n            assert np.isclose(\n                tw.dqy,\n                self.config_knobs_and_tuning[\"dqy\"][line_name],\n                rtol=1e-2,\n            ), (\n                f\"chromaticity_y is not correct for {line_name}. Expected\"\n                f\" {self.config_knobs_and_tuning['dqy'][line_name]}, got {tw.dqy}\"\n            )\n\n            assert np.isclose(\n                tw.c_minus,\n                self.config_knobs_and_tuning[\"delta_cmr\"],\n                atol=5e-3,\n            ), (\n                f\"linear coupling is not correct for {line_name}. Expected\"\n                f\" {self.config_knobs_and_tuning['delta_cmr']}, got {tw.c_minus}\"\n            )\n\n    def record_beta_functions(self, collider: xt.Multiline) -&gt; None:\n        \"\"\"\n        Records the beta functions at the IPs in the collider.\n\n        Args:\n            collider (xt.Multiline): The collider object to record the beta functions.\n\n        Returns:\n            None\n        \"\"\"\n        # Record beta functions at the IPs\n        for ip in [\"ip1\", \"ip2\", \"ip5\", \"ip8\"]:\n            tw = collider.lhcb1.twiss()\n            self.config_collider[f\"beta_x_{ip}\"] = float(np.round(float(tw[\"betx\", ip]), 5))\n            self.config_collider[f\"beta_y_{ip}\"] = float(np.round(float(tw[\"bety\", ip]), 5))\n\n    def configure_beam_beam(self, collider: xt.Multiline) -&gt; None:\n        \"\"\"\n        Configures the beam-beam interactions for the collider.\n\n        This method sets up the beam-beam interactions by configuring the number of particles per\n        bunch, the horizontal emittance (nemitt_x), and the vertical emittance (nemitt_y) based on\n        the provided configuration. Additionally, it configures the filling scheme mask and bunch\n        numbers if a filling pattern is specified in the configuration.\n\n        Args:\n            collider (xt.Multiline): The collider object to configure.\n\n        Returns:\n            None\n        \"\"\"\n        collider.configure_beambeam_interactions(\n            num_particles=self.config_beambeam[\"num_particles_per_bunch\"],\n            nemitt_x=self.config_beambeam[\"nemitt_x\"],\n            nemitt_y=self.config_beambeam[\"nemitt_y\"],\n        )\n\n        # Configure filling scheme mask and bunch numbers\n        if \"mask_with_filling_pattern\" in self.config_beambeam and (\n            \"pattern_fname\" in self.config_beambeam[\"mask_with_filling_pattern\"]\n            and self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"] is not None\n        ):\n            fname = self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"]\n            with open(fname, \"r\") as fid:\n                filling = json.load(fid)\n            filling_pattern_cw = filling[\"beam1\"]\n            filling_pattern_acw = filling[\"beam2\"]\n\n            # Initialize bunch numbers with empty values\n            i_bunch_cw = None\n            i_bunch_acw = None\n\n            # Only track bunch number if a filling pattern has been provided\n            if \"i_bunch_b1\" in self.config_beambeam[\"mask_with_filling_pattern\"]:\n                i_bunch_cw = self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"]\n            if \"i_bunch_b2\" in self.config_beambeam[\"mask_with_filling_pattern\"]:\n                i_bunch_acw = self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b2\"]\n\n            # Note that a bunch number must be provided if a filling pattern is provided\n            # Apply filling pattern\n            collider.apply_filling_pattern(\n                filling_pattern_cw=filling_pattern_cw,\n                filling_pattern_acw=filling_pattern_acw,\n                i_bunch_cw=i_bunch_cw,\n                i_bunch_acw=i_bunch_acw,\n            )\n\n    def record_final_luminosity(self, collider: xt.Multiline, l_n_collisions: list[int]) -&gt; None:\n        \"\"\"\n        Records the final luminosity and pile-up for specified interaction points (IPs)\n        in the collider, both with and without beam-beam effects.\n\n        Args:\n            collider : (xt.Multiline): The collider object configured.\n            l_n_collisions (list[int]): A list containing the number of colliding bunches for each\n                IP.\n\n        Returns:\n            None\n        \"\"\"\n        # Define IPs in which the luminosity will be computed\n        l_ip = [\"ip1\", \"ip2\", \"ip5\", \"ip8\"]\n\n        # Ensure that the final number of particles per bunch is defined, even\n        # if the leveling has been done by separation\n        if \"final_num_particles_per_bunch\" not in self.config_beambeam:\n            self.config_beambeam[\"final_num_particles_per_bunch\"] = self.config_beambeam[\n                \"num_particles_per_bunch\"\n            ]\n\n        def _twiss_and_compute_lumi(collider, l_n_collisions):\n            # Loop over each IP and record the luminosity\n            twiss_b1 = collider[\"lhcb1\"].twiss()\n            twiss_b2 = collider[\"lhcb2\"].twiss()\n            l_lumi = []\n            l_PU = []\n            for n_col, ip in zip(l_n_collisions, l_ip):\n                L = xt.lumi.luminosity_from_twiss(  # type: ignore\n                    n_colliding_bunches=n_col,\n                    num_particles_per_bunch=self.config_beambeam[\"final_num_particles_per_bunch\"],\n                    ip_name=ip,\n                    nemitt_x=self.config_beambeam[\"nemitt_x\"],\n                    nemitt_y=self.config_beambeam[\"nemitt_y\"],\n                    sigma_z=self.config_beambeam[\"sigma_z\"],\n                    twiss_b1=twiss_b1,\n                    twiss_b2=twiss_b2,\n                    crab=self.crab,\n                )\n                PU = compute_PU(\n                    L,\n                    n_col,\n                    twiss_b1[\"T_rev0\"],\n                    cross_section=self.config_beambeam[\"cross_section\"],\n                )\n\n                l_lumi.append(L)\n                l_PU.append(PU)\n\n            return l_lumi, l_PU\n\n        # Get the final luminosity in all IPs, without beam-beam\n        collider.vars[\"beambeam_scale\"] = 0\n        l_lumi, l_PU = _twiss_and_compute_lumi(collider, l_n_collisions)\n\n        # Update configuration\n        for ip, L, PU in zip(l_ip, l_lumi, l_PU):\n            self.config_beambeam[f\"luminosity_{ip}_without_beam_beam\"] = float(L)\n            self.config_beambeam[f\"Pile-up_{ip}_without_beam_beam\"] = float(PU)\n\n        # Get the final luminosity in all IPs, with beam-beam\n        collider.vars[\"beambeam_scale\"] = 1\n        l_lumi, l_PU = _twiss_and_compute_lumi(collider, l_n_collisions)\n\n        # Update configuration\n        for ip, L, PU in zip(l_ip, l_lumi, l_PU):\n            self.config_beambeam[f\"luminosity_{ip}_with_beam_beam\"] = float(L)\n            self.config_beambeam[f\"Pile-up_{ip}_with_beam_beam\"] = float(PU)\n\n    def write_collider_to_disk(self, collider, full_configuration) -&gt; None:\n        \"\"\"\n        Writes the collider object to disk in JSON format if the save_output_collider flag is set.\n\n        Args:\n            collider (Collider): The collider object to be saved.\n            full_configuration (dict): The full configuration dictionary to be deep-copied into the\n                collider's metadata.\n\n        Returns:\n            None\n        \"\"\"\n        if self.save_output_collider:\n            logging.info(\"Saving collider as json\")\n            if (\n                hasattr(collider, \"metadata\")\n                and collider.metadata is not None\n                and isinstance(collider.metadata, dict)\n            ):\n                collider.metadata.update(copy.deepcopy(full_configuration))\n            else:\n                collider.metadata = copy.deepcopy(full_configuration)\n            collider.to_json(self.path_collider_file_for_tracking_as_output)\n\n            # Compress the collider file to zip to ease the load on afs\n            if self.compress:\n                compress_and_write(self.path_collider_file_for_tracking_as_output)\n\n    @staticmethod\n    def update_configuration_knob(\n        collider: xt.Multiline, dictionnary: dict, knob_name: str\n    ) -&gt; None:\n        \"\"\"\n        Updates the given dictionary with the final value of a specified knob from the collider.\n\n        Args:\n            collider (xt.Multiline): The collider object containing various variables.\n            dictionnary (dict): The dictionary to be updated with the knob's final value.\n            knob_name (str): The name of the knob whose value is to be retrieved and stored.\n\n        Returns:\n            None\n        \"\"\"\n        if knob_name in collider.vars.keys():\n            dictionnary[f\"final_{knob_name}\"] = float(collider.vars[knob_name]._value)\n        else:\n            logging.warning(f\"Knob {knob_name} not found in the collider\")\n\n    @staticmethod\n    def return_fingerprint(collider, line_name=\"lhcb1\") -&gt; str:\n        \"\"\"\n        Generate a detailed fingerprint of the specified collider line. Useful to compare two\n        colliders.\n\n        Args:\n            collider (xt.Multiline): The collider object containing the line data.\n            line_name (str): The name of the line to analyze within the collider. Default to \"lhcb1\".\n\n        Returns:\n            str:\n                A formatted string containing detailed information about the collider line, including:\n                - Installed element types\n                - Tunes and chromaticity\n                - Synchrotron tune and slip factor\n                - Twiss parameters and phases at interaction points (IPs)\n                - Dispersion and crab dispersion at IPs\n                - Amplitude detuning coefficients\n                - Non-linear chromaticity\n                - Tunes and momentum compaction vs delta\n        \"\"\"\n        line = collider[line_name]\n\n        tw = line.twiss()\n        tt = line.get_table()\n\n        det = line.get_amplitude_detuning_coefficients(a0_sigmas=0.1, a1_sigmas=0.2, a2_sigmas=0.3)\n\n        det_table = xt.Table(\n            {\n                \"name\": np.array(list(det.keys())),\n                \"value\": np.array(list(det.values())),\n            }\n        )\n\n        nl_chrom = line.get_non_linear_chromaticity(\n            delta0_range=(-2e-4, 2e-4), num_delta=5, fit_order=3\n        )\n\n        out = \"\"\n\n        out += f\"Line: {line_name}\\n\"\n        out += \"\\n\"\n\n        out += \"Installed element types:\\n\"\n        out += repr([nn for nn in sorted(list(set(tt.element_type))) if len(nn) &gt; 0]) + \"\\n\"\n        out += \"\\n\"\n\n        out += f'Tunes:        Qx  = {tw[\"qx\"]:.5f}       Qy = {tw[\"qy\"]:.5f}\\n'\n        out += f\"\"\"Chromaticity: Q'x = {tw[\"dqx\"]:.2f}     Q'y = \"\"\" + f'{tw[\"dqy\"]:.2f}\\n'\n        out += f'c_minus:      {tw[\"c_minus\"]:.5e}\\n'\n        out += \"\\n\"\n\n        out += f'Synchrotron tune: {tw[\"qs\"]:5e}\\n'\n        out += f'Slip factor:      {tw[\"slip_factor\"]:.5e}\\n'\n        out += \"\\n\"\n\n        out += \"Twiss parameters and phases at IPs:\\n\"\n        out += (\n            tw.rows[\"ip.*\"]\n            .cols[\"name s betx bety alfx alfy mux muy\"]\n            .show(output=str, max_col_width=int(1e6), digits=8)\n        )\n        out += \"\\n\\n\"\n\n        out += \"Dispersion at IPs:\\n\"\n        out += (\n            tw.rows[\"ip.*\"]\n            .cols[\"name s dx dy dpx dpy\"]\n            .show(output=str, max_col_width=int(1e6), digits=8)\n        )\n        out += \"\\n\\n\"\n\n        out += \"Crab dispersion at IPs:\\n\"\n        out += (\n            tw.rows[\"ip.*\"]\n            .cols[\"name s dx_zeta dy_zeta dpx_zeta dpy_zeta\"]\n            .show(output=str, max_col_width=int(1e6), digits=8)\n        )\n        out += \"\\n\\n\"\n\n        out += \"Amplitude detuning coefficients:\\n\"\n        out += det_table.show(output=str, max_col_width=int(1e6), digits=6)\n        out += \"\\n\\n\"\n\n        out += \"Non-linear chromaticity:\\n\"\n        out += f'dnqx = {list(nl_chrom[\"dnqx\"])}\\n'\n        out += f'dnqy = {list(nl_chrom[\"dnqy\"])}\\n'\n        out += \"\\n\\n\"\n\n        out += \"Tunes and momentum compaction vs delta:\\n\"\n        out += nl_chrom.show(output=str, max_col_width=int(1e6), digits=6)\n        out += \"\\n\\n\"\n\n        return out\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.crab","title":"<code>crab: bool</code>  <code>property</code>","text":"<p>This method checks the configuration settings for the presence and value of the \"on_crab1\" knob. If the knob is present and its value is non-zero, it sets the <code>_crab</code> attribute to True, indicating that crab cavities are active. Otherwise, it sets <code>_crab</code> to False.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if crab cavities are active, False otherwise.</p>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.dict_orbit_correction","title":"<code>dict_orbit_correction: dict</code>  <code>property</code>","text":"<p>Generates and returns a dictionary containing orbit correction parameters.</p> <p>This method checks if the orbit correction dictionary has already been generated. If not, it determines the appropriate set of orbit correction parameters based on the version of HLLHC optics or LHC run provided.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing orbit correction parameters.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both <code>ver_hllhc_optics</code> and <code>ver_lhc_run</code> are defined.</p> <code>ValueError</code> <p>If no optics specific tools are available for the provided configuration.</p>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.__init__","title":"<code>__init__(configuration, path_collider_file_for_configuration_as_input, ver_hllhc_optics, ver_lhc_run, ions)</code>","text":"<p>Initialize the XsuiteCollider class with the given configuration and parameters.</p> <p>Parameters:</p> Name Type Description Default <code>configuration</code> <code>dict</code> <p>A dictionary containing various configuration settings. - config_beambeam (dict): Configuration for beam-beam interactions. - config_knobs_and_tuning (dict): Configuration for knobs and tuning. - config_lumi_leveling (dict): Configuration for luminosity leveling. - save_output_collider (bool): Flag to save the final collider to disk. - path_collider_file_for_tracking_as_output (str): Path to save the final collider. - config_lumi_leveling_ip1_5 (optional): Configuration for luminosity leveling at     IP1 and IP5.</p> required <code>path_collider_file_for_configuration_as_input</code> <code>str</code> <p>Path to the collider file.</p> required <code>ver_hllhc_optics</code> <code>float</code> <p>Version of the HL-LHC optics.</p> required <code>ver_lhc_run</code> <code>float</code> <p>Version of the LHC run.</p> required <code>ions</code> <code>bool</code> <p>Flag indicating if ions are used.</p> required Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def __init__(\n    self,\n    configuration: dict,\n    path_collider_file_for_configuration_as_input: str,\n    ver_hllhc_optics: float,\n    ver_lhc_run: float,\n    ions: bool,\n):\n    \"\"\"\n    Initialize the XsuiteCollider class with the given configuration and parameters.\n\n    Args:\n        configuration (dict): A dictionary containing various configuration settings.\n            - config_beambeam (dict): Configuration for beam-beam interactions.\n            - config_knobs_and_tuning (dict): Configuration for knobs and tuning.\n            - config_lumi_leveling (dict): Configuration for luminosity leveling.\n            - save_output_collider (bool): Flag to save the final collider to disk.\n            - path_collider_file_for_tracking_as_output (str): Path to save the final collider.\n            - config_lumi_leveling_ip1_5 (optional): Configuration for luminosity leveling at\n                IP1 and IP5.\n        path_collider_file_for_configuration_as_input (str): Path to the collider file.\n        ver_hllhc_optics (float): Version of the HL-LHC optics.\n        ver_lhc_run (float): Version of the LHC run.\n        ions (bool): Flag indicating if ions are used.\n    \"\"\"\n    # Collider file path\n    self.path_collider_file_for_configuration_as_input = (\n        path_collider_file_for_configuration_as_input\n    )\n\n    # Configuration variables\n    self.config_beambeam: dict[str, Any] = configuration[\"config_beambeam\"]\n    self.config_knobs_and_tuning: dict[str, Any] = configuration[\"config_knobs_and_tuning\"]\n    self.config_lumi_leveling: dict[str, Any] = configuration[\"config_lumi_leveling\"]\n\n    # self.config_lumi_leveling_ip1_5 will be None if not present in the configuration\n    self.config_lumi_leveling_ip1_5: dict[str, Any] = configuration.get(\n        \"config_lumi_leveling_ip1_5\"\n    )\n\n    # Collider configuration\n    self.config_collider: dict[str, Any] = configuration\n\n    # Optics version (needed to select the appropriate optics specific functions)\n    self.ver_hllhc_optics: float = ver_hllhc_optics\n    self.ver_lhc_run: float = ver_lhc_run\n    self.ions: bool = ions\n    self._dict_orbit_correction: dict | None = None\n\n    # Crab cavities\n    self._crab: bool | None = None\n\n    # Save collider to disk\n    self.save_output_collider = configuration[\"save_output_collider\"]\n    self.path_collider_file_for_tracking_as_output = configuration[\n        \"path_collider_file_for_tracking_as_output\"\n    ]\n    self.compress = configuration[\"compress\"]\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.add_linear_coupling","title":"<code>add_linear_coupling(collider)</code>","text":"<p>Adds linear coupling to the collider based on the version of the LHC run or HL-LHC optics.</p> <p>This method adjusts the collider variables to introduce linear coupling. The specific adjustments depend on the version of the LHC run or HL-LHC optics being used.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object to which linear coupling will be added.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the version of the optics or run is unknown.</p> Notes <ul> <li>For LHC Run 3.0, the <code>cmrs.b1_sq</code> and <code>cmrs.b2_sq</code> variables are adjusted.</li> <li>For HL-LHC optics versions 1.6, 1.5, 1.4, and 1.3, the <code>c_minus_re_b1</code> and <code>c_minus_re_b2</code> variables are adjusted.</li> </ul> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def add_linear_coupling(self, collider: xt.Multiline) -&gt; None:\n    \"\"\"\n    Adds linear coupling to the collider based on the version of the LHC run or HL-LHC optics.\n\n    This method adjusts the collider variables to introduce linear coupling. The specific\n    adjustments depend on the version of the LHC run or HL-LHC optics being used.\n\n    Args:\n        collider (xt.Multiline): The collider object to which linear coupling will be added.\n\n    Returns:\n        None\n\n    Raises:\n        ValueError: If the version of the optics or run is unknown.\n\n    Notes:\n        - For LHC Run 3.0, the `cmrs.b1_sq` and `cmrs.b2_sq` variables are adjusted.\n        - For HL-LHC optics versions 1.6, 1.5, 1.4, and 1.3, the `c_minus_re_b1` and\n        `c_minus_re_b2` variables are adjusted.\n    \"\"\"\n    # Add linear coupling as the target in the tuning of the base collider was 0\n    # (not possible to set it the target to 0.001 for now)\n    if self.ver_lhc_run == 3.0:\n        collider.vars[\"cmrs.b1_sq\"] += self.config_knobs_and_tuning[\"delta_cmr\"]\n        collider.vars[\"cmrs.b2_sq\"] += self.config_knobs_and_tuning[\"delta_cmr\"]\n    elif self.ver_hllhc_optics in [1.6, 1.5, 1.4, 1.3]:\n        collider.vars[\"c_minus_re_b1\"] += self.config_knobs_and_tuning[\"delta_cmr\"]\n        collider.vars[\"c_minus_re_b2\"] += self.config_knobs_and_tuning[\"delta_cmr\"]\n    else:\n        raise ValueError(\n            f\"Unknown version of the optics/run: {self.ver_hllhc_optics}, {self.ver_lhc_run}.\"\n        )\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.assert_tune_chroma_coupling","title":"<code>assert_tune_chroma_coupling(collider)</code>","text":"<p>Asserts that the tune, chromaticity, and linear coupling of the collider match the expected values specified in the configuration.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object containing the lines to be checked.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If any of the tune, chromaticity, or linear coupling values do not match the expected values within the specified tolerances.</p> Notes <p>The function checks the following parameters for each line (\"lhcb1\" and \"lhcb2\"): - Horizontal tune (qx) - Vertical tune (qy) - Horizontal chromaticity (dqx) - Vertical chromaticity (dqy) - Linear coupling (c_minus)</p> <p>The expected values are retrieved from the <code>self.config_knobs_and_tuning</code> dictionary.</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def assert_tune_chroma_coupling(self, collider: xt.Multiline) -&gt; None:\n    \"\"\"\n    Asserts that the tune, chromaticity, and linear coupling of the collider\n    match the expected values specified in the configuration.\n\n    Args:\n        collider (xt.Multiline): The collider object containing the lines to be checked.\n\n    Returns:\n        None\n\n    Raises:\n        AssertionError: If any of the tune, chromaticity, or linear coupling values do not match\n            the expected values within the specified tolerances.\n\n    Notes:\n        The function checks the following parameters for each line (\"lhcb1\" and \"lhcb2\"):\n        - Horizontal tune (qx)\n        - Vertical tune (qy)\n        - Horizontal chromaticity (dqx)\n        - Vertical chromaticity (dqy)\n        - Linear coupling (c_minus)\n\n    The expected values are retrieved from the `self.config_knobs_and_tuning` dictionary.\n    \"\"\"\n    for line_name in [\"lhcb1\", \"lhcb2\"]:\n        tw = collider[line_name].twiss()\n        assert np.isclose(tw.qx, self.config_knobs_and_tuning[\"qx\"][line_name], atol=1e-4), (\n            f\"tune_x is not correct for {line_name}. Expected\"\n            f\" {self.config_knobs_and_tuning['qx'][line_name]}, got {tw.qx}\"\n        )\n        assert np.isclose(tw.qy, self.config_knobs_and_tuning[\"qy\"][line_name], atol=1e-4), (\n            f\"tune_y is not correct for {line_name}. Expected\"\n            f\" {self.config_knobs_and_tuning['qy'][line_name]}, got {tw.qy}\"\n        )\n        assert np.isclose(\n            tw.dqx,\n            self.config_knobs_and_tuning[\"dqx\"][line_name],\n            rtol=1e-2,\n        ), (\n            f\"chromaticity_x is not correct for {line_name}. Expected\"\n            f\" {self.config_knobs_and_tuning['dqx'][line_name]}, got {tw.dqx}\"\n        )\n        assert np.isclose(\n            tw.dqy,\n            self.config_knobs_and_tuning[\"dqy\"][line_name],\n            rtol=1e-2,\n        ), (\n            f\"chromaticity_y is not correct for {line_name}. Expected\"\n            f\" {self.config_knobs_and_tuning['dqy'][line_name]}, got {tw.dqy}\"\n        )\n\n        assert np.isclose(\n            tw.c_minus,\n            self.config_knobs_and_tuning[\"delta_cmr\"],\n            atol=5e-3,\n        ), (\n            f\"linear coupling is not correct for {line_name}. Expected\"\n            f\" {self.config_knobs_and_tuning['delta_cmr']}, got {tw.c_minus}\"\n        )\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.compute_collision_from_scheme","title":"<code>compute_collision_from_scheme()</code>","text":"<p>This method reads a filling scheme from a JSON file specified in the configuration, converts the filling scheme into boolean arrays for two beams, and calculates the number of collisions at IP1 &amp; IP5, IP2, and IP8 by performing convolutions on the arrays.</p> <p>Returns:</p> Type Description <code>tuple[int, int, int]</code> <p>tuple[int, int, int]: A tuple containing the number of collisions at IP1 &amp; IP5, IP2, and IP8 respectively.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the filling scheme file is not in JSON format.</p> <code>AssertionError</code> <p>If the length of the beam arrays is not 3564.</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def compute_collision_from_scheme(self) -&gt; tuple[int, int, int]:\n    \"\"\"\n    This method reads a filling scheme from a JSON file specified in the configuration, converts\n    the filling scheme into boolean arrays for two beams, and calculates the number of\n    collisions at IP1 &amp; IP5, IP2, and IP8 by performing convolutions on the arrays.\n\n    Returns:\n        tuple[int, int, int]: A tuple containing the number of collisions at IP1 &amp; IP5, IP2, and\n            IP8 respectively.\n\n    Raises:\n        ValueError: If the filling scheme file is not in JSON format.\n        AssertionError: If the length of the beam arrays is not 3564.\n    \"\"\"\n    # Get the filling scheme path (in json or csv format)\n    filling_scheme_path = self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"]\n\n    # Load the filling scheme\n    if not filling_scheme_path.endswith(\".json\"):\n        raise ValueError(\n            f\"Unknown filling scheme file format: {filling_scheme_path}. It you provided a csv\"\n            \" file, it should have been automatically convert when running the script\"\n            \" 001_make_folders.py. Something went wrong.\"\n        )\n\n    with open(filling_scheme_path, \"r\") as fid:\n        filling_scheme = json.load(fid)\n\n    # Extract booleans beam arrays\n    array_b1 = np.array(filling_scheme[\"beam1\"])\n    array_b2 = np.array(filling_scheme[\"beam2\"])\n\n    # Assert that the arrays have the required length, and do the convolution\n    assert len(array_b1) == len(array_b2) == 3564\n    n_collisions_ip1_and_5 = array_b1 @ array_b2\n    n_collisions_ip2 = np.roll(array_b1, 891) @ array_b2\n    n_collisions_ip8 = np.roll(array_b1, 2670) @ array_b2\n\n    return int(n_collisions_ip1_and_5), int(n_collisions_ip2), int(n_collisions_ip8)\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.configure_beam_beam","title":"<code>configure_beam_beam(collider)</code>","text":"<p>Configures the beam-beam interactions for the collider.</p> <p>This method sets up the beam-beam interactions by configuring the number of particles per bunch, the horizontal emittance (nemitt_x), and the vertical emittance (nemitt_y) based on the provided configuration. Additionally, it configures the filling scheme mask and bunch numbers if a filling pattern is specified in the configuration.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object to configure.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def configure_beam_beam(self, collider: xt.Multiline) -&gt; None:\n    \"\"\"\n    Configures the beam-beam interactions for the collider.\n\n    This method sets up the beam-beam interactions by configuring the number of particles per\n    bunch, the horizontal emittance (nemitt_x), and the vertical emittance (nemitt_y) based on\n    the provided configuration. Additionally, it configures the filling scheme mask and bunch\n    numbers if a filling pattern is specified in the configuration.\n\n    Args:\n        collider (xt.Multiline): The collider object to configure.\n\n    Returns:\n        None\n    \"\"\"\n    collider.configure_beambeam_interactions(\n        num_particles=self.config_beambeam[\"num_particles_per_bunch\"],\n        nemitt_x=self.config_beambeam[\"nemitt_x\"],\n        nemitt_y=self.config_beambeam[\"nemitt_y\"],\n    )\n\n    # Configure filling scheme mask and bunch numbers\n    if \"mask_with_filling_pattern\" in self.config_beambeam and (\n        \"pattern_fname\" in self.config_beambeam[\"mask_with_filling_pattern\"]\n        and self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"] is not None\n    ):\n        fname = self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"]\n        with open(fname, \"r\") as fid:\n            filling = json.load(fid)\n        filling_pattern_cw = filling[\"beam1\"]\n        filling_pattern_acw = filling[\"beam2\"]\n\n        # Initialize bunch numbers with empty values\n        i_bunch_cw = None\n        i_bunch_acw = None\n\n        # Only track bunch number if a filling pattern has been provided\n        if \"i_bunch_b1\" in self.config_beambeam[\"mask_with_filling_pattern\"]:\n            i_bunch_cw = self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"]\n        if \"i_bunch_b2\" in self.config_beambeam[\"mask_with_filling_pattern\"]:\n            i_bunch_acw = self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b2\"]\n\n        # Note that a bunch number must be provided if a filling pattern is provided\n        # Apply filling pattern\n        collider.apply_filling_pattern(\n            filling_pattern_cw=filling_pattern_cw,\n            filling_pattern_acw=filling_pattern_acw,\n            i_bunch_cw=i_bunch_cw,\n            i_bunch_acw=i_bunch_acw,\n        )\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.install_beam_beam_wrapper","title":"<code>install_beam_beam_wrapper(collider)</code>","text":"<p>This method installs beam-beam interactions in the collider with the specified parameters. The beam-beam lenses are initially inactive and not configured.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object where the beam-beam interactions will be installed.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def install_beam_beam_wrapper(self, collider: xt.Multiline) -&gt; None:\n    \"\"\"\n    This method installs beam-beam interactions in the collider with the specified\n    parameters. The beam-beam lenses are initially inactive and not configured.\n\n    Args:\n        collider (xt.Multiline): The collider object where the beam-beam interactions\n            will be installed.\n\n    Returns:\n        None\n    \"\"\"\n    # Install beam-beam lenses (inactive and not configured)\n    collider.install_beambeam_interactions(\n        clockwise_line=\"lhcb1\",\n        anticlockwise_line=\"lhcb2\",\n        ip_names=[\"ip1\", \"ip2\", \"ip5\", \"ip8\"],\n        delay_at_ips_slots=[0, 891, 0, 2670],\n        num_long_range_encounters_per_side=self.config_beambeam[\n            \"num_long_range_encounters_per_side\"\n        ],\n        num_slices_head_on=self.config_beambeam[\"num_slices_head_on\"],\n        harmonic_number=35640,\n        bunch_spacing_buckets=self.config_beambeam[\"bunch_spacing_buckets\"],\n        sigmaz=self.config_beambeam[\"sigma_z\"],\n    )\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.level_all_by_separation","title":"<code>level_all_by_separation(n_collisions_ip1_and_5, n_collisions_ip2, n_collisions_ip8, collider)</code>","text":"<p>This method updates the number of colliding bunches for IP1, IP2, IP5, and IP8 in the configuration file and performs luminosity leveling using the provided collider object. It also updates the separation knobs for the collider based on the new configuration.</p> <p>Parameters:</p> Name Type Description Default <code>n_collisions_ip1_and_5</code> <code>int</code> <p>Number of collisions at interaction points 1 and 5.</p> required <code>n_collisions_ip2</code> <code>int</code> <p>Number of collisions at interaction point 2.</p> required <code>n_collisions_ip8</code> <code>int</code> <p>Number of collisions at interaction point 8.</p> required <code>collider</code> <code>Multiline</code> <p>The collider object to be used for luminosity leveling.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def level_all_by_separation(\n    self,\n    n_collisions_ip1_and_5: int,\n    n_collisions_ip2: int,\n    n_collisions_ip8: int,\n    collider: xt.Multiline,\n) -&gt; None:\n    \"\"\"\n    This method updates the number of colliding bunches for IP1, IP2, IP5, and IP8 in the\n    configuration file and performs luminosity leveling using the provided collider object.\n    It also updates the separation knobs for the collider based on the new configuration.\n\n    Args:\n        n_collisions_ip1_and_5 (int): Number of collisions at interaction points 1 and 5.\n        n_collisions_ip2 (int): Number of collisions at interaction point 2.\n        n_collisions_ip8 (int): Number of collisions at interaction point 8.\n        collider (xt.Multiline): The collider object to be used for luminosity leveling.\n\n    Returns:\n        None\n    \"\"\"\n    # Update the number of bunches in the configuration file\n    l_n_collisions = [\n        n_collisions_ip1_and_5,\n        n_collisions_ip2,\n        n_collisions_ip1_and_5,\n        n_collisions_ip8,\n    ]\n    for ip, n_collisions in zip([\"ip1\", \"ip2\", \"ip5\", \"ip8\"], l_n_collisions):\n        if ip in self.config_lumi_leveling:\n            self.config_lumi_leveling[ip][\"num_colliding_bunches\"] = n_collisions\n        else:\n            logging.warning(f\"IP {ip} is not in the configuration\")\n\n    # ! Crabs are not handled in the following function\n    xm.lhc.luminosity_leveling(  # type: ignore\n        collider,\n        config_lumi_leveling=self.config_lumi_leveling,\n        config_beambeam=self.config_beambeam,\n    )\n\n    # Update configuration\n    if \"ip1\" in self.config_lumi_leveling:\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip1\"], \"on_sep1\")\n    if \"ip2\" in self.config_lumi_leveling:\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2\")\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2h\")\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2v\")\n    if \"ip5\" in self.config_lumi_leveling:\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip5\"], \"on_sep5\")\n    if \"ip8\" in self.config_lumi_leveling:\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8\")\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8h\")\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8v\")\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.level_ip1_5_by_bunch_intensity","title":"<code>level_ip1_5_by_bunch_intensity(collider, n_collisions_ip1_and_5)</code>","text":"<p>This method modifies the bunch intensity to achieve the desired luminosity levels in IP 1 and 5. It updates the configuration with the new intensity values.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object containing the beam and lattice configuration.</p> required <code>n_collisions_ip1_and_5</code> <code>int</code> <p>The number of collisions in IP 1 and 5.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def level_ip1_5_by_bunch_intensity(\n    self,\n    collider: xt.Multiline,\n    n_collisions_ip1_and_5: int,\n) -&gt; None:\n    \"\"\"\n    This method modifies the bunch intensity to achieve the desired luminosity\n    levels in IP 1 and 5. It updates the configuration with the new intensity values.\n\n    Args:\n        collider (xt.Multiline): The collider object containing the beam and lattice\n            configuration.\n        n_collisions_ip1_and_5 (int):\n            The number of collisions in IP 1 and 5.\n\n    Returns:\n        None\n    \"\"\"\n    # Initial intensity\n    bunch_intensity = self.config_beambeam[\"num_particles_per_bunch\"]\n\n    # First level luminosity in IP 1/5 changing the intensity\n    if (\n        self.config_lumi_leveling_ip1_5 is not None\n        and not self.config_lumi_leveling_ip1_5[\"skip_leveling\"]\n    ):\n        logging.info(\"Leveling luminosity in IP 1/5 varying the intensity\")\n        # Update the number of bunches in the configuration file\n        self.config_lumi_leveling_ip1_5[\"num_colliding_bunches\"] = n_collisions_ip1_and_5\n\n        # Do the levelling\n        bunch_intensity = luminosity_leveling_ip1_5(\n            collider,\n            self.config_lumi_leveling_ip1_5,\n            self.config_beambeam,\n            crab=self.crab,\n            cross_section=self.config_beambeam[\"cross_section\"],\n        )\n\n    # Update the configuration\n    self.config_beambeam[\"final_num_particles_per_bunch\"] = float(bunch_intensity)\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.level_ip2_8_by_separation","title":"<code>level_ip2_8_by_separation(n_collisions_ip2, n_collisions_ip8, collider)</code>","text":"<p>This method updates the number of colliding bunches for IP2 and IP8 in the configuration file, performs luminosity leveling for the specified collider, and updates the separation knobs for both interaction points.</p> <p>Parameters:</p> Name Type Description Default <code>n_collisions_ip2</code> <code>int</code> <p>The number of collisions at interaction point 2 (IP2).</p> required <code>n_collisions_ip8</code> <code>int</code> <p>The number of collisions at interaction point 8 (IP8).</p> required <code>collider</code> <code>Multiline</code> <p>The collider object for which the luminosity leveling is to be performed.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def level_ip2_8_by_separation(\n    self,\n    n_collisions_ip2: int,\n    n_collisions_ip8: int,\n    collider: xt.Multiline,\n) -&gt; None:\n    \"\"\"\n    This method updates the number of colliding bunches for IP2 and IP8 in the configuration\n    file, performs luminosity leveling for the specified collider, and updates the separation\n    knobs for both interaction points.\n\n    Args:\n        n_collisions_ip2 (int): The number of collisions at interaction point 2 (IP2).\n        n_collisions_ip8 (int): The number of collisions at interaction point 8 (IP8).\n        collider (xt.Multiline): The collider object for which the luminosity leveling is to be\n            performed.\n\n    Returns:\n        None\n    \"\"\"\n    # Update the number of bunches in the configuration file\n    if \"ip2\" in self.config_lumi_leveling:\n        self.config_lumi_leveling[\"ip2\"][\"num_colliding_bunches\"] = n_collisions_ip2\n    if \"ip8\" in self.config_lumi_leveling:\n        self.config_lumi_leveling[\"ip8\"][\"num_colliding_bunches\"] = n_collisions_ip8\n\n    # Ensure the the num particles per bunch corresponds to the final one\n    temp_num_particles_per_bunch = self.config_beambeam[\"num_particles_per_bunch\"]\n    if \"final_num_particles_per_bunch\" in self.config_beambeam:\n        self.config_beambeam[\"num_particles_per_bunch\"] = self.config_beambeam[\n            \"final_num_particles_per_bunch\"\n        ]\n    # Do levelling in IP2 and IP8\n    xm.lhc.luminosity_leveling(  # type: ignore\n        collider,\n        config_lumi_leveling=self.config_lumi_leveling,\n        config_beambeam=self.config_beambeam,\n    )\n\n    # Update configuration\n    if \"ip2\" in self.config_lumi_leveling:\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2\")\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2h\")\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2v\")\n    if \"ip8\" in self.config_lumi_leveling:\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8\")\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8h\")\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8v\")\n\n    # Set back the num particles per bunch to its initial value\n    self.config_beambeam[\"num_particles_per_bunch\"] = temp_num_particles_per_bunch\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.load_collider","title":"<code>load_collider()</code>","text":"<p>Load a collider configuration from a file.</p> <p>If the file path ends with \".zip\", the file is uncompressed locally and the collider configuration is loaded from the uncompressed file. Otherwise, the collider configuration is loaded directly from the file.</p> <p>Returns:</p> Type Description <code>Multiline</code> <p>xt.Multiline: The loaded collider configuration.</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def load_collider(self) -&gt; xt.Multiline:\n    \"\"\"\n    Load a collider configuration from a file.\n\n    If the file path ends with \".zip\", the file is uncompressed locally\n    and the collider configuration is loaded from the uncompressed file.\n    Otherwise, the collider configuration is loaded directly from the file.\n\n    Returns:\n        xt.Multiline: The loaded collider configuration.\n    \"\"\"\n    return self._load_collider(self.path_collider_file_for_configuration_as_input)\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.match_tune_and_chroma","title":"<code>match_tune_and_chroma(collider, match_linear_coupling_to_zero=True)</code>","text":"<p>This method adjusts the tune and chromaticity of the specified collider lines (\"lhcb1\" and \"lhcb2\") to the target values defined in the configuration. It also optionally matches the linear coupling to zero.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object containing the lines to be tuned.</p> required <code>match_linear_coupling_to_zero</code> <code>bool</code> <p>If True, linear coupling will be matched to zero. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def match_tune_and_chroma(\n    self, collider: xt.Multiline, match_linear_coupling_to_zero: bool = True\n) -&gt; None:\n    \"\"\"\n    This method adjusts the tune and chromaticity of the specified collider lines\n    (\"lhcb1\" and \"lhcb2\") to the target values defined in the configuration. It also\n    optionally matches the linear coupling to zero.\n\n    Args:\n        collider (xt.Multiline): The collider object containing the lines to be tuned.\n        match_linear_coupling_to_zero (bool, optional): If True, linear coupling will be\n            matched to zero. Defaults to True.\n\n    Returns:\n        None\n    \"\"\"\n    for line_name in [\"lhcb1\", \"lhcb2\"]:\n        knob_names = self.config_knobs_and_tuning[\"knob_names\"][line_name]\n\n        targets = {\n            \"qx\": self.config_knobs_and_tuning[\"qx\"][line_name],\n            \"qy\": self.config_knobs_and_tuning[\"qy\"][line_name],\n            \"dqx\": self.config_knobs_and_tuning[\"dqx\"][line_name],\n            \"dqy\": self.config_knobs_and_tuning[\"dqy\"][line_name],\n        }\n\n        xm.machine_tuning(\n            line=collider[line_name],\n            enable_closed_orbit_correction=True,\n            enable_linear_coupling_correction=match_linear_coupling_to_zero,\n            enable_tune_correction=True,\n            enable_chromaticity_correction=True,\n            knob_names=knob_names,\n            targets=targets,\n            line_co_ref=collider[f\"{line_name}_co_ref\"],\n            co_corr_config=self.dict_orbit_correction[line_name],\n        )\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.record_beta_functions","title":"<code>record_beta_functions(collider)</code>","text":"<p>Records the beta functions at the IPs in the collider.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object to record the beta functions.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def record_beta_functions(self, collider: xt.Multiline) -&gt; None:\n    \"\"\"\n    Records the beta functions at the IPs in the collider.\n\n    Args:\n        collider (xt.Multiline): The collider object to record the beta functions.\n\n    Returns:\n        None\n    \"\"\"\n    # Record beta functions at the IPs\n    for ip in [\"ip1\", \"ip2\", \"ip5\", \"ip8\"]:\n        tw = collider.lhcb1.twiss()\n        self.config_collider[f\"beta_x_{ip}\"] = float(np.round(float(tw[\"betx\", ip]), 5))\n        self.config_collider[f\"beta_y_{ip}\"] = float(np.round(float(tw[\"bety\", ip]), 5))\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.record_final_luminosity","title":"<code>record_final_luminosity(collider, l_n_collisions)</code>","text":"<p>Records the final luminosity and pile-up for specified interaction points (IPs) in the collider, both with and without beam-beam effects.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <p>(xt.Multiline): The collider object configured.</p> required <code>l_n_collisions</code> <code>list[int]</code> <p>A list containing the number of colliding bunches for each IP.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def record_final_luminosity(self, collider: xt.Multiline, l_n_collisions: list[int]) -&gt; None:\n    \"\"\"\n    Records the final luminosity and pile-up for specified interaction points (IPs)\n    in the collider, both with and without beam-beam effects.\n\n    Args:\n        collider : (xt.Multiline): The collider object configured.\n        l_n_collisions (list[int]): A list containing the number of colliding bunches for each\n            IP.\n\n    Returns:\n        None\n    \"\"\"\n    # Define IPs in which the luminosity will be computed\n    l_ip = [\"ip1\", \"ip2\", \"ip5\", \"ip8\"]\n\n    # Ensure that the final number of particles per bunch is defined, even\n    # if the leveling has been done by separation\n    if \"final_num_particles_per_bunch\" not in self.config_beambeam:\n        self.config_beambeam[\"final_num_particles_per_bunch\"] = self.config_beambeam[\n            \"num_particles_per_bunch\"\n        ]\n\n    def _twiss_and_compute_lumi(collider, l_n_collisions):\n        # Loop over each IP and record the luminosity\n        twiss_b1 = collider[\"lhcb1\"].twiss()\n        twiss_b2 = collider[\"lhcb2\"].twiss()\n        l_lumi = []\n        l_PU = []\n        for n_col, ip in zip(l_n_collisions, l_ip):\n            L = xt.lumi.luminosity_from_twiss(  # type: ignore\n                n_colliding_bunches=n_col,\n                num_particles_per_bunch=self.config_beambeam[\"final_num_particles_per_bunch\"],\n                ip_name=ip,\n                nemitt_x=self.config_beambeam[\"nemitt_x\"],\n                nemitt_y=self.config_beambeam[\"nemitt_y\"],\n                sigma_z=self.config_beambeam[\"sigma_z\"],\n                twiss_b1=twiss_b1,\n                twiss_b2=twiss_b2,\n                crab=self.crab,\n            )\n            PU = compute_PU(\n                L,\n                n_col,\n                twiss_b1[\"T_rev0\"],\n                cross_section=self.config_beambeam[\"cross_section\"],\n            )\n\n            l_lumi.append(L)\n            l_PU.append(PU)\n\n        return l_lumi, l_PU\n\n    # Get the final luminosity in all IPs, without beam-beam\n    collider.vars[\"beambeam_scale\"] = 0\n    l_lumi, l_PU = _twiss_and_compute_lumi(collider, l_n_collisions)\n\n    # Update configuration\n    for ip, L, PU in zip(l_ip, l_lumi, l_PU):\n        self.config_beambeam[f\"luminosity_{ip}_without_beam_beam\"] = float(L)\n        self.config_beambeam[f\"Pile-up_{ip}_without_beam_beam\"] = float(PU)\n\n    # Get the final luminosity in all IPs, with beam-beam\n    collider.vars[\"beambeam_scale\"] = 1\n    l_lumi, l_PU = _twiss_and_compute_lumi(collider, l_n_collisions)\n\n    # Update configuration\n    for ip, L, PU in zip(l_ip, l_lumi, l_PU):\n        self.config_beambeam[f\"luminosity_{ip}_with_beam_beam\"] = float(L)\n        self.config_beambeam[f\"Pile-up_{ip}_with_beam_beam\"] = float(PU)\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.return_fingerprint","title":"<code>return_fingerprint(collider, line_name='lhcb1')</code>  <code>staticmethod</code>","text":"<p>Generate a detailed fingerprint of the specified collider line. Useful to compare two colliders.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object containing the line data.</p> required <code>line_name</code> <code>str</code> <p>The name of the line to analyze within the collider. Default to \"lhcb1\".</p> <code>'lhcb1'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A formatted string containing detailed information about the collider line, including: - Installed element types - Tunes and chromaticity - Synchrotron tune and slip factor - Twiss parameters and phases at interaction points (IPs) - Dispersion and crab dispersion at IPs - Amplitude detuning coefficients - Non-linear chromaticity - Tunes and momentum compaction vs delta</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>@staticmethod\ndef return_fingerprint(collider, line_name=\"lhcb1\") -&gt; str:\n    \"\"\"\n    Generate a detailed fingerprint of the specified collider line. Useful to compare two\n    colliders.\n\n    Args:\n        collider (xt.Multiline): The collider object containing the line data.\n        line_name (str): The name of the line to analyze within the collider. Default to \"lhcb1\".\n\n    Returns:\n        str:\n            A formatted string containing detailed information about the collider line, including:\n            - Installed element types\n            - Tunes and chromaticity\n            - Synchrotron tune and slip factor\n            - Twiss parameters and phases at interaction points (IPs)\n            - Dispersion and crab dispersion at IPs\n            - Amplitude detuning coefficients\n            - Non-linear chromaticity\n            - Tunes and momentum compaction vs delta\n    \"\"\"\n    line = collider[line_name]\n\n    tw = line.twiss()\n    tt = line.get_table()\n\n    det = line.get_amplitude_detuning_coefficients(a0_sigmas=0.1, a1_sigmas=0.2, a2_sigmas=0.3)\n\n    det_table = xt.Table(\n        {\n            \"name\": np.array(list(det.keys())),\n            \"value\": np.array(list(det.values())),\n        }\n    )\n\n    nl_chrom = line.get_non_linear_chromaticity(\n        delta0_range=(-2e-4, 2e-4), num_delta=5, fit_order=3\n    )\n\n    out = \"\"\n\n    out += f\"Line: {line_name}\\n\"\n    out += \"\\n\"\n\n    out += \"Installed element types:\\n\"\n    out += repr([nn for nn in sorted(list(set(tt.element_type))) if len(nn) &gt; 0]) + \"\\n\"\n    out += \"\\n\"\n\n    out += f'Tunes:        Qx  = {tw[\"qx\"]:.5f}       Qy = {tw[\"qy\"]:.5f}\\n'\n    out += f\"\"\"Chromaticity: Q'x = {tw[\"dqx\"]:.2f}     Q'y = \"\"\" + f'{tw[\"dqy\"]:.2f}\\n'\n    out += f'c_minus:      {tw[\"c_minus\"]:.5e}\\n'\n    out += \"\\n\"\n\n    out += f'Synchrotron tune: {tw[\"qs\"]:5e}\\n'\n    out += f'Slip factor:      {tw[\"slip_factor\"]:.5e}\\n'\n    out += \"\\n\"\n\n    out += \"Twiss parameters and phases at IPs:\\n\"\n    out += (\n        tw.rows[\"ip.*\"]\n        .cols[\"name s betx bety alfx alfy mux muy\"]\n        .show(output=str, max_col_width=int(1e6), digits=8)\n    )\n    out += \"\\n\\n\"\n\n    out += \"Dispersion at IPs:\\n\"\n    out += (\n        tw.rows[\"ip.*\"]\n        .cols[\"name s dx dy dpx dpy\"]\n        .show(output=str, max_col_width=int(1e6), digits=8)\n    )\n    out += \"\\n\\n\"\n\n    out += \"Crab dispersion at IPs:\\n\"\n    out += (\n        tw.rows[\"ip.*\"]\n        .cols[\"name s dx_zeta dy_zeta dpx_zeta dpy_zeta\"]\n        .show(output=str, max_col_width=int(1e6), digits=8)\n    )\n    out += \"\\n\\n\"\n\n    out += \"Amplitude detuning coefficients:\\n\"\n    out += det_table.show(output=str, max_col_width=int(1e6), digits=6)\n    out += \"\\n\\n\"\n\n    out += \"Non-linear chromaticity:\\n\"\n    out += f'dnqx = {list(nl_chrom[\"dnqx\"])}\\n'\n    out += f'dnqy = {list(nl_chrom[\"dnqy\"])}\\n'\n    out += \"\\n\\n\"\n\n    out += \"Tunes and momentum compaction vs delta:\\n\"\n    out += nl_chrom.show(output=str, max_col_width=int(1e6), digits=6)\n    out += \"\\n\\n\"\n\n    return out\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.set_filling_and_bunch_tracked","title":"<code>set_filling_and_bunch_tracked(ask_worst_bunch=False)</code>","text":"<p>Sets the filling scheme and determines the bunch to be tracked for beam-beam interactions.</p> <p>This method performs the following steps: 1. Retrieves the filling scheme path from the configuration. 2. Checks if the filling scheme path needs to be obtained from the template schemes. 3. Loads and verifies the filling scheme, potentially converting it if necessary. 4. Updates the configuration with the correct filling scheme path. 5. Determines the number of long-range encounters to consider. 6. If the bunch number for beam 1 is not provided, it identifies the bunch with the largest number of long-range interactions.    - If <code>ask_worst_bunch</code> is True, prompts the user to confirm or provide a bunch number.    - Otherwise, automatically selects the worst bunch. 7. If the bunch number for beam 2 is not provided, it automatically selects the worst bunch.</p> <p>Parameters:</p> Name Type Description Default <code>ask_worst_bunch</code> <code>bool</code> <p>If True, prompts the user to confirm or provide the bunch number for beam 1. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def set_filling_and_bunch_tracked(self, ask_worst_bunch: bool = False) -&gt; None:\n    \"\"\"\n    Sets the filling scheme and determines the bunch to be tracked for beam-beam interactions.\n\n    This method performs the following steps:\n    1. Retrieves the filling scheme path from the configuration.\n    2. Checks if the filling scheme path needs to be obtained from the template schemes.\n    3. Loads and verifies the filling scheme, potentially converting it if necessary.\n    4. Updates the configuration with the correct filling scheme path.\n    5. Determines the number of long-range encounters to consider.\n    6. If the bunch number for beam 1 is not provided, it identifies the bunch with the largest\n    number of long-range interactions.\n       - If `ask_worst_bunch` is True, prompts the user to confirm or provide a bunch number.\n       - Otherwise, automatically selects the worst bunch.\n    7. If the bunch number for beam 2 is not provided, it automatically selects the worst bunch.\n\n    Args:\n        ask_worst_bunch (bool): If True, prompts the user to confirm or provide the bunch number\n            for beam 1. Defaults to False.\n\n    Returns:\n        None\n    \"\"\"\n    # Get the filling scheme path\n    filling_scheme_path = self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"]\n\n    # Check if the filling scheme path must be obtained from the template schemes\n    scheme_folder = (\n        pathlib.Path(__file__).parent.parent.parent.resolve().joinpath(\"assets/filling_schemes\")\n    )\n    if filling_scheme_path in os.listdir(scheme_folder):\n        filling_scheme_path = str(scheme_folder.joinpath(filling_scheme_path))\n        self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"] = filling_scheme_path\n\n    # Load and check filling scheme, potentially convert it\n    filling_scheme_path = load_and_check_filling_scheme(filling_scheme_path)\n\n    # Correct filling scheme in config, as it might have been converted\n    self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"] = filling_scheme_path\n\n    # Get number of LR to consider\n    n_LR = self.config_beambeam[\"num_long_range_encounters_per_side\"][\"ip1\"]\n\n    # If the bunch number is None, the bunch with the largest number of long-range interactions is used\n    if self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] is None:\n        # Case the bunch number has not been provided\n        worst_bunch_b1 = get_worst_bunch(\n            filling_scheme_path, number_of_LR_to_consider=n_LR, beam=\"beam_1\"\n        )\n        if ask_worst_bunch:\n            while self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] is None:\n                bool_inp = input(\n                    \"The bunch number for beam 1 has not been provided. Do you want to use the\"\n                    \" bunch with the largest number of long-range interactions? It is the bunch\"\n                    \" number \" + str(worst_bunch_b1) + \" (y/n): \"\n                )\n                if bool_inp == \"y\":\n                    self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] = (\n                        worst_bunch_b1\n                    )\n                elif bool_inp == \"n\":\n                    self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] = int(\n                        input(\"Please enter the bunch number for beam 1: \")\n                    )\n        else:\n            self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] = worst_bunch_b1\n\n    if self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b2\"] is None:\n        worst_bunch_b2 = get_worst_bunch(\n            filling_scheme_path, number_of_LR_to_consider=n_LR, beam=\"beam_2\"\n        )\n        # For beam 2, just select the worst bunch by default\n        self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b2\"] = worst_bunch_b2\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.set_knobs","title":"<code>set_knobs(collider)</code>","text":"<p>Set all knobs for the collider, including crossing angles, dispersion correction, RF, crab cavities, experimental magnets, etc.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object to which the knob settings will be applied.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def set_knobs(self, collider: xt.Multiline) -&gt; None:\n    \"\"\"\n    Set all knobs for the collider, including crossing angles, dispersion correction,\n    RF, crab cavities, experimental magnets, etc.\n\n    Args:\n        collider (xt.Multiline): The collider object to which the knob settings will be applied.\n\n    Returns:\n        None\n    \"\"\"\n    # Set all knobs (crossing angles, dispersion correction, rf, crab cavities,\n    # experimental magnets, etc.)\n    for kk, vv in self.config_knobs_and_tuning[\"knob_settings\"].items():\n        collider.vars[kk] = vv\n\n    # Crab fix (if needed)\n    if self.ver_hllhc_optics is not None and self.ver_hllhc_optics == 1.3:\n        apply_crab_fix(collider, self.config_knobs_and_tuning)\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.update_configuration_knob","title":"<code>update_configuration_knob(collider, dictionnary, knob_name)</code>  <code>staticmethod</code>","text":"<p>Updates the given dictionary with the final value of a specified knob from the collider.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object containing various variables.</p> required <code>dictionnary</code> <code>dict</code> <p>The dictionary to be updated with the knob's final value.</p> required <code>knob_name</code> <code>str</code> <p>The name of the knob whose value is to be retrieved and stored.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>@staticmethod\ndef update_configuration_knob(\n    collider: xt.Multiline, dictionnary: dict, knob_name: str\n) -&gt; None:\n    \"\"\"\n    Updates the given dictionary with the final value of a specified knob from the collider.\n\n    Args:\n        collider (xt.Multiline): The collider object containing various variables.\n        dictionnary (dict): The dictionary to be updated with the knob's final value.\n        knob_name (str): The name of the knob whose value is to be retrieved and stored.\n\n    Returns:\n        None\n    \"\"\"\n    if knob_name in collider.vars.keys():\n        dictionnary[f\"final_{knob_name}\"] = float(collider.vars[knob_name]._value)\n    else:\n        logging.warning(f\"Knob {knob_name} not found in the collider\")\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.write_collider_to_disk","title":"<code>write_collider_to_disk(collider, full_configuration)</code>","text":"<p>Writes the collider object to disk in JSON format if the save_output_collider flag is set.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Collider</code> <p>The collider object to be saved.</p> required <code>full_configuration</code> <code>dict</code> <p>The full configuration dictionary to be deep-copied into the collider's metadata.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def write_collider_to_disk(self, collider, full_configuration) -&gt; None:\n    \"\"\"\n    Writes the collider object to disk in JSON format if the save_output_collider flag is set.\n\n    Args:\n        collider (Collider): The collider object to be saved.\n        full_configuration (dict): The full configuration dictionary to be deep-copied into the\n            collider's metadata.\n\n    Returns:\n        None\n    \"\"\"\n    if self.save_output_collider:\n        logging.info(\"Saving collider as json\")\n        if (\n            hasattr(collider, \"metadata\")\n            and collider.metadata is not None\n            and isinstance(collider.metadata, dict)\n        ):\n            collider.metadata.update(copy.deepcopy(full_configuration))\n        else:\n            collider.metadata = copy.deepcopy(full_configuration)\n        collider.to_json(self.path_collider_file_for_tracking_as_output)\n\n        # Compress the collider file to zip to ease the load on afs\n        if self.compress:\n            compress_and_write(self.path_collider_file_for_tracking_as_output)\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_leveling.html","title":"xsuite_leveling","text":"<p>This modules contains functions used for luminosity leveling.</p>"},{"location":"reference/study_da/generate/master_classes/xsuite_leveling.html#study_da.generate.master_classes.xsuite_leveling.compute_PU","title":"<code>compute_PU(luminosity, num_colliding_bunches, T_rev0, cross_section)</code>","text":"<p>Compute the Pile-Up (PU) value.</p> <p>Parameters:</p> Name Type Description Default <code>luminosity</code> <code>float</code> <p>The luminosity of the collider.</p> required <code>num_colliding_bunches</code> <code>int</code> <p>The number of colliding bunches.</p> required <code>T_rev0</code> <code>float</code> <p>The revolution time of the collider.</p> required <code>cross_section</code> <code>float</code> <p>The cross-section value.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The computed Pile-Up (PU) value.</p> Source code in <code>study_da/generate/master_classes/xsuite_leveling.py</code> <pre><code>def compute_PU(\n    luminosity: float, num_colliding_bunches: int, T_rev0: float, cross_section: float\n) -&gt; float:\n    \"\"\"\n    Compute the Pile-Up (PU) value.\n\n    Args:\n        luminosity (float): The luminosity of the collider.\n        num_colliding_bunches (int): The number of colliding bunches.\n        T_rev0 (float): The revolution time of the collider.\n        cross_section (float): The cross-section value.\n\n    Returns:\n        float: The computed Pile-Up (PU) value.\n    \"\"\"\n    return luminosity / num_colliding_bunches * cross_section * T_rev0\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_leveling.html#study_da.generate.master_classes.xsuite_leveling.luminosity_leveling_ip1_5","title":"<code>luminosity_leveling_ip1_5(collider, config_lumi_leveling_ip1_5, config_beambeam, crab=False, cross_section=8.1e-26)</code>","text":"<p>Perform luminosity leveling for interaction points IP1 and IP5.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>dict</code> <p>Dictionary containing collider objects for beams 'lhcb1' and 'lhcb2'.</p> required <code>config_lumi_leveling_ip1_5</code> <code>dict</code> <p>Configuration dictionary for luminosity leveling at IP1 and IP5. Must contain 'num_colliding_bunches' and 'constraints' with 'max_intensity' and 'max_PU'.</p> required <code>config_beambeam</code> <code>dict</code> <p>Configuration dictionary for beam-beam parameters. Must contain 'nemitt_x', 'nemitt_y', and 'sigma_z'.</p> required <code>crab</code> <code>bool</code> <p>Flag to indicate if crab cavities are used. Default to False.</p> <code>False</code> <code>cross_section</code> <code>float</code> <p>Cross-section value in square meters. Default to 81e-27.</p> <code>8.1e-26</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Optimized bunch intensity for leveling in IP1 and IP5.</p> <p>Raises:</p> Type Description <code>Warning</code> <p>If the optimization for leveling in IP1/5 fails, a warning is logged.</p> Source code in <code>study_da/generate/master_classes/xsuite_leveling.py</code> <pre><code>def luminosity_leveling_ip1_5(\n    collider: xt.Multiline,\n    config_lumi_leveling_ip1_5: dict[str, Any],\n    config_beambeam: dict[str, Any],\n    crab: bool = False,\n    cross_section: float = 81e-27,\n) -&gt; float:\n    \"\"\"\n    Perform luminosity leveling for interaction points IP1 and IP5.\n\n    Args:\n        collider (dict): Dictionary containing collider objects for beams 'lhcb1' and 'lhcb2'.\n        config_lumi_leveling_ip1_5 (dict): Configuration dictionary for luminosity leveling at IP1\n            and IP5. Must contain 'num_colliding_bunches' and 'constraints' with 'max_intensity'\n            and 'max_PU'.\n        config_beambeam (dict): Configuration dictionary for beam-beam parameters. Must contain\n            'nemitt_x', 'nemitt_y', and 'sigma_z'.\n        crab (bool): Flag to indicate if crab cavities are used. Default to False.\n        cross_section (float): Cross-section value in square meters. Default to 81e-27.\n\n    Returns:\n        float: Optimized bunch intensity for leveling in IP1 and IP5.\n\n    Raises:\n        Warning: If the optimization for leveling in IP1/5 fails, a warning is logged.\n    \"\"\"\n    # Get Twiss\n    twiss_b1 = collider[\"lhcb1\"].twiss()\n    twiss_b2 = collider[\"lhcb2\"].twiss()\n\n    # Get the number of colliding bunches in IP1/5\n    n_colliding_IP1_5 = config_lumi_leveling_ip1_5[\"num_colliding_bunches\"]\n\n    # Get max intensity in IP1/5\n    max_intensity_IP1_5 = float(config_lumi_leveling_ip1_5[\"constraints\"][\"max_intensity\"])\n\n    def _compute_lumi(bunch_intensity):\n        luminosity = xt.lumi.luminosity_from_twiss(  # type: ignore\n            n_colliding_bunches=n_colliding_IP1_5,\n            num_particles_per_bunch=bunch_intensity,\n            ip_name=\"ip1\",\n            nemitt_x=config_beambeam[\"nemitt_x\"],\n            nemitt_y=config_beambeam[\"nemitt_y\"],\n            sigma_z=config_beambeam[\"sigma_z\"],\n            twiss_b1=twiss_b1,\n            twiss_b2=twiss_b2,\n            crab=crab,\n        )\n        return luminosity\n\n    def f(bunch_intensity):\n        luminosity = _compute_lumi(bunch_intensity)\n\n        max_PU_IP_1_5 = config_lumi_leveling_ip1_5[\"constraints\"][\"max_PU\"]\n\n        target_luminosity_IP_1_5 = config_lumi_leveling_ip1_5[\"luminosity\"]\n        PU = compute_PU(\n            luminosity,\n            n_colliding_IP1_5,\n            twiss_b1[\"T_rev0\"],\n            cross_section,\n        )\n\n        penalty_PU = max(0, (PU - max_PU_IP_1_5) * 1e35)  # in units of 1e-35\n        penalty_excess_lumi = max(\n            0, (luminosity - target_luminosity_IP_1_5) * 10\n        )  # in units of 1e-35 if luminosity is in units of 1e34\n\n        return abs(luminosity - target_luminosity_IP_1_5) + penalty_PU + penalty_excess_lumi\n\n    # Do the optimization\n    res = minimize_scalar(\n        f,\n        bounds=(\n            1e10,\n            max_intensity_IP1_5,\n        ),\n        method=\"bounded\",\n        options={\"xatol\": 1e7},\n    )\n    if not res.success:  # type: ignore\n        logging.warning(\"Optimization for leveling in IP 1/5 failed. Please check the constraints.\")\n    else:\n        logging.info(\n            f\"Optimization for leveling in IP 1/5 succeeded with I={res.x:.2e} particles per bunch\"  # type: ignore\n        )\n    return res.x  # type: ignore\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_tracking.html","title":"xsuite_tracking","text":"<p>This class is used to build a Xsuite collider from a madx sequence and optics.</p>"},{"location":"reference/study_da/generate/master_classes/xsuite_tracking.html#study_da.generate.master_classes.xsuite_tracking.XsuiteTracking","title":"<code>XsuiteTracking</code>","text":"<p>XsuiteTracking class for managing particle tracking simulations.</p> <p>Attributes:</p> Name Type Description <code>context_str</code> <code>str</code> <p>The context for the simulation (e.g., \"cupy\", \"opencl\", \"cpu\").</p> <code>device_number</code> <code>int</code> <p>The device number for GPU contexts.</p> <code>_context</code> <code>Context</code> <p>The context object for the simulation.</p> <code>beam</code> <code>str</code> <p>The beam configuration.</p> <code>distribution_file</code> <code>str</code> <p>The file path to the particle data.</p> <code>delta_max</code> <code>float</code> <p>The maximum delta value for particles.</p> <code>n_turns</code> <code>int</code> <p>The number of turns for the simulation.</p> <code>nemitt_x</code> <code>float</code> <p>The normalized emittance in the x direction.</p> <code>nemitt_y</code> <code>float</code> <p>The normalized emittance in the y direction.</p> <p>Methods:</p> Name Description <code>context</code> <p>Get the context object for the simulation.</p> <code>prepare_particle_distribution_for_tracking</code> <p>Prepare the particle distribution for tracking.</p> <code>track</code> <p>Track the particles in the collider.</p> Source code in <code>study_da/generate/master_classes/xsuite_tracking.py</code> <pre><code>class XsuiteTracking:\n    \"\"\"\n    XsuiteTracking class for managing particle tracking simulations.\n\n    Attributes:\n        context_str (str): The context for the simulation (e.g., \"cupy\", \"opencl\", \"cpu\").\n        device_number (int): The device number for GPU contexts.\n        _context (xo.Context): The context object for the simulation.\n        beam (str): The beam configuration.\n        distribution_file (str): The file path to the particle data.\n        delta_max (float): The maximum delta value for particles.\n        n_turns (int): The number of turns for the simulation.\n        nemitt_x (float): The normalized emittance in the x direction.\n        nemitt_y (float): The normalized emittance in the y direction.\n\n    Methods:\n        context: Get the context object for the simulation.\n        prepare_particle_distribution_for_tracking: Prepare the particle distribution for tracking.\n        track: Track the particles in the collider.\n    \"\"\"\n\n    def __init__(self, configuration: dict, nemitt_x: float, nemitt_y: float) -&gt; None:\n        \"\"\"\n        Initialize the tracking configuration.\n\n        Args:\n            configuration (dict): A dictionary containing the configuration parameters.\n                Expected keys:\n                - \"context\": str, context string for the simulation.\n                - \"device_number\": int, device number for the simulation.\n                - \"beam\": str, beam type for the simulation.\n                - \"distribution_file\": str, path to the particle file.\n                - \"delta_max\": float, maximum delta value for the simulation.\n                - \"n_turns\": int, number of turns for the simulation.\n            nemitt_x (float): Normalized emittance in the x-plane.\n            nemitt_y (float): Normalized emittance in the y-plane.\n        \"\"\"\n        # Context parameters\n        self.context_str: str = configuration[\"context\"]\n        self.device_number: int = configuration[\"device_number\"]\n        self._context = None\n\n        # Simulation parameters\n        self.beam: str = configuration[\"beam\"]\n        self.distribution_file: str = configuration[\"distribution_file\"]\n        self.path_distribution_folder_input: str = configuration[\"path_distribution_folder_input\"]\n        self.particle_path: str = f\"{self.path_distribution_folder_input}/{self.distribution_file}\"\n        self.delta_max: float = configuration[\"delta_max\"]\n        self.n_turns: int = configuration[\"n_turns\"]\n\n        # Beambeam parameters\n        self.nemitt_x: float = nemitt_x\n        self.nemitt_y: float = nemitt_y\n\n    @property\n    def context(self) -&gt; Any:\n        \"\"\"\n        Returns the context for the current instance. If the context is not already set,\n        it initializes the context based on the `context_str` attribute. The context can\n        be one of the following:\n\n        - \"cupy\": Uses `xo.ContextCupy`. If `device_number` is specified, it initializes\n            the context with the given device number.\n        - \"opencl\": Uses `xo.ContextPyopencl`.\n        - \"cpu\": Uses `xo.ContextCpu`.\n        - Any other value: Logs a warning and defaults to `xo.ContextCpu`.\n\n        If `device_number` is specified but the context is not \"cupy\", a warning is logged\n        indicating that the device number will be ignored.\n\n        Returns:\n            Any: The initialized context.\n        \"\"\"\n        if self._context is None:\n            if self.device_number is not None and self.context_str not in [\"cupy\"]:\n                logging.warning(\"Device number will be ignored since context is not cupy\")\n            match self.context_str:\n                case \"cupy\":\n                    if self.device_number is not None:\n                        self._context = xo.ContextCupy(device=self.device_number)\n                    else:\n                        self._context = xo.ContextCupy()\n                case \"opencl\":\n                    self._context = xo.ContextPyopencl()\n                case \"cpu\":\n                    self._context = xo.ContextCpu()\n                case _:\n                    logging.warning(\"Context not recognized, using cpu\")\n                    self._context = xo.ContextCpu()\n        return self._context\n\n    # ? I removed type hints for the output as I get an unclear linting error\n    # TODO: Check the proper type hints for the output\n    def prepare_particle_distribution_for_tracking(self, collider: xt.Multiline) -&gt; tuple:\n        \"\"\"\n        Prepare a particle distribution for tracking in the collider.\n\n        This method reads particle data from a parquet file, processes the data to\n        generate normalized amplitudes and angles, and then builds particles for\n        tracking in the collider. If the context is set to use GPU, the collider\n        trackers are reset and rebuilt accordingly.\n\n        Args:\n            collider (xt.Multiline): The collider object containing the beam and\n                tracking information.\n\n        Returns:\n            tuple: A tuple containing:\n                - xp.Particles: The particles ready for tracking.\n                - np.ndarray: Array of particle IDs.\n                - np.ndarray: Array of normalized amplitudes in the xy-plane.\n                - np.ndarray: Array of angles in the xy-plane in radians.\n        \"\"\"\n        # Reset the tracker to go to GPU if needed\n        if self.context_str in [\"cupy\", \"opencl\"]:\n            collider.discard_trackers()\n            collider.build_trackers(_context=self.context)\n\n        particle_df = pd.read_parquet(self.particle_path)\n\n        r_vect = particle_df[\"normalized amplitude in xy-plane\"].values\n        theta_vect = particle_df[\"angle in xy-plane [deg]\"].values * np.pi / 180  # type: ignore # [rad]\n\n        A1_in_sigma = r_vect * np.cos(theta_vect)\n        A2_in_sigma = r_vect * np.sin(theta_vect)\n\n        particles = collider[self.beam].build_particles(\n            x_norm=A1_in_sigma,\n            y_norm=A2_in_sigma,\n            delta=self.delta_max,\n            scale_with_transverse_norm_emitt=(\n                self.nemitt_x,\n                self.nemitt_y,\n            ),\n            _context=self.context,\n        )\n\n        particle_id = particle_df.particle_id.values\n        return particles, particle_id, r_vect, theta_vect\n\n    def track(self, collider: xt.Multiline, particles: xp.Particles) -&gt; dict:\n        \"\"\"\n        Tracks particles through a collider for a specified number of turns and logs the elapsed time.\n\n        Args:\n            collider (xt.Multiline): The collider object containing the beamline to be tracked.\n            particles (xp.Particles): The particles to be tracked.\n\n        Returns:\n            dict: A dictionary representation of the tracked particles.\n        \"\"\"\n        # Optimize line for tracking\n        collider[self.beam].optimize_for_tracking()\n\n        # Track\n        num_turns = self.n_turns\n        a = time.time()\n        collider[self.beam].track(particles, turn_by_turn_monitor=False, num_turns=num_turns)\n        b = time.time()\n\n        logging.info(f\"Elapsed time: {b-a} s\")\n        logging.info(\n            f\"Elapsed time per particle per turn: {(b-a)/particles._capacity/num_turns*1e6} us\"\n        )\n\n        return particles.to_dict()\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_tracking.html#study_da.generate.master_classes.xsuite_tracking.XsuiteTracking.context","title":"<code>context: Any</code>  <code>property</code>","text":"<p>Returns the context for the current instance. If the context is not already set, it initializes the context based on the <code>context_str</code> attribute. The context can be one of the following:</p> <ul> <li>\"cupy\": Uses <code>xo.ContextCupy</code>. If <code>device_number</code> is specified, it initializes     the context with the given device number.</li> <li>\"opencl\": Uses <code>xo.ContextPyopencl</code>.</li> <li>\"cpu\": Uses <code>xo.ContextCpu</code>.</li> <li>Any other value: Logs a warning and defaults to <code>xo.ContextCpu</code>.</li> </ul> <p>If <code>device_number</code> is specified but the context is not \"cupy\", a warning is logged indicating that the device number will be ignored.</p> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The initialized context.</p>"},{"location":"reference/study_da/generate/master_classes/xsuite_tracking.html#study_da.generate.master_classes.xsuite_tracking.XsuiteTracking.__init__","title":"<code>__init__(configuration, nemitt_x, nemitt_y)</code>","text":"<p>Initialize the tracking configuration.</p> <p>Parameters:</p> Name Type Description Default <code>configuration</code> <code>dict</code> <p>A dictionary containing the configuration parameters. Expected keys: - \"context\": str, context string for the simulation. - \"device_number\": int, device number for the simulation. - \"beam\": str, beam type for the simulation. - \"distribution_file\": str, path to the particle file. - \"delta_max\": float, maximum delta value for the simulation. - \"n_turns\": int, number of turns for the simulation.</p> required <code>nemitt_x</code> <code>float</code> <p>Normalized emittance in the x-plane.</p> required <code>nemitt_y</code> <code>float</code> <p>Normalized emittance in the y-plane.</p> required Source code in <code>study_da/generate/master_classes/xsuite_tracking.py</code> <pre><code>def __init__(self, configuration: dict, nemitt_x: float, nemitt_y: float) -&gt; None:\n    \"\"\"\n    Initialize the tracking configuration.\n\n    Args:\n        configuration (dict): A dictionary containing the configuration parameters.\n            Expected keys:\n            - \"context\": str, context string for the simulation.\n            - \"device_number\": int, device number for the simulation.\n            - \"beam\": str, beam type for the simulation.\n            - \"distribution_file\": str, path to the particle file.\n            - \"delta_max\": float, maximum delta value for the simulation.\n            - \"n_turns\": int, number of turns for the simulation.\n        nemitt_x (float): Normalized emittance in the x-plane.\n        nemitt_y (float): Normalized emittance in the y-plane.\n    \"\"\"\n    # Context parameters\n    self.context_str: str = configuration[\"context\"]\n    self.device_number: int = configuration[\"device_number\"]\n    self._context = None\n\n    # Simulation parameters\n    self.beam: str = configuration[\"beam\"]\n    self.distribution_file: str = configuration[\"distribution_file\"]\n    self.path_distribution_folder_input: str = configuration[\"path_distribution_folder_input\"]\n    self.particle_path: str = f\"{self.path_distribution_folder_input}/{self.distribution_file}\"\n    self.delta_max: float = configuration[\"delta_max\"]\n    self.n_turns: int = configuration[\"n_turns\"]\n\n    # Beambeam parameters\n    self.nemitt_x: float = nemitt_x\n    self.nemitt_y: float = nemitt_y\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_tracking.html#study_da.generate.master_classes.xsuite_tracking.XsuiteTracking.prepare_particle_distribution_for_tracking","title":"<code>prepare_particle_distribution_for_tracking(collider)</code>","text":"<p>Prepare a particle distribution for tracking in the collider.</p> <p>This method reads particle data from a parquet file, processes the data to generate normalized amplitudes and angles, and then builds particles for tracking in the collider. If the context is set to use GPU, the collider trackers are reset and rebuilt accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object containing the beam and tracking information.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing: - xp.Particles: The particles ready for tracking. - np.ndarray: Array of particle IDs. - np.ndarray: Array of normalized amplitudes in the xy-plane. - np.ndarray: Array of angles in the xy-plane in radians.</p> Source code in <code>study_da/generate/master_classes/xsuite_tracking.py</code> <pre><code>def prepare_particle_distribution_for_tracking(self, collider: xt.Multiline) -&gt; tuple:\n    \"\"\"\n    Prepare a particle distribution for tracking in the collider.\n\n    This method reads particle data from a parquet file, processes the data to\n    generate normalized amplitudes and angles, and then builds particles for\n    tracking in the collider. If the context is set to use GPU, the collider\n    trackers are reset and rebuilt accordingly.\n\n    Args:\n        collider (xt.Multiline): The collider object containing the beam and\n            tracking information.\n\n    Returns:\n        tuple: A tuple containing:\n            - xp.Particles: The particles ready for tracking.\n            - np.ndarray: Array of particle IDs.\n            - np.ndarray: Array of normalized amplitudes in the xy-plane.\n            - np.ndarray: Array of angles in the xy-plane in radians.\n    \"\"\"\n    # Reset the tracker to go to GPU if needed\n    if self.context_str in [\"cupy\", \"opencl\"]:\n        collider.discard_trackers()\n        collider.build_trackers(_context=self.context)\n\n    particle_df = pd.read_parquet(self.particle_path)\n\n    r_vect = particle_df[\"normalized amplitude in xy-plane\"].values\n    theta_vect = particle_df[\"angle in xy-plane [deg]\"].values * np.pi / 180  # type: ignore # [rad]\n\n    A1_in_sigma = r_vect * np.cos(theta_vect)\n    A2_in_sigma = r_vect * np.sin(theta_vect)\n\n    particles = collider[self.beam].build_particles(\n        x_norm=A1_in_sigma,\n        y_norm=A2_in_sigma,\n        delta=self.delta_max,\n        scale_with_transverse_norm_emitt=(\n            self.nemitt_x,\n            self.nemitt_y,\n        ),\n        _context=self.context,\n    )\n\n    particle_id = particle_df.particle_id.values\n    return particles, particle_id, r_vect, theta_vect\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_tracking.html#study_da.generate.master_classes.xsuite_tracking.XsuiteTracking.track","title":"<code>track(collider, particles)</code>","text":"<p>Tracks particles through a collider for a specified number of turns and logs the elapsed time.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object containing the beamline to be tracked.</p> required <code>particles</code> <code>Particles</code> <p>The particles to be tracked.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the tracked particles.</p> Source code in <code>study_da/generate/master_classes/xsuite_tracking.py</code> <pre><code>def track(self, collider: xt.Multiline, particles: xp.Particles) -&gt; dict:\n    \"\"\"\n    Tracks particles through a collider for a specified number of turns and logs the elapsed time.\n\n    Args:\n        collider (xt.Multiline): The collider object containing the beamline to be tracked.\n        particles (xp.Particles): The particles to be tracked.\n\n    Returns:\n        dict: A dictionary representation of the tracked particles.\n    \"\"\"\n    # Optimize line for tracking\n    collider[self.beam].optimize_for_tracking()\n\n    # Track\n    num_turns = self.n_turns\n    a = time.time()\n    collider[self.beam].track(particles, turn_by_turn_monitor=False, num_turns=num_turns)\n    b = time.time()\n\n    logging.info(f\"Elapsed time: {b-a} s\")\n    logging.info(\n        f\"Elapsed time per particle per turn: {(b-a)/particles._capacity/num_turns*1e6} us\"\n    )\n\n    return particles.to_dict()\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/index.html","title":"version_specific_files","text":""},{"location":"reference/study_da/generate/version_specific_files/hllhc13/index.html","title":"hllhc13","text":""},{"location":"reference/study_da/generate/version_specific_files/hllhc13/index.html#study_da.generate.version_specific_files.hllhc13.apply_crab_fix","title":"<code>apply_crab_fix(collider, config_knobs_and_tuning)</code>","text":"<p>Apply crab fix beam 2 crabs for HLLHC13</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The Xtrack collider object</p> required <code>config_knobs_and_tuning</code> <code>dict</code> <p>The configuration of the knobs and tuning from the configuration file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/hllhc13/crab_fix.py</code> <pre><code>def apply_crab_fix(collider: xt.Multiline, config_knobs_and_tuning: dict) -&gt; None:\n    \"\"\"Apply crab fix beam 2 crabs for HLLHC13\n\n    Args:\n        collider (xt.Multiline): The Xtrack collider object\n        config_knobs_and_tuning (dict): The configuration of the knobs and tuning from the\n            configuration file.\n\n    Returns:\n        None\n    \"\"\"\n    if \"on_crab5\" in config_knobs_and_tuning[\"knob_settings\"]:\n        collider.vars[\"avcrab_r5b2\"] = -collider.vars[\"avcrab_r5b2\"]._get_value()\n        collider.vars[\"ahcrab_r5b2\"] = -collider.vars[\"ahcrab_r5b2\"]._get_value()\n        collider.vars[\"avcrab_l5b2\"] = -collider.vars[\"avcrab_l5b2\"]._get_value()\n        collider.vars[\"ahcrab_l5b2\"] = -collider.vars[\"ahcrab_l5b2\"]._get_value()\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/hllhc13/index.html#study_da.generate.version_specific_files.hllhc13.generate_orbit_correction_setup","title":"<code>generate_orbit_correction_setup()</code>","text":"<p>Return a dictionary with the setup for the orbit correction.</p> Source code in <code>study_da/generate/version_specific_files/hllhc13/orbit_correction.py</code> <pre><code>def generate_orbit_correction_setup() -&gt; dict:\n    \"\"\"Return a dictionary with the setup for the orbit correction.\"\"\"\n    return {\n        \"lhcb1\": {\n            \"IR1 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.r8.b1\",\n                end=\"e.ds.l1.b1\",\n                vary=(\n                    \"corr_co_acbh14.l1b1\",\n                    \"corr_co_acbh12.l1b1\",\n                    \"corr_co_acbv15.l1b1\",\n                    \"corr_co_acbv13.l1b1\",\n                ),\n                targets=(\"e.ds.l1.b1\",),\n            ),\n            \"IR1 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r1.b1\",\n                end=\"s.ds.l2.b1\",\n                vary=(\n                    \"corr_co_acbh13.r1b1\",\n                    \"corr_co_acbh15.r1b1\",\n                    \"corr_co_acbv12.r1b1\",\n                    \"corr_co_acbv14.r1b1\",\n                ),\n                targets=(\"s.ds.l2.b1\",),\n            ),\n            \"IR5 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.r4.b1\",\n                end=\"e.ds.l5.b1\",\n                vary=(\n                    \"corr_co_acbh14.l5b1\",\n                    \"corr_co_acbh12.l5b1\",\n                    \"corr_co_acbv15.l5b1\",\n                    \"corr_co_acbv13.l5b1\",\n                ),\n                targets=(\"e.ds.l5.b1\",),\n            ),\n            \"IR5 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r5.b1\",\n                end=\"s.ds.l6.b1\",\n                vary=(\n                    \"corr_co_acbh13.r5b1\",\n                    \"corr_co_acbh15.r5b1\",\n                    \"corr_co_acbv12.r5b1\",\n                    \"corr_co_acbv14.r5b1\",\n                ),\n                targets=(\"s.ds.l6.b1\",),\n            ),\n            \"IP1\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l1.b1\",\n                end=\"s.ds.r1.b1\",\n                vary=(\n                    \"corr_co_acbch6.l1b1\",\n                    \"corr_co_acbyvs5.l1b1\",\n                    \"corr_co_acbyhs5.r1b1\",\n                    \"corr_co_acbcv6.r1b1\",\n                    \"corr_co_acbyhs4.l1b1\",\n                    \"corr_co_acbyhs4.r1b1\",\n                    \"corr_co_acbyvs4.l1b1\",\n                    \"corr_co_acbyvs4.r1b1\",\n                ),\n                targets=(\"ip1\", \"s.ds.r1.b1\"),\n            ),\n            \"IP2\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l2.b1\",\n                end=\"s.ds.r2.b1\",\n                vary=(\n                    \"corr_co_acbyhs5.l2b1\",\n                    \"corr_co_acbchs5.r2b1\",\n                    \"corr_co_acbyvs5.l2b1\",\n                    \"corr_co_acbcvs5.r2b1\",\n                    \"corr_co_acbyhs4.l2b1\",\n                    \"corr_co_acbyhs4.r2b1\",\n                    \"corr_co_acbyvs4.l2b1\",\n                    \"corr_co_acbyvs4.r2b1\",\n                ),\n                targets=(\"ip2\", \"s.ds.r2.b1\"),\n            ),\n            \"IP5\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l5.b1\",\n                end=\"s.ds.r5.b1\",\n                vary=(\n                    \"corr_co_acbch6.l5b1\",\n                    \"corr_co_acbyvs5.l5b1\",\n                    \"corr_co_acbyhs5.r5b1\",\n                    \"corr_co_acbcv6.r5b1\",\n                    \"corr_co_acbyhs4.l5b1\",\n                    \"corr_co_acbyhs4.r5b1\",\n                    \"corr_co_acbyvs4.l5b1\",\n                    \"corr_co_acbyvs4.r5b1\",\n                ),\n                targets=(\"ip5\", \"s.ds.r5.b1\"),\n            ),\n            \"IP8\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l8.b1\",\n                end=\"s.ds.r8.b1\",\n                vary=(\n                    \"corr_co_acbch5.l8b1\",\n                    \"corr_co_acbyhs4.l8b1\",\n                    \"corr_co_acbyhs4.r8b1\",\n                    \"corr_co_acbyhs5.r8b1\",\n                    \"corr_co_acbcvs5.l8b1\",\n                    \"corr_co_acbyvs4.l8b1\",\n                    \"corr_co_acbyvs4.r8b1\",\n                    \"corr_co_acbyvs5.r8b1\",\n                ),\n                targets=(\"ip8\", \"s.ds.r8.b1\"),\n            ),\n        },\n        \"lhcb2\": {\n            \"IR1 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l1.b2\",\n                end=\"e.ds.r8.b2\",\n                vary=(\n                    \"corr_co_acbh13.l1b2\",\n                    \"corr_co_acbh15.l1b2\",\n                    \"corr_co_acbv12.l1b2\",\n                    \"corr_co_acbv14.l1b2\",\n                ),\n                targets=(\"e.ds.r8.b2\",),\n            ),\n            \"IR1 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.l2.b2\",\n                end=\"s.ds.r1.b2\",\n                vary=(\n                    \"corr_co_acbh12.r1b2\",\n                    \"corr_co_acbh14.r1b2\",\n                    \"corr_co_acbv13.r1b2\",\n                    \"corr_co_acbv15.r1b2\",\n                ),\n                targets=(\"s.ds.r1.b2\",),\n            ),\n            \"IR5 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l5.b2\",\n                end=\"e.ds.r4.b2\",\n                vary=(\n                    \"corr_co_acbh13.l5b2\",\n                    \"corr_co_acbh15.l5b2\",\n                    \"corr_co_acbv12.l5b2\",\n                    \"corr_co_acbv14.l5b2\",\n                ),\n                targets=(\"e.ds.r4.b2\",),\n            ),\n            \"IR5 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.l6.b2\",\n                end=\"s.ds.r5.b2\",\n                vary=(\n                    \"corr_co_acbh12.r5b2\",\n                    \"corr_co_acbh14.r5b2\",\n                    \"corr_co_acbv13.r5b2\",\n                    \"corr_co_acbv15.r5b2\",\n                ),\n                targets=(\"s.ds.r5.b2\",),\n            ),\n            \"IP1\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r1.b2\",\n                end=\"e.ds.l1.b2\",\n                vary=(\n                    \"corr_co_acbch6.r1b2\",\n                    \"corr_co_acbyvs5.r1b2\",\n                    \"corr_co_acbyhs5.l1b2\",\n                    \"corr_co_acbcv6.l1b2\",\n                    \"corr_co_acbyhs4.l1b2\",\n                    \"corr_co_acbyhs4.r1b2\",\n                    \"corr_co_acbyvs4.l1b2\",\n                    \"corr_co_acbyvs4.r1b2\",\n                ),\n                targets=(\n                    \"ip1\",\n                    \"e.ds.l1.b2\",\n                ),\n            ),\n            \"IP2\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r2.b2\",\n                end=\"e.ds.l2.b2\",\n                vary=(\n                    \"corr_co_acbyhs5.l2b2\",\n                    \"corr_co_acbchs5.r2b2\",\n                    \"corr_co_acbyvs5.l2b2\",\n                    \"corr_co_acbcvs5.r2b2\",\n                    \"corr_co_acbyhs4.l2b2\",\n                    \"corr_co_acbyhs4.r2b2\",\n                    \"corr_co_acbyvs4.l2b2\",\n                    \"corr_co_acbyvs4.r2b2\",\n                ),\n                targets=(\"ip2\", \"e.ds.l2.b2\"),\n            ),\n            \"IP5\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r5.b2\",\n                end=\"e.ds.l5.b2\",\n                vary=(\n                    \"corr_co_acbch6.r5b2\",\n                    \"corr_co_acbyvs5.r5b2\",\n                    \"corr_co_acbyhs5.l5b2\",\n                    \"corr_co_acbcv6.l5b2\",\n                    \"corr_co_acbyhs4.l5b2\",\n                    \"corr_co_acbyhs4.r5b2\",\n                    \"corr_co_acbyvs4.l5b2\",\n                    \"corr_co_acbyvs4.r5b2\",\n                ),\n                targets=(\n                    \"ip5\",\n                    \"e.ds.l5.b2\",\n                ),\n            ),\n            \"IP8\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r8.b2\",\n                end=\"e.ds.l8.b2\",\n                vary=(\n                    \"corr_co_acbchs5.l8b2\",\n                    \"corr_co_acbyhs5.r8b2\",\n                    \"corr_co_acbcvs5.l8b2\",\n                    \"corr_co_acbyvs5.r8b2\",\n                    \"corr_co_acbyhs4.l8b2\",\n                    \"corr_co_acbyhs4.r8b2\",\n                    \"corr_co_acbyvs4.l8b2\",\n                    \"corr_co_acbyvs4.r8b2\",\n                ),\n                targets=(\n                    \"ip8\",\n                    \"e.ds.l8.b2\",\n                ),\n            ),\n        },\n    }\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/hllhc13/crab_fix.html","title":"crab_fix","text":""},{"location":"reference/study_da/generate/version_specific_files/hllhc13/crab_fix.html#study_da.generate.version_specific_files.hllhc13.crab_fix.apply_crab_fix","title":"<code>apply_crab_fix(collider, config_knobs_and_tuning)</code>","text":"<p>Apply crab fix beam 2 crabs for HLLHC13</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The Xtrack collider object</p> required <code>config_knobs_and_tuning</code> <code>dict</code> <p>The configuration of the knobs and tuning from the configuration file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/hllhc13/crab_fix.py</code> <pre><code>def apply_crab_fix(collider: xt.Multiline, config_knobs_and_tuning: dict) -&gt; None:\n    \"\"\"Apply crab fix beam 2 crabs for HLLHC13\n\n    Args:\n        collider (xt.Multiline): The Xtrack collider object\n        config_knobs_and_tuning (dict): The configuration of the knobs and tuning from the\n            configuration file.\n\n    Returns:\n        None\n    \"\"\"\n    if \"on_crab5\" in config_knobs_and_tuning[\"knob_settings\"]:\n        collider.vars[\"avcrab_r5b2\"] = -collider.vars[\"avcrab_r5b2\"]._get_value()\n        collider.vars[\"ahcrab_r5b2\"] = -collider.vars[\"ahcrab_r5b2\"]._get_value()\n        collider.vars[\"avcrab_l5b2\"] = -collider.vars[\"avcrab_l5b2\"]._get_value()\n        collider.vars[\"ahcrab_l5b2\"] = -collider.vars[\"ahcrab_l5b2\"]._get_value()\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/hllhc13/optics_specific_tools.html","title":"optics_specific_tools","text":""},{"location":"reference/study_da/generate/version_specific_files/hllhc13/optics_specific_tools.html#study_da.generate.version_specific_files.hllhc13.optics_specific_tools.apply_optics","title":"<code>apply_optics(*args, **kwargs)</code>","text":"<p>Apply the optics to the MAD-X model.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>See hllhc16.optics_specific_tools.apply_optics.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>See hllhc16.optics_specific_tools.apply_optics.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/hllhc13/optics_specific_tools.py</code> <pre><code>def apply_optics(*args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Apply the optics to the MAD-X model.\n\n    Args:\n        *args (Any): See hllhc16.optics_specific_tools.apply_optics.\n        **kwargs (Any): See hllhc16.optics_specific_tools.apply_optics.\n\n    Returns:\n        None\n    \"\"\"\n    apply_optics_hllhc16(*args, **kwargs)\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/hllhc13/optics_specific_tools.html#study_da.generate.version_specific_files.hllhc13.optics_specific_tools.build_sequence","title":"<code>build_sequence(mad, mylhcbeam, beam_config, ignore_cycling=False, slice_factor=None, BFPP=False)</code>","text":"<p>Build the sequence for the (HL-)LHC, for a given beam.</p> <p>Parameters:</p> Name Type Description Default <code>mad</code> <code>Madx</code> <p>The MAD-X object used to build the sequence.</p> required <code>mylhcbeam</code> <code>int</code> <p>The beam number (1, 2 or 4).</p> required <code>beam_config</code> <code>dict[str, Any]</code> <p>The configuration of the beam from the configuration file.</p> required <code>ignore_cycling</code> <code>bool</code> <p>Whether to ignore cycling to have IP3 at position s=0. Defaults to False.</p> <code>False</code> <code>slice_factor</code> <code>int | None</code> <p>The slice factor if optic is not thin. Defaults to None.</p> <code>None</code> <code>BFPP</code> <code>bool</code> <p>Whether to use the BFPP knob. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/hllhc13/optics_specific_tools.py</code> <pre><code>def build_sequence(\n    mad: Madx,\n    mylhcbeam: int,\n    beam_config: dict[str, Any],  # Not used but important for consistency with other optics\n    ignore_cycling: bool = False,\n    slice_factor: int | None = None,  # Not used but important for consistency with other optics\n    BFPP: bool = False,  # Not used but important for consistency with other optics\n) -&gt; None:\n    \"\"\"Build the sequence for the (HL-)LHC, for a given beam.\n\n    Args:\n        mad (Madx): The MAD-X object used to build the sequence.\n        mylhcbeam (int): The beam number (1, 2 or 4).\n        beam_config (dict[str, Any]): The configuration of the beam from the configuration file.\n        ignore_cycling (bool, optional): Whether to ignore cycling to have IP3 at position s=0.\n            Defaults to False.\n        slice_factor (int | None, optional): The slice factor if optic is not thin. Defaults to None.\n        BFPP (bool, optional): Whether to use the BFPP knob. Defaults to False.\n\n    Returns:\n        None\n    \"\"\"\n\n    # Select beam\n    mad.input(f\"mylhcbeam = {mylhcbeam}\")\n\n    # Build sequence\n    mad.input(\"\"\"\n      ! Build sequence\n      option, -echo,-warn,-info;\n      if (mylhcbeam==4){\n        call,file=\"acc-models-lhc/lhcb4.seq\";\n      } else {\n        call,file=\"acc-models-lhc/lhc.seq\";\n      };\n      !Install HL-LHC\n      call, file=\n        \"acc-models-lhc/hllhc_sequence.madx\";\n      ! Get the toolkit\n      call,file=\n        \"acc-models-lhc/toolkit/macro.madx\";\n      option, -echo, warn,-info;\n      \"\"\")\n\n    mad.input(\"\"\"\n      ! Slice nominal sequence\n      exec, myslice;\n      \"\"\")\n\n    mad.input(\"\"\"exec,mk_beam(7000);\"\"\")\n\n    install_errors_placeholders_hllhc(mad)\n\n    if not ignore_cycling:\n        mad.input(\"\"\"\n        !Cycling w.r.t. to IP3 (mandatory to find closed orbit in collision in the presence of errors)\n        if (mylhcbeam&lt;3){\n        seqedit, sequence=lhcb1; flatten; cycle, start=IP3; flatten; endedit;\n        };\n        seqedit, sequence=lhcb2; flatten; cycle, start=IP3; flatten; endedit;\n        \"\"\")\n\n    # Incorporate crab-cavities\n    mad.input(\"\"\"\n    ! Install crab cavities (they are off)\n    call, file='acc-models-lhc/toolkit/enable_crabcavities.madx';\n    on_crab1 = 0;\n    on_crab5 = 0;\n    \"\"\")\n\n    mad.input(\"\"\"\n        ! Set twiss formats for MAD-X parts (macro from opt. toolkit)\n        exec, twiss_opt;\n        \"\"\")\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/hllhc13/optics_specific_tools.html#study_da.generate.version_specific_files.hllhc13.optics_specific_tools.check_madx_lattices","title":"<code>check_madx_lattices(*args, **kwargs)</code>","text":"<p>Check the consistency of the MAD-X lattice for the (HL-)LHC.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>See hllhc16.optics_specific_tools.check_madx_lattices.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>See hllhc16.optics_specific_tools.check_madx_lattices.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/hllhc13/optics_specific_tools.py</code> <pre><code>def check_madx_lattices(*args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Check the consistency of the MAD-X lattice for the (HL-)LHC.\n\n    Args:\n        *args (Any): See hllhc16.optics_specific_tools.check_madx_lattices.\n        **kwargs (Any): See hllhc16.optics_specific_tools.check_madx_lattices.\n\n    Returns:\n        None\n    \"\"\"\n    check_madx_lattices_hllhc16(*args, **kwargs)\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/hllhc13/orbit_correction.html","title":"orbit_correction","text":""},{"location":"reference/study_da/generate/version_specific_files/hllhc13/orbit_correction.html#study_da.generate.version_specific_files.hllhc13.orbit_correction.generate_orbit_correction_setup","title":"<code>generate_orbit_correction_setup()</code>","text":"<p>Return a dictionary with the setup for the orbit correction.</p> Source code in <code>study_da/generate/version_specific_files/hllhc13/orbit_correction.py</code> <pre><code>def generate_orbit_correction_setup() -&gt; dict:\n    \"\"\"Return a dictionary with the setup for the orbit correction.\"\"\"\n    return {\n        \"lhcb1\": {\n            \"IR1 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.r8.b1\",\n                end=\"e.ds.l1.b1\",\n                vary=(\n                    \"corr_co_acbh14.l1b1\",\n                    \"corr_co_acbh12.l1b1\",\n                    \"corr_co_acbv15.l1b1\",\n                    \"corr_co_acbv13.l1b1\",\n                ),\n                targets=(\"e.ds.l1.b1\",),\n            ),\n            \"IR1 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r1.b1\",\n                end=\"s.ds.l2.b1\",\n                vary=(\n                    \"corr_co_acbh13.r1b1\",\n                    \"corr_co_acbh15.r1b1\",\n                    \"corr_co_acbv12.r1b1\",\n                    \"corr_co_acbv14.r1b1\",\n                ),\n                targets=(\"s.ds.l2.b1\",),\n            ),\n            \"IR5 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.r4.b1\",\n                end=\"e.ds.l5.b1\",\n                vary=(\n                    \"corr_co_acbh14.l5b1\",\n                    \"corr_co_acbh12.l5b1\",\n                    \"corr_co_acbv15.l5b1\",\n                    \"corr_co_acbv13.l5b1\",\n                ),\n                targets=(\"e.ds.l5.b1\",),\n            ),\n            \"IR5 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r5.b1\",\n                end=\"s.ds.l6.b1\",\n                vary=(\n                    \"corr_co_acbh13.r5b1\",\n                    \"corr_co_acbh15.r5b1\",\n                    \"corr_co_acbv12.r5b1\",\n                    \"corr_co_acbv14.r5b1\",\n                ),\n                targets=(\"s.ds.l6.b1\",),\n            ),\n            \"IP1\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l1.b1\",\n                end=\"s.ds.r1.b1\",\n                vary=(\n                    \"corr_co_acbch6.l1b1\",\n                    \"corr_co_acbyvs5.l1b1\",\n                    \"corr_co_acbyhs5.r1b1\",\n                    \"corr_co_acbcv6.r1b1\",\n                    \"corr_co_acbyhs4.l1b1\",\n                    \"corr_co_acbyhs4.r1b1\",\n                    \"corr_co_acbyvs4.l1b1\",\n                    \"corr_co_acbyvs4.r1b1\",\n                ),\n                targets=(\"ip1\", \"s.ds.r1.b1\"),\n            ),\n            \"IP2\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l2.b1\",\n                end=\"s.ds.r2.b1\",\n                vary=(\n                    \"corr_co_acbyhs5.l2b1\",\n                    \"corr_co_acbchs5.r2b1\",\n                    \"corr_co_acbyvs5.l2b1\",\n                    \"corr_co_acbcvs5.r2b1\",\n                    \"corr_co_acbyhs4.l2b1\",\n                    \"corr_co_acbyhs4.r2b1\",\n                    \"corr_co_acbyvs4.l2b1\",\n                    \"corr_co_acbyvs4.r2b1\",\n                ),\n                targets=(\"ip2\", \"s.ds.r2.b1\"),\n            ),\n            \"IP5\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l5.b1\",\n                end=\"s.ds.r5.b1\",\n                vary=(\n                    \"corr_co_acbch6.l5b1\",\n                    \"corr_co_acbyvs5.l5b1\",\n                    \"corr_co_acbyhs5.r5b1\",\n                    \"corr_co_acbcv6.r5b1\",\n                    \"corr_co_acbyhs4.l5b1\",\n                    \"corr_co_acbyhs4.r5b1\",\n                    \"corr_co_acbyvs4.l5b1\",\n                    \"corr_co_acbyvs4.r5b1\",\n                ),\n                targets=(\"ip5\", \"s.ds.r5.b1\"),\n            ),\n            \"IP8\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l8.b1\",\n                end=\"s.ds.r8.b1\",\n                vary=(\n                    \"corr_co_acbch5.l8b1\",\n                    \"corr_co_acbyhs4.l8b1\",\n                    \"corr_co_acbyhs4.r8b1\",\n                    \"corr_co_acbyhs5.r8b1\",\n                    \"corr_co_acbcvs5.l8b1\",\n                    \"corr_co_acbyvs4.l8b1\",\n                    \"corr_co_acbyvs4.r8b1\",\n                    \"corr_co_acbyvs5.r8b1\",\n                ),\n                targets=(\"ip8\", \"s.ds.r8.b1\"),\n            ),\n        },\n        \"lhcb2\": {\n            \"IR1 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l1.b2\",\n                end=\"e.ds.r8.b2\",\n                vary=(\n                    \"corr_co_acbh13.l1b2\",\n                    \"corr_co_acbh15.l1b2\",\n                    \"corr_co_acbv12.l1b2\",\n                    \"corr_co_acbv14.l1b2\",\n                ),\n                targets=(\"e.ds.r8.b2\",),\n            ),\n            \"IR1 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.l2.b2\",\n                end=\"s.ds.r1.b2\",\n                vary=(\n                    \"corr_co_acbh12.r1b2\",\n                    \"corr_co_acbh14.r1b2\",\n                    \"corr_co_acbv13.r1b2\",\n                    \"corr_co_acbv15.r1b2\",\n                ),\n                targets=(\"s.ds.r1.b2\",),\n            ),\n            \"IR5 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l5.b2\",\n                end=\"e.ds.r4.b2\",\n                vary=(\n                    \"corr_co_acbh13.l5b2\",\n                    \"corr_co_acbh15.l5b2\",\n                    \"corr_co_acbv12.l5b2\",\n                    \"corr_co_acbv14.l5b2\",\n                ),\n                targets=(\"e.ds.r4.b2\",),\n            ),\n            \"IR5 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.l6.b2\",\n                end=\"s.ds.r5.b2\",\n                vary=(\n                    \"corr_co_acbh12.r5b2\",\n                    \"corr_co_acbh14.r5b2\",\n                    \"corr_co_acbv13.r5b2\",\n                    \"corr_co_acbv15.r5b2\",\n                ),\n                targets=(\"s.ds.r5.b2\",),\n            ),\n            \"IP1\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r1.b2\",\n                end=\"e.ds.l1.b2\",\n                vary=(\n                    \"corr_co_acbch6.r1b2\",\n                    \"corr_co_acbyvs5.r1b2\",\n                    \"corr_co_acbyhs5.l1b2\",\n                    \"corr_co_acbcv6.l1b2\",\n                    \"corr_co_acbyhs4.l1b2\",\n                    \"corr_co_acbyhs4.r1b2\",\n                    \"corr_co_acbyvs4.l1b2\",\n                    \"corr_co_acbyvs4.r1b2\",\n                ),\n                targets=(\n                    \"ip1\",\n                    \"e.ds.l1.b2\",\n                ),\n            ),\n            \"IP2\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r2.b2\",\n                end=\"e.ds.l2.b2\",\n                vary=(\n                    \"corr_co_acbyhs5.l2b2\",\n                    \"corr_co_acbchs5.r2b2\",\n                    \"corr_co_acbyvs5.l2b2\",\n                    \"corr_co_acbcvs5.r2b2\",\n                    \"corr_co_acbyhs4.l2b2\",\n                    \"corr_co_acbyhs4.r2b2\",\n                    \"corr_co_acbyvs4.l2b2\",\n                    \"corr_co_acbyvs4.r2b2\",\n                ),\n                targets=(\"ip2\", \"e.ds.l2.b2\"),\n            ),\n            \"IP5\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r5.b2\",\n                end=\"e.ds.l5.b2\",\n                vary=(\n                    \"corr_co_acbch6.r5b2\",\n                    \"corr_co_acbyvs5.r5b2\",\n                    \"corr_co_acbyhs5.l5b2\",\n                    \"corr_co_acbcv6.l5b2\",\n                    \"corr_co_acbyhs4.l5b2\",\n                    \"corr_co_acbyhs4.r5b2\",\n                    \"corr_co_acbyvs4.l5b2\",\n                    \"corr_co_acbyvs4.r5b2\",\n                ),\n                targets=(\n                    \"ip5\",\n                    \"e.ds.l5.b2\",\n                ),\n            ),\n            \"IP8\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r8.b2\",\n                end=\"e.ds.l8.b2\",\n                vary=(\n                    \"corr_co_acbchs5.l8b2\",\n                    \"corr_co_acbyhs5.r8b2\",\n                    \"corr_co_acbcvs5.l8b2\",\n                    \"corr_co_acbyvs5.r8b2\",\n                    \"corr_co_acbyhs4.l8b2\",\n                    \"corr_co_acbyhs4.r8b2\",\n                    \"corr_co_acbyvs4.l8b2\",\n                    \"corr_co_acbyvs4.r8b2\",\n                ),\n                targets=(\n                    \"ip8\",\n                    \"e.ds.l8.b2\",\n                ),\n            ),\n        },\n    }\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/hllhc16/index.html","title":"hllhc16","text":""},{"location":"reference/study_da/generate/version_specific_files/hllhc16/index.html#study_da.generate.version_specific_files.hllhc16.generate_orbit_correction_setup","title":"<code>generate_orbit_correction_setup()</code>","text":"<p>Return a dictionary with the setup for the orbit correction.</p> Source code in <code>study_da/generate/version_specific_files/hllhc16/orbit_correction.py</code> <pre><code>def generate_orbit_correction_setup() -&gt; dict:\n    \"\"\"Return a dictionary with the setup for the orbit correction.\"\"\"\n    return {\n        \"lhcb1\": {\n            \"IR1 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.r8.b1\",\n                end=\"e.ds.l1.b1\",\n                vary=(\n                    \"corr_co_acbh14.l1b1\",\n                    \"corr_co_acbh12.l1b1\",\n                    \"corr_co_acbv15.l1b1\",\n                    \"corr_co_acbv13.l1b1\",\n                ),\n                targets=(\"e.ds.l1.b1\",),\n            ),\n            \"IR1 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r1.b1\",\n                end=\"s.ds.l2.b1\",\n                vary=(\n                    \"corr_co_acbh13.r1b1\",\n                    \"corr_co_acbh15.r1b1\",\n                    \"corr_co_acbv12.r1b1\",\n                    \"corr_co_acbv14.r1b1\",\n                ),\n                targets=(\"s.ds.l2.b1\",),\n            ),\n            \"IR5 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.r4.b1\",\n                end=\"e.ds.l5.b1\",\n                vary=(\n                    \"corr_co_acbh14.l5b1\",\n                    \"corr_co_acbh12.l5b1\",\n                    \"corr_co_acbv15.l5b1\",\n                    \"corr_co_acbv13.l5b1\",\n                ),\n                targets=(\"e.ds.l5.b1\",),\n            ),\n            \"IR5 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r5.b1\",\n                end=\"s.ds.l6.b1\",\n                vary=(\n                    \"corr_co_acbh13.r5b1\",\n                    \"corr_co_acbh15.r5b1\",\n                    \"corr_co_acbv12.r5b1\",\n                    \"corr_co_acbv14.r5b1\",\n                ),\n                targets=(\"s.ds.l6.b1\",),\n            ),\n            \"IP1\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l1.b1\",\n                end=\"s.ds.r1.b1\",\n                vary=(\n                    \"corr_co_acbch6.l1b1\",\n                    \"corr_co_acbcv5.l1b1\",\n                    \"corr_co_acbch5.r1b1\",\n                    \"corr_co_acbcv6.r1b1\",\n                    \"corr_co_acbyhs4.l1b1\",\n                    \"corr_co_acbyhs4.r1b1\",\n                    \"corr_co_acbyvs4.l1b1\",\n                    \"corr_co_acbyvs4.r1b1\",\n                ),\n                targets=(\"ip1\", \"s.ds.r1.b1\"),\n            ),\n            \"IP2\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l2.b1\",\n                end=\"s.ds.r2.b1\",\n                vary=(\n                    \"corr_co_acbyhs5.l2b1\",\n                    \"corr_co_acbchs5.r2b1\",\n                    \"corr_co_acbyvs5.l2b1\",\n                    \"corr_co_acbcvs5.r2b1\",\n                    \"corr_co_acbyhs4.l2b1\",\n                    \"corr_co_acbyhs4.r2b1\",\n                    \"corr_co_acbyvs4.l2b1\",\n                    \"corr_co_acbyvs4.r2b1\",\n                ),\n                targets=(\"ip2\", \"s.ds.r2.b1\"),\n            ),\n            \"IP5\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l5.b1\",\n                end=\"s.ds.r5.b1\",\n                vary=(\n                    \"corr_co_acbch6.l5b1\",\n                    \"corr_co_acbcv5.l5b1\",\n                    \"corr_co_acbch5.r5b1\",\n                    \"corr_co_acbcv6.r5b1\",\n                    \"corr_co_acbyhs4.l5b1\",\n                    \"corr_co_acbyhs4.r5b1\",\n                    \"corr_co_acbyvs4.l5b1\",\n                    \"corr_co_acbyvs4.r5b1\",\n                ),\n                targets=(\"ip5\", \"s.ds.r5.b1\"),\n            ),\n            \"IP8\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l8.b1\",\n                end=\"s.ds.r8.b1\",\n                vary=(\n                    \"corr_co_acbch5.l8b1\",\n                    \"corr_co_acbyhs4.l8b1\",\n                    \"corr_co_acbyhs4.r8b1\",\n                    \"corr_co_acbyhs5.r8b1\",\n                    \"corr_co_acbcvs5.l8b1\",\n                    \"corr_co_acbyvs4.l8b1\",\n                    \"corr_co_acbyvs4.r8b1\",\n                    \"corr_co_acbyvs5.r8b1\",\n                ),\n                targets=(\"ip8\", \"s.ds.r8.b1\"),\n            ),\n        },\n        \"lhcb2\": {\n            \"IR1 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l1.b2\",\n                end=\"e.ds.r8.b2\",\n                vary=(\n                    \"corr_co_acbh13.l1b2\",\n                    \"corr_co_acbh15.l1b2\",\n                    \"corr_co_acbv12.l1b2\",\n                    \"corr_co_acbv14.l1b2\",\n                ),\n                targets=(\"e.ds.r8.b2\",),\n            ),\n            \"IR1 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.l2.b2\",\n                end=\"s.ds.r1.b2\",\n                vary=(\n                    \"corr_co_acbh12.r1b2\",\n                    \"corr_co_acbh14.r1b2\",\n                    \"corr_co_acbv13.r1b2\",\n                    \"corr_co_acbv15.r1b2\",\n                ),\n                targets=(\"s.ds.r1.b2\",),\n            ),\n            \"IR5 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l5.b2\",\n                end=\"e.ds.r4.b2\",\n                vary=(\n                    \"corr_co_acbh13.l5b2\",\n                    \"corr_co_acbh15.l5b2\",\n                    \"corr_co_acbv12.l5b2\",\n                    \"corr_co_acbv14.l5b2\",\n                ),\n                targets=(\"e.ds.r4.b2\",),\n            ),\n            \"IR5 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.l6.b2\",\n                end=\"s.ds.r5.b2\",\n                vary=(\n                    \"corr_co_acbh12.r5b2\",\n                    \"corr_co_acbh14.r5b2\",\n                    \"corr_co_acbv13.r5b2\",\n                    \"corr_co_acbv15.r5b2\",\n                ),\n                targets=(\"s.ds.r5.b2\",),\n            ),\n            \"IP1\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r1.b2\",\n                end=\"e.ds.l1.b2\",\n                vary=(\n                    \"corr_co_acbch6.r1b2\",\n                    \"corr_co_acbcv5.r1b2\",\n                    \"corr_co_acbch5.l1b2\",\n                    \"corr_co_acbcv6.l1b2\",\n                    \"corr_co_acbyhs4.l1b2\",\n                    \"corr_co_acbyhs4.r1b2\",\n                    \"corr_co_acbyvs4.l1b2\",\n                    \"corr_co_acbyvs4.r1b2\",\n                ),\n                targets=(\n                    \"ip1\",\n                    \"e.ds.l1.b2\",\n                ),\n            ),\n            \"IP2\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r2.b2\",\n                end=\"e.ds.l2.b2\",\n                vary=(\n                    \"corr_co_acbyhs5.l2b2\",\n                    \"corr_co_acbchs5.r2b2\",\n                    \"corr_co_acbyvs5.l2b2\",\n                    \"corr_co_acbcvs5.r2b2\",\n                    \"corr_co_acbyhs4.l2b2\",\n                    \"corr_co_acbyhs4.r2b2\",\n                    \"corr_co_acbyvs4.l2b2\",\n                    \"corr_co_acbyvs4.r2b2\",\n                ),\n                targets=(\"ip2\", \"e.ds.l2.b2\"),\n            ),\n            \"IP5\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r5.b2\",\n                end=\"e.ds.l5.b2\",\n                vary=(\n                    \"corr_co_acbch6.r5b2\",\n                    \"corr_co_acbcv5.r5b2\",\n                    \"corr_co_acbch5.l5b2\",\n                    \"corr_co_acbcv6.l5b2\",\n                    \"corr_co_acbyhs4.l5b2\",\n                    \"corr_co_acbyhs4.r5b2\",\n                    \"corr_co_acbyvs4.l5b2\",\n                    \"corr_co_acbyvs4.r5b2\",\n                ),\n                targets=(\n                    \"ip5\",\n                    \"e.ds.l5.b2\",\n                ),\n            ),\n            \"IP8\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r8.b2\",\n                end=\"e.ds.l8.b2\",\n                vary=(\n                    \"corr_co_acbchs5.l8b2\",\n                    \"corr_co_acbyhs5.r8b2\",\n                    \"corr_co_acbcvs5.l8b2\",\n                    \"corr_co_acbyvs5.r8b2\",\n                    \"corr_co_acbyhs4.l8b2\",\n                    \"corr_co_acbyhs4.r8b2\",\n                    \"corr_co_acbyvs4.l8b2\",\n                    \"corr_co_acbyvs4.r8b2\",\n                ),\n                targets=(\n                    \"ip8\",\n                    \"e.ds.l8.b2\",\n                ),\n            ),\n        },\n    }\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/hllhc16/optics_specific_tools.html","title":"optics_specific_tools","text":""},{"location":"reference/study_da/generate/version_specific_files/hllhc16/optics_specific_tools.html#study_da.generate.version_specific_files.hllhc16.optics_specific_tools.apply_optics","title":"<code>apply_optics(mad, optics_file)</code>","text":"<p>Apply the optics to the MAD-X model.</p> <p>Parameters:</p> Name Type Description Default <code>mad</code> <code>Madx</code> <p>The MAD-X object used to build the sequence.</p> required <code>optics_file</code> <code>str</code> <p>The path to the optics file to apply.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/hllhc16/optics_specific_tools.py</code> <pre><code>def apply_optics(mad: Madx, optics_file: str) -&gt; None:\n    \"\"\"Apply the optics to the MAD-X model.\n\n    Args:\n        mad (Madx): The MAD-X object used to build the sequence.\n        optics_file (str): The path to the optics file to apply.\n\n    Returns:\n        None\n    \"\"\"\n    mad.call(optics_file)\n    # A knob redefinition\n    mad.input(\"on_alice := on_alice_normalized * 7000./nrj;\")\n    mad.input(\"on_lhcb := on_lhcb_normalized * 7000./nrj;\")\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/hllhc16/optics_specific_tools.html#study_da.generate.version_specific_files.hllhc16.optics_specific_tools.build_sequence","title":"<code>build_sequence(mad, mylhcbeam, beam_config, ignore_cycling=False, slice_factor=None, BFPP=False)</code>","text":"<p>Build the sequence for the (HL-)LHC, for a given beam.</p> <p>Parameters:</p> Name Type Description Default <code>mad</code> <code>Madx</code> <p>The MAD-X object used to build the sequence.</p> required <code>mylhcbeam</code> <code>int</code> <p>The beam number (1, 2 or 4).</p> required <code>beam_config</code> <code>dict[str, Any]</code> <p>The configuration of the beam from the configuration file.</p> required <code>ignore_cycling</code> <code>bool</code> <p>Whether to ignore cycling to have IP3 at position s=0. Defaults to False.</p> <code>False</code> <code>slice_factor</code> <code>int | None</code> <p>The slice factor if optic is not thin. Defaults to None.</p> <code>None</code> <code>BFPP</code> <code>bool</code> <p>Whether to use the BFPP knob. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/hllhc16/optics_specific_tools.py</code> <pre><code>def build_sequence(\n    mad: Madx,\n    mylhcbeam: int,\n    beam_config: dict[str, Any],  # Not used but important for consistency with other optics\n    ignore_cycling: bool = False,\n    slice_factor: int | None = None,  # Not used but important for consistency with other optics\n    BFPP: bool = False,  # Not used but important for consistency with other optics\n) -&gt; None:\n    \"\"\"Build the sequence for the (HL-)LHC, for a given beam.\n\n    Args:\n        mad (Madx): The MAD-X object used to build the sequence.\n        mylhcbeam (int): The beam number (1, 2 or 4).\n        beam_config (dict[str, Any]): The configuration of the beam from the configuration file.\n        ignore_cycling (bool, optional): Whether to ignore cycling to have IP3 at position s=0.\n            Defaults to False.\n        slice_factor (int | None, optional): The slice factor if optic is not thin. Defaults to None.\n        BFPP (bool, optional): Whether to use the BFPP knob. Defaults to False.\n\n    Returns:\n        None\n    \"\"\"\n    # Select beam\n    mad.input(f\"mylhcbeam = {mylhcbeam}\")\n\n    # Build sequence\n    mad.input(\"\"\"\n      ! Build sequence\n      option, -echo,-warn,-info;\n      if (mylhcbeam==4){\n        call,file=\"acc-models-lhc/lhcb4.seq\";\n      } else {\n        call,file=\"acc-models-lhc/lhc.seq\";\n      };\n      !Install HL-LHC\n      call, file=\n        \"acc-models-lhc/hllhc_sequence.madx\";\n      ! Get the toolkit\n      call,file=\n        \"acc-models-lhc/toolkit/macro.madx\";\n      option, -echo, warn,-info;\n      \"\"\")\n\n    # Fix for hllhc16\n    mad.input(\"\"\"\n    l.mbh = 0.001000;\n    ACSCA, HARMON := HRF400;\n\n    ACSCA.D5L4.B1, VOLT := VRF400/8, LAG := LAGRF400.B1, HARMON := HRF400;\n    ACSCA.C5L4.B1, VOLT := VRF400/8, LAG := LAGRF400.B1, HARMON := HRF400;\n    ACSCA.B5L4.B1, VOLT := VRF400/8, LAG := LAGRF400.B1, HARMON := HRF400;\n    ACSCA.A5L4.B1, VOLT := VRF400/8, LAG := LAGRF400.B1, HARMON := HRF400;\n    ACSCA.A5R4.B1, VOLT := VRF400/8, LAG := LAGRF400.B1, HARMON := HRF400;\n    ACSCA.B5R4.B1, VOLT := VRF400/8, LAG := LAGRF400.B1, HARMON := HRF400;\n    ACSCA.C5R4.B1, VOLT := VRF400/8, LAG := LAGRF400.B1, HARMON := HRF400;\n    ACSCA.D5R4.B1, VOLT := VRF400/8, LAG := LAGRF400.B1, HARMON := HRF400;\n    ACSCA.D5L4.B2, VOLT := VRF400/8, LAG := LAGRF400.B2, HARMON := HRF400;\n    ACSCA.C5L4.B2, VOLT := VRF400/8, LAG := LAGRF400.B2, HARMON := HRF400;\n    ACSCA.B5L4.B2, VOLT := VRF400/8, LAG := LAGRF400.B2, HARMON := HRF400;\n    ACSCA.A5L4.B2, VOLT := VRF400/8, LAG := LAGRF400.B2, HARMON := HRF400;\n    ACSCA.A5R4.B2, VOLT := VRF400/8, LAG := LAGRF400.B2, HARMON := HRF400;\n    ACSCA.B5R4.B2, VOLT := VRF400/8, LAG := LAGRF400.B2, HARMON := HRF400;\n    ACSCA.C5R4.B2, VOLT := VRF400/8, LAG := LAGRF400.B2, HARMON := HRF400;\n    ACSCA.D5R4.B2, VOLT := VRF400/8, LAG := LAGRF400.B2, HARMON := HRF400;\n    \"\"\")\n\n    mad.input(\"\"\"\n      ! Slice nominal sequence\n      exec, myslice;\n      \"\"\")\n\n    if mylhcbeam &lt; 3:\n        mad.input(\"\"\"\n      nrj=7000;\n      beam,particle=proton,sequence=lhcb1,energy=nrj,npart=1.15E11,sige=4.5e-4;\n      beam,particle=proton,sequence=lhcb2,energy=nrj,bv = -1,npart=1.15E11,sige=4.5e-4;\n      \"\"\")\n\n    install_errors_placeholders_hllhc(mad)\n\n    if not ignore_cycling:\n        mad.input(\"\"\"\n        !Cycling w.r.t. to IP3 (mandatory to find closed orbit in collision in the presence of errors)\n        if (mylhcbeam&lt;3){\n        seqedit, sequence=lhcb1; flatten; cycle, start=IP3; flatten; endedit;\n        };\n        seqedit, sequence=lhcb2; flatten; cycle, start=IP3; flatten; endedit;\n        \"\"\")\n\n    # Incorporate crab-cavities\n    mad.input(\"\"\"\n    ! Install crab cavities (they are off)\n    call, file='acc-models-lhc/toolkit/enable_crabcavities.madx';\n    on_crab1 = 0;\n    on_crab5 = 0;\n    \"\"\")\n\n    mad.input(\"\"\"\n        ! Set twiss formats for MAD-X parts (macro from opt. toolkit)\n        exec, twiss_opt;\n        \"\"\")\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/hllhc16/optics_specific_tools.html#study_da.generate.version_specific_files.hllhc16.optics_specific_tools.check_madx_lattices","title":"<code>check_madx_lattices(mad)</code>","text":"<p>Check the consistency of the MAD-X lattice for the (HL-)LHC.</p> <p>Parameters:</p> Name Type Description Default <code>mad</code> <code>Madx</code> <p>The MAD-X object used to build the sequence.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/hllhc16/optics_specific_tools.py</code> <pre><code>def check_madx_lattices(mad: Madx) -&gt; None:  # sourcery skip: extract-method\n    \"\"\"Check the consistency of the MAD-X lattice for the (HL-)LHC.\n\n    Args:\n        mad (Madx): The MAD-X object used to build the sequence.\n\n    Returns:\n        None\n    \"\"\"\n    assert mad.globals[\"qxb1\"] == mad.globals[\"qxb2\"]\n    assert mad.globals[\"qyb1\"] == mad.globals[\"qyb2\"]\n    assert mad.globals[\"qpxb1\"] == mad.globals[\"qpxb2\"]\n    assert mad.globals[\"qpyb1\"] == mad.globals[\"qpyb2\"]\n\n    assert np.isclose(mad.table.summ.q1, mad.globals[\"qxb1\"], atol=1e-02)\n    assert np.isclose(mad.table.summ.q2, mad.globals[\"qyb1\"], atol=1e-02)\n\n    try:\n        assert np.isclose(mad.table.summ.dq1, mad.globals[\"qpxb1\"], atol=1e-01)\n        assert np.isclose(mad.table.summ.dq2, mad.globals[\"qpyb1\"], atol=1e-01)\n\n        df = mad.table.twiss.dframe()\n        for my_ip in [1, 2, 5, 8]:\n            assert np.isclose(df.loc[f\"ip{my_ip}\"].betx, mad.globals[f\"betx_IP{my_ip}\"], rtol=1e-02)\n            assert np.isclose(df.loc[f\"ip{my_ip}\"].bety, mad.globals[f\"bety_IP{my_ip}\"], rtol=1e-02)\n\n        assert df[\"x\"].std() &lt; 1e-6\n        assert df[\"y\"].std() &lt; 1e-6\n    except AssertionError:\n        logging.warning(\"WARNING: Some sanity checks have failed during the madx lattice check\")\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/hllhc16/orbit_correction.html","title":"orbit_correction","text":""},{"location":"reference/study_da/generate/version_specific_files/hllhc16/orbit_correction.html#study_da.generate.version_specific_files.hllhc16.orbit_correction.generate_orbit_correction_setup","title":"<code>generate_orbit_correction_setup()</code>","text":"<p>Return a dictionary with the setup for the orbit correction.</p> Source code in <code>study_da/generate/version_specific_files/hllhc16/orbit_correction.py</code> <pre><code>def generate_orbit_correction_setup() -&gt; dict:\n    \"\"\"Return a dictionary with the setup for the orbit correction.\"\"\"\n    return {\n        \"lhcb1\": {\n            \"IR1 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.r8.b1\",\n                end=\"e.ds.l1.b1\",\n                vary=(\n                    \"corr_co_acbh14.l1b1\",\n                    \"corr_co_acbh12.l1b1\",\n                    \"corr_co_acbv15.l1b1\",\n                    \"corr_co_acbv13.l1b1\",\n                ),\n                targets=(\"e.ds.l1.b1\",),\n            ),\n            \"IR1 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r1.b1\",\n                end=\"s.ds.l2.b1\",\n                vary=(\n                    \"corr_co_acbh13.r1b1\",\n                    \"corr_co_acbh15.r1b1\",\n                    \"corr_co_acbv12.r1b1\",\n                    \"corr_co_acbv14.r1b1\",\n                ),\n                targets=(\"s.ds.l2.b1\",),\n            ),\n            \"IR5 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.r4.b1\",\n                end=\"e.ds.l5.b1\",\n                vary=(\n                    \"corr_co_acbh14.l5b1\",\n                    \"corr_co_acbh12.l5b1\",\n                    \"corr_co_acbv15.l5b1\",\n                    \"corr_co_acbv13.l5b1\",\n                ),\n                targets=(\"e.ds.l5.b1\",),\n            ),\n            \"IR5 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r5.b1\",\n                end=\"s.ds.l6.b1\",\n                vary=(\n                    \"corr_co_acbh13.r5b1\",\n                    \"corr_co_acbh15.r5b1\",\n                    \"corr_co_acbv12.r5b1\",\n                    \"corr_co_acbv14.r5b1\",\n                ),\n                targets=(\"s.ds.l6.b1\",),\n            ),\n            \"IP1\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l1.b1\",\n                end=\"s.ds.r1.b1\",\n                vary=(\n                    \"corr_co_acbch6.l1b1\",\n                    \"corr_co_acbcv5.l1b1\",\n                    \"corr_co_acbch5.r1b1\",\n                    \"corr_co_acbcv6.r1b1\",\n                    \"corr_co_acbyhs4.l1b1\",\n                    \"corr_co_acbyhs4.r1b1\",\n                    \"corr_co_acbyvs4.l1b1\",\n                    \"corr_co_acbyvs4.r1b1\",\n                ),\n                targets=(\"ip1\", \"s.ds.r1.b1\"),\n            ),\n            \"IP2\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l2.b1\",\n                end=\"s.ds.r2.b1\",\n                vary=(\n                    \"corr_co_acbyhs5.l2b1\",\n                    \"corr_co_acbchs5.r2b1\",\n                    \"corr_co_acbyvs5.l2b1\",\n                    \"corr_co_acbcvs5.r2b1\",\n                    \"corr_co_acbyhs4.l2b1\",\n                    \"corr_co_acbyhs4.r2b1\",\n                    \"corr_co_acbyvs4.l2b1\",\n                    \"corr_co_acbyvs4.r2b1\",\n                ),\n                targets=(\"ip2\", \"s.ds.r2.b1\"),\n            ),\n            \"IP5\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l5.b1\",\n                end=\"s.ds.r5.b1\",\n                vary=(\n                    \"corr_co_acbch6.l5b1\",\n                    \"corr_co_acbcv5.l5b1\",\n                    \"corr_co_acbch5.r5b1\",\n                    \"corr_co_acbcv6.r5b1\",\n                    \"corr_co_acbyhs4.l5b1\",\n                    \"corr_co_acbyhs4.r5b1\",\n                    \"corr_co_acbyvs4.l5b1\",\n                    \"corr_co_acbyvs4.r5b1\",\n                ),\n                targets=(\"ip5\", \"s.ds.r5.b1\"),\n            ),\n            \"IP8\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l8.b1\",\n                end=\"s.ds.r8.b1\",\n                vary=(\n                    \"corr_co_acbch5.l8b1\",\n                    \"corr_co_acbyhs4.l8b1\",\n                    \"corr_co_acbyhs4.r8b1\",\n                    \"corr_co_acbyhs5.r8b1\",\n                    \"corr_co_acbcvs5.l8b1\",\n                    \"corr_co_acbyvs4.l8b1\",\n                    \"corr_co_acbyvs4.r8b1\",\n                    \"corr_co_acbyvs5.r8b1\",\n                ),\n                targets=(\"ip8\", \"s.ds.r8.b1\"),\n            ),\n        },\n        \"lhcb2\": {\n            \"IR1 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l1.b2\",\n                end=\"e.ds.r8.b2\",\n                vary=(\n                    \"corr_co_acbh13.l1b2\",\n                    \"corr_co_acbh15.l1b2\",\n                    \"corr_co_acbv12.l1b2\",\n                    \"corr_co_acbv14.l1b2\",\n                ),\n                targets=(\"e.ds.r8.b2\",),\n            ),\n            \"IR1 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.l2.b2\",\n                end=\"s.ds.r1.b2\",\n                vary=(\n                    \"corr_co_acbh12.r1b2\",\n                    \"corr_co_acbh14.r1b2\",\n                    \"corr_co_acbv13.r1b2\",\n                    \"corr_co_acbv15.r1b2\",\n                ),\n                targets=(\"s.ds.r1.b2\",),\n            ),\n            \"IR5 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l5.b2\",\n                end=\"e.ds.r4.b2\",\n                vary=(\n                    \"corr_co_acbh13.l5b2\",\n                    \"corr_co_acbh15.l5b2\",\n                    \"corr_co_acbv12.l5b2\",\n                    \"corr_co_acbv14.l5b2\",\n                ),\n                targets=(\"e.ds.r4.b2\",),\n            ),\n            \"IR5 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.l6.b2\",\n                end=\"s.ds.r5.b2\",\n                vary=(\n                    \"corr_co_acbh12.r5b2\",\n                    \"corr_co_acbh14.r5b2\",\n                    \"corr_co_acbv13.r5b2\",\n                    \"corr_co_acbv15.r5b2\",\n                ),\n                targets=(\"s.ds.r5.b2\",),\n            ),\n            \"IP1\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r1.b2\",\n                end=\"e.ds.l1.b2\",\n                vary=(\n                    \"corr_co_acbch6.r1b2\",\n                    \"corr_co_acbcv5.r1b2\",\n                    \"corr_co_acbch5.l1b2\",\n                    \"corr_co_acbcv6.l1b2\",\n                    \"corr_co_acbyhs4.l1b2\",\n                    \"corr_co_acbyhs4.r1b2\",\n                    \"corr_co_acbyvs4.l1b2\",\n                    \"corr_co_acbyvs4.r1b2\",\n                ),\n                targets=(\n                    \"ip1\",\n                    \"e.ds.l1.b2\",\n                ),\n            ),\n            \"IP2\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r2.b2\",\n                end=\"e.ds.l2.b2\",\n                vary=(\n                    \"corr_co_acbyhs5.l2b2\",\n                    \"corr_co_acbchs5.r2b2\",\n                    \"corr_co_acbyvs5.l2b2\",\n                    \"corr_co_acbcvs5.r2b2\",\n                    \"corr_co_acbyhs4.l2b2\",\n                    \"corr_co_acbyhs4.r2b2\",\n                    \"corr_co_acbyvs4.l2b2\",\n                    \"corr_co_acbyvs4.r2b2\",\n                ),\n                targets=(\"ip2\", \"e.ds.l2.b2\"),\n            ),\n            \"IP5\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r5.b2\",\n                end=\"e.ds.l5.b2\",\n                vary=(\n                    \"corr_co_acbch6.r5b2\",\n                    \"corr_co_acbcv5.r5b2\",\n                    \"corr_co_acbch5.l5b2\",\n                    \"corr_co_acbcv6.l5b2\",\n                    \"corr_co_acbyhs4.l5b2\",\n                    \"corr_co_acbyhs4.r5b2\",\n                    \"corr_co_acbyvs4.l5b2\",\n                    \"corr_co_acbyvs4.r5b2\",\n                ),\n                targets=(\n                    \"ip5\",\n                    \"e.ds.l5.b2\",\n                ),\n            ),\n            \"IP8\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r8.b2\",\n                end=\"e.ds.l8.b2\",\n                vary=(\n                    \"corr_co_acbchs5.l8b2\",\n                    \"corr_co_acbyhs5.r8b2\",\n                    \"corr_co_acbcvs5.l8b2\",\n                    \"corr_co_acbyvs5.r8b2\",\n                    \"corr_co_acbyhs4.l8b2\",\n                    \"corr_co_acbyhs4.r8b2\",\n                    \"corr_co_acbyvs4.l8b2\",\n                    \"corr_co_acbyvs4.r8b2\",\n                ),\n                targets=(\n                    \"ip8\",\n                    \"e.ds.l8.b2\",\n                ),\n            ),\n        },\n    }\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/runIII/index.html","title":"runIII","text":""},{"location":"reference/study_da/generate/version_specific_files/runIII/index.html#study_da.generate.version_specific_files.runIII.generate_orbit_correction_setup","title":"<code>generate_orbit_correction_setup()</code>","text":"<p>Return a dictionary with the setup for the orbit correction.</p> Source code in <code>study_da/generate/version_specific_files/hllhc16/orbit_correction.py</code> <pre><code>def generate_orbit_correction_setup() -&gt; dict:\n    \"\"\"Return a dictionary with the setup for the orbit correction.\"\"\"\n    return {\n        \"lhcb1\": {\n            \"IR1 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.r8.b1\",\n                end=\"e.ds.l1.b1\",\n                vary=(\n                    \"corr_co_acbh14.l1b1\",\n                    \"corr_co_acbh12.l1b1\",\n                    \"corr_co_acbv15.l1b1\",\n                    \"corr_co_acbv13.l1b1\",\n                ),\n                targets=(\"e.ds.l1.b1\",),\n            ),\n            \"IR1 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r1.b1\",\n                end=\"s.ds.l2.b1\",\n                vary=(\n                    \"corr_co_acbh13.r1b1\",\n                    \"corr_co_acbh15.r1b1\",\n                    \"corr_co_acbv12.r1b1\",\n                    \"corr_co_acbv14.r1b1\",\n                ),\n                targets=(\"s.ds.l2.b1\",),\n            ),\n            \"IR5 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.r4.b1\",\n                end=\"e.ds.l5.b1\",\n                vary=(\n                    \"corr_co_acbh14.l5b1\",\n                    \"corr_co_acbh12.l5b1\",\n                    \"corr_co_acbv15.l5b1\",\n                    \"corr_co_acbv13.l5b1\",\n                ),\n                targets=(\"e.ds.l5.b1\",),\n            ),\n            \"IR5 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r5.b1\",\n                end=\"s.ds.l6.b1\",\n                vary=(\n                    \"corr_co_acbh13.r5b1\",\n                    \"corr_co_acbh15.r5b1\",\n                    \"corr_co_acbv12.r5b1\",\n                    \"corr_co_acbv14.r5b1\",\n                ),\n                targets=(\"s.ds.l6.b1\",),\n            ),\n            \"IP1\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l1.b1\",\n                end=\"s.ds.r1.b1\",\n                vary=(\n                    \"corr_co_acbch6.l1b1\",\n                    \"corr_co_acbcv5.l1b1\",\n                    \"corr_co_acbch5.r1b1\",\n                    \"corr_co_acbcv6.r1b1\",\n                    \"corr_co_acbyhs4.l1b1\",\n                    \"corr_co_acbyhs4.r1b1\",\n                    \"corr_co_acbyvs4.l1b1\",\n                    \"corr_co_acbyvs4.r1b1\",\n                ),\n                targets=(\"ip1\", \"s.ds.r1.b1\"),\n            ),\n            \"IP2\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l2.b1\",\n                end=\"s.ds.r2.b1\",\n                vary=(\n                    \"corr_co_acbyhs5.l2b1\",\n                    \"corr_co_acbchs5.r2b1\",\n                    \"corr_co_acbyvs5.l2b1\",\n                    \"corr_co_acbcvs5.r2b1\",\n                    \"corr_co_acbyhs4.l2b1\",\n                    \"corr_co_acbyhs4.r2b1\",\n                    \"corr_co_acbyvs4.l2b1\",\n                    \"corr_co_acbyvs4.r2b1\",\n                ),\n                targets=(\"ip2\", \"s.ds.r2.b1\"),\n            ),\n            \"IP5\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l5.b1\",\n                end=\"s.ds.r5.b1\",\n                vary=(\n                    \"corr_co_acbch6.l5b1\",\n                    \"corr_co_acbcv5.l5b1\",\n                    \"corr_co_acbch5.r5b1\",\n                    \"corr_co_acbcv6.r5b1\",\n                    \"corr_co_acbyhs4.l5b1\",\n                    \"corr_co_acbyhs4.r5b1\",\n                    \"corr_co_acbyvs4.l5b1\",\n                    \"corr_co_acbyvs4.r5b1\",\n                ),\n                targets=(\"ip5\", \"s.ds.r5.b1\"),\n            ),\n            \"IP8\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l8.b1\",\n                end=\"s.ds.r8.b1\",\n                vary=(\n                    \"corr_co_acbch5.l8b1\",\n                    \"corr_co_acbyhs4.l8b1\",\n                    \"corr_co_acbyhs4.r8b1\",\n                    \"corr_co_acbyhs5.r8b1\",\n                    \"corr_co_acbcvs5.l8b1\",\n                    \"corr_co_acbyvs4.l8b1\",\n                    \"corr_co_acbyvs4.r8b1\",\n                    \"corr_co_acbyvs5.r8b1\",\n                ),\n                targets=(\"ip8\", \"s.ds.r8.b1\"),\n            ),\n        },\n        \"lhcb2\": {\n            \"IR1 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l1.b2\",\n                end=\"e.ds.r8.b2\",\n                vary=(\n                    \"corr_co_acbh13.l1b2\",\n                    \"corr_co_acbh15.l1b2\",\n                    \"corr_co_acbv12.l1b2\",\n                    \"corr_co_acbv14.l1b2\",\n                ),\n                targets=(\"e.ds.r8.b2\",),\n            ),\n            \"IR1 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.l2.b2\",\n                end=\"s.ds.r1.b2\",\n                vary=(\n                    \"corr_co_acbh12.r1b2\",\n                    \"corr_co_acbh14.r1b2\",\n                    \"corr_co_acbv13.r1b2\",\n                    \"corr_co_acbv15.r1b2\",\n                ),\n                targets=(\"s.ds.r1.b2\",),\n            ),\n            \"IR5 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l5.b2\",\n                end=\"e.ds.r4.b2\",\n                vary=(\n                    \"corr_co_acbh13.l5b2\",\n                    \"corr_co_acbh15.l5b2\",\n                    \"corr_co_acbv12.l5b2\",\n                    \"corr_co_acbv14.l5b2\",\n                ),\n                targets=(\"e.ds.r4.b2\",),\n            ),\n            \"IR5 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.l6.b2\",\n                end=\"s.ds.r5.b2\",\n                vary=(\n                    \"corr_co_acbh12.r5b2\",\n                    \"corr_co_acbh14.r5b2\",\n                    \"corr_co_acbv13.r5b2\",\n                    \"corr_co_acbv15.r5b2\",\n                ),\n                targets=(\"s.ds.r5.b2\",),\n            ),\n            \"IP1\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r1.b2\",\n                end=\"e.ds.l1.b2\",\n                vary=(\n                    \"corr_co_acbch6.r1b2\",\n                    \"corr_co_acbcv5.r1b2\",\n                    \"corr_co_acbch5.l1b2\",\n                    \"corr_co_acbcv6.l1b2\",\n                    \"corr_co_acbyhs4.l1b2\",\n                    \"corr_co_acbyhs4.r1b2\",\n                    \"corr_co_acbyvs4.l1b2\",\n                    \"corr_co_acbyvs4.r1b2\",\n                ),\n                targets=(\n                    \"ip1\",\n                    \"e.ds.l1.b2\",\n                ),\n            ),\n            \"IP2\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r2.b2\",\n                end=\"e.ds.l2.b2\",\n                vary=(\n                    \"corr_co_acbyhs5.l2b2\",\n                    \"corr_co_acbchs5.r2b2\",\n                    \"corr_co_acbyvs5.l2b2\",\n                    \"corr_co_acbcvs5.r2b2\",\n                    \"corr_co_acbyhs4.l2b2\",\n                    \"corr_co_acbyhs4.r2b2\",\n                    \"corr_co_acbyvs4.l2b2\",\n                    \"corr_co_acbyvs4.r2b2\",\n                ),\n                targets=(\"ip2\", \"e.ds.l2.b2\"),\n            ),\n            \"IP5\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r5.b2\",\n                end=\"e.ds.l5.b2\",\n                vary=(\n                    \"corr_co_acbch6.r5b2\",\n                    \"corr_co_acbcv5.r5b2\",\n                    \"corr_co_acbch5.l5b2\",\n                    \"corr_co_acbcv6.l5b2\",\n                    \"corr_co_acbyhs4.l5b2\",\n                    \"corr_co_acbyhs4.r5b2\",\n                    \"corr_co_acbyvs4.l5b2\",\n                    \"corr_co_acbyvs4.r5b2\",\n                ),\n                targets=(\n                    \"ip5\",\n                    \"e.ds.l5.b2\",\n                ),\n            ),\n            \"IP8\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r8.b2\",\n                end=\"e.ds.l8.b2\",\n                vary=(\n                    \"corr_co_acbchs5.l8b2\",\n                    \"corr_co_acbyhs5.r8b2\",\n                    \"corr_co_acbcvs5.l8b2\",\n                    \"corr_co_acbyvs5.r8b2\",\n                    \"corr_co_acbyhs4.l8b2\",\n                    \"corr_co_acbyhs4.r8b2\",\n                    \"corr_co_acbyvs4.l8b2\",\n                    \"corr_co_acbyvs4.r8b2\",\n                ),\n                targets=(\n                    \"ip8\",\n                    \"e.ds.l8.b2\",\n                ),\n            ),\n        },\n    }\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/runIII/optics_specific_tools.html","title":"optics_specific_tools","text":""},{"location":"reference/study_da/generate/version_specific_files/runIII/optics_specific_tools.html#study_da.generate.version_specific_files.runIII.optics_specific_tools.apply_optics","title":"<code>apply_optics(mad, optics_file)</code>","text":"<p>Apply the optics to the MAD-X model.</p> <p>Parameters:</p> Name Type Description Default <code>mad</code> <code>Madx</code> <p>The MAD-X object used to build the sequence.</p> required <code>optics_file</code> <code>str</code> <p>The path to the optics file to apply.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/runIII/optics_specific_tools.py</code> <pre><code>def apply_optics(mad: Madx, optics_file: str) -&gt; None:\n    \"\"\"Apply the optics to the MAD-X model.\n\n    Args:\n        mad (Madx): The MAD-X object used to build the sequence.\n        optics_file (str): The path to the optics file to apply.\n\n    Returns:\n        None\n    \"\"\"\n    mad.call(optics_file)\n    # A knob redefinition\n    mad.input(\"on_alice := on_alice_normalized * 7000./nrj;\")\n    mad.input(\"on_lhcb := on_lhcb_normalized * 7000./nrj;\")\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/runIII/optics_specific_tools.html#study_da.generate.version_specific_files.runIII.optics_specific_tools.build_sequence","title":"<code>build_sequence(mad, mylhcbeam, beam_config, ignore_cycling=False, slice_factor=4, BFPP=False)</code>","text":"<p>Build the sequence for the (HL-)LHC, for a given beam.</p> <p>Parameters:</p> Name Type Description Default <code>mad</code> <code>Madx</code> <p>The MAD-X object used to build the sequence.</p> required <code>mylhcbeam</code> <code>int</code> <p>The beam number (1, 2 or 4).</p> required <code>beam_config</code> <code>dict[str, Any]</code> <p>The configuration of the beam from the configuration file.</p> required <code>ignore_cycling</code> <code>bool</code> <p>Whether to ignore cycling to have IP3 at position s=0. Defaults to False.</p> <code>False</code> <code>slice_factor</code> <code>int | None</code> <p>The slice factor if optic is not thin. Defaults to 4.</p> <code>4</code> <code>BFPP</code> <code>bool</code> <p>Whether to use the BFPP knob. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/runIII/optics_specific_tools.py</code> <pre><code>def build_sequence(\n    mad: Madx,\n    mylhcbeam: int,\n    beam_config: dict[str, Any],\n    ignore_cycling: bool = False,\n    slice_factor: int | None = 4,\n    BFPP: bool = False,\n) -&gt; None:\n    \"\"\"Build the sequence for the (HL-)LHC, for a given beam.\n\n    Args:\n        mad (Madx): The MAD-X object used to build the sequence.\n        mylhcbeam (int): The beam number (1, 2 or 4).\n        beam_config (dict[str, Any]): The configuration of the beam from the configuration file.\n        ignore_cycling (bool, optional): Whether to ignore cycling to have IP3 at position s=0.\n            Defaults to False.\n        slice_factor (int | None, optional): The slice factor if optic is not thin. Defaults to 4.\n        BFPP (bool, optional): Whether to use the BFPP knob. Defaults to False.\n\n    Returns:\n        None\n    \"\"\"  # Select beam\n    mad.input(f\"mylhcbeam = {mylhcbeam}\")\n\n    mad.input(\"\"\"\n      ! Get the toolkit\n      call,file=\n        \"acc-models-lhc/toolkit/macro.madx\";\n      \"\"\")\n\n    mad.input(\"\"\"\n      ! Build sequence\n      option, -echo,-warn,-info;\n      if (mylhcbeam==4){\n        call,file=\"acc-models-lhc/lhc_acc-models-lhc_b4.seq\";\n      } else {\n        call,file=\"acc-models-lhc/lhc_acc-models-lhc.seq\";\n      };\n      option, -echo, warn,-info;\n      \"\"\")\n\n    # Redefine macro for myslice\n    if slice_factor is not None:\n        my_slice(mad, slice_factor=slice_factor)\n\n    # Slice nominal sequence\n    mad.input(\"exec, myslice;\")\n\n    mad.input(\"\"\"\n    nrj=6800;\n    beam,particle=proton,sequence=lhcb1,energy=nrj,npart=1.15E11,sige=4.5e-4;\n    beam,particle=proton,sequence=lhcb2,energy=nrj,bv = -1,npart=1.15E11,sige=4.5e-4;\n    \"\"\")\n\n    if not ignore_cycling:\n        mad.input(\"\"\"\n        !Cycling w.r.t. to IP3 (mandatory to find closed orbit in collision in the presence of errors)\n        if (mylhcbeam&lt;3){\n        seqedit, sequence=lhcb1; flatten; cycle, start=IP3; flatten; endedit;\n        };\n        seqedit, sequence=lhcb2; flatten; cycle, start=IP3; flatten; endedit;\n        \"\"\")\n\n    mad.input(\"\"\"\n        ! Set twiss formats for MAD-X parts (macro from opt. toolkit)\n        exec, twiss_opt;\n        \"\"\")\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/runIII/optics_specific_tools.html#study_da.generate.version_specific_files.runIII.optics_specific_tools.check_madx_lattices","title":"<code>check_madx_lattices(mad)</code>","text":"<p>Check the consistency of the MAD-X lattice for the (HL-)LHC.</p> <p>Parameters:</p> Name Type Description Default <code>mad</code> <code>Madx</code> <p>The MAD-X object used to build the sequence.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/runIII/optics_specific_tools.py</code> <pre><code>def check_madx_lattices(mad: Madx) -&gt; None:  # sourcery skip: extract-method\n    \"\"\"Check the consistency of the MAD-X lattice for the (HL-)LHC.\n\n    Args:\n        mad (Madx): The MAD-X object used to build the sequence.\n\n    Returns:\n        None\n    \"\"\"\n    assert mad.globals[\"qxb1\"] == mad.globals[\"qxb2\"]\n    assert mad.globals[\"qyb1\"] == mad.globals[\"qyb2\"]\n    assert mad.globals[\"qpxb1\"] == mad.globals[\"qpxb2\"]\n    assert mad.globals[\"qpyb1\"] == mad.globals[\"qpyb2\"]\n\n    assert np.isclose(mad.table.summ.q1, mad.globals[\"qxb1\"], atol=1e-05)\n    assert np.isclose(mad.table.summ.q2, mad.globals[\"qyb1\"], atol=1e-05)\n\n    try:\n        assert np.isclose(mad.table.summ.dq1, mad.globals[\"qpxb1\"], atol=1e-01)\n        assert np.isclose(mad.table.summ.dq2, mad.globals[\"qpyb1\"], atol=1e-01)\n\n        df = mad.table.twiss.dframe()\n        for my_ip in [1, 2, 5, 8]:\n            assert np.isclose(df.loc[f\"ip{my_ip}\"].betx, mad.globals[f\"betx_IP{my_ip}\"], rtol=1e-03)\n            assert np.isclose(df.loc[f\"ip{my_ip}\"].bety, mad.globals[f\"bety_IP{my_ip}\"], rtol=1e-03)\n\n        assert df[\"x\"].std() &lt; 1e-8\n        assert df[\"y\"].std() &lt; 1e-8\n    except AssertionError:\n        logging.warning(\"WARNING: Some sanity checks have failed during the madx lattice check\")\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/runIII/optics_specific_tools.html#study_da.generate.version_specific_files.runIII.optics_specific_tools.my_slice","title":"<code>my_slice(mad, slice_factor=2)</code>","text":"<p>Redefine the macro myslice for the LHC, to make a sequence thin.</p> <p>Parameters:</p> Name Type Description Default <code>mad</code> <code>Madx</code> <p>The MAD-X object used to build the sequence.</p> required <code>slice_factor</code> <code>int</code> <p>The slice factor. Defaults to 2.</p> <code>2</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/runIII/optics_specific_tools.py</code> <pre><code>def my_slice(mad: Madx, slice_factor: int = 2) -&gt; None:\n    \"\"\"Redefine the macro myslice for the LHC, to make a sequence thin.\n\n    Args:\n        mad (Madx): The MAD-X object used to build the sequence.\n        slice_factor (int, optional): The slice factor. Defaults to 2.\n\n    Returns:\n        None\n    \"\"\"\n    mad.input(f\"slicefactor = {slice_factor};\")\n    mad.input(\"\"\"\n        myslice: macro = {\n        if (MBX.4L2-&gt;l&gt;0) {\n          select, flag=makethin, clear;\n          select, flag=makethin, class=mb, slice=2;\n          select, flag=makethin, class=mq, slice=2 * slicefactor;\n          select, flag=makethin, class=mqxa,  slice=16 * slicefactor;  !old triplet\n          select, flag=makethin, class=mqxb,  slice=16 * slicefactor;  !old triplet\n          select, flag=makethin, class=mqxc,  slice=16 * slicefactor;  !new mqxa (q1,q3)\n          select, flag=makethin, class=mqxd,  slice=16 * slicefactor;  !new mqxb (q2a,q2b)\n          select, flag=makethin, class=mqxfa, slice=16 * slicefactor;  !new (q1,q3 v1.1)\n          select, flag=makethin, class=mqxfb, slice=16 * slicefactor;  !new (q2a,q2b v1.1)\n          select, flag=makethin, class=mbxa,  slice=4;   !new d1\n          select, flag=makethin, class=mbxf,  slice=4;   !new d1 (v1.1)\n          select, flag=makethin, class=mbrd,  slice=4;   !new d2 (if needed)\n          select, flag=makethin, class=mqyy,  slice=4 * slicefactor;;   !new q4\n          select, flag=makethin, class=mqyl,  slice=4 * slicefactor;;   !new q5\n          select, flag=makethin, class=mbh,   slice=4;   !11T dipoles\n          select, flag=makethin, pattern=mbx\\.,    slice=4;\n          select, flag=makethin, pattern=mbrb\\.,   slice=4;\n          select, flag=makethin, pattern=mbrc\\.,   slice=4;\n          select, flag=makethin, pattern=mbrs\\.,   slice=4;\n          select, flag=makethin, pattern=mbh\\.,    slice=4;\n          select, flag=makethin, pattern=mqwa\\.,   slice=4 * slicefactor;\n          select, flag=makethin, pattern=mqwb\\.,   slice=4 * slicefactor;\n          select, flag=makethin, pattern=mqy\\.,    slice=4 * slicefactor;\n          select, flag=makethin, pattern=mqm\\.,    slice=4 * slicefactor;\n          select, flag=makethin, pattern=mqmc\\.,   slice=4 * slicefactor;\n          select, flag=makethin, pattern=mqml\\.,   slice=4 * slicefactor;\n          select, flag=makethin, pattern=mqtlh\\.,  slice=2 * slicefactor;\n          select, flag=makethin, pattern=mqtli\\.,  slice=2 * slicefactor;\n          select, flag=makethin, pattern=mqt\\.  ,  slice=2 * slicefactor;\n          !thin lens\n          option rbarc=false; beam;\n          use,sequence=lhcb1; makethin,sequence=lhcb1,makedipedge=true,style=teapot;\n          use,sequence=lhcb2; makethin,sequence=lhcb2,makedipedge=true,style=teapot;\n          option rbarc=true;\n        } else {\n          print, text=\"Sequence is already thin\";\n        };\n          is_thin=1;\n        };\n    \"\"\")\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/runIII_ions/index.html","title":"runIII_ions","text":""},{"location":"reference/study_da/generate/version_specific_files/runIII_ions/index.html#study_da.generate.version_specific_files.runIII_ions.generate_orbit_correction_setup","title":"<code>generate_orbit_correction_setup()</code>","text":"<p>Return a dictionary with the setup for the orbit correction.</p> Source code in <code>study_da/generate/version_specific_files/hllhc16/orbit_correction.py</code> <pre><code>def generate_orbit_correction_setup() -&gt; dict:\n    \"\"\"Return a dictionary with the setup for the orbit correction.\"\"\"\n    return {\n        \"lhcb1\": {\n            \"IR1 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.r8.b1\",\n                end=\"e.ds.l1.b1\",\n                vary=(\n                    \"corr_co_acbh14.l1b1\",\n                    \"corr_co_acbh12.l1b1\",\n                    \"corr_co_acbv15.l1b1\",\n                    \"corr_co_acbv13.l1b1\",\n                ),\n                targets=(\"e.ds.l1.b1\",),\n            ),\n            \"IR1 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r1.b1\",\n                end=\"s.ds.l2.b1\",\n                vary=(\n                    \"corr_co_acbh13.r1b1\",\n                    \"corr_co_acbh15.r1b1\",\n                    \"corr_co_acbv12.r1b1\",\n                    \"corr_co_acbv14.r1b1\",\n                ),\n                targets=(\"s.ds.l2.b1\",),\n            ),\n            \"IR5 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.r4.b1\",\n                end=\"e.ds.l5.b1\",\n                vary=(\n                    \"corr_co_acbh14.l5b1\",\n                    \"corr_co_acbh12.l5b1\",\n                    \"corr_co_acbv15.l5b1\",\n                    \"corr_co_acbv13.l5b1\",\n                ),\n                targets=(\"e.ds.l5.b1\",),\n            ),\n            \"IR5 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r5.b1\",\n                end=\"s.ds.l6.b1\",\n                vary=(\n                    \"corr_co_acbh13.r5b1\",\n                    \"corr_co_acbh15.r5b1\",\n                    \"corr_co_acbv12.r5b1\",\n                    \"corr_co_acbv14.r5b1\",\n                ),\n                targets=(\"s.ds.l6.b1\",),\n            ),\n            \"IP1\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l1.b1\",\n                end=\"s.ds.r1.b1\",\n                vary=(\n                    \"corr_co_acbch6.l1b1\",\n                    \"corr_co_acbcv5.l1b1\",\n                    \"corr_co_acbch5.r1b1\",\n                    \"corr_co_acbcv6.r1b1\",\n                    \"corr_co_acbyhs4.l1b1\",\n                    \"corr_co_acbyhs4.r1b1\",\n                    \"corr_co_acbyvs4.l1b1\",\n                    \"corr_co_acbyvs4.r1b1\",\n                ),\n                targets=(\"ip1\", \"s.ds.r1.b1\"),\n            ),\n            \"IP2\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l2.b1\",\n                end=\"s.ds.r2.b1\",\n                vary=(\n                    \"corr_co_acbyhs5.l2b1\",\n                    \"corr_co_acbchs5.r2b1\",\n                    \"corr_co_acbyvs5.l2b1\",\n                    \"corr_co_acbcvs5.r2b1\",\n                    \"corr_co_acbyhs4.l2b1\",\n                    \"corr_co_acbyhs4.r2b1\",\n                    \"corr_co_acbyvs4.l2b1\",\n                    \"corr_co_acbyvs4.r2b1\",\n                ),\n                targets=(\"ip2\", \"s.ds.r2.b1\"),\n            ),\n            \"IP5\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l5.b1\",\n                end=\"s.ds.r5.b1\",\n                vary=(\n                    \"corr_co_acbch6.l5b1\",\n                    \"corr_co_acbcv5.l5b1\",\n                    \"corr_co_acbch5.r5b1\",\n                    \"corr_co_acbcv6.r5b1\",\n                    \"corr_co_acbyhs4.l5b1\",\n                    \"corr_co_acbyhs4.r5b1\",\n                    \"corr_co_acbyvs4.l5b1\",\n                    \"corr_co_acbyvs4.r5b1\",\n                ),\n                targets=(\"ip5\", \"s.ds.r5.b1\"),\n            ),\n            \"IP8\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l8.b1\",\n                end=\"s.ds.r8.b1\",\n                vary=(\n                    \"corr_co_acbch5.l8b1\",\n                    \"corr_co_acbyhs4.l8b1\",\n                    \"corr_co_acbyhs4.r8b1\",\n                    \"corr_co_acbyhs5.r8b1\",\n                    \"corr_co_acbcvs5.l8b1\",\n                    \"corr_co_acbyvs4.l8b1\",\n                    \"corr_co_acbyvs4.r8b1\",\n                    \"corr_co_acbyvs5.r8b1\",\n                ),\n                targets=(\"ip8\", \"s.ds.r8.b1\"),\n            ),\n        },\n        \"lhcb2\": {\n            \"IR1 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l1.b2\",\n                end=\"e.ds.r8.b2\",\n                vary=(\n                    \"corr_co_acbh13.l1b2\",\n                    \"corr_co_acbh15.l1b2\",\n                    \"corr_co_acbv12.l1b2\",\n                    \"corr_co_acbv14.l1b2\",\n                ),\n                targets=(\"e.ds.r8.b2\",),\n            ),\n            \"IR1 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.l2.b2\",\n                end=\"s.ds.r1.b2\",\n                vary=(\n                    \"corr_co_acbh12.r1b2\",\n                    \"corr_co_acbh14.r1b2\",\n                    \"corr_co_acbv13.r1b2\",\n                    \"corr_co_acbv15.r1b2\",\n                ),\n                targets=(\"s.ds.r1.b2\",),\n            ),\n            \"IR5 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l5.b2\",\n                end=\"e.ds.r4.b2\",\n                vary=(\n                    \"corr_co_acbh13.l5b2\",\n                    \"corr_co_acbh15.l5b2\",\n                    \"corr_co_acbv12.l5b2\",\n                    \"corr_co_acbv14.l5b2\",\n                ),\n                targets=(\"e.ds.r4.b2\",),\n            ),\n            \"IR5 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.l6.b2\",\n                end=\"s.ds.r5.b2\",\n                vary=(\n                    \"corr_co_acbh12.r5b2\",\n                    \"corr_co_acbh14.r5b2\",\n                    \"corr_co_acbv13.r5b2\",\n                    \"corr_co_acbv15.r5b2\",\n                ),\n                targets=(\"s.ds.r5.b2\",),\n            ),\n            \"IP1\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r1.b2\",\n                end=\"e.ds.l1.b2\",\n                vary=(\n                    \"corr_co_acbch6.r1b2\",\n                    \"corr_co_acbcv5.r1b2\",\n                    \"corr_co_acbch5.l1b2\",\n                    \"corr_co_acbcv6.l1b2\",\n                    \"corr_co_acbyhs4.l1b2\",\n                    \"corr_co_acbyhs4.r1b2\",\n                    \"corr_co_acbyvs4.l1b2\",\n                    \"corr_co_acbyvs4.r1b2\",\n                ),\n                targets=(\n                    \"ip1\",\n                    \"e.ds.l1.b2\",\n                ),\n            ),\n            \"IP2\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r2.b2\",\n                end=\"e.ds.l2.b2\",\n                vary=(\n                    \"corr_co_acbyhs5.l2b2\",\n                    \"corr_co_acbchs5.r2b2\",\n                    \"corr_co_acbyvs5.l2b2\",\n                    \"corr_co_acbcvs5.r2b2\",\n                    \"corr_co_acbyhs4.l2b2\",\n                    \"corr_co_acbyhs4.r2b2\",\n                    \"corr_co_acbyvs4.l2b2\",\n                    \"corr_co_acbyvs4.r2b2\",\n                ),\n                targets=(\"ip2\", \"e.ds.l2.b2\"),\n            ),\n            \"IP5\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r5.b2\",\n                end=\"e.ds.l5.b2\",\n                vary=(\n                    \"corr_co_acbch6.r5b2\",\n                    \"corr_co_acbcv5.r5b2\",\n                    \"corr_co_acbch5.l5b2\",\n                    \"corr_co_acbcv6.l5b2\",\n                    \"corr_co_acbyhs4.l5b2\",\n                    \"corr_co_acbyhs4.r5b2\",\n                    \"corr_co_acbyvs4.l5b2\",\n                    \"corr_co_acbyvs4.r5b2\",\n                ),\n                targets=(\n                    \"ip5\",\n                    \"e.ds.l5.b2\",\n                ),\n            ),\n            \"IP8\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r8.b2\",\n                end=\"e.ds.l8.b2\",\n                vary=(\n                    \"corr_co_acbchs5.l8b2\",\n                    \"corr_co_acbyhs5.r8b2\",\n                    \"corr_co_acbcvs5.l8b2\",\n                    \"corr_co_acbyvs5.r8b2\",\n                    \"corr_co_acbyhs4.l8b2\",\n                    \"corr_co_acbyhs4.r8b2\",\n                    \"corr_co_acbyvs4.l8b2\",\n                    \"corr_co_acbyvs4.r8b2\",\n                ),\n                targets=(\n                    \"ip8\",\n                    \"e.ds.l8.b2\",\n                ),\n            ),\n        },\n    }\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/runIII_ions/optics_specific_tools.html","title":"optics_specific_tools","text":""},{"location":"reference/study_da/generate/version_specific_files/runIII_ions/optics_specific_tools.html#study_da.generate.version_specific_files.runIII_ions.optics_specific_tools.apply_BFPP","title":"<code>apply_BFPP(mad)</code>","text":"<p>Apply the BFPP knob to the MAD-X model.</p> <p>Parameters:</p> Name Type Description Default <code>mad</code> <code>Madx</code> <p>The MAD-X object used to build the sequence.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/runIII_ions/optics_specific_tools.py</code> <pre><code>def apply_BFPP(mad: Madx) -&gt; None:\n    \"\"\"Apply the BFPP knob to the MAD-X model.\n\n    Args:\n        mad (Madx): The MAD-X object used to build the sequence.\n\n    Returns:\n        None\n    \"\"\"\n    mad.input(\"\"\"acbch8.r2b1        :=   6.336517325e-05 * ON_BFPP.R2 / 7.8;\n                acbch10.r2b1       :=   2.102863759e-05 * ON_BFPP.R2 / 7.8;\n                acbh12.r2b1        :=   4.404997133e-05 * ON_BFPP.R2 / 7.8;\n\n                acbch7.r1b1        :=   4.259479019e-06 * ON_BFPP.R1 / 2.5;\n                acbch9.r1b1        :=   1.794045373e-05 * ON_BFPP.R1 / 2.5;\n                acbh13.r1b1        :=   1.371178403e-05 * ON_BFPP.R1 / 2.5;\n\n                acbch7.r5b1        :=   2.153161387e-06 * ON_BFPP.R5 / 1.3;\n                acbch9.r5b1        :=   9.314782805e-06 * ON_BFPP.R5 / 1.3;\n                acbh13.r5b1        :=   7.12996247e-06 * ON_BFPP.R5 / 1.3;\n\n                acbch8.r8b1        :=   3.521812667e-05 * ON_BFPP.R8 / 4.6;\n                acbch10.r8b1       :=   1.064966564e-05 * ON_BFPP.R8 / 4.6;\n                acbh12.r8b1        :=   2.786990521e-05 * ON_BFPP.R8 / 4.6;\n            \"\"\")\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/runIII_ions/optics_specific_tools.html#study_da.generate.version_specific_files.runIII_ions.optics_specific_tools.apply_ir7_strengths","title":"<code>apply_ir7_strengths(mad)</code>","text":"<p>Apply the IR7 strengths fix to the MAD-X model.</p> <p>Parameters:</p> Name Type Description Default <code>mad</code> <code>Madx</code> <p>The MAD-X object used to build the sequence.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/runIII_ions/optics_specific_tools.py</code> <pre><code>def apply_ir7_strengths(mad: Madx) -&gt; None:\n    \"\"\"Apply the IR7 strengths fix to the MAD-X model.\n\n    Args:\n        mad (Madx): The MAD-X object used to build the sequence.\n\n    Returns:\n        None\n    \"\"\"\n    mad.input(\"\"\"!***IR7 Optics***\n            KQ4.LR7     :=    0.131382724100E-02 ;\n            KQT4.L7     :=    0.331689344000E-03 ;\n            KQT4.R7     :=    0.331689344000E-03 ;\n            KQ5.LR7     :=   -0.133553657300E-02 ;\n            KQT5.L7     :=    0.000000000000E+00 ;\n            KQT5.R7     :=    0.000000000000E+00 ;\n\n            !Beam1\n            KQ6.L7B1    :=    0.332380383100E-02 ;\n            KQ6.R7B1    :=   -0.281821059300E-02 ;\n            KQTL7.L7B1  :=    0.307231360100E-03 ;\n            KQTL7.R7B1  :=    0.411775382800E-02 ;\n            KQTL8.L7B1  :=    0.535631538200E-03 ;\n            KQTL8.R7B1  :=    0.180061251400E-02 ;\n            KQTL9.L7B1  :=    0.104649831600E-03 ;\n            KQTL9.R7B1  :=    0.316515736800E-02 ;\n            KQTL10.L7B1 :=    0.469149843300E-02 ;\n            KQTL10.R7B1 :=    0.234006504200E-03 ;\n            KQTL11.L7B1 :=    0.109300381500E-02 ;\n            KQTL11.R7B1 :=   -0.129517571700E-03 ;\n            KQT12.L7B1  :=    0.203869506000E-02 ;\n            KQT12.R7B1  :=    0.414855502900E-03 ;\n            KQT13.L7B1  :=   -0.647047560500E-03 ;\n            KQT13.R7B1  :=    0.163470209700E-03 ;\n\n            !Beam2\n            KQ6.L7B2    :=   -0.278052285800E-02 ;\n            KQ6.R7B2    :=    0.330261896100E-02 ;\n            KQTL7.L7B2  :=    0.391109869200E-02 ;\n            KQTL7.R7B2  :=    0.307913213400E-03 ;\n            KQTL8.L7B2  :=    0.141328062600E-02 ;\n            KQTL8.R7B2  :=    0.139274871000E-02 ;\n            KQTL9.L7B2  :=    0.363516060400E-02 ;\n            KQTL9.R7B2  :=    0.692028108000E-04 ;\n            KQTL10.L7B2 :=    0.156243369200E-03 ;\n            KQTL10.R7B2 :=    0.451207010600E-02 ;\n            KQTL11.L7B2 :=    0.360602594900E-03 ;\n            KQTL11.R7B2 :=    0.131920025500E-02 ;\n            KQT12.L7B2  :=   -0.705199531300E-03 ;\n            KQT12.R7B2  :=   -0.138620184600E-02 ;\n            KQT13.L7B2  :=   -0.606647736700E-03 ;\n            KQT13.R7B2  :=   -0.585571959400E-03 ;\"\"\")\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/runIII_ions/optics_specific_tools.html#study_da.generate.version_specific_files.runIII_ions.optics_specific_tools.apply_optics","title":"<code>apply_optics(mad, optics_file)</code>","text":"<p>Apply the optics to the MAD-X model.</p> <p>Parameters:</p> Name Type Description Default <code>mad</code> <code>Madx</code> <p>The MAD-X object used to build the sequence.</p> required <code>optics_file</code> <code>str</code> <p>The path to the optics file to apply.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/runIII_ions/optics_specific_tools.py</code> <pre><code>def apply_optics(mad: Madx, optics_file: str) -&gt; None:\n    \"\"\"Apply the optics to the MAD-X model.\n\n    Args:\n        mad (Madx): The MAD-X object used to build the sequence.\n        optics_file (str): The path to the optics file to apply.\n\n    Returns:\n        None\n    \"\"\"\n    mad.call(optics_file)\n    apply_ir7_strengths(mad)\n    mad.input(\"on_alice := on_alice_normalized * 7000. / nrj;\")\n    mad.input(\"on_lhcb := on_lhcb_normalized * 7000. / nrj;\")\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/runIII_ions/optics_specific_tools.html#study_da.generate.version_specific_files.runIII_ions.optics_specific_tools.build_sequence","title":"<code>build_sequence(mad, mylhcbeam, beam_config, ignore_cycling=False, slice_factor=8, BFPP=True)</code>","text":"<p>Build the sequence for the (HL-)LHC, for a given beam.</p> <p>Parameters:</p> Name Type Description Default <code>mad</code> <code>Madx</code> <p>The MAD-X object used to build the sequence.</p> required <code>mylhcbeam</code> <code>int</code> <p>The beam number (1, 2 or 4).</p> required <code>beam_config</code> <code>dict[str, Any]</code> <p>The configuration of the beam from the configuration file.</p> required <code>ignore_cycling</code> <code>bool</code> <p>Whether to ignore cycling to have IP3 at position s=0. Defaults to False.</p> <code>False</code> <code>slice_factor</code> <code>int | None</code> <p>The slice factor if optic is not thin. Defaults to 8.</p> <code>8</code> <code>BFPP</code> <code>bool</code> <p>Whether to use the BFPP knob. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/runIII_ions/optics_specific_tools.py</code> <pre><code>def build_sequence(\n    mad: Madx,\n    mylhcbeam: int,\n    beam_config: dict[str, Any],\n    ignore_cycling: bool = False,\n    slice_factor: int | None = 8,\n    BFPP: bool = True,\n) -&gt; None:\n    \"\"\"Build the sequence for the (HL-)LHC, for a given beam.\n\n    Args:\n        mad (Madx): The MAD-X object used to build the sequence.\n        mylhcbeam (int): The beam number (1, 2 or 4).\n        beam_config (dict[str, Any]): The configuration of the beam from the configuration file.\n        ignore_cycling (bool, optional): Whether to ignore cycling to have IP3 at position s=0.\n            Defaults to False.\n        slice_factor (int | None, optional): The slice factor if optic is not thin. Defaults to 8.\n        BFPP (bool, optional): Whether to use the BFPP knob. Defaults to True.\n\n    Returns:\n        None\n    \"\"\"\n    # Select beam\n    mad.input(f\"mylhcbeam = {mylhcbeam}\")\n    mad.input(\"option, -echo,warn, -info;\")\n\n    # optics dependent macros (for splitting)\n    mad.call(\"acc-models-lhc/runII/2018/toolkit/macro.madx\")\n\n    assert mylhcbeam in {1, 2, 4}, \"Invalid mylhcbeam (it should be in [1, 2, 4])\"\n\n    if mylhcbeam in {1, 2}:\n        mad.call(\"acc-models-lhc/runII/2018/lhc_as-built.seq\")\n    else:\n        mad.call(\"acc-models-lhc/runII/2018/lhcb4_as-built.seq\")\n\n    # New IR7 MQW layout and cabling\n    mad.call(\"acc-models-lhc/runIII/RunIII_dev/IR7-Run3seqedit.madx\")\n\n    # Makethin part\n    if slice_factor is not None and slice_factor &gt; 0:\n        # the variable in the macro is slice_factor\n        mad.input(f\"slicefactor={slice_factor};\")\n        mad.call(\"acc-models-lhc/runII/2018/toolkit/myslice.madx\")\n        if mylhcbeam == 1:\n            xm.attach_beam_to_sequence(mad.sequence[\"lhcb1\"], 1, beam_config[\"lhcb1\"])\n            xm.attach_beam_to_sequence(mad.sequence[\"lhcb2\"], 2, beam_config[\"lhcb2\"])\n        elif mylhcbeam == 4:\n            xm.attach_beam_to_sequence(mad.sequence[\"lhcb2\"], 4, beam_config[\"lhcb2\"])\n        else:\n            raise ValueError(\"Invalid mylhcbeam\")\n        # mad.beam()\n        for my_sequence in [\"lhcb1\", \"lhcb2\"]:\n            if my_sequence in list(mad.sequence):\n                mad.input(\n                    f\"use, sequence={my_sequence}; makethin,\"\n                    f\"sequence={my_sequence}, style=teapot, makedipedge=true;\"\n                )\n    else:\n        logging.warning(\"WARNING: The sequences are not thin!\")\n\n    # Cycling w.r.t. to IP3 (mandatory to find closed orbit in collision in the presence of errors)\n    if not ignore_cycling:\n        for my_sequence in [\"lhcb1\", \"lhcb2\"]:\n            if my_sequence in list(mad.sequence):\n                mad.input(\n                    f\"seqedit, sequence={my_sequence}; flatten;\"\n                    \"cycle, start=IP3; flatten; endedit;\"\n                )\n\n    # BFPP\n    if mylhcbeam == 1 and BFPP:\n        apply_BFPP(mad)\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/runIII_ions/optics_specific_tools.html#study_da.generate.version_specific_files.runIII_ions.optics_specific_tools.check_madx_lattices","title":"<code>check_madx_lattices(mad)</code>","text":"<p>Check the consistency of the MAD-X lattice for the (HL-)LHC.</p> <p>Parameters:</p> Name Type Description Default <code>mad</code> <code>Madx</code> <p>The MAD-X object used to build the sequence.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/runIII_ions/optics_specific_tools.py</code> <pre><code>def check_madx_lattices(mad: Madx) -&gt; None:\n    \"\"\"Check the consistency of the MAD-X lattice for the (HL-)LHC.\n\n    Args:\n        mad (Madx): The MAD-X object used to build the sequence.\n\n    Returns:\n        None\n    \"\"\"\n    assert mad.globals[\"qxb1\"] == mad.globals[\"qxb2\"]\n    assert mad.globals[\"qyb1\"] == mad.globals[\"qyb2\"]\n    assert mad.globals[\"qpxb1\"] == mad.globals[\"qpxb2\"]\n    assert mad.globals[\"qpyb1\"] == mad.globals[\"qpyb2\"]\n\n    try:\n        assert np.isclose(mad.table.summ.q1, mad.globals[\"qxb1\"], atol=1e-02)\n        assert np.isclose(mad.table.summ.q2, mad.globals[\"qyb1\"], atol=1e-02)\n        assert np.isclose(mad.table.summ.dq1, mad.globals[\"qpxb1\"], atol=5e-01)\n        assert np.isclose(mad.table.summ.dq2, mad.globals[\"qpyb1\"], atol=5e-01)\n    except AssertionError:\n        logging.warning(\n            \"Warning: some of the Qx, Qy, DQx, DQy values are not close to the expected ones\"\n        )\n\n    df = mad.table.twiss.dframe()\n    for my_ip in [1, 2, 5, 8]:\n        # assert np.isclose(df.loc[f\"ip{my_ip}\"].betx, mad.globals[f\"betx_IP{my_ip}\"], rtol=1e-02)\n        # assert np.isclose(df.loc[f\"ip{my_ip}\"].bety, mad.globals[f\"bety_IP{my_ip}\"], rtol=1e-02)\n        assert np.isclose(df.loc[f\"ip{my_ip}\"].betx, mad.globals[f\"betxIP{my_ip}b1\"], rtol=1e-02)\n        assert np.isclose(df.loc[f\"ip{my_ip}\"].bety, mad.globals[f\"betyIP{my_ip}b1\"], rtol=1e-02)\n\n    mad.twiss()\n    df = mad.table.twiss.dframe()\n\n    try:\n        assert df[\"x\"].std() &lt; 1e-6\n        assert df[\"y\"].std() &lt; 1e-6\n    except AssertionError:\n        logging.warning(\"Warning: the standard deviation of x and y are not close to zero\")\n</code></pre>"},{"location":"reference/study_da/plot/index.html","title":"plot","text":""},{"location":"reference/study_da/plot/index.html#study_da.plot.get_title_from_configuration","title":"<code>get_title_from_configuration(dataframe_data, ions=False, crossing_type=None, display_LHC_version=True, display_energy=True, display_bunch_index=True, display_CC_crossing=True, display_bunch_intensity=True, display_beta=True, display_crossing_IP_1=True, display_crossing_IP_2=True, display_crossing_IP_5=True, display_crossing_IP_8=True, display_bunch_length=True, display_polarity_IP_2_8=True, display_emittance=True, display_chromaticity=True, display_octupole_intensity=True, display_coupling=True, display_filling_scheme=True, display_horizontal_tune=None, display_vertical_tune=None, display_tune=True, display_luminosity_1=True, display_luminosity_2=True, display_luminosity_5=True, display_luminosity_8=True, display_PU_1=True, display_PU_2=True, display_PU_5=True, display_PU_8=True, display_number_of_turns=False)</code>","text":"<p>Generates a title string from the configuration data.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing configuration data.</p> required <code>ions</code> <code>bool</code> <p>Whether the beam is composed of ions. Defaults to False.</p> <code>False</code> <code>crossing_type</code> <code>str</code> <p>The type of crossing: 'vh' or 'hv'. Defaults to None, meaning it will try to be inferred from the optics file name. Back to 'hv' if not found.</p> <code>None</code> <code>display_betx_bety</code> <code>bool</code> <p>Whether to display the beta functions. Defaults to True.</p> required <code>display_LHC_version</code> <code>bool</code> <p>Whether to display the LHC version. Defaults to True.</p> <code>True</code> <code>display_energy</code> <code>bool</code> <p>Whether to display the energy. Defaults to True.</p> <code>True</code> <code>display_bunch_index</code> <code>bool</code> <p>Whether to display the bunch index. Defaults to True.</p> <code>True</code> <code>display_CC_crossing</code> <code>bool</code> <p>Whether to display the CC crossing. Defaults to True.</p> <code>True</code> <code>display_bunch_intensity</code> <code>bool</code> <p>Whether to display the bunch intensity. Defaults to True.</p> <code>True</code> <code>display_beta</code> <code>bool</code> <p>Whether to display the beta function. Defaults to True.</p> <code>True</code> <code>display_crossing_IP_1</code> <code>bool</code> <p>Whether to display the crossing at IP1. Defaults to True.</p> <code>True</code> <code>display_crossing_IP_2</code> <code>bool</code> <p>Whether to display the crossing at IP2. Defaults to True.</p> <code>True</code> <code>display_crossing_IP_5</code> <code>bool</code> <p>Whether to display the crossing at IP5. Defaults to True.</p> <code>True</code> <code>display_crossing_IP_8</code> <code>bool</code> <p>Whether to display the crossing at IP8. Defaults to True.</p> <code>True</code> <code>display_bunch_length</code> <code>bool</code> <p>Whether to display the bunch length. Defaults to True.</p> <code>True</code> <code>display_polarity_IP_2_8</code> <code>bool</code> <p>Whether to display the polarity at IP2 and IP8. Defaults to True.</p> <code>True</code> <code>display_emittance</code> <code>bool</code> <p>Whether to display the emittance. Defaults to True.</p> <code>True</code> <code>display_chromaticity</code> <code>bool</code> <p>Whether to display the chromaticity. Defaults to True.</p> <code>True</code> <code>display_octupole_intensity</code> <code>bool</code> <p>Whether to display the octupole intensity. Defaults to True.</p> <code>True</code> <code>display_coupling</code> <code>bool</code> <p>Whether to display the coupling. Defaults to True.</p> <code>True</code> <code>display_filling_scheme</code> <code>bool</code> <p>Whether to display the filling scheme. Defaults to True.</p> <code>True</code> <code>display_horizontal_tune</code> <code>bool</code> <p>Whether to display the horizontal tune. Defaults to None. Takes precedence over display_tune.</p> <code>None</code> <code>display_vertical_tune</code> <code>bool</code> <p>Whether to display the vertical tune. Defaults to None. Takes precedence over display_tune.</p> <code>None</code> <code>display_tune</code> <code>bool</code> <p>Whether to display the tune. Defaults to True.</p> <code>True</code> <code>display_luminosity_1</code> <code>bool</code> <p>Whether to display the luminosity at IP1. Defaults to True.</p> <code>True</code> <code>display_luminosity_2</code> <code>bool</code> <p>Whether to display the luminosity at IP2. Defaults to True.</p> <code>True</code> <code>display_luminosity_5</code> <code>bool</code> <p>Whether to display the luminosity at IP5. Defaults to True.</p> <code>True</code> <code>display_luminosity_8</code> <code>bool</code> <p>Whether to display the luminosity at IP8. Defaults to True.</p> <code>True</code> <code>display_PU_1</code> <code>bool</code> <p>Whether to display the PU at IP1. Defaults to True.</p> <code>True</code> <code>display_PU_2</code> <code>bool</code> <p>Whether to display the PU at IP2. Defaults to True.</p> <code>True</code> <code>display_PU_5</code> <code>bool</code> <p>Whether to display the PU at IP5. Defaults to True.</p> <code>True</code> <code>display_PU_8</code> <code>bool</code> <p>Whether to display the PU at IP8. Defaults to True.</p> <code>True</code> <code>display_number_of_turns</code> <code>bool</code> <p>Whether to display the number of turns. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The generated title string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_title_from_configuration(\n    dataframe_data: pd.DataFrame,\n    ions: bool = False,\n    crossing_type: Optional[str] = None,\n    display_LHC_version: bool = True,\n    display_energy: bool = True,\n    display_bunch_index: bool = True,\n    display_CC_crossing: bool = True,\n    display_bunch_intensity: bool = True,\n    display_beta: bool = True,\n    display_crossing_IP_1: bool = True,\n    display_crossing_IP_2: bool = True,\n    display_crossing_IP_5: bool = True,\n    display_crossing_IP_8: bool = True,\n    display_bunch_length: bool = True,\n    display_polarity_IP_2_8: bool = True,\n    display_emittance: bool = True,\n    display_chromaticity: bool = True,\n    display_octupole_intensity: bool = True,\n    display_coupling: bool = True,\n    display_filling_scheme: bool = True,\n    display_horizontal_tune: Optional[bool] = None,\n    display_vertical_tune: Optional[bool] = None,\n    display_tune: bool = True,\n    display_luminosity_1: bool = True,\n    display_luminosity_2: bool = True,\n    display_luminosity_5: bool = True,\n    display_luminosity_8: bool = True,\n    display_PU_1: bool = True,\n    display_PU_2: bool = True,\n    display_PU_5: bool = True,\n    display_PU_8: bool = True,\n    display_number_of_turns=False,\n) -&gt; str:\n    \"\"\"\n    Generates a title string from the configuration data.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing configuration data.\n        ions (bool, optional): Whether the beam is composed of ions. Defaults to False.\n        crossing_type (str, optional): The type of crossing: 'vh' or 'hv'. Defaults to None, meaning\n            it will try to be inferred from the optics file name. Back to 'hv' if not found.\n        display_betx_bety (bool, optional): Whether to display the beta functions. Defaults to True.\n        display_LHC_version (bool, optional): Whether to display the LHC version. Defaults to True.\n        display_energy (bool, optional): Whether to display the energy. Defaults to True.\n        display_bunch_index (bool, optional): Whether to display the bunch index. Defaults to True.\n        display_CC_crossing (bool, optional): Whether to display the CC crossing. Defaults to True.\n        display_bunch_intensity (bool, optional): Whether to display the bunch intensity. Defaults\n            to True.\n        display_beta (bool, optional): Whether to display the beta function. Defaults to True.\n        display_crossing_IP_1 (bool, optional): Whether to display the crossing at IP1. Defaults to\n            True.\n        display_crossing_IP_2 (bool, optional): Whether to display the crossing at IP2. Defaults to\n            True.\n        display_crossing_IP_5 (bool, optional): Whether to display the crossing at IP5. Defaults to\n            True.\n        display_crossing_IP_8 (bool, optional): Whether to display the crossing at IP8. Defaults to\n            True.\n        display_bunch_length (bool, optional): Whether to display the bunch length. Defaults to\n            True.\n        display_polarity_IP_2_8 (bool, optional): Whether to display the polarity at IP2 and IP8.\n            Defaults to True.\n        display_emittance (bool, optional): Whether to display the emittance. Defaults to True.\n        display_chromaticity (bool, optional): Whether to display the chromaticity.\n            Defaults to True.\n        display_octupole_intensity (bool, optional): Whether to display the octupole intensity.\n            Defaults to True.\n        display_coupling (bool, optional): Whether to display the coupling. Defaults to True.\n        display_filling_scheme (bool, optional): Whether to display the filling scheme. Defaults to\n            True.\n        display_horizontal_tune (bool, optional): Whether to display the horizontal tune. Defaults to\n            None. Takes precedence over display_tune.\n        display_vertical_tune (bool, optional): Whether to display the vertical tune. Defaults to\n            None. Takes precedence over display_tune.\n        display_tune (bool, optional): Whether to display the tune. Defaults to True.\n        display_luminosity_1 (bool, optional): Whether to display the luminosity at IP1. Defaults to\n            True.\n        display_luminosity_2 (bool, optional): Whether to display the luminosity at IP2. Defaults to\n            True.\n        display_luminosity_5 (bool, optional): Whether to display the luminosity at IP5. Defaults to\n            True.\n        display_luminosity_8 (bool, optional): Whether to display the luminosity at IP8. Defaults to\n            True.\n        display_PU_1 (bool, optional): Whether to display the PU at IP1. Defaults to True.\n        display_PU_2 (bool, optional): Whether to display the PU at IP2. Defaults to True.\n        display_PU_5 (bool, optional): Whether to display the PU at IP5. Defaults to True.\n        display_PU_8 (bool, optional): Whether to display the PU at IP8. Defaults to True.\n        display_number_of_turns (bool, optional): Whether to display the number of turns. Defaults to\n            False.\n\n    Returns:\n        str: The generated title string.\n    \"\"\"\n\n    # Warn about tune definition\n    if (\n        display_horizontal_tune is not None or display_vertical_tune is not None\n    ) and not display_tune:\n        logging.warning(\n            \"You have defined display_horizontal_tune or display_vertical_tune, but not \"\n            \"display_tune. The horizontal and/or vertical tunes will still be displayed.\"\n        )\n\n    # Find out what is the crossing type\n    if crossing_type is None:\n        crossing_type = get_crossing_type(dataframe_data)\n\n    # Collect all the information to display\n    LHC_version_str = get_LHC_version_str(dataframe_data, ions)\n    energy_str = get_energy_str(dataframe_data, ions)\n    bunch_index_str = get_bunch_index_str(dataframe_data)\n    CC_crossing_str = get_CC_crossing_str(dataframe_data)\n    bunch_intensity_str = get_bunch_intensity_str(dataframe_data)\n    beta_str = get_beta_str(dataframe_data)\n    xing_IP1_str, xing_IP5_str = get_crossing_IP_1_5_str(dataframe_data, crossing_type)\n    xing_IP2_str, xing_IP8_str = get_crossing_IP_2_8_str(dataframe_data)\n    bunch_length_str = get_bunch_length_str(dataframe_data)\n    polarity_str = get_polarity_IP_2_8_str(dataframe_data)\n    emittance_str = get_normalized_emittance_str(dataframe_data)\n    chromaticity_str = get_chromaticity_str(dataframe_data)\n    octupole_intensity_str = get_octupole_intensity_str(dataframe_data)\n    coupling_str = get_linear_coupling_str(dataframe_data)\n    filling_scheme_str = get_filling_scheme_str(dataframe_data)\n    tune_str = get_tune_str(dataframe_data, display_horizontal_tune, display_vertical_tune)\n    n_turns_str = get_number_of_turns_str(dataframe_data)\n\n    # Collect luminosity and PU strings at each IP\n    dic_lumi_PU_str = {\n        \"with_beam_beam\": {\"lumi\": {}, \"PU\": {}},\n        \"without_beam_beam\": {\"lumi\": {}, \"PU\": {}},\n    }\n    for beam_beam in [\"with_beam_beam\", \"without_beam_beam\"]:\n        for ip in [1, 2, 5, 8]:\n            dic_lumi_PU_str[beam_beam][\"lumi\"][ip] = get_luminosity_at_ip_str(\n                dataframe_data, ip, beam_beam=True\n            )\n            dic_lumi_PU_str[beam_beam][\"PU\"][ip] = get_PU_at_IP_str(\n                dataframe_data, ip, beam_beam=True\n            )\n\n    def test_if_empty_and_add_period(string: str) -&gt; str:\n        \"\"\"\n        Test if a string is empty and add a period if not.\n\n        Args:\n            string (str): The string to test.\n\n        Returns:\n            str: The string with a period if not empty.\n        \"\"\"\n        return f\"{string}. \" if string != \"\" else \"\"\n\n    # Make the final title (order is the same as in the past)\n    title = \"\"\n    if display_LHC_version:\n        title += test_if_empty_and_add_period(LHC_version_str)\n    if display_energy:\n        title += test_if_empty_and_add_period(energy_str)\n    if display_CC_crossing:\n        title += test_if_empty_and_add_period(CC_crossing_str)\n    if display_bunch_intensity:\n        title += test_if_empty_and_add_period(bunch_intensity_str)\n    # Jump to the next line\n    title += \"\\n\"\n    if display_luminosity_1:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"lumi\"][1])\n    if display_PU_1:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"PU\"][1])\n    if display_luminosity_5:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"lumi\"][5])\n    if display_PU_5:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"PU\"][5])\n    # Jump to the next line\n    title += \"\\n\"\n    if display_luminosity_2:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"lumi\"][2])\n    if display_PU_2:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"PU\"][2])\n    if display_luminosity_8:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"lumi\"][8])\n    if display_PU_8:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"PU\"][8])\n    # Jump to the next line\n    title += \"\\n\"\n    if display_beta:\n        title += test_if_empty_and_add_period(beta_str)\n    if display_polarity_IP_2_8:\n        title += test_if_empty_and_add_period(polarity_str)\n    if display_bunch_length:\n        title += test_if_empty_and_add_period(bunch_length_str)\n    # Jump to the next line\n    title += \"\\n\"\n    if display_crossing_IP_1:\n        title += test_if_empty_and_add_period(xing_IP1_str)\n    if display_crossing_IP_5:\n        title += test_if_empty_and_add_period(xing_IP5_str)\n    if display_crossing_IP_2:\n        title += test_if_empty_and_add_period(xing_IP2_str)\n    if display_crossing_IP_8:\n        title += test_if_empty_and_add_period(xing_IP8_str)\n\n    # Jump to the next line\n    title += \"\\n\"\n    if display_emittance:\n        title += test_if_empty_and_add_period(emittance_str)\n    if display_chromaticity:\n        title += test_if_empty_and_add_period(chromaticity_str)\n    if display_octupole_intensity:\n        title += test_if_empty_and_add_period(octupole_intensity_str)\n    if display_coupling:\n        title += test_if_empty_and_add_period(coupling_str)\n    if display_tune:\n        title += test_if_empty_and_add_period(tune_str)\n    # Jump to the next line\n    title += \"\\n\"\n    if display_filling_scheme:\n        title += test_if_empty_and_add_period(filling_scheme_str)\n    if display_bunch_index:\n        title += test_if_empty_and_add_period(bunch_index_str)\n    # Jump to the next line\n    if display_number_of_turns:\n        title += \"\\n\"\n        title += test_if_empty_and_add_period(n_turns_str)\n\n    # Filter final title for empty lines\n    title = \"\\n\".join([line for line in title.split(\"\\n\") if line.strip() != \"\"])\n\n    return title\n</code></pre>"},{"location":"reference/study_da/plot/index.html#study_da.plot.plot_3D","title":"<code>plot_3D(dataframe_data, x_variable, y_variable, z_variable, color_variable, xlabel=None, ylabel=None, z_label=None, title='', vmin=4.5, vmax=7.5, surface_count=30, opacity=0.2, figsize=(1000, 1000), colormap='RdBu', colorbar_title_text='Minimum DA (\u03c3)', display_colormap=False, output_path='output.png', output_path_html='output.html', display_plot=True, dark_theme=False)</code>","text":"<p>Plots a 3D volume rendering from the given dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing the data to plot.</p> required <code>x_variable</code> <code>str</code> <p>The variable to plot on the x-axis.</p> required <code>y_variable</code> <code>str</code> <p>The variable to plot on the y-axis.</p> required <code>z_variable</code> <code>str</code> <p>The variable to plot on the z-axis.</p> required <code>color_variable</code> <code>str</code> <p>The variable to use for the color scale.</p> required <code>xlabel</code> <code>Optional[str]</code> <p>The label for the x-axis. Defaults to None.</p> <code>None</code> <code>ylabel</code> <code>Optional[str]</code> <p>The label for the y-axis. Defaults to None.</p> <code>None</code> <code>z_label</code> <code>Optional[str]</code> <p>The label for the z-axis. Defaults to None.</p> <code>None</code> <code>title</code> <code>str</code> <p>The title of the plot. Defaults to \"\".</p> <code>''</code> <code>vmin</code> <code>float</code> <p>The minimum value for the color scale. Defaults to 4.5.</p> <code>4.5</code> <code>vmax</code> <code>float</code> <p>The maximum value for the color scale. Defaults to 7.5.</p> <code>7.5</code> <code>surface_count</code> <code>int</code> <p>The number of surfaces for volume rendering. Defaults to 30.</p> <code>30</code> <code>opacity</code> <code>float</code> <p>The opacity of the volume rendering. Defaults to 0.2.</p> <code>0.2</code> <code>figsize</code> <code>tuple[float, float]</code> <p>The size of the figure. Defaults to (1000, 1000).</p> <code>(1000, 1000)</code> <code>colormap</code> <code>str</code> <p>The colormap to use. Defaults to \"RdBu\".</p> <code>'RdBu'</code> <code>colorbar_title_text</code> <code>str</code> <p>The label for the colorbar. Defaults to \"Minimum DA (\u03c3)\".</p> <code>'Minimum DA (\u03c3)'</code> <code>display_colormap</code> <code>bool</code> <p>Whether to display the colormap. Defaults to False.</p> <code>False</code> <code>output_path</code> <code>str</code> <p>The path to save the plot image. Defaults to \"output.png\".</p> <code>'output.png'</code> <code>output_path_html</code> <code>str</code> <p>The path to save the plot HTML. Defaults to \"output.html\".</p> <code>'output.html'</code> <code>display_plot</code> <code>bool</code> <p>Whether to display the plot. Defaults to True.</p> <code>True</code> <code>dark_theme</code> <code>bool</code> <p>Whether to use a dark theme. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>go.Figure: The plotly figure object.</p> Source code in <code>study_da/plot/plot_study.py</code> <pre><code>def plot_3D(\n    dataframe_data: pd.DataFrame,\n    x_variable: str,\n    y_variable: str,\n    z_variable: str,\n    color_variable: str,\n    xlabel: Optional[str] = None,\n    ylabel: Optional[str] = None,\n    z_label: Optional[str] = None,\n    title: str = \"\",\n    vmin: float = 4.5,\n    vmax: float = 7.5,\n    surface_count: int = 30,\n    opacity: float = 0.2,\n    figsize: tuple[float, float] = (1000, 1000),\n    colormap: str = \"RdBu\",\n    colorbar_title_text: str = \"Minimum DA (\u03c3)\",\n    display_colormap: bool = False,\n    output_path: str = \"output.png\",\n    output_path_html: str = \"output.html\",\n    display_plot: bool = True,\n    dark_theme: bool = False,\n) -&gt; Any:\n    \"\"\"\n    Plots a 3D volume rendering from the given dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing the data to plot.\n        x_variable (str): The variable to plot on the x-axis.\n        y_variable (str): The variable to plot on the y-axis.\n        z_variable (str): The variable to plot on the z-axis.\n        color_variable (str): The variable to use for the color scale.\n        xlabel (Optional[str], optional): The label for the x-axis. Defaults to None.\n        ylabel (Optional[str], optional): The label for the y-axis. Defaults to None.\n        z_label (Optional[str], optional): The label for the z-axis. Defaults to None.\n        title (str, optional): The title of the plot. Defaults to \"\".\n        vmin (float, optional): The minimum value for the color scale. Defaults to 4.5.\n        vmax (float, optional): The maximum value for the color scale. Defaults to 7.5.\n        surface_count (int, optional): The number of surfaces for volume rendering. Defaults to 30.\n        opacity (float, optional): The opacity of the volume rendering. Defaults to 0.2.\n        figsize (tuple[float, float], optional): The size of the figure. Defaults to (1000, 1000).\n        colormap (str, optional): The colormap to use. Defaults to \"RdBu\".\n        colorbar_title_text (str, optional): The label for the colorbar. Defaults to \"Minimum DA (\u03c3)\".\n        display_colormap (bool, optional): Whether to display the colormap. Defaults to False.\n        output_path (str, optional): The path to save the plot image. Defaults to \"output.png\".\n        output_path_html (str, optional): The path to save the plot HTML. Defaults to \"output.html\".\n        display_plot (bool, optional): Whether to display the plot. Defaults to True.\n        dark_theme (bool, optional): Whether to use a dark theme. Defaults to False.\n\n    Returns:\n        go.Figure: The plotly figure object.\n    \"\"\"\n    # Check if plotly is installed\n    try:\n        import plotly.graph_objects as go\n    except ImportError as e:\n        raise ImportError(\"Please install plotly to use this function\") from e\n\n    X = np.array(dataframe_data[x_variable])\n    Y = np.array(dataframe_data[y_variable])\n    Z = np.array(dataframe_data[z_variable])\n    values = np.array(dataframe_data[color_variable])\n    fig = go.Figure(\n        data=go.Volume(\n            x=X.flatten(),\n            y=Y.flatten(),\n            z=Z.flatten(),\n            value=values.flatten(),\n            isomin=vmin,\n            isomax=vmax,\n            opacity=opacity,  # needs to be small to see through all surfaces\n            surface_count=surface_count,  # needs to be a large number for good volume rendering\n            colorscale=colormap,\n            colorbar_title_text=colorbar_title_text,\n        )\n    )\n\n    fig.update_layout(\n        scene_xaxis_title_text=xlabel,\n        scene_yaxis_title_text=ylabel,\n        scene_zaxis_title_text=z_label,\n        title=title,\n    )\n\n    # Get a good initial view, dezoomed\n    fig.update_layout(scene_camera=dict(eye=dict(x=1.5, y=1.5, z=1.5)))\n\n    # Center the title\n    fig.update_layout(title_x=0.5, title_y=0.9, title_xanchor=\"center\", title_yanchor=\"top\")\n\n    # Specify the width and height of the figure\n    fig.update_layout(width=figsize[0], height=figsize[1])\n\n    # Remove margins and padding\n    fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\n\n    # Display the colormap\n    if not display_colormap:\n        fig.update_layout(coloraxis_showscale=False)\n        fig.update_traces(showscale=False)\n    else:\n        # Make colorbar smaller\n        fig.update_layout(coloraxis_colorbar=dict(thickness=10, len=0.5))\n\n    # Set the theme\n    if dark_theme:\n        fig.update_layout(template=\"plotly_dark\")\n\n    # Display/save/return the figure\n    if output_path is not None:\n        fig.write_image(output_path)\n\n    if output_path_html is not None:\n        fig.write_html(output_path_html)\n\n    if display_plot:\n        fig.show()\n\n    return fig\n</code></pre>"},{"location":"reference/study_da/plot/index.html#study_da.plot.plot_heatmap","title":"<code>plot_heatmap(dataframe_data, horizontal_variable, vertical_variable, color_variable, link=None, position_qr='top-right', plot_contours=True, xlabel=None, ylabel=None, tick_interval=2, round_xticks=None, round_yticks=None, symmetric_missing=False, mask_lower_triangle=False, mask_upper_triangle=False, plot_diagonal_lines=True, shift_diagonal_lines=1, xaxis_ticks_on_top=True, title='', vmin=4.5, vmax=7.5, k_masking=-1, green_contour=6.0, min_level_contours=1, max_level_contours=15, delta_levels_contours=0.5, figsize=None, label_cbar='Minimum DA (' + '$\\\\sigma$' + ')', colormap='coolwarm_r', style='ggplot', output_path='output.png', display_plot=True, latex_fonts=True, vectorize=False, fill_missing_value_with=None, dpi=300)</code>","text":"<p>Plots a heatmap from the given dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing the data to plot.</p> required <code>horizontal_variable</code> <code>str</code> <p>The variable to plot on the horizontal axis.</p> required <code>vertical_variable</code> <code>str</code> <p>The variable to plot on the vertical axis.</p> required <code>color_variable</code> <code>str</code> <p>The variable to use for the color scale.</p> required <code>link</code> <code>Optional[str]</code> <p>A link to encode in a QR code. Defaults to None.</p> <code>None</code> <code>plot_contours</code> <code>bool</code> <p>Whether to plot contours. Defaults to True.</p> <code>True</code> <code>xlabel</code> <code>Optional[str]</code> <p>The label for the x-axis. Defaults to None.</p> <code>None</code> <code>ylabel</code> <code>Optional[str]</code> <p>The label for the y-axis. Defaults to None.</p> <code>None</code> <code>tick_interval</code> <code>int</code> <p>The interval for the ticks. Defaults to 2.</p> <code>2</code> <code>round_xticks</code> <code>Optional[int]</code> <p>The number of decimal places to round the x-ticks to. Defaults to None.</p> <code>None</code> <code>round_yticks</code> <code>Optional[int]</code> <p>The number of decimal places to round the y-ticks to. Defaults to None.</p> <code>None</code> <code>symmetric_missing</code> <code>bool</code> <p>Whether to make the matrix symmetric by replacing the lower triangle with the upper triangle. Defaults to False.</p> <code>False</code> <code>mask_lower_triangle</code> <code>bool</code> <p>Whether to mask the lower triangle. Defaults to False.</p> <code>False</code> <code>mask_upper_triangle</code> <code>bool</code> <p>Whether to mask the upper triangle. Defaults to False.</p> <code>False</code> <code>plot_diagonal_lines</code> <code>bool</code> <p>Whether to plot diagonal lines. Defaults to True.</p> <code>True</code> <code>shift_diagonal_lines</code> <code>int</code> <p>The shift for the diagonal lines. Defaults to 1.</p> <code>1</code> <code>xaxis_ticks_on_top</code> <code>bool</code> <p>Whether to place the x-axis ticks on top. Defaults to True.</p> <code>True</code> <code>title</code> <code>str</code> <p>The title of the plot. Defaults to \"\".</p> <code>''</code> <code>vmin</code> <code>float</code> <p>The minimum value for the color scale. Defaults to 4.5.</p> <code>4.5</code> <code>vmax</code> <code>float</code> <p>The maximum value for the color scale. Defaults to 7.5.</p> <code>7.5</code> <code>k_masking</code> <code>int</code> <p>The k parameter for masking. Defaults to -1.</p> <code>-1</code> <code>green_contour</code> <code>Optional[float]</code> <p>The value for the green contour line. Defaults to 6.0.</p> <code>6.0</code> <code>min_level_contours</code> <code>float</code> <p>The minimum level for the contours. Defaults to 1.</p> <code>1</code> <code>max_level_contours</code> <code>float</code> <p>The maximum level for the contours. Defaults to 15.</p> <code>15</code> <code>delta_levels_contours</code> <code>float</code> <p>The delta between contour levels. Defaults to 0.5.</p> <code>0.5</code> <code>figsize</code> <code>Optional[tuple[float, float]]</code> <p>The size of the figure. Defaults to None.</p> <code>None</code> <code>label_cbar</code> <code>str</code> <p>The label for the colorbar. Defaults to \"Minimum DA ($\\sigma$)\".</p> <code>'Minimum DA (' + '$\\\\sigma$' + ')'</code> <code>colormap</code> <code>str</code> <p>The colormap to use. Defaults to \"coolwarm_r\".</p> <code>'coolwarm_r'</code> <code>style</code> <code>str</code> <p>The style to use for the plot. Defaults to \"ggplot\".</p> <code>'ggplot'</code> <code>output_path</code> <code>str</code> <p>The path to save the plot. Defaults to \"output.pdf\".</p> <code>'output.png'</code> <code>display_plot</code> <code>bool</code> <p>Whether to display the plot. Defaults to True.</p> <code>True</code> <code>latex_fonts</code> <code>bool</code> <p>Whether to use LaTeX fonts. Defaults to True.</p> <code>True</code> <code>vectorize</code> <code>bool</code> <p>Whether to vectorize the plot. Defaults to False.</p> <code>False</code> <code>fill_missing_value_with</code> <code>Optional[str | float]</code> <p>The value to fill missing values with. Can be a number or 'interpolate'. Defaults to None.</p> <code>None</code> <code>dpi</code> <code>int</code> <p>The DPI for the plot. Defaults to 300.</p> <code>300</code> <p>Returns:</p> Type Description <code>tuple[Figure, Axes]</code> <p>tuple[plt.Figure, plt.Axes]: The figure and axes of the plot.</p> Source code in <code>study_da/plot/plot_study.py</code> <pre><code>def plot_heatmap(\n    dataframe_data: pd.DataFrame,\n    horizontal_variable: str,\n    vertical_variable: str,\n    color_variable: str,\n    link: Optional[str] = None,\n    position_qr: Optional[str] = \"top-right\",\n    plot_contours: bool = True,\n    xlabel: Optional[str] = None,\n    ylabel: Optional[str] = None,\n    tick_interval: int = 2,\n    round_xticks: Optional[int] = None,\n    round_yticks: Optional[int] = None,\n    symmetric_missing: bool = False,\n    mask_lower_triangle: bool = False,\n    mask_upper_triangle: bool = False,\n    plot_diagonal_lines: bool = True,\n    shift_diagonal_lines: int = 1,\n    xaxis_ticks_on_top: bool = True,\n    title: str = \"\",\n    vmin: float = 4.5,\n    vmax: float = 7.5,\n    k_masking: int = -1,\n    green_contour: Optional[float] = 6.0,\n    min_level_contours: float = 1,\n    max_level_contours: float = 15,\n    delta_levels_contours: float = 0.5,\n    figsize: Optional[tuple[float, float]] = None,\n    label_cbar: str = \"Minimum DA (\" + r\"$\\sigma$\" + \")\",\n    colormap: str = \"coolwarm_r\",\n    style: str = \"ggplot\",\n    output_path: str = \"output.png\",\n    display_plot: bool = True,\n    latex_fonts: bool = True,\n    vectorize: bool = False,\n    fill_missing_value_with: Optional[str | float] = None,\n    dpi=300,\n) -&gt; tuple[plt.Figure, plt.Axes]:\n    \"\"\"\n    Plots a heatmap from the given dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing the data to plot.\n        horizontal_variable (str): The variable to plot on the horizontal axis.\n        vertical_variable (str): The variable to plot on the vertical axis.\n        color_variable (str): The variable to use for the color scale.\n        link (Optional[str], optional): A link to encode in a QR code. Defaults to None.\n        plot_contours (bool, optional): Whether to plot contours. Defaults to True.\n        xlabel (Optional[str], optional): The label for the x-axis. Defaults to None.\n        ylabel (Optional[str], optional): The label for the y-axis. Defaults to None.\n        tick_interval (int, optional): The interval for the ticks. Defaults to 2.\n        round_xticks (Optional[int], optional): The number of decimal places to round the x-ticks to.\n            Defaults to None.\n        round_yticks (Optional[int], optional): The number of decimal places to round the y-ticks to.\n            Defaults to None.\n        symmetric_missing (bool, optional): Whether to make the matrix symmetric by replacing the\n            lower triangle with the upper triangle. Defaults to False.\n        mask_lower_triangle (bool, optional): Whether to mask the lower triangle. Defaults to False.\n        mask_upper_triangle (bool, optional): Whether to mask the upper triangle. Defaults to False.\n        plot_diagonal_lines (bool, optional): Whether to plot diagonal lines. Defaults to True.\n        shift_diagonal_lines (int, optional): The shift for the diagonal lines. Defaults to 1.\n        xaxis_ticks_on_top (bool, optional): Whether to place the x-axis ticks on top. Defaults to True.\n        title (str, optional): The title of the plot. Defaults to \"\".\n        vmin (float, optional): The minimum value for the color scale. Defaults to 4.5.\n        vmax (float, optional): The maximum value for the color scale. Defaults to 7.5.\n        k_masking (int, optional): The k parameter for masking. Defaults to -1.\n        green_contour (Optional[float], optional): The value for the green contour line. Defaults to 6.0.\n        min_level_contours (float, optional): The minimum level for the contours. Defaults to 1.\n        max_level_contours (float, optional): The maximum level for the contours. Defaults to 15.\n        delta_levels_contours (float, optional): The delta between contour levels. Defaults to 0.5.\n        figsize (Optional[tuple[float, float]], optional): The size of the figure. Defaults to None.\n        label_cbar (str, optional): The label for the colorbar. Defaults to \"Minimum DA ($\\sigma$)\".\n        colormap (str, optional): The colormap to use. Defaults to \"coolwarm_r\".\n        style (str, optional): The style to use for the plot. Defaults to \"ggplot\".\n        output_path (str, optional): The path to save the plot. Defaults to \"output.pdf\".\n        display_plot (bool, optional): Whether to display the plot. Defaults to True.\n        latex_fonts (bool, optional): Whether to use LaTeX fonts. Defaults to True.\n        vectorize (bool, optional): Whether to vectorize the plot. Defaults to False.\n        fill_missing_value_with (Optional[str | float], optional): The value to fill missing values\n            with. Can be a number or 'interpolate'. Defaults to None.\n        dpi (int, optional): The DPI for the plot. Defaults to 300.\n\n    Returns:\n        tuple[plt.Figure, plt.Axes]: The figure and axes of the plot.\n    \"\"\"\n    # Use the requested style\n    _set_style(style, latex_fonts, vectorize)\n\n    # Get the dataframe to plot\n    df_to_plot = dataframe_data.pivot(\n        index=vertical_variable, columns=horizontal_variable, values=color_variable\n    )\n\n    # Get numpy array from dataframe\n    data_array = df_to_plot.to_numpy(dtype=float)\n\n    # Replace NaNs with a value if requested\n    if fill_missing_value_with is not None:\n        if isinstance(fill_missing_value_with, (int, float)):\n            data_array[np.isnan(data_array)] = fill_missing_value_with\n        elif fill_missing_value_with == \"interpolate\":\n            # Interpolate missing values with griddata\n            x = np.arange(data_array.shape[1])\n            y = np.arange(data_array.shape[0])\n            xx, yy = np.meshgrid(x, y)\n            x = xx[~np.isnan(data_array)]\n            y = yy[~np.isnan(data_array)]\n            z = data_array[~np.isnan(data_array)]\n            data_array = griddata((x, y), z, (xx, yy), method=\"cubic\")\n\n    # Mask the lower or upper triangle (checks are done in the function)\n    data_array_masked, mask_main_array = _mask(\n        mask_lower_triangle, mask_upper_triangle, data_array, k_masking\n    )\n\n    # Define colormap and set NaNs to white\n    cmap = matplotlib.colormaps.get_cmap(colormap)\n    cmap.set_bad(\"w\")\n\n    # Build heatmap, with inverted y axis\n    fig, ax = plt.subplots()\n    if figsize is not None:\n        fig.set_size_inches(figsize)\n    im = ax.imshow(data_array_masked, cmap=cmap, vmin=vmin, vmax=vmax)\n    ax.invert_yaxis()\n\n    # Add text annotations\n    ax = _add_text_annotation(df_to_plot, data_array, ax, vmin, vmax)\n\n    # Smooth data for contours\n    mx = _smooth(data_array, symmetric_missing)\n\n    # Plot contours if requested\n    if plot_contours:\n        ax = _add_contours(\n            ax,\n            data_array,\n            mx,\n            green_contour,\n            min_level_contours,\n            max_level_contours,\n            delta_levels_contours,\n            mask_main_array,\n        )\n\n    if plot_diagonal_lines:\n        # Diagonal lines must be plotted after the contour lines, because of bug in matplotlib\n        # Shift might need to be adjusted\n        ax = _add_diagonal_lines(ax, shift=shift_diagonal_lines)\n\n    # Define title and axis labels\n    ax.set_title(\n        title,\n        fontsize=10,\n    )\n\n    # Set axis labels\n    ax = _set_labels(\n        ax,\n        df_to_plot,\n        data_array,\n        horizontal_variable,\n        vertical_variable,\n        xlabel,\n        ylabel,\n        xaxis_ticks_on_top,\n        tick_interval,\n        round_xticks,\n        round_yticks,\n    )\n\n    # Create colorbar\n    cbar = ax.figure.colorbar(im, ax=ax, fraction=0.026, pad=0.04)\n    cbar.ax.set_ylabel(label_cbar, rotation=90, va=\"bottom\", labelpad=15)\n\n    # Remove potential grid\n    plt.grid(visible=None)\n\n    # Add QR code with a link to the topright side (a bit experimental, might need adjustments)\n    if link is not None:\n        fig = add_QR_code(fig, link, position_qr)\n\n    # Save and potentially display the plot\n    if output_path is not None:\n        if output_path.endswith(\".pdf\") and not vectorize:\n            raise ValueError(\"Please set vectorize=True to save as PDF\")\n        elif not output_path.endswith(\".pdf\") and vectorize:\n            raise ValueError(\"Please set vectorize=False to save as PNG or JPG\")\n        plt.savefig(output_path, bbox_inches=\"tight\", dpi=dpi)\n\n    if display_plot:\n        plt.show()\n    return fig, ax\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html","title":"build_title","text":"<p>This module provides functions to compute LaTeX strings for plot titles based on data from a pandas DataFrame. The functions extract various parameters such as crossing type, LHC version, energy, bunch index, crab cavity crossing angle, bunch intensity, beta functions, crossing angles at different interaction points (IPs), bunch length, polarity, normalized emittance, chromaticity, octupole intensity, linear coupling, filling scheme, tune, luminosity, and pile-up.</p> <p>Functions:</p> Name Description <code>latex_float</code> <p>float, precision: int = 3) -&gt; str:</p> <code>get_crossing_type</code> <p>pd.DataFrame) -&gt; str:</p> <code>get_LHC_version_str</code> <p>pd.DataFrame) -&gt; str:</p> <code>get_energy_str</code> <p>pd.DataFrame) -&gt; str:</p> <code>get_bunch_index_str</code> <p>pd.DataFrame) -&gt; str:</p> <code>get_CC_crossing_str</code> <p>pd.DataFrame) -&gt; str:</p> <code>get_bunch_intensity_str</code> <p>pd.DataFrame) -&gt; str:</p> <code>get_beta_str</code> <p>float, bety_value: float) -&gt; str:</p> <code>_get_plane_crossing_IP_1_5_str</code> <p>pd.DataFrame, type_crossing: str) -&gt; tuple[str, str]:</p> <code>_get_crossing_value_IP_1_5</code> <p>pd.DataFrame, ip: int) -&gt; float:</p> <code>get_crossing_IP_1_5_str</code> <p>pd.DataFrame, type_crossing: str) -&gt; tuple[str, str]:</p> <code>get_crossing_IP_2_8_str</code> <p>pd.DataFrame) -&gt; list[str]:</p> <code>get_bunch_length_str</code> <p>pd.DataFrame) -&gt; str:</p> <code>get_polarity_IP_2_8_str</code> <p>pd.DataFrame) -&gt; str:</p> <code>get_normalized_emittance_str</code> <p>pd.DataFrame) -&gt; str:</p> <code>get_chromaticity_str</code> <p>pd.DataFrame) -&gt; str:</p> <code>get_octupole_intensity_str</code> <p>pd.DataFrame) -&gt; str:</p> <code>get_linear_coupling_str</code> <p>pd.DataFrame) -&gt; str:</p> <code>get_filling_scheme_str</code> <p>pd.DataFrame) -&gt; str:</p> <code>get_tune_str</code> <p>pd.DataFrame) -&gt; str:</p> <code>get_luminosity_at_ip_str</code> <p>pd.DataFrame, ip: int, beam_beam=True) -&gt; str:</p> <code>get_PU_at_IP_str</code> <p>pd.DataFrame, ip: int, beam_beam=True) -&gt; str:</p> <code>get_title_from_configuration</code>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_CC_crossing_str","title":"<code>get_CC_crossing_str(dataframe_data)</code>","text":"<p>Retrieves the crab cavity crossing angle from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing crab cavity crossing angle information.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The crab cavity crossing angle string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_CC_crossing_str(dataframe_data: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Retrieves the crab cavity crossing angle from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing crab cavity crossing angle\n            information.\n\n    Returns:\n        str: The crab cavity crossing angle string.\n    \"\"\"\n    if \"on_crab1\" in dataframe_data.columns:\n        CC_crossing_value = dataframe_data[\"on_crab1\"].unique()[0]\n        return f\"$CC = {{{CC_crossing_value:.1f}}}$ $\\mu rad$\"\n    else:\n        logging.warning(\"CC crossing not found in the dataframe\")\n        return \"\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_LHC_version_str","title":"<code>get_LHC_version_str(dataframe_data, ions=False)</code>","text":"<p>Retrieves the LHC version from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing LHC version information.</p> required <code>ions</code> <code>bool</code> <p>Whether the study is for ions. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The LHC version string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_LHC_version_str(dataframe_data: pd.DataFrame, ions: bool = False) -&gt; str:\n    \"\"\"\n    Retrieves the LHC version from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing LHC version information.\n        ions (bool, optional): Whether the study is for ions. Defaults to False.\n\n    Returns:\n        str: The LHC version string.\n    \"\"\"\n    string_HL_LHC = None\n    string_LHC = None\n    ions_string = \" (ions)\" if ions else \"\"\n    if \"ver_hllhc_optics\" in dataframe_data.columns:\n        ver_hllhc_optics = dataframe_data[\"ver_hllhc_optics\"].unique()[0]\n        if ver_hllhc_optics is not None and not np.isnan(ver_hllhc_optics):\n            string_HL_LHC = f\"HL-LHC v{ver_hllhc_optics:.1f}\"\n    if \"ver_lhc_run\" in dataframe_data.columns:\n        ver_lhc_run = dataframe_data[\"ver_lhc_run\"].unique()[0]\n        if ver_lhc_run is not None and not np.isnan(ver_lhc_run):\n            string_LHC = f\"LHC Run {int(ver_lhc_run)}\"\n\n    if string_HL_LHC is not None and string_LHC is not None:\n        raise ValueError(\"Both HL-LHC and LHC Run versions found in the dataframe. Please check.\")\n    elif string_HL_LHC is not None:\n        return string_HL_LHC + ions_string\n    elif string_LHC is not None:\n        return string_LHC + ions_string\n    logging.warning(\"LHC version not found in the dataframe\")\n    return \"\" + ions_string\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_PU_at_IP_str","title":"<code>get_PU_at_IP_str(dataframe_data, ip, beam_beam=True)</code>","text":"<p>Retrieves the pile-up at a given IP from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing pile-up information.</p> required <code>ip</code> <code>int</code> <p>The IP number.</p> required <code>beam_beam</code> <code>bool</code> <p>Whether to consider beam-beam pile-up. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The pile-up string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_PU_at_IP_str(dataframe_data: pd.DataFrame, ip: int, beam_beam=True) -&gt; str:\n    \"\"\"\n    Retrieves the pile-up at a given IP from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing pile-up information.\n        ip (int): The IP number.\n        beam_beam (bool, optional): Whether to consider beam-beam pile-up. Defaults to True.\n\n    Returns:\n        str: The pile-up string.\n    \"\"\"\n    # sourcery skip: merge-else-if-into-elif, simplify-fstring-formatting\n    if beam_beam:\n        if f\"Pile-up_ip{ip}_with_beam_beam\" in dataframe_data.columns:\n            PU_value = np.mean(dataframe_data[f\"Pile-up_ip{ip}_with_beam_beam\"].unique())\n            return f\"$PU_{{{ip}}} \\simeq ${latex_float(float(PU_value))}\"\n        else:\n            logging.warning(f\"Pile-up at IP{ip} with beam-beam not found in the dataframe\")\n            return \"\"\n    else:\n        if f\"Pile-up_ip{ip}_without_beam_beam\" in dataframe_data.columns:\n            PU_value = np.mean(dataframe_data[f\"Pile-up_ip{ip}_without_beam_beam\"].unique())\n            return f\"$PU_{{{ip}}} \\simeq ${latex_float(float(PU_value))}\"\n        else:\n            logging.warning(f\"Pile-up at IP{ip} without beam-beam not found in the dataframe\")\n            return \"\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_beta_str","title":"<code>get_beta_str(dataframe_data)</code>","text":"<p>Retrieves the beta function string from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing beta function information.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The beta function string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_beta_str(dataframe_data: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Retrieves the beta function string from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing beta function information.\n\n    Returns:\n        str: The beta function string.\n    \"\"\"\n    if \"beta_x_ip1\" in dataframe_data.columns and \"beta_y_ip1\" in dataframe_data.columns:\n        betx_value = round(dataframe_data[\"beta_x_ip1\"].unique()[0], 2)\n        bety_value = round(dataframe_data[\"beta_y_ip1\"].unique()[0], 2)\n    else:\n        logging.warning(\"Beta functions not found in the dataframe\")\n        betx_value = 0\n        bety_value = 0\n\n    betx_str = r\"$\\beta^{*}_{x,1}$\"\n    bety_str = r\"$\\beta^{*}_{y,1}$\"\n    return f\"{betx_str}$= {{{betx_value}}}$ m, {bety_str}\" + f\"$= {{{bety_value}}}$\" + \" m\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_bunch_index_str","title":"<code>get_bunch_index_str(dataframe_data)</code>","text":"<p>Retrieves the bunch index from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing bunch index information.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The bunch index string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_bunch_index_str(dataframe_data: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Retrieves the bunch index from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing bunch index information.\n\n    Returns:\n        str: The bunch index string.\n    \"\"\"\n    if \"i_bunch_b1\" in dataframe_data.columns:\n        bunch_index_value = dataframe_data[\"i_bunch_b1\"].unique()[0]\n        return f\"Bunch {bunch_index_value}\"\n    else:\n        logging.warning(\"Bunch index not found in the dataframe\")\n        return \"\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_bunch_intensity_str","title":"<code>get_bunch_intensity_str(dataframe_data)</code>","text":"<p>Retrieves the bunch intensity string from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing bunch intensity information.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The bunch intensity string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_bunch_intensity_str(dataframe_data: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Retrieves the bunch intensity string from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing bunch intensity information.\n\n    Returns:\n        str: The bunch intensity string.\n    \"\"\"\n\n    if \"final_num_particles_per_bunch\" in dataframe_data.columns:\n        bunch_intensity_value = dataframe_data[\"final_num_particles_per_bunch\"].unique()[0]\n        return f\"$N_b \\simeq ${latex_float(float(bunch_intensity_value))} ppb\"\n    elif \"num_particles_per_bunch\" in dataframe_data.columns:\n        logging.warning(\n            \"final_num_particles_per_bunch not found in the dataframe.\"\n            \"Using num_particles_per_bunch instead.\"\n        )\n        bunch_intensity_value = dataframe_data[\"num_particles_per_bunch\"].unique()[0]\n        return f\"$N_b \\simeq ${latex_float(float(bunch_intensity_value))} ppb\"\n    else:\n        logging.warning(\"Bunch intensity not found in the dataframe\")\n        return \"\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_bunch_length_str","title":"<code>get_bunch_length_str(dataframe_data)</code>","text":"<p>Retrieves the bunch length from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing bunch length information.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The bunch length string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_bunch_length_str(dataframe_data: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Retrieves the bunch length from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing bunch length information.\n\n    Returns:\n        str: The bunch length string.\n    \"\"\"\n    if \"sigma_z\" in dataframe_data.columns:\n        bunch_length_value = dataframe_data[\"sigma_z\"].unique()[0] * 100\n        return f\"$\\sigma_{{z}} = {{{bunch_length_value}}}$ $cm$\"\n    else:\n        logging.warning(\"Bunch length not found in the dataframe\")\n        return \"\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_chromaticity_str","title":"<code>get_chromaticity_str(dataframe_data)</code>","text":"<p>Retrieves the chromaticity from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing chromaticity information.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The chromaticity string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_chromaticity_str(dataframe_data: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Retrieves the chromaticity from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing chromaticity information.\n\n    Returns:\n        str: The chromaticity string.\n    \"\"\"\n    if \"dqx_b1\" in dataframe_data.columns:\n        chroma_value = dataframe_data[\"dqx_b1\"].unique()[0]\n        return f\"$Q' = {{{chroma_value}}}$\"\n    else:\n        logging.warning(\"Chromaticity not found in the dataframe\")\n        return \"\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_crossing_IP_1_5_str","title":"<code>get_crossing_IP_1_5_str(dataframe_data, type_crossing)</code>","text":"<p>Retrieves the crossing angle strings for IP1 and IP5.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing crossing angle information.</p> required <code>type_crossing</code> <code>str</code> <p>The type of crossing. Either \"hv\" or \"vh\".</p> required <p>Returns:</p> Type Description <code>tuple[str, str]</code> <p>tuple[str, str]: The crossing angle strings for IP1 and IP5.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_crossing_IP_1_5_str(dataframe_data: pd.DataFrame, type_crossing: str) -&gt; tuple[str, str]:\n    \"\"\"\n    Retrieves the crossing angle strings for IP1 and IP5.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing crossing angle information.\n        type_crossing (str): The type of crossing. Either \"hv\" or \"vh\".\n\n    Returns:\n        tuple[str, str]: The crossing angle strings for IP1 and IP5.\n    \"\"\"\n    # Get crossing plane at IP1/5\n    phi_1_str, phi_5_str = _get_plane_crossing_IP_1_5_str(dataframe_data, type_crossing)\n\n    # Get crossing angle values at IP1 and IP5\n    xing_value_IP1 = _get_crossing_value_IP_1_5(dataframe_data, ip=1)\n    xing_value_IP5 = _get_crossing_value_IP_1_5(dataframe_data, ip=5)\n\n    # Get corresponding strings\n    xing_IP1_str = f\"{phi_1_str}$= {{{xing_value_IP1:.0f}}}$\" + \" $\\mu rad$\"\n    xing_IP5_str = f\"{phi_5_str}$= {{{xing_value_IP5:.0f}}}$\" + \" $\\mu rad$\"\n\n    return xing_IP1_str, xing_IP5_str\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_crossing_IP_2_8_str","title":"<code>get_crossing_IP_2_8_str(dataframe_data)</code>","text":"<p>Retrieves the crossing angle strings for IP2 and IP8.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing crossing angle information.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>tuple[str, str]: The crossing angle strings for IP2 and IP8.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_crossing_IP_2_8_str(dataframe_data: pd.DataFrame) -&gt; list[str]:\n    \"\"\"\n    Retrieves the crossing angle strings for IP2 and IP8.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing crossing angle information.\n\n    Returns:\n        tuple[str, str]: The crossing angle strings for IP2 and IP8.\n    \"\"\"\n    # First collect crossing angle values\n    dic_xing_values = {}\n    for ip in [2, 8]:\n        dic_xing_values[ip] = {}\n        for type_angle in [\"\", \"h\", \"v\"]:\n            if f\"on_x{ip}{type_angle}_final\" in dataframe_data.columns:\n                xing_value = dataframe_data[f\"on_x{ip}{type_angle}_final\"].unique()[0]\n            elif f\"on_x{ip}{type_angle}\" in dataframe_data.columns:\n                logging.warning(\n                    f\"on_x{ip}{type_angle}_final not found in the dataframe. \"\n                    f\"Using on_x{ip}{type_angle} instead.\"\n                )\n                xing_value = dataframe_data[f\"on_x{ip}{type_angle}\"].unique()[0]\n            else:\n                xing_value = 0\n            dic_xing_values[ip][type_angle] = xing_value\n\n    # Then create the strings\n    l_xing_IP_str = []\n    for ip in [2, 8]:\n        if dic_xing_values[ip][\"h\"] != 0 and dic_xing_values[ip][\"v\"] == 0:\n            xing_IP_str = (\n                r\"$\\Phi/2_{\"\n                + f\"{ip},H\"\n                + r\"}$\"\n                + f\"$= {{{dic_xing_values[ip]['h']:.0f}}}$ $\\mu rad$\"\n            )\n        elif dic_xing_values[ip][\"h\"] == 0 and dic_xing_values[ip][\"v\"] != 0:\n            xing_IP_str = (\n                r\"$\\Phi/2_{\"\n                + f\"{ip},V\"\n                + r\"}$\"\n                + f\"$= {{{dic_xing_values[ip]['v']:.0f}}}$ $\\mu rad$\"\n            )\n        elif dic_xing_values[ip][\"h\"] != 0 and dic_xing_values[ip][\"v\"] != 0:\n            logging.warning(\n                f\"It seems that the crossing angles at IP{ip} are not orthogonal... \"\n                f\"Only keeping the plane with the maximum crossing angle, but you might want to \"\n                f\"double-check this.\"\n            )\n            xing_IP_str = (\n                r\"$\\Phi/2_{\"\n                + f\"{ip},H\"\n                + r\"}$\"\n                + f\"$= {{{dic_xing_values[ip]['h']:.0f}}}$ $\\mu rad$\"\n                if dic_xing_values[ip][\"h\"] &gt; dic_xing_values[ip][\"v\"]\n                else r\"$\\Phi/2_{\"\n                + f\"{ip},V\"\n                + r\"}$\"\n                + f\"$= {{{dic_xing_values[ip]['v']:.0f}}}$ $\\mu rad$\"\n            )\n        elif dic_xing_values[ip][\"\"] != 0:\n            xing_IP_str = (\n                r\"$\\Phi/2_{\" + f\"{ip}\" + r\"}$\" + f\"$= {{{dic_xing_values[ip]['']:.0f}}}$ $\\mu rad$\"\n            )\n        else:\n            logging.warning(f\"Crossing angle at IP{ip} seems to be 0. Maybe double-check.\")\n            xing_IP_str = r\"$\\Phi/2_{\" + f\"{ip}\" + r\"}$\" + f\"$= 0$ $\\mu rad$\"\n        l_xing_IP_str.append(xing_IP_str)\n\n    return l_xing_IP_str\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_crossing_type","title":"<code>get_crossing_type(dataframe_data)</code>","text":"<p>Retrieves the crossing type from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing crossing type information.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The crossing type string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_crossing_type(dataframe_data: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Retrieves the crossing type from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing crossing type information.\n\n    Returns:\n        str: The crossing type string.\n    \"\"\"\n    if \"optics_file\" in dataframe_data.columns:\n        optics_file = dataframe_data[\"optics_file\"].unique()[0]\n        if \"flatvh\" in optics_file or \"vh\" in optics_file:\n            return \"vh\"\n        elif \"flathv\" in optics_file or \"hv\" in optics_file:\n            return \"hv\"\n\n    logging.warning(\"Crossing type not found in the dataframe. Falling back to hv.\")\n    return \"hv\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_energy_str","title":"<code>get_energy_str(dataframe_data, ions=False)</code>","text":"<p>Retrieves the energy from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing energy information.</p> required <code>ions</code> <code>bool</code> <p>Whether the study is for ions. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The energy string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_energy_str(dataframe_data: pd.DataFrame, ions: bool = False) -&gt; str:\n    \"\"\"\n    Retrieves the energy from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing energy information.\n        ions (bool, optional): Whether the study is for ions. Defaults to False.\n\n    Returns:\n        str: The energy string.\n    \"\"\"\n    if \"beam_energy_tot_b1\" in dataframe_data.columns:\n        energy_value = dataframe_data[\"beam_energy_tot_b1\"].unique()[0] / 1000\n        if not ions:\n            return f\"$E = {{{energy_value:.1f}}}$ $TeV$\"\n        else:\n            return f\"$E = {{{energy_value/82:.1f}}}$ Z $TeV$\"\n    else:\n        logging.warning(\"Energy not found in the dataframe\")\n        return \"\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_filling_scheme_str","title":"<code>get_filling_scheme_str(dataframe_data)</code>","text":"<p>Retrieves the filling scheme from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing filling scheme information.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The filling scheme string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_filling_scheme_str(dataframe_data: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Retrieves the filling scheme from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing filling scheme information.\n\n    Returns:\n        str: The filling scheme string.\n    \"\"\"\n    if \"pattern_fname\" in dataframe_data.columns:\n        filling_scheme_value = dataframe_data[\"pattern_fname\"].unique()[0]\n        # Only keep the last part of the path, which is the filling scheme\n        filling_scheme_value = filling_scheme_value.split(\"/\")[-1]\n        # Clean\n        if \"12inj\" in filling_scheme_value:\n            filling_scheme_value = filling_scheme_value.split(\"12inj\")[0] + \"12inj\"\n        return f\"{filling_scheme_value}\"\n    else:\n        logging.warning(\"Filling scheme not found in the dataframe\")\n        return \"\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_linear_coupling_str","title":"<code>get_linear_coupling_str(dataframe_data)</code>","text":"<p>Retrieves the linear coupling from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing linear coupling information.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The linear coupling string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_linear_coupling_str(dataframe_data: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Retrieves the linear coupling from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing linear coupling information.\n\n    Returns:\n        str: The linear coupling string.\n    \"\"\"\n    if \"delta_cmr\" in dataframe_data.columns:\n        coupling_value = dataframe_data[\"delta_cmr\"].unique()[0]\n        return f\"$C^- = {{{coupling_value}}}$\"\n    else:\n        logging.warning(\"Linear coupling not found in the dataframe\")\n        return \"\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_luminosity_at_ip_str","title":"<code>get_luminosity_at_ip_str(dataframe_data, ip, beam_beam=True)</code>","text":"<p>Retrieves the luminosity at a given IP from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing luminosity information.</p> required <code>ip</code> <code>int</code> <p>The IP number.</p> required <code>beam_beam</code> <code>bool</code> <p>Whether to consider beam-beam luminosity. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The luminosity string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_luminosity_at_ip_str(dataframe_data: pd.DataFrame, ip: int, beam_beam=True) -&gt; str:\n    \"\"\"\n    Retrieves the luminosity at a given IP from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing luminosity information.\n        ip (int): The IP number.\n        beam_beam (bool, optional): Whether to consider beam-beam luminosity. Defaults to True.\n\n    Returns:\n        str: The luminosity string.\n    \"\"\"\n    # sourcery skip: merge-else-if-into-elif, simplify-fstring-formatting\n    unit_luminosity = \"cm$^{-2}$s$^{-1}$\"\n    if beam_beam:\n        if f\"luminosity_ip{ip}_with_beam_beam\" in dataframe_data.columns:\n            luminosity_value = np.mean(dataframe_data[f\"luminosity_ip{ip}_with_beam_beam\"].unique())\n            return f\"$L_{{{ip}}} \\simeq ${latex_float(float(luminosity_value))} {unit_luminosity}\"\n        else:\n            logging.warning(f\"Luminosity at IP{ip} with beam-beam not found in the dataframe\")\n            return \"\"\n    else:\n        if f\"luminosity_ip{ip}_without_beam_beam\" in dataframe_data.columns:\n            luminosity_value = np.mean(\n                dataframe_data[f\"luminosity_ip{ip}_without_beam_beam\"].unique()\n            )\n            return f\"$L_{{{ip}}} \\simeq ${latex_float(float(luminosity_value))} {unit_luminosity}\"\n        else:\n            logging.warning(f\"Luminosity at IP{ip} without beam-beam not found in the dataframe\")\n            return \"\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_normalized_emittance_str","title":"<code>get_normalized_emittance_str(dataframe_data)</code>","text":"<p>Retrieves the normalized emittance from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing normalized emittance information.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The normalized emittance string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_normalized_emittance_str(dataframe_data: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Retrieves the normalized emittance from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing normalized emittance information.\n\n    Returns:\n        str: The normalized emittance string.\n    \"\"\"\n    if \"nemitt_x\" in dataframe_data.columns:\n        emittance_value = dataframe_data[\"nemitt_x\"].unique()[0] / 1e-6\n        # Round to 5 digits\n        emittance_value = round(emittance_value, 5)\n        return f\"$\\epsilon_{{n}} = {{{emittance_value}}}$ $\\mu m$\"\n    else:\n        logging.warning(\"Emittance not found in the dataframe\")\n        return \"\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_number_of_turns_str","title":"<code>get_number_of_turns_str(dataframe_data)</code>","text":"<p>Retrieves the number of turns from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing the number of turns information.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The number of turns string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_number_of_turns_str(dataframe_data: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Retrieves the number of turns from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing the number of turns information.\n\n    Returns:\n        str: The number of turns string.\n    \"\"\"\n    if \"n_turns\" in dataframe_data.columns:\n        n_turns_value = dataframe_data[\"n_turns\"].unique()[0]\n        return f\"$N_{{turns}} = {{{n_turns_value}}}$\"\n    else:\n        logging.warning(\"Number of turns not found in the dataframe\")\n        return \"\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_octupole_intensity_str","title":"<code>get_octupole_intensity_str(dataframe_data)</code>","text":"<p>Retrieves the octupole intensity from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing octupole intensity information.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The octupole intensity string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_octupole_intensity_str(dataframe_data: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Retrieves the octupole intensity from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing octupole intensity information.\n\n    Returns:\n        str: The octupole intensity string.\n    \"\"\"\n    if \"i_oct_b1\" in dataframe_data.columns:\n        octupole_intensity_value = dataframe_data[\"i_oct_b1\"].unique()[0]\n        return f\"$I_{{OCT}} = {{{octupole_intensity_value}}}$ $A$\"\n    else:\n        logging.warning(\"Octupole intensity not found in the dataframe\")\n        return \"\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_polarity_IP_2_8_str","title":"<code>get_polarity_IP_2_8_str(dataframe_data)</code>","text":"<p>Retrieves the polarity at IP2 and IP8 from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing polarity information.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The polarity string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_polarity_IP_2_8_str(dataframe_data: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Retrieves the polarity at IP2 and IP8 from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing polarity information.\n\n    Returns:\n        str: The polarity string.\n    \"\"\"\n    if (\n        \"on_alice_normalized\" in dataframe_data.columns\n        and \"on_lhcb_normalized\" in dataframe_data.columns\n    ):\n        polarity_value_IP2 = dataframe_data[\"on_alice_normalized\"].unique()[0]\n        polarity_value_IP8 = dataframe_data[\"on_lhcb_normalized\"].unique()[0]\n        return f\"$polarity$ $IP_{{2/8}} = {{{polarity_value_IP2}}}/{{{polarity_value_IP8}}}$\"\n    else:\n        logging.warning(\"Polarity at IP2 and IP8 not found in the dataframe\")\n        return \"\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_title_from_configuration","title":"<code>get_title_from_configuration(dataframe_data, ions=False, crossing_type=None, display_LHC_version=True, display_energy=True, display_bunch_index=True, display_CC_crossing=True, display_bunch_intensity=True, display_beta=True, display_crossing_IP_1=True, display_crossing_IP_2=True, display_crossing_IP_5=True, display_crossing_IP_8=True, display_bunch_length=True, display_polarity_IP_2_8=True, display_emittance=True, display_chromaticity=True, display_octupole_intensity=True, display_coupling=True, display_filling_scheme=True, display_horizontal_tune=None, display_vertical_tune=None, display_tune=True, display_luminosity_1=True, display_luminosity_2=True, display_luminosity_5=True, display_luminosity_8=True, display_PU_1=True, display_PU_2=True, display_PU_5=True, display_PU_8=True, display_number_of_turns=False)</code>","text":"<p>Generates a title string from the configuration data.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing configuration data.</p> required <code>ions</code> <code>bool</code> <p>Whether the beam is composed of ions. Defaults to False.</p> <code>False</code> <code>crossing_type</code> <code>str</code> <p>The type of crossing: 'vh' or 'hv'. Defaults to None, meaning it will try to be inferred from the optics file name. Back to 'hv' if not found.</p> <code>None</code> <code>display_betx_bety</code> <code>bool</code> <p>Whether to display the beta functions. Defaults to True.</p> required <code>display_LHC_version</code> <code>bool</code> <p>Whether to display the LHC version. Defaults to True.</p> <code>True</code> <code>display_energy</code> <code>bool</code> <p>Whether to display the energy. Defaults to True.</p> <code>True</code> <code>display_bunch_index</code> <code>bool</code> <p>Whether to display the bunch index. Defaults to True.</p> <code>True</code> <code>display_CC_crossing</code> <code>bool</code> <p>Whether to display the CC crossing. Defaults to True.</p> <code>True</code> <code>display_bunch_intensity</code> <code>bool</code> <p>Whether to display the bunch intensity. Defaults to True.</p> <code>True</code> <code>display_beta</code> <code>bool</code> <p>Whether to display the beta function. Defaults to True.</p> <code>True</code> <code>display_crossing_IP_1</code> <code>bool</code> <p>Whether to display the crossing at IP1. Defaults to True.</p> <code>True</code> <code>display_crossing_IP_2</code> <code>bool</code> <p>Whether to display the crossing at IP2. Defaults to True.</p> <code>True</code> <code>display_crossing_IP_5</code> <code>bool</code> <p>Whether to display the crossing at IP5. Defaults to True.</p> <code>True</code> <code>display_crossing_IP_8</code> <code>bool</code> <p>Whether to display the crossing at IP8. Defaults to True.</p> <code>True</code> <code>display_bunch_length</code> <code>bool</code> <p>Whether to display the bunch length. Defaults to True.</p> <code>True</code> <code>display_polarity_IP_2_8</code> <code>bool</code> <p>Whether to display the polarity at IP2 and IP8. Defaults to True.</p> <code>True</code> <code>display_emittance</code> <code>bool</code> <p>Whether to display the emittance. Defaults to True.</p> <code>True</code> <code>display_chromaticity</code> <code>bool</code> <p>Whether to display the chromaticity. Defaults to True.</p> <code>True</code> <code>display_octupole_intensity</code> <code>bool</code> <p>Whether to display the octupole intensity. Defaults to True.</p> <code>True</code> <code>display_coupling</code> <code>bool</code> <p>Whether to display the coupling. Defaults to True.</p> <code>True</code> <code>display_filling_scheme</code> <code>bool</code> <p>Whether to display the filling scheme. Defaults to True.</p> <code>True</code> <code>display_horizontal_tune</code> <code>bool</code> <p>Whether to display the horizontal tune. Defaults to None. Takes precedence over display_tune.</p> <code>None</code> <code>display_vertical_tune</code> <code>bool</code> <p>Whether to display the vertical tune. Defaults to None. Takes precedence over display_tune.</p> <code>None</code> <code>display_tune</code> <code>bool</code> <p>Whether to display the tune. Defaults to True.</p> <code>True</code> <code>display_luminosity_1</code> <code>bool</code> <p>Whether to display the luminosity at IP1. Defaults to True.</p> <code>True</code> <code>display_luminosity_2</code> <code>bool</code> <p>Whether to display the luminosity at IP2. Defaults to True.</p> <code>True</code> <code>display_luminosity_5</code> <code>bool</code> <p>Whether to display the luminosity at IP5. Defaults to True.</p> <code>True</code> <code>display_luminosity_8</code> <code>bool</code> <p>Whether to display the luminosity at IP8. Defaults to True.</p> <code>True</code> <code>display_PU_1</code> <code>bool</code> <p>Whether to display the PU at IP1. Defaults to True.</p> <code>True</code> <code>display_PU_2</code> <code>bool</code> <p>Whether to display the PU at IP2. Defaults to True.</p> <code>True</code> <code>display_PU_5</code> <code>bool</code> <p>Whether to display the PU at IP5. Defaults to True.</p> <code>True</code> <code>display_PU_8</code> <code>bool</code> <p>Whether to display the PU at IP8. Defaults to True.</p> <code>True</code> <code>display_number_of_turns</code> <code>bool</code> <p>Whether to display the number of turns. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The generated title string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_title_from_configuration(\n    dataframe_data: pd.DataFrame,\n    ions: bool = False,\n    crossing_type: Optional[str] = None,\n    display_LHC_version: bool = True,\n    display_energy: bool = True,\n    display_bunch_index: bool = True,\n    display_CC_crossing: bool = True,\n    display_bunch_intensity: bool = True,\n    display_beta: bool = True,\n    display_crossing_IP_1: bool = True,\n    display_crossing_IP_2: bool = True,\n    display_crossing_IP_5: bool = True,\n    display_crossing_IP_8: bool = True,\n    display_bunch_length: bool = True,\n    display_polarity_IP_2_8: bool = True,\n    display_emittance: bool = True,\n    display_chromaticity: bool = True,\n    display_octupole_intensity: bool = True,\n    display_coupling: bool = True,\n    display_filling_scheme: bool = True,\n    display_horizontal_tune: Optional[bool] = None,\n    display_vertical_tune: Optional[bool] = None,\n    display_tune: bool = True,\n    display_luminosity_1: bool = True,\n    display_luminosity_2: bool = True,\n    display_luminosity_5: bool = True,\n    display_luminosity_8: bool = True,\n    display_PU_1: bool = True,\n    display_PU_2: bool = True,\n    display_PU_5: bool = True,\n    display_PU_8: bool = True,\n    display_number_of_turns=False,\n) -&gt; str:\n    \"\"\"\n    Generates a title string from the configuration data.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing configuration data.\n        ions (bool, optional): Whether the beam is composed of ions. Defaults to False.\n        crossing_type (str, optional): The type of crossing: 'vh' or 'hv'. Defaults to None, meaning\n            it will try to be inferred from the optics file name. Back to 'hv' if not found.\n        display_betx_bety (bool, optional): Whether to display the beta functions. Defaults to True.\n        display_LHC_version (bool, optional): Whether to display the LHC version. Defaults to True.\n        display_energy (bool, optional): Whether to display the energy. Defaults to True.\n        display_bunch_index (bool, optional): Whether to display the bunch index. Defaults to True.\n        display_CC_crossing (bool, optional): Whether to display the CC crossing. Defaults to True.\n        display_bunch_intensity (bool, optional): Whether to display the bunch intensity. Defaults\n            to True.\n        display_beta (bool, optional): Whether to display the beta function. Defaults to True.\n        display_crossing_IP_1 (bool, optional): Whether to display the crossing at IP1. Defaults to\n            True.\n        display_crossing_IP_2 (bool, optional): Whether to display the crossing at IP2. Defaults to\n            True.\n        display_crossing_IP_5 (bool, optional): Whether to display the crossing at IP5. Defaults to\n            True.\n        display_crossing_IP_8 (bool, optional): Whether to display the crossing at IP8. Defaults to\n            True.\n        display_bunch_length (bool, optional): Whether to display the bunch length. Defaults to\n            True.\n        display_polarity_IP_2_8 (bool, optional): Whether to display the polarity at IP2 and IP8.\n            Defaults to True.\n        display_emittance (bool, optional): Whether to display the emittance. Defaults to True.\n        display_chromaticity (bool, optional): Whether to display the chromaticity.\n            Defaults to True.\n        display_octupole_intensity (bool, optional): Whether to display the octupole intensity.\n            Defaults to True.\n        display_coupling (bool, optional): Whether to display the coupling. Defaults to True.\n        display_filling_scheme (bool, optional): Whether to display the filling scheme. Defaults to\n            True.\n        display_horizontal_tune (bool, optional): Whether to display the horizontal tune. Defaults to\n            None. Takes precedence over display_tune.\n        display_vertical_tune (bool, optional): Whether to display the vertical tune. Defaults to\n            None. Takes precedence over display_tune.\n        display_tune (bool, optional): Whether to display the tune. Defaults to True.\n        display_luminosity_1 (bool, optional): Whether to display the luminosity at IP1. Defaults to\n            True.\n        display_luminosity_2 (bool, optional): Whether to display the luminosity at IP2. Defaults to\n            True.\n        display_luminosity_5 (bool, optional): Whether to display the luminosity at IP5. Defaults to\n            True.\n        display_luminosity_8 (bool, optional): Whether to display the luminosity at IP8. Defaults to\n            True.\n        display_PU_1 (bool, optional): Whether to display the PU at IP1. Defaults to True.\n        display_PU_2 (bool, optional): Whether to display the PU at IP2. Defaults to True.\n        display_PU_5 (bool, optional): Whether to display the PU at IP5. Defaults to True.\n        display_PU_8 (bool, optional): Whether to display the PU at IP8. Defaults to True.\n        display_number_of_turns (bool, optional): Whether to display the number of turns. Defaults to\n            False.\n\n    Returns:\n        str: The generated title string.\n    \"\"\"\n\n    # Warn about tune definition\n    if (\n        display_horizontal_tune is not None or display_vertical_tune is not None\n    ) and not display_tune:\n        logging.warning(\n            \"You have defined display_horizontal_tune or display_vertical_tune, but not \"\n            \"display_tune. The horizontal and/or vertical tunes will still be displayed.\"\n        )\n\n    # Find out what is the crossing type\n    if crossing_type is None:\n        crossing_type = get_crossing_type(dataframe_data)\n\n    # Collect all the information to display\n    LHC_version_str = get_LHC_version_str(dataframe_data, ions)\n    energy_str = get_energy_str(dataframe_data, ions)\n    bunch_index_str = get_bunch_index_str(dataframe_data)\n    CC_crossing_str = get_CC_crossing_str(dataframe_data)\n    bunch_intensity_str = get_bunch_intensity_str(dataframe_data)\n    beta_str = get_beta_str(dataframe_data)\n    xing_IP1_str, xing_IP5_str = get_crossing_IP_1_5_str(dataframe_data, crossing_type)\n    xing_IP2_str, xing_IP8_str = get_crossing_IP_2_8_str(dataframe_data)\n    bunch_length_str = get_bunch_length_str(dataframe_data)\n    polarity_str = get_polarity_IP_2_8_str(dataframe_data)\n    emittance_str = get_normalized_emittance_str(dataframe_data)\n    chromaticity_str = get_chromaticity_str(dataframe_data)\n    octupole_intensity_str = get_octupole_intensity_str(dataframe_data)\n    coupling_str = get_linear_coupling_str(dataframe_data)\n    filling_scheme_str = get_filling_scheme_str(dataframe_data)\n    tune_str = get_tune_str(dataframe_data, display_horizontal_tune, display_vertical_tune)\n    n_turns_str = get_number_of_turns_str(dataframe_data)\n\n    # Collect luminosity and PU strings at each IP\n    dic_lumi_PU_str = {\n        \"with_beam_beam\": {\"lumi\": {}, \"PU\": {}},\n        \"without_beam_beam\": {\"lumi\": {}, \"PU\": {}},\n    }\n    for beam_beam in [\"with_beam_beam\", \"without_beam_beam\"]:\n        for ip in [1, 2, 5, 8]:\n            dic_lumi_PU_str[beam_beam][\"lumi\"][ip] = get_luminosity_at_ip_str(\n                dataframe_data, ip, beam_beam=True\n            )\n            dic_lumi_PU_str[beam_beam][\"PU\"][ip] = get_PU_at_IP_str(\n                dataframe_data, ip, beam_beam=True\n            )\n\n    def test_if_empty_and_add_period(string: str) -&gt; str:\n        \"\"\"\n        Test if a string is empty and add a period if not.\n\n        Args:\n            string (str): The string to test.\n\n        Returns:\n            str: The string with a period if not empty.\n        \"\"\"\n        return f\"{string}. \" if string != \"\" else \"\"\n\n    # Make the final title (order is the same as in the past)\n    title = \"\"\n    if display_LHC_version:\n        title += test_if_empty_and_add_period(LHC_version_str)\n    if display_energy:\n        title += test_if_empty_and_add_period(energy_str)\n    if display_CC_crossing:\n        title += test_if_empty_and_add_period(CC_crossing_str)\n    if display_bunch_intensity:\n        title += test_if_empty_and_add_period(bunch_intensity_str)\n    # Jump to the next line\n    title += \"\\n\"\n    if display_luminosity_1:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"lumi\"][1])\n    if display_PU_1:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"PU\"][1])\n    if display_luminosity_5:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"lumi\"][5])\n    if display_PU_5:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"PU\"][5])\n    # Jump to the next line\n    title += \"\\n\"\n    if display_luminosity_2:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"lumi\"][2])\n    if display_PU_2:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"PU\"][2])\n    if display_luminosity_8:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"lumi\"][8])\n    if display_PU_8:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"PU\"][8])\n    # Jump to the next line\n    title += \"\\n\"\n    if display_beta:\n        title += test_if_empty_and_add_period(beta_str)\n    if display_polarity_IP_2_8:\n        title += test_if_empty_and_add_period(polarity_str)\n    if display_bunch_length:\n        title += test_if_empty_and_add_period(bunch_length_str)\n    # Jump to the next line\n    title += \"\\n\"\n    if display_crossing_IP_1:\n        title += test_if_empty_and_add_period(xing_IP1_str)\n    if display_crossing_IP_5:\n        title += test_if_empty_and_add_period(xing_IP5_str)\n    if display_crossing_IP_2:\n        title += test_if_empty_and_add_period(xing_IP2_str)\n    if display_crossing_IP_8:\n        title += test_if_empty_and_add_period(xing_IP8_str)\n\n    # Jump to the next line\n    title += \"\\n\"\n    if display_emittance:\n        title += test_if_empty_and_add_period(emittance_str)\n    if display_chromaticity:\n        title += test_if_empty_and_add_period(chromaticity_str)\n    if display_octupole_intensity:\n        title += test_if_empty_and_add_period(octupole_intensity_str)\n    if display_coupling:\n        title += test_if_empty_and_add_period(coupling_str)\n    if display_tune:\n        title += test_if_empty_and_add_period(tune_str)\n    # Jump to the next line\n    title += \"\\n\"\n    if display_filling_scheme:\n        title += test_if_empty_and_add_period(filling_scheme_str)\n    if display_bunch_index:\n        title += test_if_empty_and_add_period(bunch_index_str)\n    # Jump to the next line\n    if display_number_of_turns:\n        title += \"\\n\"\n        title += test_if_empty_and_add_period(n_turns_str)\n\n    # Filter final title for empty lines\n    title = \"\\n\".join([line for line in title.split(\"\\n\") if line.strip() != \"\"])\n\n    return title\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_tune_str","title":"<code>get_tune_str(dataframe_data, display_horizontal_tune=None, display_vertical_tune=None)</code>","text":"<p>Retrieves the tune from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing tune information.</p> required <code>display_horizontal_tune</code> <code>bool</code> <p>Whether to display the horizontal tune. Defaults to None.</p> <code>None</code> <code>display_vertical_tune</code> <code>bool</code> <p>Whether to display the vertical tune. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The tune string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_tune_str(\n    dataframe_data: pd.DataFrame,\n    display_horizontal_tune: Optional[bool] = None,\n    display_vertical_tune: Optional[bool] = None,\n) -&gt; str:\n    \"\"\"\n    Retrieves the tune from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing tune information.\n        display_horizontal_tune (bool, optional): Whether to display the horizontal tune. Defaults to\n            None.\n        display_vertical_tune (bool, optional): Whether to display the vertical tune. Defaults to\n            None.\n\n    Returns:\n        str: The tune string.\n    \"\"\"\n    if \"qx_b1\" in dataframe_data.columns and \"qy_b1\" in dataframe_data.columns:\n        tune_h_value = dataframe_data[\"qx_b1\"].unique()[0]\n        tune_v_value = dataframe_data[\"qy_b1\"].unique()[0]\n        if (\n            (\n                display_horizontal_tune is not None\n                and display_vertical_tune is not None\n                and display_horizontal_tune\n                and display_vertical_tune\n            )\n            or display_horizontal_tune is None\n            and display_vertical_tune is None\n        ):\n            return f\"$Q_x = {tune_h_value}$, $Q_y = {tune_v_value}$\"\n        elif display_horizontal_tune is not None and display_horizontal_tune:\n            return f\"$Q_x = {tune_h_value}$\"\n        elif display_vertical_tune is not None and display_vertical_tune:\n            return f\"$Q_y = {tune_v_value}$\"\n        else:\n            return \"\"\n    else:\n        logging.warning(\"Tune not found in the dataframe\")\n        return \"\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.latex_float","title":"<code>latex_float(f, precision=3)</code>","text":"<p>Converts a float to a scientific LaTeX format string.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>float</code> <p>The float to convert.</p> required <code>precision</code> <code>int</code> <p>The precision of the float. Defaults to 3.</p> <code>3</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The float in scientific LaTeX format.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def latex_float(f: float, precision: int = 3) -&gt; str:\n    \"\"\"\n    Converts a float to a scientific LaTeX format string.\n\n    Args:\n        f (float): The float to convert.\n        precision (int, optional): The precision of the float. Defaults to 3.\n\n    Returns:\n        str: The float in scientific LaTeX format.\n    \"\"\"\n    float_str = \"{0:.{1}g}\".format(f, precision)\n\n    # In case the float is an integer, don't use scientific notation\n    if \"e\" not in float_str:\n        return float_str\n\n    # Otherwise, split the float into base and exponent\n    base, exponent = float_str.split(\"e\")\n    return r\"${0} \\times 10^{{{1}}}$\".format(base, int(exponent))\n</code></pre>"},{"location":"reference/study_da/plot/plot_study.html","title":"plot_study","text":"<p>This module provides functions to create and customize study plots, including heatmaps and 3D volume renderings.</p> <p>Functions:</p> Name Description <code>_set_style</code> <code>_add_text_annotation</code> <code>_smooth</code> <code>_mask</code> <code>_add_contours</code> <code>_add_diagonal_lines</code> <code>add_QR_code</code> <code>_set_labels</code> <code>plot_heatmap</code> <code>plot_3D</code>"},{"location":"reference/study_da/plot/plot_study.html#study_da.plot.plot_study.add_QR_code","title":"<code>add_QR_code(fig, link, position_qr='top-right')</code>","text":"<p>Adds a QR code pointing to the given link to the figure.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>The figure to add the QR code to.</p> required <code>link</code> <code>str</code> <p>The link to encode in the QR code.</p> required <p>Returns:</p> Type Description <code>Figure</code> <p>plt.Figure: The figure with the QR code.</p> Source code in <code>study_da/plot/plot_study.py</code> <pre><code>def add_QR_code(fig: plt.Figure, link: str, position_qr=\"top-right\") -&gt; plt.Figure:\n    \"\"\"\n    Adds a QR code pointing to the given link to the figure.\n\n    Args:\n        fig (plt.Figure): The figure to add the QR code to.\n        link (str): The link to encode in the QR code.\n\n    Returns:\n        plt.Figure: The figure with the QR code.\n    \"\"\"\n    # Add QR code pointing to the github repository\n    qr = qrcode.QRCode(\n        # version=None,\n        box_size=10,\n        border=1,\n    )\n    qr.add_data(link)\n    qr.make(fit=False)\n    im = qr.make_image(fill_color=\"black\", back_color=\"transparent\")\n    if position_qr == \"top-right\":\n        newax = fig.add_axes([0.9, 0.9, 0.05, 0.05], anchor=\"NE\", zorder=1)\n    elif position_qr == \"bottom-right\":\n        newax = fig.add_axes([0.9, 0.1, 0.05, 0.05], anchor=\"SE\", zorder=1)\n    elif position_qr == \"bottom-left\":\n        newax = fig.add_axes([0.1, 0.1, 0.05, 0.05], anchor=\"SW\", zorder=1)\n    elif position_qr == \"top-left\":\n        newax = fig.add_axes([0.1, 0.9, 0.05, 0.05], anchor=\"NW\", zorder=1)\n    else:\n        raise ValueError(f\"Position {position_qr} not recognized\")\n    newax.imshow(im, resample=False, interpolation=\"none\", filternorm=False)\n    # Add link below qrcode\n    newax.plot([0, 0], [0, 0], color=\"white\", label=\"link\")\n    _ = newax.annotate(\n        \"lin\",\n        xy=(0, 300),\n        xytext=(0, 300),\n        fontsize=30,\n        url=link,\n        bbox=dict(color=\"white\", alpha=1e-6, url=link),\n        alpha=0,\n    )\n    # Hide X and Y axes label marks\n    newax.xaxis.set_tick_params(labelbottom=False)\n    newax.yaxis.set_tick_params(labelleft=False)\n    # Hide X and Y axes tick marks\n    newax.set_xticks([])\n    newax.set_yticks([])\n    newax.set_axis_off()\n\n    return fig\n</code></pre>"},{"location":"reference/study_da/plot/plot_study.html#study_da.plot.plot_study.plot_3D","title":"<code>plot_3D(dataframe_data, x_variable, y_variable, z_variable, color_variable, xlabel=None, ylabel=None, z_label=None, title='', vmin=4.5, vmax=7.5, surface_count=30, opacity=0.2, figsize=(1000, 1000), colormap='RdBu', colorbar_title_text='Minimum DA (\u03c3)', display_colormap=False, output_path='output.png', output_path_html='output.html', display_plot=True, dark_theme=False)</code>","text":"<p>Plots a 3D volume rendering from the given dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing the data to plot.</p> required <code>x_variable</code> <code>str</code> <p>The variable to plot on the x-axis.</p> required <code>y_variable</code> <code>str</code> <p>The variable to plot on the y-axis.</p> required <code>z_variable</code> <code>str</code> <p>The variable to plot on the z-axis.</p> required <code>color_variable</code> <code>str</code> <p>The variable to use for the color scale.</p> required <code>xlabel</code> <code>Optional[str]</code> <p>The label for the x-axis. Defaults to None.</p> <code>None</code> <code>ylabel</code> <code>Optional[str]</code> <p>The label for the y-axis. Defaults to None.</p> <code>None</code> <code>z_label</code> <code>Optional[str]</code> <p>The label for the z-axis. Defaults to None.</p> <code>None</code> <code>title</code> <code>str</code> <p>The title of the plot. Defaults to \"\".</p> <code>''</code> <code>vmin</code> <code>float</code> <p>The minimum value for the color scale. Defaults to 4.5.</p> <code>4.5</code> <code>vmax</code> <code>float</code> <p>The maximum value for the color scale. Defaults to 7.5.</p> <code>7.5</code> <code>surface_count</code> <code>int</code> <p>The number of surfaces for volume rendering. Defaults to 30.</p> <code>30</code> <code>opacity</code> <code>float</code> <p>The opacity of the volume rendering. Defaults to 0.2.</p> <code>0.2</code> <code>figsize</code> <code>tuple[float, float]</code> <p>The size of the figure. Defaults to (1000, 1000).</p> <code>(1000, 1000)</code> <code>colormap</code> <code>str</code> <p>The colormap to use. Defaults to \"RdBu\".</p> <code>'RdBu'</code> <code>colorbar_title_text</code> <code>str</code> <p>The label for the colorbar. Defaults to \"Minimum DA (\u03c3)\".</p> <code>'Minimum DA (\u03c3)'</code> <code>display_colormap</code> <code>bool</code> <p>Whether to display the colormap. Defaults to False.</p> <code>False</code> <code>output_path</code> <code>str</code> <p>The path to save the plot image. Defaults to \"output.png\".</p> <code>'output.png'</code> <code>output_path_html</code> <code>str</code> <p>The path to save the plot HTML. Defaults to \"output.html\".</p> <code>'output.html'</code> <code>display_plot</code> <code>bool</code> <p>Whether to display the plot. Defaults to True.</p> <code>True</code> <code>dark_theme</code> <code>bool</code> <p>Whether to use a dark theme. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>go.Figure: The plotly figure object.</p> Source code in <code>study_da/plot/plot_study.py</code> <pre><code>def plot_3D(\n    dataframe_data: pd.DataFrame,\n    x_variable: str,\n    y_variable: str,\n    z_variable: str,\n    color_variable: str,\n    xlabel: Optional[str] = None,\n    ylabel: Optional[str] = None,\n    z_label: Optional[str] = None,\n    title: str = \"\",\n    vmin: float = 4.5,\n    vmax: float = 7.5,\n    surface_count: int = 30,\n    opacity: float = 0.2,\n    figsize: tuple[float, float] = (1000, 1000),\n    colormap: str = \"RdBu\",\n    colorbar_title_text: str = \"Minimum DA (\u03c3)\",\n    display_colormap: bool = False,\n    output_path: str = \"output.png\",\n    output_path_html: str = \"output.html\",\n    display_plot: bool = True,\n    dark_theme: bool = False,\n) -&gt; Any:\n    \"\"\"\n    Plots a 3D volume rendering from the given dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing the data to plot.\n        x_variable (str): The variable to plot on the x-axis.\n        y_variable (str): The variable to plot on the y-axis.\n        z_variable (str): The variable to plot on the z-axis.\n        color_variable (str): The variable to use for the color scale.\n        xlabel (Optional[str], optional): The label for the x-axis. Defaults to None.\n        ylabel (Optional[str], optional): The label for the y-axis. Defaults to None.\n        z_label (Optional[str], optional): The label for the z-axis. Defaults to None.\n        title (str, optional): The title of the plot. Defaults to \"\".\n        vmin (float, optional): The minimum value for the color scale. Defaults to 4.5.\n        vmax (float, optional): The maximum value for the color scale. Defaults to 7.5.\n        surface_count (int, optional): The number of surfaces for volume rendering. Defaults to 30.\n        opacity (float, optional): The opacity of the volume rendering. Defaults to 0.2.\n        figsize (tuple[float, float], optional): The size of the figure. Defaults to (1000, 1000).\n        colormap (str, optional): The colormap to use. Defaults to \"RdBu\".\n        colorbar_title_text (str, optional): The label for the colorbar. Defaults to \"Minimum DA (\u03c3)\".\n        display_colormap (bool, optional): Whether to display the colormap. Defaults to False.\n        output_path (str, optional): The path to save the plot image. Defaults to \"output.png\".\n        output_path_html (str, optional): The path to save the plot HTML. Defaults to \"output.html\".\n        display_plot (bool, optional): Whether to display the plot. Defaults to True.\n        dark_theme (bool, optional): Whether to use a dark theme. Defaults to False.\n\n    Returns:\n        go.Figure: The plotly figure object.\n    \"\"\"\n    # Check if plotly is installed\n    try:\n        import plotly.graph_objects as go\n    except ImportError as e:\n        raise ImportError(\"Please install plotly to use this function\") from e\n\n    X = np.array(dataframe_data[x_variable])\n    Y = np.array(dataframe_data[y_variable])\n    Z = np.array(dataframe_data[z_variable])\n    values = np.array(dataframe_data[color_variable])\n    fig = go.Figure(\n        data=go.Volume(\n            x=X.flatten(),\n            y=Y.flatten(),\n            z=Z.flatten(),\n            value=values.flatten(),\n            isomin=vmin,\n            isomax=vmax,\n            opacity=opacity,  # needs to be small to see through all surfaces\n            surface_count=surface_count,  # needs to be a large number for good volume rendering\n            colorscale=colormap,\n            colorbar_title_text=colorbar_title_text,\n        )\n    )\n\n    fig.update_layout(\n        scene_xaxis_title_text=xlabel,\n        scene_yaxis_title_text=ylabel,\n        scene_zaxis_title_text=z_label,\n        title=title,\n    )\n\n    # Get a good initial view, dezoomed\n    fig.update_layout(scene_camera=dict(eye=dict(x=1.5, y=1.5, z=1.5)))\n\n    # Center the title\n    fig.update_layout(title_x=0.5, title_y=0.9, title_xanchor=\"center\", title_yanchor=\"top\")\n\n    # Specify the width and height of the figure\n    fig.update_layout(width=figsize[0], height=figsize[1])\n\n    # Remove margins and padding\n    fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\n\n    # Display the colormap\n    if not display_colormap:\n        fig.update_layout(coloraxis_showscale=False)\n        fig.update_traces(showscale=False)\n    else:\n        # Make colorbar smaller\n        fig.update_layout(coloraxis_colorbar=dict(thickness=10, len=0.5))\n\n    # Set the theme\n    if dark_theme:\n        fig.update_layout(template=\"plotly_dark\")\n\n    # Display/save/return the figure\n    if output_path is not None:\n        fig.write_image(output_path)\n\n    if output_path_html is not None:\n        fig.write_html(output_path_html)\n\n    if display_plot:\n        fig.show()\n\n    return fig\n</code></pre>"},{"location":"reference/study_da/plot/plot_study.html#study_da.plot.plot_study.plot_heatmap","title":"<code>plot_heatmap(dataframe_data, horizontal_variable, vertical_variable, color_variable, link=None, position_qr='top-right', plot_contours=True, xlabel=None, ylabel=None, tick_interval=2, round_xticks=None, round_yticks=None, symmetric_missing=False, mask_lower_triangle=False, mask_upper_triangle=False, plot_diagonal_lines=True, shift_diagonal_lines=1, xaxis_ticks_on_top=True, title='', vmin=4.5, vmax=7.5, k_masking=-1, green_contour=6.0, min_level_contours=1, max_level_contours=15, delta_levels_contours=0.5, figsize=None, label_cbar='Minimum DA (' + '$\\\\sigma$' + ')', colormap='coolwarm_r', style='ggplot', output_path='output.png', display_plot=True, latex_fonts=True, vectorize=False, fill_missing_value_with=None, dpi=300)</code>","text":"<p>Plots a heatmap from the given dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing the data to plot.</p> required <code>horizontal_variable</code> <code>str</code> <p>The variable to plot on the horizontal axis.</p> required <code>vertical_variable</code> <code>str</code> <p>The variable to plot on the vertical axis.</p> required <code>color_variable</code> <code>str</code> <p>The variable to use for the color scale.</p> required <code>link</code> <code>Optional[str]</code> <p>A link to encode in a QR code. Defaults to None.</p> <code>None</code> <code>plot_contours</code> <code>bool</code> <p>Whether to plot contours. Defaults to True.</p> <code>True</code> <code>xlabel</code> <code>Optional[str]</code> <p>The label for the x-axis. Defaults to None.</p> <code>None</code> <code>ylabel</code> <code>Optional[str]</code> <p>The label for the y-axis. Defaults to None.</p> <code>None</code> <code>tick_interval</code> <code>int</code> <p>The interval for the ticks. Defaults to 2.</p> <code>2</code> <code>round_xticks</code> <code>Optional[int]</code> <p>The number of decimal places to round the x-ticks to. Defaults to None.</p> <code>None</code> <code>round_yticks</code> <code>Optional[int]</code> <p>The number of decimal places to round the y-ticks to. Defaults to None.</p> <code>None</code> <code>symmetric_missing</code> <code>bool</code> <p>Whether to make the matrix symmetric by replacing the lower triangle with the upper triangle. Defaults to False.</p> <code>False</code> <code>mask_lower_triangle</code> <code>bool</code> <p>Whether to mask the lower triangle. Defaults to False.</p> <code>False</code> <code>mask_upper_triangle</code> <code>bool</code> <p>Whether to mask the upper triangle. Defaults to False.</p> <code>False</code> <code>plot_diagonal_lines</code> <code>bool</code> <p>Whether to plot diagonal lines. Defaults to True.</p> <code>True</code> <code>shift_diagonal_lines</code> <code>int</code> <p>The shift for the diagonal lines. Defaults to 1.</p> <code>1</code> <code>xaxis_ticks_on_top</code> <code>bool</code> <p>Whether to place the x-axis ticks on top. Defaults to True.</p> <code>True</code> <code>title</code> <code>str</code> <p>The title of the plot. Defaults to \"\".</p> <code>''</code> <code>vmin</code> <code>float</code> <p>The minimum value for the color scale. Defaults to 4.5.</p> <code>4.5</code> <code>vmax</code> <code>float</code> <p>The maximum value for the color scale. Defaults to 7.5.</p> <code>7.5</code> <code>k_masking</code> <code>int</code> <p>The k parameter for masking. Defaults to -1.</p> <code>-1</code> <code>green_contour</code> <code>Optional[float]</code> <p>The value for the green contour line. Defaults to 6.0.</p> <code>6.0</code> <code>min_level_contours</code> <code>float</code> <p>The minimum level for the contours. Defaults to 1.</p> <code>1</code> <code>max_level_contours</code> <code>float</code> <p>The maximum level for the contours. Defaults to 15.</p> <code>15</code> <code>delta_levels_contours</code> <code>float</code> <p>The delta between contour levels. Defaults to 0.5.</p> <code>0.5</code> <code>figsize</code> <code>Optional[tuple[float, float]]</code> <p>The size of the figure. Defaults to None.</p> <code>None</code> <code>label_cbar</code> <code>str</code> <p>The label for the colorbar. Defaults to \"Minimum DA ($\\sigma$)\".</p> <code>'Minimum DA (' + '$\\\\sigma$' + ')'</code> <code>colormap</code> <code>str</code> <p>The colormap to use. Defaults to \"coolwarm_r\".</p> <code>'coolwarm_r'</code> <code>style</code> <code>str</code> <p>The style to use for the plot. Defaults to \"ggplot\".</p> <code>'ggplot'</code> <code>output_path</code> <code>str</code> <p>The path to save the plot. Defaults to \"output.pdf\".</p> <code>'output.png'</code> <code>display_plot</code> <code>bool</code> <p>Whether to display the plot. Defaults to True.</p> <code>True</code> <code>latex_fonts</code> <code>bool</code> <p>Whether to use LaTeX fonts. Defaults to True.</p> <code>True</code> <code>vectorize</code> <code>bool</code> <p>Whether to vectorize the plot. Defaults to False.</p> <code>False</code> <code>fill_missing_value_with</code> <code>Optional[str | float]</code> <p>The value to fill missing values with. Can be a number or 'interpolate'. Defaults to None.</p> <code>None</code> <code>dpi</code> <code>int</code> <p>The DPI for the plot. Defaults to 300.</p> <code>300</code> <p>Returns:</p> Type Description <code>tuple[Figure, Axes]</code> <p>tuple[plt.Figure, plt.Axes]: The figure and axes of the plot.</p> Source code in <code>study_da/plot/plot_study.py</code> <pre><code>def plot_heatmap(\n    dataframe_data: pd.DataFrame,\n    horizontal_variable: str,\n    vertical_variable: str,\n    color_variable: str,\n    link: Optional[str] = None,\n    position_qr: Optional[str] = \"top-right\",\n    plot_contours: bool = True,\n    xlabel: Optional[str] = None,\n    ylabel: Optional[str] = None,\n    tick_interval: int = 2,\n    round_xticks: Optional[int] = None,\n    round_yticks: Optional[int] = None,\n    symmetric_missing: bool = False,\n    mask_lower_triangle: bool = False,\n    mask_upper_triangle: bool = False,\n    plot_diagonal_lines: bool = True,\n    shift_diagonal_lines: int = 1,\n    xaxis_ticks_on_top: bool = True,\n    title: str = \"\",\n    vmin: float = 4.5,\n    vmax: float = 7.5,\n    k_masking: int = -1,\n    green_contour: Optional[float] = 6.0,\n    min_level_contours: float = 1,\n    max_level_contours: float = 15,\n    delta_levels_contours: float = 0.5,\n    figsize: Optional[tuple[float, float]] = None,\n    label_cbar: str = \"Minimum DA (\" + r\"$\\sigma$\" + \")\",\n    colormap: str = \"coolwarm_r\",\n    style: str = \"ggplot\",\n    output_path: str = \"output.png\",\n    display_plot: bool = True,\n    latex_fonts: bool = True,\n    vectorize: bool = False,\n    fill_missing_value_with: Optional[str | float] = None,\n    dpi=300,\n) -&gt; tuple[plt.Figure, plt.Axes]:\n    \"\"\"\n    Plots a heatmap from the given dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing the data to plot.\n        horizontal_variable (str): The variable to plot on the horizontal axis.\n        vertical_variable (str): The variable to plot on the vertical axis.\n        color_variable (str): The variable to use for the color scale.\n        link (Optional[str], optional): A link to encode in a QR code. Defaults to None.\n        plot_contours (bool, optional): Whether to plot contours. Defaults to True.\n        xlabel (Optional[str], optional): The label for the x-axis. Defaults to None.\n        ylabel (Optional[str], optional): The label for the y-axis. Defaults to None.\n        tick_interval (int, optional): The interval for the ticks. Defaults to 2.\n        round_xticks (Optional[int], optional): The number of decimal places to round the x-ticks to.\n            Defaults to None.\n        round_yticks (Optional[int], optional): The number of decimal places to round the y-ticks to.\n            Defaults to None.\n        symmetric_missing (bool, optional): Whether to make the matrix symmetric by replacing the\n            lower triangle with the upper triangle. Defaults to False.\n        mask_lower_triangle (bool, optional): Whether to mask the lower triangle. Defaults to False.\n        mask_upper_triangle (bool, optional): Whether to mask the upper triangle. Defaults to False.\n        plot_diagonal_lines (bool, optional): Whether to plot diagonal lines. Defaults to True.\n        shift_diagonal_lines (int, optional): The shift for the diagonal lines. Defaults to 1.\n        xaxis_ticks_on_top (bool, optional): Whether to place the x-axis ticks on top. Defaults to True.\n        title (str, optional): The title of the plot. Defaults to \"\".\n        vmin (float, optional): The minimum value for the color scale. Defaults to 4.5.\n        vmax (float, optional): The maximum value for the color scale. Defaults to 7.5.\n        k_masking (int, optional): The k parameter for masking. Defaults to -1.\n        green_contour (Optional[float], optional): The value for the green contour line. Defaults to 6.0.\n        min_level_contours (float, optional): The minimum level for the contours. Defaults to 1.\n        max_level_contours (float, optional): The maximum level for the contours. Defaults to 15.\n        delta_levels_contours (float, optional): The delta between contour levels. Defaults to 0.5.\n        figsize (Optional[tuple[float, float]], optional): The size of the figure. Defaults to None.\n        label_cbar (str, optional): The label for the colorbar. Defaults to \"Minimum DA ($\\sigma$)\".\n        colormap (str, optional): The colormap to use. Defaults to \"coolwarm_r\".\n        style (str, optional): The style to use for the plot. Defaults to \"ggplot\".\n        output_path (str, optional): The path to save the plot. Defaults to \"output.pdf\".\n        display_plot (bool, optional): Whether to display the plot. Defaults to True.\n        latex_fonts (bool, optional): Whether to use LaTeX fonts. Defaults to True.\n        vectorize (bool, optional): Whether to vectorize the plot. Defaults to False.\n        fill_missing_value_with (Optional[str | float], optional): The value to fill missing values\n            with. Can be a number or 'interpolate'. Defaults to None.\n        dpi (int, optional): The DPI for the plot. Defaults to 300.\n\n    Returns:\n        tuple[plt.Figure, plt.Axes]: The figure and axes of the plot.\n    \"\"\"\n    # Use the requested style\n    _set_style(style, latex_fonts, vectorize)\n\n    # Get the dataframe to plot\n    df_to_plot = dataframe_data.pivot(\n        index=vertical_variable, columns=horizontal_variable, values=color_variable\n    )\n\n    # Get numpy array from dataframe\n    data_array = df_to_plot.to_numpy(dtype=float)\n\n    # Replace NaNs with a value if requested\n    if fill_missing_value_with is not None:\n        if isinstance(fill_missing_value_with, (int, float)):\n            data_array[np.isnan(data_array)] = fill_missing_value_with\n        elif fill_missing_value_with == \"interpolate\":\n            # Interpolate missing values with griddata\n            x = np.arange(data_array.shape[1])\n            y = np.arange(data_array.shape[0])\n            xx, yy = np.meshgrid(x, y)\n            x = xx[~np.isnan(data_array)]\n            y = yy[~np.isnan(data_array)]\n            z = data_array[~np.isnan(data_array)]\n            data_array = griddata((x, y), z, (xx, yy), method=\"cubic\")\n\n    # Mask the lower or upper triangle (checks are done in the function)\n    data_array_masked, mask_main_array = _mask(\n        mask_lower_triangle, mask_upper_triangle, data_array, k_masking\n    )\n\n    # Define colormap and set NaNs to white\n    cmap = matplotlib.colormaps.get_cmap(colormap)\n    cmap.set_bad(\"w\")\n\n    # Build heatmap, with inverted y axis\n    fig, ax = plt.subplots()\n    if figsize is not None:\n        fig.set_size_inches(figsize)\n    im = ax.imshow(data_array_masked, cmap=cmap, vmin=vmin, vmax=vmax)\n    ax.invert_yaxis()\n\n    # Add text annotations\n    ax = _add_text_annotation(df_to_plot, data_array, ax, vmin, vmax)\n\n    # Smooth data for contours\n    mx = _smooth(data_array, symmetric_missing)\n\n    # Plot contours if requested\n    if plot_contours:\n        ax = _add_contours(\n            ax,\n            data_array,\n            mx,\n            green_contour,\n            min_level_contours,\n            max_level_contours,\n            delta_levels_contours,\n            mask_main_array,\n        )\n\n    if plot_diagonal_lines:\n        # Diagonal lines must be plotted after the contour lines, because of bug in matplotlib\n        # Shift might need to be adjusted\n        ax = _add_diagonal_lines(ax, shift=shift_diagonal_lines)\n\n    # Define title and axis labels\n    ax.set_title(\n        title,\n        fontsize=10,\n    )\n\n    # Set axis labels\n    ax = _set_labels(\n        ax,\n        df_to_plot,\n        data_array,\n        horizontal_variable,\n        vertical_variable,\n        xlabel,\n        ylabel,\n        xaxis_ticks_on_top,\n        tick_interval,\n        round_xticks,\n        round_yticks,\n    )\n\n    # Create colorbar\n    cbar = ax.figure.colorbar(im, ax=ax, fraction=0.026, pad=0.04)\n    cbar.ax.set_ylabel(label_cbar, rotation=90, va=\"bottom\", labelpad=15)\n\n    # Remove potential grid\n    plt.grid(visible=None)\n\n    # Add QR code with a link to the topright side (a bit experimental, might need adjustments)\n    if link is not None:\n        fig = add_QR_code(fig, link, position_qr)\n\n    # Save and potentially display the plot\n    if output_path is not None:\n        if output_path.endswith(\".pdf\") and not vectorize:\n            raise ValueError(\"Please set vectorize=True to save as PDF\")\n        elif not output_path.endswith(\".pdf\") and vectorize:\n            raise ValueError(\"Please set vectorize=False to save as PNG or JPG\")\n        plt.savefig(output_path, bbox_inches=\"tight\", dpi=dpi)\n\n    if display_plot:\n        plt.show()\n    return fig, ax\n</code></pre>"},{"location":"reference/study_da/plot/utils/index.html","title":"utils","text":""},{"location":"reference/study_da/plot/utils/index.html#study_da.plot.utils.apply_high_quality","title":"<code>apply_high_quality(vectorial=False)</code>","text":"<p>Sets the matplotlib output format to high quality or vectorial plots.</p> <p>Parameters:</p> Name Type Description Default <code>vectorial</code> <code>bool</code> <p>If True, sets the format to 'svg'. Otherwise, sets it to 'retina'. Defaults to False.</p> <code>False</code> Source code in <code>study_da/plot/utils/maplotlib_utils.py</code> <pre><code>def apply_high_quality(vectorial: bool = False) -&gt; None:\n    \"\"\"\n    Sets the matplotlib output format to high quality or vectorial plots.\n\n    Args:\n        vectorial (bool, optional): If True, sets the format to 'svg'. Otherwise, sets it to\n            'retina'. Defaults to False.\n    \"\"\"\n    if vectorial:\n        matplotlib_inline.backend_inline.set_matplotlib_formats(\"svg\")\n    else:\n        matplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\n</code></pre>"},{"location":"reference/study_da/plot/utils/index.html#study_da.plot.utils.apply_nicer_style","title":"<code>apply_nicer_style(remove_right_upper_spines=True)</code>","text":"<p>Applies a nicer style to plots, using the whitegrid seaborn theme.</p> <p>Parameters:</p> Name Type Description Default <code>remove_right_upper_spines</code> <code>bool</code> <p>If True, removes the right and upper spines from the plots. Defaults to True.</p> <code>True</code> Source code in <code>study_da/plot/utils/maplotlib_utils.py</code> <pre><code>def apply_nicer_style(remove_right_upper_spines: bool = True) -&gt; None:\n    \"\"\"\n    Applies a nicer style to plots, using the whitegrid seaborn theme.\n\n    Args:\n        remove_right_upper_spines (bool, optional): If True, removes the right and upper spines\n            from the plots. Defaults to True.\n    \"\"\"\n    sns.set_theme(style=\"whitegrid\")\n\n    if remove_right_upper_spines:\n        custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\n        sns.set_theme(style=\"ticks\", rc=custom_params)\n\n    # sns.set(font='Adobe Devanagari')\n    sns.set_context(\"paper\", font_scale=1, rc={\"lines.linewidth\": 0.5, \"grid.linewidth\": 0.3})\n</code></pre>"},{"location":"reference/study_da/plot/utils/index.html#study_da.plot.utils.apply_standard_quality","title":"<code>apply_standard_quality()</code>","text":"<p>Sets the matplotlib output format to standard quality (\"png\" argument).</p> Source code in <code>study_da/plot/utils/maplotlib_utils.py</code> <pre><code>def apply_standard_quality() -&gt; None:\n    \"\"\"\n    Sets the matplotlib output format to standard quality (\"png\" argument).\n    \"\"\"\n    matplotlib_inline.backend_inline.set_matplotlib_formats(\"png\")\n</code></pre>"},{"location":"reference/study_da/plot/utils/index.html#study_da.plot.utils.use_default_fonts","title":"<code>use_default_fonts()</code>","text":"<p>Configures matplotlib to use the default DejaVu Sans fonts.</p> Source code in <code>study_da/plot/utils/maplotlib_utils.py</code> <pre><code>def use_default_fonts() -&gt; None:\n    \"\"\"\n    Configures matplotlib to use the default DejaVu Sans fonts.\n    \"\"\"\n    matplotlib.rcParams[\"mathtext.fontset\"] = \"dejavusans\"\n    matplotlib.rcParams[\"font.family\"] = \"DejaVu Sans\"\n    matplotlib.rcParams[\"mathtext.default\"] = \"it\"\n    matplotlib.rcParams[\"font.weight\"] = \"normal\"\n</code></pre>"},{"location":"reference/study_da/plot/utils/index.html#study_da.plot.utils.use_latex_fonts","title":"<code>use_latex_fonts(italic=False)</code>","text":"<p>Configures matplotlib to use LaTeX fonts.</p> <p>Parameters:</p> Name Type Description Default <code>italic</code> <code>bool</code> <p>If True, sets the default mathtext to italic. Otherwise, sets it to regular. Defaults to False.</p> <code>False</code> Source code in <code>study_da/plot/utils/maplotlib_utils.py</code> <pre><code>def use_latex_fonts(italic: bool = False) -&gt; None:\n    \"\"\"\n    Configures matplotlib to use LaTeX fonts.\n\n    Args:\n        italic (bool, optional): If True, sets the default mathtext to italic. Otherwise, sets it\n            to regular. Defaults to False.\n    \"\"\"\n    matplotlib.rcParams[\"mathtext.fontset\"] = \"cm\"\n    matplotlib.rcParams[\"font.family\"] = \"STIXGeneral\"\n    if not italic:\n        matplotlib.rcParams[\"mathtext.default\"] = \"regular\"\n        matplotlib.rcParams[\"font.weight\"] = \"light\"\n</code></pre>"},{"location":"reference/study_da/plot/utils/maplotlib_utils.html","title":"maplotlib_utils","text":""},{"location":"reference/study_da/plot/utils/maplotlib_utils.html#study_da.plot.utils.maplotlib_utils.apply_high_quality","title":"<code>apply_high_quality(vectorial=False)</code>","text":"<p>Sets the matplotlib output format to high quality or vectorial plots.</p> <p>Parameters:</p> Name Type Description Default <code>vectorial</code> <code>bool</code> <p>If True, sets the format to 'svg'. Otherwise, sets it to 'retina'. Defaults to False.</p> <code>False</code> Source code in <code>study_da/plot/utils/maplotlib_utils.py</code> <pre><code>def apply_high_quality(vectorial: bool = False) -&gt; None:\n    \"\"\"\n    Sets the matplotlib output format to high quality or vectorial plots.\n\n    Args:\n        vectorial (bool, optional): If True, sets the format to 'svg'. Otherwise, sets it to\n            'retina'. Defaults to False.\n    \"\"\"\n    if vectorial:\n        matplotlib_inline.backend_inline.set_matplotlib_formats(\"svg\")\n    else:\n        matplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\n</code></pre>"},{"location":"reference/study_da/plot/utils/maplotlib_utils.html#study_da.plot.utils.maplotlib_utils.apply_nicer_style","title":"<code>apply_nicer_style(remove_right_upper_spines=True)</code>","text":"<p>Applies a nicer style to plots, using the whitegrid seaborn theme.</p> <p>Parameters:</p> Name Type Description Default <code>remove_right_upper_spines</code> <code>bool</code> <p>If True, removes the right and upper spines from the plots. Defaults to True.</p> <code>True</code> Source code in <code>study_da/plot/utils/maplotlib_utils.py</code> <pre><code>def apply_nicer_style(remove_right_upper_spines: bool = True) -&gt; None:\n    \"\"\"\n    Applies a nicer style to plots, using the whitegrid seaborn theme.\n\n    Args:\n        remove_right_upper_spines (bool, optional): If True, removes the right and upper spines\n            from the plots. Defaults to True.\n    \"\"\"\n    sns.set_theme(style=\"whitegrid\")\n\n    if remove_right_upper_spines:\n        custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\n        sns.set_theme(style=\"ticks\", rc=custom_params)\n\n    # sns.set(font='Adobe Devanagari')\n    sns.set_context(\"paper\", font_scale=1, rc={\"lines.linewidth\": 0.5, \"grid.linewidth\": 0.3})\n</code></pre>"},{"location":"reference/study_da/plot/utils/maplotlib_utils.html#study_da.plot.utils.maplotlib_utils.apply_standard_quality","title":"<code>apply_standard_quality()</code>","text":"<p>Sets the matplotlib output format to standard quality (\"png\" argument).</p> Source code in <code>study_da/plot/utils/maplotlib_utils.py</code> <pre><code>def apply_standard_quality() -&gt; None:\n    \"\"\"\n    Sets the matplotlib output format to standard quality (\"png\" argument).\n    \"\"\"\n    matplotlib_inline.backend_inline.set_matplotlib_formats(\"png\")\n</code></pre>"},{"location":"reference/study_da/plot/utils/maplotlib_utils.html#study_da.plot.utils.maplotlib_utils.use_default_fonts","title":"<code>use_default_fonts()</code>","text":"<p>Configures matplotlib to use the default DejaVu Sans fonts.</p> Source code in <code>study_da/plot/utils/maplotlib_utils.py</code> <pre><code>def use_default_fonts() -&gt; None:\n    \"\"\"\n    Configures matplotlib to use the default DejaVu Sans fonts.\n    \"\"\"\n    matplotlib.rcParams[\"mathtext.fontset\"] = \"dejavusans\"\n    matplotlib.rcParams[\"font.family\"] = \"DejaVu Sans\"\n    matplotlib.rcParams[\"mathtext.default\"] = \"it\"\n    matplotlib.rcParams[\"font.weight\"] = \"normal\"\n</code></pre>"},{"location":"reference/study_da/plot/utils/maplotlib_utils.html#study_da.plot.utils.maplotlib_utils.use_latex_fonts","title":"<code>use_latex_fonts(italic=False)</code>","text":"<p>Configures matplotlib to use LaTeX fonts.</p> <p>Parameters:</p> Name Type Description Default <code>italic</code> <code>bool</code> <p>If True, sets the default mathtext to italic. Otherwise, sets it to regular. Defaults to False.</p> <code>False</code> Source code in <code>study_da/plot/utils/maplotlib_utils.py</code> <pre><code>def use_latex_fonts(italic: bool = False) -&gt; None:\n    \"\"\"\n    Configures matplotlib to use LaTeX fonts.\n\n    Args:\n        italic (bool, optional): If True, sets the default mathtext to italic. Otherwise, sets it\n            to regular. Defaults to False.\n    \"\"\"\n    matplotlib.rcParams[\"mathtext.fontset\"] = \"cm\"\n    matplotlib.rcParams[\"font.family\"] = \"STIXGeneral\"\n    if not italic:\n        matplotlib.rcParams[\"mathtext.default\"] = \"regular\"\n        matplotlib.rcParams[\"font.weight\"] = \"light\"\n</code></pre>"},{"location":"reference/study_da/postprocess/index.html","title":"postprocess","text":""},{"location":"reference/study_da/postprocess/index.html#study_da.postprocess.aggregate_output_data","title":"<code>aggregate_output_data(path_tree, l_group_by_parameters, function_to_aggregate=min, generation_of_interest=2, name_output='output_particles.parquet', write_output=True, path_output=None, only_keep_lost_particles=True, dic_parameters_of_interest=None, l_parameters_to_keep=None, name_template_parameters='parameters_lhc.yaml', path_template_parameters=None, force_overwrite=False)</code>","text":"<p>Aggregates output data from simulation files.</p> <p>Parameters:</p> Name Type Description Default <code>path_tree</code> <code>str</code> <p>The path to the tree file.</p> required <code>l_group_by_parameters</code> <code>list</code> <p>List of parameters to group by.</p> required <code>function_to_aggregate</code> <code>callable</code> <p>Function to aggregate the grouped data.</p> <code>min</code> <code>generation_of_interest</code> <code>int</code> <p>The generation of interest. Defaults to 2.</p> <code>2</code> <code>name_output</code> <code>str</code> <p>The name of the output file. Defaults to \"output_particles.parquet\".</p> <code>'output_particles.parquet'</code> <code>write_output</code> <code>bool</code> <p>Flag to indicate if the output should be written to a file. Defaults to True.</p> <code>True</code> <code>path_output</code> <code>str</code> <p>The path to the output file. If not provided, the default output file will be in the study folder as 'da.parquet'. Defaults to None.</p> <code>None</code> <code>only_keep_lost_particles</code> <code>bool</code> <p>Flag to indicate if only lost particles should be kept. Defaults to True.</p> <code>True</code> <code>dic_parameters_of_interest</code> <code>dict</code> <p>Dictionary of parameters of interest. Defaults to None.</p> <code>None</code> <code>l_parameters_to_keep</code> <code>list</code> <p>List of parameters to keep. Defaults to None.</p> <code>None</code> <code>name_template_parameters</code> <code>str</code> <p>The name of the template parameters file associating each parameter to a list of keys. Defaults to \"parameters_lhc.yaml\", which is already contained in the study-da package, and includes the main usual parameters.</p> <code>'parameters_lhc.yaml'</code> <code>path_template_parameters</code> <code>str</code> <p>The path to the template parameters file. Must be provided if a no template already contained in study-da is provided through the argument name_template_parameters. Defaults to None.</p> <code>None</code> <code>force_overwrite</code> <code>bool</code> <p>Flag to indicate if the output file should be overwritten if it already exists. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The final aggregated DataFrame.</p> Source code in <code>study_da/postprocess/postprocess.py</code> <pre><code>def aggregate_output_data(\n    path_tree: str,\n    l_group_by_parameters: List[str],\n    function_to_aggregate: Callable = min,\n    generation_of_interest: int = 2,\n    name_output: str = \"output_particles.parquet\",\n    write_output: bool = True,\n    path_output: Optional[str] = None,\n    only_keep_lost_particles: bool = True,\n    dic_parameters_of_interest: Optional[Dict[str, List[str]]] = None,\n    l_parameters_to_keep: Optional[List[str]] = None,\n    name_template_parameters: str = \"parameters_lhc.yaml\",\n    path_template_parameters: Optional[str] = None,\n    force_overwrite: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Aggregates output data from simulation files.\n\n    Args:\n        path_tree (str): The path to the tree file.\n        l_group_by_parameters (list): List of parameters to group by.\n        function_to_aggregate (callable, optional): Function to aggregate the grouped data.\n        generation_of_interest (int, optional): The generation of interest. Defaults to 2.\n        name_output (str, optional): The name of the output file. Defaults to \"output_particles.parquet\".\n        write_output (bool, optional): Flag to indicate if the output should be written to a file.\n            Defaults to True.\n        path_output (str, optional): The path to the output file. If not provided, the default\n            output file will be in the study folder as 'da.parquet'. Defaults to None.\n        only_keep_lost_particles (bool, optional): Flag to indicate if only lost particles should be\n            kept. Defaults to True.\n        dic_parameters_of_interest (dict, optional): Dictionary of parameters of interest. Defaults\n            to None.\n        l_parameters_to_keep (list, optional): List of parameters to keep. Defaults to None.\n        name_template_parameters (str, optional): The name of the template parameters file\n            associating each parameter to a list of keys. Defaults to \"parameters_lhc.yaml\", which\n            is already contained in the study-da package, and includes the main usual parameters.\n        path_template_parameters (str, optional): The path to the template parameters file. Must\n            be provided if a no template already contained in study-da is provided through the\n            argument name_template_parameters. Defaults to None.\n        force_overwrite (bool, optional): Flag to indicate if the output file should be overwritten\n            if it already exists. Defaults to False.\n\n    Returns:\n        pd.DataFrame: The final aggregated DataFrame.\n    \"\"\"\n    # Check it the output doesn't already exist and ask for confirmation to overwrite\n    dic_tree, _ = load_dic_from_path(path_tree)\n    absolute_path_study = dic_tree[\"absolute_path\"]\n    if path_output is None:\n        path_output = os.path.join(absolute_path_study, \"da.parquet\")\n    if os.path.exists(path_output) and not force_overwrite:\n        input_user = input(\n            f\"The output file {path_output} already exists. Do you want to overwrite it? (y/n) \"\n        )\n        if input_user.lower() != \"y\":\n            logging.warning(\"Output file not overwritten\")\n            return pd.read_parquet(path_output)\n\n    logging.info(\"Analysis of output simulation files started\")\n\n    dic_all_jobs = ConfigJobs(dic_tree,starting_depth=-len(Path(path_tree).parts) + 2).find_all_jobs()\n\n    l_df_sim = get_particles_data(\n        dic_all_jobs, absolute_path_study, generation_of_interest, name_output\n    )\n\n    default_path_template_parameters = False\n    if dic_parameters_of_interest is None:\n        if path_template_parameters is not None:\n            logging.info(\"Loading parameters of interest from the provided configuration file\")\n        else:\n            if name_template_parameters is None:\n                raise ValueError(\n                    \"No template configuration file provided for the parameters of interest\"\n                )\n            logging.info(\"Loading parameters of interest from the template configuration file\")\n            path_template_parameters = os.path.join(\n                os.path.dirname(inspect.getfile(aggregate_output_data)),\n                \"configs\",\n                name_template_parameters,\n            )\n            default_path_template_parameters = True\n        dic_parameters_of_interest, _ = load_dic_from_path(path_template_parameters)\n\n    l_df_output = add_parameters_from_config(\n        l_df_sim, dic_parameters_of_interest, default_path_template_parameters\n    )\n\n    df_final = merge_and_group_by_parameters_of_interest(\n        l_df_output,\n        l_group_by_parameters,\n        only_keep_lost_particles,\n        l_parameters_to_keep,\n        function_to_aggregate,\n    )\n\n    # Fix the LHC version type\n    df_final = fix_LHC_version(df_final)\n\n    if write_output:\n        df_final.to_parquet(path_output)\n    elif path_output is not None:\n        logging.warning(\"Output path provided but write_output set to False, no output saved\")\n\n    logging.info(\"Final dataframe for current set of simulations: %s\", df_final)\n    return df_final\n</code></pre>"},{"location":"reference/study_da/postprocess/postprocess.html","title":"postprocess","text":"<p>This module provides functions to process and analyze simulation output data.</p> <p>Functions:</p> Name Description <code>get_particles_data</code> <code>add_parameters_from_config</code> <code>merge_and_group_by_parameters_of_interest</code> <code>aggregate_output_data</code> <code>fix_LHC_version</code>"},{"location":"reference/study_da/postprocess/postprocess.html#study_da.postprocess.postprocess.add_parameters_from_config","title":"<code>add_parameters_from_config(l_df_output, dic_parameters_of_interest, default_path_template_parameters=False)</code>","text":"<p>Adds parameters from the configuration to the output data.</p> <p>Parameters:</p> Name Type Description Default <code>l_df_output</code> <code>list</code> <p>List of DataFrames containing the output data.</p> required <code>dic_parameters_of_interest</code> <code>dict</code> <p>Dictionary of parameters of interest.</p> required <code>default_path_template_parameters</code> <code>bool</code> <p>Flag to indicate if the default path template parameters are used. If True, less caution is applied in the checking of the parameters. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>list</code> <code>List[DataFrame]</code> <p>A list of DataFrames with added parameters.</p> Source code in <code>study_da/postprocess/postprocess.py</code> <pre><code>def add_parameters_from_config(\n    l_df_output: List[pd.DataFrame],\n    dic_parameters_of_interest: Dict[str, List[str]],\n    default_path_template_parameters: bool = False,\n) -&gt; List[pd.DataFrame]:\n    \"\"\"\n    Adds parameters from the configuration to the output data.\n\n    Args:\n        l_df_output (list): List of DataFrames containing the output data.\n        dic_parameters_of_interest (dict): Dictionary of parameters of interest.\n        default_path_template_parameters (bool, optional): Flag to indicate if the default path\n            template parameters are used. If True, less caution is applied in the checking of the\n            parameters. Defaults to False.\n\n    Returns:\n        list: A list of DataFrames with added parameters.\n    \"\"\"\n\n    for df_output in l_df_output:\n        # Get generation configurations as dictionnaries for parameter assignation\n        dic_configuration = df_output.attrs[\"configuration\"]\n\n        # Select simulations parameters of interest\n        for name_param, l_path_param in dic_parameters_of_interest.items():\n            try:\n                df_output[name_param] = nested_get(dic_configuration, l_path_param)\n            except KeyError:\n                # Only be verbose if the dic_parameters_of_interest has not been provided by the user\n                if not default_path_template_parameters:\n                    logging.warning(f\"Parameter {name_param} not found in the configuration file\")\n\n    return l_df_output\n</code></pre>"},{"location":"reference/study_da/postprocess/postprocess.html#study_da.postprocess.postprocess.aggregate_output_data","title":"<code>aggregate_output_data(path_tree, l_group_by_parameters, function_to_aggregate=min, generation_of_interest=2, name_output='output_particles.parquet', write_output=True, path_output=None, only_keep_lost_particles=True, dic_parameters_of_interest=None, l_parameters_to_keep=None, name_template_parameters='parameters_lhc.yaml', path_template_parameters=None, force_overwrite=False)</code>","text":"<p>Aggregates output data from simulation files.</p> <p>Parameters:</p> Name Type Description Default <code>path_tree</code> <code>str</code> <p>The path to the tree file.</p> required <code>l_group_by_parameters</code> <code>list</code> <p>List of parameters to group by.</p> required <code>function_to_aggregate</code> <code>callable</code> <p>Function to aggregate the grouped data.</p> <code>min</code> <code>generation_of_interest</code> <code>int</code> <p>The generation of interest. Defaults to 2.</p> <code>2</code> <code>name_output</code> <code>str</code> <p>The name of the output file. Defaults to \"output_particles.parquet\".</p> <code>'output_particles.parquet'</code> <code>write_output</code> <code>bool</code> <p>Flag to indicate if the output should be written to a file. Defaults to True.</p> <code>True</code> <code>path_output</code> <code>str</code> <p>The path to the output file. If not provided, the default output file will be in the study folder as 'da.parquet'. Defaults to None.</p> <code>None</code> <code>only_keep_lost_particles</code> <code>bool</code> <p>Flag to indicate if only lost particles should be kept. Defaults to True.</p> <code>True</code> <code>dic_parameters_of_interest</code> <code>dict</code> <p>Dictionary of parameters of interest. Defaults to None.</p> <code>None</code> <code>l_parameters_to_keep</code> <code>list</code> <p>List of parameters to keep. Defaults to None.</p> <code>None</code> <code>name_template_parameters</code> <code>str</code> <p>The name of the template parameters file associating each parameter to a list of keys. Defaults to \"parameters_lhc.yaml\", which is already contained in the study-da package, and includes the main usual parameters.</p> <code>'parameters_lhc.yaml'</code> <code>path_template_parameters</code> <code>str</code> <p>The path to the template parameters file. Must be provided if a no template already contained in study-da is provided through the argument name_template_parameters. Defaults to None.</p> <code>None</code> <code>force_overwrite</code> <code>bool</code> <p>Flag to indicate if the output file should be overwritten if it already exists. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The final aggregated DataFrame.</p> Source code in <code>study_da/postprocess/postprocess.py</code> <pre><code>def aggregate_output_data(\n    path_tree: str,\n    l_group_by_parameters: List[str],\n    function_to_aggregate: Callable = min,\n    generation_of_interest: int = 2,\n    name_output: str = \"output_particles.parquet\",\n    write_output: bool = True,\n    path_output: Optional[str] = None,\n    only_keep_lost_particles: bool = True,\n    dic_parameters_of_interest: Optional[Dict[str, List[str]]] = None,\n    l_parameters_to_keep: Optional[List[str]] = None,\n    name_template_parameters: str = \"parameters_lhc.yaml\",\n    path_template_parameters: Optional[str] = None,\n    force_overwrite: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Aggregates output data from simulation files.\n\n    Args:\n        path_tree (str): The path to the tree file.\n        l_group_by_parameters (list): List of parameters to group by.\n        function_to_aggregate (callable, optional): Function to aggregate the grouped data.\n        generation_of_interest (int, optional): The generation of interest. Defaults to 2.\n        name_output (str, optional): The name of the output file. Defaults to \"output_particles.parquet\".\n        write_output (bool, optional): Flag to indicate if the output should be written to a file.\n            Defaults to True.\n        path_output (str, optional): The path to the output file. If not provided, the default\n            output file will be in the study folder as 'da.parquet'. Defaults to None.\n        only_keep_lost_particles (bool, optional): Flag to indicate if only lost particles should be\n            kept. Defaults to True.\n        dic_parameters_of_interest (dict, optional): Dictionary of parameters of interest. Defaults\n            to None.\n        l_parameters_to_keep (list, optional): List of parameters to keep. Defaults to None.\n        name_template_parameters (str, optional): The name of the template parameters file\n            associating each parameter to a list of keys. Defaults to \"parameters_lhc.yaml\", which\n            is already contained in the study-da package, and includes the main usual parameters.\n        path_template_parameters (str, optional): The path to the template parameters file. Must\n            be provided if a no template already contained in study-da is provided through the\n            argument name_template_parameters. Defaults to None.\n        force_overwrite (bool, optional): Flag to indicate if the output file should be overwritten\n            if it already exists. Defaults to False.\n\n    Returns:\n        pd.DataFrame: The final aggregated DataFrame.\n    \"\"\"\n    # Check it the output doesn't already exist and ask for confirmation to overwrite\n    dic_tree, _ = load_dic_from_path(path_tree)\n    absolute_path_study = dic_tree[\"absolute_path\"]\n    if path_output is None:\n        path_output = os.path.join(absolute_path_study, \"da.parquet\")\n    if os.path.exists(path_output) and not force_overwrite:\n        input_user = input(\n            f\"The output file {path_output} already exists. Do you want to overwrite it? (y/n) \"\n        )\n        if input_user.lower() != \"y\":\n            logging.warning(\"Output file not overwritten\")\n            return pd.read_parquet(path_output)\n\n    logging.info(\"Analysis of output simulation files started\")\n\n    dic_all_jobs = ConfigJobs(dic_tree,starting_depth=-len(Path(path_tree).parts) + 2).find_all_jobs()\n\n    l_df_sim = get_particles_data(\n        dic_all_jobs, absolute_path_study, generation_of_interest, name_output\n    )\n\n    default_path_template_parameters = False\n    if dic_parameters_of_interest is None:\n        if path_template_parameters is not None:\n            logging.info(\"Loading parameters of interest from the provided configuration file\")\n        else:\n            if name_template_parameters is None:\n                raise ValueError(\n                    \"No template configuration file provided for the parameters of interest\"\n                )\n            logging.info(\"Loading parameters of interest from the template configuration file\")\n            path_template_parameters = os.path.join(\n                os.path.dirname(inspect.getfile(aggregate_output_data)),\n                \"configs\",\n                name_template_parameters,\n            )\n            default_path_template_parameters = True\n        dic_parameters_of_interest, _ = load_dic_from_path(path_template_parameters)\n\n    l_df_output = add_parameters_from_config(\n        l_df_sim, dic_parameters_of_interest, default_path_template_parameters\n    )\n\n    df_final = merge_and_group_by_parameters_of_interest(\n        l_df_output,\n        l_group_by_parameters,\n        only_keep_lost_particles,\n        l_parameters_to_keep,\n        function_to_aggregate,\n    )\n\n    # Fix the LHC version type\n    df_final = fix_LHC_version(df_final)\n\n    if write_output:\n        df_final.to_parquet(path_output)\n    elif path_output is not None:\n        logging.warning(\"Output path provided but write_output set to False, no output saved\")\n\n    logging.info(\"Final dataframe for current set of simulations: %s\", df_final)\n    return df_final\n</code></pre>"},{"location":"reference/study_da/postprocess/postprocess.html#study_da.postprocess.postprocess.fix_LHC_version","title":"<code>fix_LHC_version(df)</code>","text":"<p>Fixes the LHC version type in the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to fix.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The fixed DataFrame.</p> Source code in <code>study_da/postprocess/postprocess.py</code> <pre><code>def fix_LHC_version(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Fixes the LHC version type in the DataFrame.\n\n    Args:\n        df (pd.DataFrame): The DataFrame to fix.\n\n    Returns:\n        pd.DataFrame: The fixed DataFrame.\n    \"\"\"\n    # Fix the LHC version type\n    if \"ver_lhc_run\" in df.columns and not df[\"ver_lhc_run\"].empty:\n        if df[\"ver_lhc_run\"].isna().sum() != 0:\n            logging.warning(\"Some LHC version numbers are NaN... Ignoring\")\n        else:\n            df[\"ver_lhc_run\"] = df[\"ver_lhc_run\"].astype(\"int32\")\n    if \"ver_hllhc_optics\" in df.columns and not df[\"ver_hllhc_optics\"].empty:\n        if df[\"ver_hllhc_optics\"].isna().sum() != 0:\n            logging.warning(\"Some HLLHC optics version numbers are NaN... Ignoring\")\n        else:\n            df[\"ver_hllhc_optics\"] = df[\"ver_hllhc_optics\"].astype(\"float32\")\n\n    return df\n</code></pre>"},{"location":"reference/study_da/postprocess/postprocess.html#study_da.postprocess.postprocess.get_particles_data","title":"<code>get_particles_data(dic_all_jobs, absolute_path_study, generation_of_interest=2, name_output='output_particles.parquet')</code>","text":"<p>Retrieves particle data from simulation output files.</p> <p>Parameters:</p> Name Type Description Default <code>dic_all_jobs</code> <code>dict</code> <p>Dictionary containing all jobs and their details.</p> required <code>absolute_path_study</code> <code>str</code> <p>The absolute path to the study directory.</p> required <code>generation_of_interest</code> <code>int</code> <p>The generation of interest. Defaults to 2.</p> <code>2</code> <code>name_output</code> <code>str</code> <p>The name of the output file. Defaults to \"output_particles.parquet\".</p> <code>'output_particles.parquet'</code> <p>Returns:</p> Name Type Description <code>list</code> <code>List[DataFrame]</code> <p>A list of DataFrames containing the particle data.</p> Source code in <code>study_da/postprocess/postprocess.py</code> <pre><code>def get_particles_data(\n    dic_all_jobs: Dict[str, Dict[str, List[str]]],\n    absolute_path_study: str,\n    generation_of_interest: int = 2,\n    name_output: str = \"output_particles.parquet\",\n) -&gt; List[pd.DataFrame]:\n    \"\"\"\n    Retrieves particle data from simulation output files.\n\n    Args:\n        dic_all_jobs (dict): Dictionary containing all jobs and their details.\n        absolute_path_study (str): The absolute path to the study directory.\n        generation_of_interest (int, optional): The generation of interest. Defaults to 2.\n        name_output (str, optional): The name of the output file.\n            Defaults to \"output_particles.parquet\".\n\n    Returns:\n        list: A list of DataFrames containing the particle data.\n    \"\"\"\n\n    # Loop over all jobs and extract the output data\n    l_df_output = []\n    for relative_path_job, dic_job in dic_all_jobs.items():\n        if dic_job[\"gen\"] != generation_of_interest:\n            continue\n        absolute_path_job = os.path.join(absolute_path_study, relative_path_job)\n        absolute_folder_job = os.path.dirname(absolute_path_job)\n        try:\n            df_output = pd.read_parquet(os.path.join(absolute_folder_job, name_output))\n        except FileNotFoundError as e:\n            logging.warning(f\"File not found: {e}\")\n            continue\n        except Exception as e:\n            logging.warning(f\"Error reading parquet file: {e}\")\n            continue\n\n        # Register path of the job\n        df_output[\"name base collider\"] = relative_path_job\n\n        # Add to the list\n        l_df_output.append(df_output)\n\n    return l_df_output\n</code></pre>"},{"location":"reference/study_da/postprocess/postprocess.html#study_da.postprocess.postprocess.merge_and_group_by_parameters_of_interest","title":"<code>merge_and_group_by_parameters_of_interest(l_df_output, l_group_by_parameters, only_keep_lost_particles=True, l_parameters_to_keep=None, function_to_aggregate=min)</code>","text":"<p>Merges and groups the output data by parameters of interest.</p> <p>Parameters:</p> Name Type Description Default <code>l_df_output</code> <code>list</code> <p>List of DataFrames containing the output data.</p> required <code>l_group_by_parameters</code> <code>list</code> <p>List of parameters to group by.</p> required <code>only_keep_lost_particles</code> <code>bool</code> <p>Flag to indicate if only lost particles should be kept. Defaults to True.</p> <code>True</code> <code>l_parameters_to_keep</code> <code>list</code> <p>List of parameters to keep. Defaults to None.</p> <code>None</code> <code>function_to_aggregate</code> <code>callable</code> <p>Function to aggregate the grouped data. Defaults to min.</p> <code>min</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The merged and grouped DataFrame.</p> Source code in <code>study_da/postprocess/postprocess.py</code> <pre><code>def merge_and_group_by_parameters_of_interest(\n    l_df_output: List[pd.DataFrame],\n    l_group_by_parameters: List[str],\n    only_keep_lost_particles: bool = True,\n    l_parameters_to_keep: Optional[List[str]] = None,\n    function_to_aggregate: Callable = min,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Merges and groups the output data by parameters of interest.\n\n    Args:\n        l_df_output (list): List of DataFrames containing the output data.\n        l_group_by_parameters (list): List of parameters to group by.\n        only_keep_lost_particles (bool, optional): Flag to indicate if only lost particles should\n            be kept. Defaults to True.\n        l_parameters_to_keep (list, optional): List of parameters to keep. Defaults to None.\n        function_to_aggregate (callable, optional): Function to aggregate the grouped data.\n            Defaults to min.\n\n    Returns:\n        pd.DataFrame: The merged and grouped DataFrame.\n    \"\"\"\n    # Merge all dataframes\n    df_all_sim = pd.concat(l_df_output)\n\n    # Handle mutable default arguments\n    if l_parameters_to_keep is None:\n        logging.info(\"No list of parameters to keep provided, keeping all available parameters\")\n        l_parameters_to_keep = list(df_all_sim.columns)\n\n    if only_keep_lost_particles:\n        # Extract the particles that were lost for DA computation\n        df_all_sim = df_all_sim[df_all_sim[\"state\"] != 1]\n\n    # Check if the dataframe is empty\n    if df_all_sim.empty:\n        logging.warning(\"No unstable particles found, the output dataframe will be empty.\")\n\n    # Group by parameters of interest\n    df_grouped = df_all_sim.groupby(l_group_by_parameters)\n\n    # Return the grouped dataframe, keeping only the minimum values of the parameters of interest\n    # (should not have impact except for DA, which we want to be minimal)\n    # return pd.DataFrame(\n    #     [df_grouped[parameter].min() for parameter in l_parameters_to_keep]\n    # ).transpose()\n    return pd.DataFrame(\n        [df_grouped[parameter].agg(function_to_aggregate) for parameter in l_parameters_to_keep]\n    ).transpose()\n</code></pre>"},{"location":"reference/study_da/submit/index.html","title":"submit","text":""},{"location":"reference/study_da/submit/ask_user_config.html","title":"ask_user_config","text":"<p>This module contains functions to prompt the user for various job configuration settings.</p> <p>Functions:</p> Name Description <code>ask_and_set_gpu</code> <p>dict[str, Any]) -&gt; None:</p> <code>ask_and_set_htc_flavour</code> <p>dict[str, Any]) -&gt; None:</p> <code>ask_and_set_run_on</code> <p>dict[str, Any]) -&gt; None:</p> <code>ask_keep_setting</code> <p>str) -&gt; bool:</p> <code>ask_skip_configured_jobs</code>"},{"location":"reference/study_da/submit/ask_user_config.html#study_da.submit.ask_user_config.ask_and_set_gpu","title":"<code>ask_and_set_gpu(dic_gen)</code>","text":"<p>Prompts the user if a GPU must be used for the job and sets it in the provided dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>dic_gen</code> <code>dict[str, Any]</code> <p>The dictionary containing job configuration.</p> required Source code in <code>study_da/submit/ask_user_config.py</code> <pre><code>def ask_and_set_gpu(dic_gen: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Prompts the user if a GPU must be used for the job and sets it in the provided dictionary.\n\n    Args:\n        dic_gen (dict[str, Any]): The dictionary containing job configuration.\n    \"\"\"\n    while True:\n        gpu = input(\n            f\"Do you want to request a GPU for job {dic_gen['file']}?\" \" (y/n). Default is n.\"\n        )\n        gpu = \"n\" if gpu == \"\" else gpu\n        if gpu in [\"y\", \"n\", \"\"]:\n            break\n\n    dic_gen[\"request_gpu\"] = gpu == \"y\"\n</code></pre>"},{"location":"reference/study_da/submit/ask_user_config.html#study_da.submit.ask_user_config.ask_and_set_htc_flavour","title":"<code>ask_and_set_htc_flavour(dic_gen)</code>","text":"<p>Prompts the user to select an HTCondor job flavor and sets it in the provided dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>dic_gen</code> <code>dict[str, Any]</code> <p>The dictionary containing job configuration.</p> required Source code in <code>study_da/submit/ask_user_config.py</code> <pre><code>def ask_and_set_htc_flavour(dic_gen: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Prompts the user to select an HTCondor job flavor and sets it in the provided dictionary.\n\n    Args:\n        dic_gen (dict[str, Any]): The dictionary containing job configuration.\n    \"\"\"\n    while True:\n        try:\n            submission_type = input(\n                f\"What type of htc job flavour do you want to use for job {dic_gen['file']}?\"\n                f\" 1: espresso, 2: microcentury, 3: longlunch, 4: workday, 5: tomorrow,\"\n                f\" 6: testmatch, 7: nextweek. Default is espresso.\"\n            )\n            submission_type = 1 if submission_type == \"\" else int(submission_type)\n            if submission_type in range(1, 8):\n                break\n            else:\n                raise ValueError\n        except ValueError:\n            print(\"Invalid input. Please enter a number between 1 and 7.\")\n\n    dict_flavour_type = {\n        1: \"espresso\",\n        2: \"microcentury\",\n        3: \"longlunch\",\n        4: \"workday\",\n        5: \"tomorrow\",\n        6: \"testmatch\",\n        7: \"nextweek\",\n    }\n    dic_gen[\"htc_flavor\"] = dict_flavour_type[submission_type]\n</code></pre>"},{"location":"reference/study_da/submit/ask_user_config.html#study_da.submit.ask_user_config.ask_and_set_run_on","title":"<code>ask_and_set_run_on(dic_gen)</code>","text":"<p>Prompts the user to select a submission type and sets it in the provided dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>dic_gen</code> <code>dict[str, Any]</code> <p>The dictionary containing job configuration.</p> required Source code in <code>study_da/submit/ask_user_config.py</code> <pre><code>def ask_and_set_run_on(dic_gen: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Prompts the user to select a submission type and sets it in the provided dictionary.\n\n    Args:\n        dic_gen (dict[str, Any]): The dictionary containing job configuration.\n    \"\"\"\n    while True:\n        try:\n            submission_type = input(\n                f\"What type of submission do you want to use for job {dic_gen['file']}?\"\n                \" 1: local, 2: htc, 3: htc_docker, 4: slurm, 5: slurm_docker. Default is local.\"\n            )\n            submission_type = 1 if submission_type == \"\" else int(submission_type)\n            if submission_type in range(1, 6):\n                break\n            else:\n                raise ValueError\n        except ValueError:\n            print(\"Invalid input. Please enter a number between 1 and 5.\")\n\n    dict_submission_type = {\n        1: \"local\",\n        2: \"htc\",\n        3: \"htc_docker\",\n        4: \"slurm\",\n        5: \"slurm_docker\",\n    }\n    dic_gen[\"submission_type\"] = dict_submission_type[submission_type]\n</code></pre>"},{"location":"reference/study_da/submit/ask_user_config.html#study_da.submit.ask_user_config.ask_keep_setting","title":"<code>ask_keep_setting(job_name)</code>","text":"<p>Prompts the user to decide whether to keep the same settings for identical jobs.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the user wants to keep the same settings, False otherwise.</p> Source code in <code>study_da/submit/ask_user_config.py</code> <pre><code>def ask_keep_setting(job_name: str) -&gt; bool:\n    \"\"\"\n    Prompts the user to decide whether to keep the same settings for identical jobs.\n\n    Returns:\n        bool: True if the user wants to keep the same settings, False otherwise.\n    \"\"\"\n    keep_setting = input(\n        f\"Do you want to keep the same setting for all jobs of the type {job_name} ? (y/n).\"\n        f\"Default is y.\"\n    )\n    while keep_setting not in [\"\", \"y\", \"n\"]:\n        keep_setting = input(\"Invalid input. Please enter y, n or skip question.\")\n    if keep_setting == \"\":\n        keep_setting = \"y\"\n    return keep_setting == \"y\"\n</code></pre>"},{"location":"reference/study_da/submit/ask_user_config.html#study_da.submit.ask_user_config.ask_skip_configured_jobs","title":"<code>ask_skip_configured_jobs()</code>","text":"<p>Prompts the user to decide whether to skip already configured jobs.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the user wants to skip already configured jobs, False otherwise.</p> Source code in <code>study_da/submit/ask_user_config.py</code> <pre><code>def ask_skip_configured_jobs() -&gt; bool:\n    \"\"\"\n    Prompts the user to decide whether to skip already configured jobs.\n\n    Returns:\n        bool: True if the user wants to skip already configured jobs, False otherwise.\n    \"\"\"\n    skip_configured_jobs = input(\n        \"Some jobs to submit seem to be configured already. Do you want to skip them? (y/n). \"\n        \"Default is y.\"\n    )\n    while skip_configured_jobs not in [\"\", \"y\", \"n\"]:\n        skip_configured_jobs = input(\"Invalid input. Please enter y, n or skip question.\")\n    if skip_configured_jobs == \"\":\n        skip_configured_jobs = \"y\"\n    return skip_configured_jobs == \"y\"\n</code></pre>"},{"location":"reference/study_da/submit/config_jobs.html","title":"config_jobs","text":"<p>This module contains the ConfigJobs class that allows to configure jobs in the tree file.</p>"},{"location":"reference/study_da/submit/config_jobs.html#study_da.submit.config_jobs.ConfigJobs","title":"<code>ConfigJobs</code>","text":"<p>A class to configure jobs in the tree file.</p> <p>Attributes:</p> Name Type Description <code>dic_tree</code> <code>dict</code> <p>The dictionary representing the job tree.</p> <p>Methods:</p> Name Description <code>_find_and_configure_jobs_recursion</code> <p>Recursively finds and configures jobs.</p> <code>find_and_configure_jobs</code> <p>Finds and configures all jobs in the tree.</p> <code>find_all_jobs</code> <p>Finds all jobs in the tree.</p> Source code in <code>study_da/submit/config_jobs.py</code> <pre><code>class ConfigJobs:\n    \"\"\"\n    A class to configure jobs in the tree file.\n\n    Attributes:\n        dic_tree (dict): The dictionary representing the job tree.\n\n    Methods:\n        _find_and_configure_jobs_recursion(dic_gen, depth=0, l_keys=None, find_only=False):\n            Recursively finds and configures jobs.\n        find_and_configure_jobs(): Finds and configures all jobs in the tree.\n        find_all_jobs(): Finds all jobs in the tree.\n    \"\"\"\n\n    def __init__(self, dic_tree: dict,starting_depth: int = 0):\n        \"\"\"\n        Initializes the ConfigJobs class.\n\n        Args:\n            dic_tree (dict): The dictionary representing the job tree.\n\n        \"\"\"\n        self.dic_tree: dict = dic_tree\n\n        # Flag set to True if self.find_all_jobs() has been called at least once\n        self.all_jobs_found: bool = False\n\n        # Variables to store the jobs and their configuration\n        self.dic_all_jobs: dict[str, Any] = {}\n\n        # Variable to adjust the depth of the tree \n        self.starting_depth: int = starting_depth\n\n    def _find_and_configure_jobs_recursion(\n        self,\n        dic_gen: dict[str, Any],\n        depth: int = 0,\n        l_keys: list[str] | None = None,\n        find_only: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Recursively finds and configures jobs in the tree.\n\n        Args:\n            dic_gen (dict[str, Any]): The dictionary representing the current level of the job tree.\n            depth (int, optional): The current depth in the tree. Defaults to 0.\n            l_keys (list[str], optional): The list of keys representing the path in the tree.\n                Defaults to None.\n            find_only (bool, optional): If True, only finds jobs without configuring them.\n                Defaults to False.\n\n        Raises:\n            AttributeError: If required attributes are not set before calling this method.\n        \"\"\"\n        if l_keys is None:\n            l_keys = []\n\n        if not hasattr(self, \"dic_all_jobs\"):\n            raise AttributeError(\"dic_all_jobs should be set before calling this method\")\n\n        # Recursively look for job key in the tree, keeping track of the depth\n        # of the job in the tree\n        # Browse a list of keys rather than than the keys() generator not to create mutation errors\n        for key in list(dic_gen.keys()):\n            value = dic_gen[key]\n            if isinstance(value, dict):\n                self._find_and_configure_jobs_recursion(\n                    dic_gen=value, depth=depth + 1, l_keys=l_keys + [key], find_only=find_only\n                )\n            elif key == \"file\":\n                # Add job the the list of all jobs\n                # In theory, the list of keys can be obtained from the job path\n                # but it's safer to keep it in the dict\n                self.dic_all_jobs[value] = {\n                    \"gen\": depth,\n                    \"l_keys\": copy.copy(l_keys),\n                }\n\n                # Stop the browsing if we only want to find the jobs\n                if find_only:\n                    return\n\n                # Otherwise ensure that the job can be configured\n                if not hasattr(self, \"dic_config_jobs\") or not hasattr(\n                    self, \"skip_configured_jobs\"\n                ):\n                    raise AttributeError(\n                        \"dic_config_jobs and skip_configured_jobs should be set before calling\"\n                        \"this method\"\n                    )\n\n                # Put path_run to None if it exists\n                if \"path_run\" in dic_gen:\n                    logging.warning(\n                        f\"Job {value} has a path_run attribute. It will be set to None.\"\n                    )\n                    dic_gen[\"path_run\"] = None\n\n                # If all is fine so far, get job name and configure\n                job_name = value.split(\"/\")[-1]\n\n                # Ensure configuration is not already set\n                if \"submission_type\" in dic_gen:\n                    if self.skip_configured_jobs is None:\n                        self.skip_configured_jobs = ask_skip_configured_jobs()\n                    if self.skip_configured_jobs:\n                        return\n\n                # If it's the first time we find the job, ask for gpu and run_on\n                # Note that a job can be configured and not be in self.dic_config_jobs\n                # self.dic_config_jobs contains the archetypical main jobs (one per gen), not all jobs\n                if job_name not in self.dic_config_jobs:\n                    self._get_gpu_and_run_on(depth, value, dic_gen, job_name)\n                else:\n                    # Check that the config for the current job is ok\n                    self.check_config_jobs(job_name)\n\n                    # Merge the configuration of the job with the existing one\n                    dic_gen |= self.dic_config_jobs[job_name]\n\n    def check_config_jobs(self, job_name: str) -&gt; None:\n        \"\"\"\n        Check the configuration of a job and ensure that it is properly set.\n        Useful when the dic_config_jobs is provided externally.\n\n        Args:\n            job_name (str): The name of the job to be configured.\n\n        Returns:\n            dict: The updated job configuration.\n        \"\"\"\n\n        # Ensure flavour is set for htc jobs\n        if (\n            self.dic_config_jobs[job_name][\"submission_type\"] in [\"htc\", \"htc_docker\"]\n            and \"htc_flavor\" not in self.dic_config_jobs[job_name]\n        ):\n            raise ValueError(\n                f\"Job {job_name} is not properly configured. Please set the htc_flavor.\"\n            )\n\n        # Set status to to_submit if not already set\n        if \"status\" not in self.dic_config_jobs[job_name]:\n            self.dic_config_jobs[job_name][\"status\"] = \"to_submit\"\n\n    def _get_gpu_and_run_on(self, depth: int, value: str, dic_gen: dict, job_name: str) -&gt; None:\n        \"\"\"\n        Sets the gpu request and run-on parameters for a job, updates the job configuration,\n        and stores it in the job dictionary if the user chooses to keep the settings.\n\n        Args:\n            depth (int): The depth level of the job in the hierarchy.\n            value (str): The value associated with the job.\n            dic_gen (dict): A dictionary containing general job configuration parameters.\n            job_name (str): The name of the job to be configured.\n\n        Returns:\n            None\n        \"\"\"\n        logging.info(f\"Found job at depth {depth}: {value}\")\n        # Set GPU request and run_on\n        ask_and_set_gpu(dic_gen)\n        ask_and_set_run_on(dic_gen)\n        if dic_gen[\"submission_type\"] in [\"htc\", \"htc_docker\"]:\n            ask_and_set_htc_flavour(dic_gen)\n        else:\n            dic_gen[\"htc_flavor\"] = None\n        dic_gen[\"status\"] = \"to_submit\"\n\n        # Compute all jobs to see if there are at least two jobs in the current generation\n        self.find_all_jobs()\n        # Ensure there are more than one job of the same type to ask the user\n        # sourcery skip: merge-nested-ifs\n        if [x[\"gen\"] == depth for x in self.dic_all_jobs.values()].count(True) &gt; 1:\n            # Ask the user if they want to keep the settings for all jobs of the same type\n            if ask_keep_setting(job_name):\n                self.dic_config_jobs[job_name] = {\n                    \"request_gpu\": dic_gen[\"request_gpu\"],\n                    \"submission_type\": dic_gen[\"submission_type\"],\n                    \"status\": dic_gen[\"status\"],\n                    \"htc_flavor\": dic_gen[\"htc_flavor\"],\n                }\n\n    def find_and_configure_jobs(\n        self, dic_config_jobs: Optional[dict[str, dict[str, Any]]] = None\n    ) -&gt; dict:\n        \"\"\"\n        Finds and configures all jobs in the tree.\n\n        Args:\n            dic_config_jobs (dict[str, dict[str, Any]], optional): A dictionary containing the\n                configuration of the jobs. Defaults to None.\n\n        Returns:\n            dict: The updated job tree with configurations.\n        \"\"\"\n        # Variables to store the jobs and their configuration\n        self.dic_config_jobs = dic_config_jobs if dic_config_jobs is not None else {}\n        self.skip_configured_jobs = None\n        self._log_and_find(\"Finding and configuring jobs in the tree\", False)\n        return self.dic_tree\n\n    def find_all_jobs(self) -&gt; dict:\n        \"\"\"\n        Finds all jobs in the tree.\n\n        Returns:\n            dict: A dictionary containing all jobs and their details.\n        \"\"\"\n        if not self.all_jobs_found:\n            self._log_and_find(\"Finding all jobs in the tree\", True)\n        else:\n            logging.info(\"All jobs have already been found. Returning the existing dictionary.\")\n\n        return self.dic_all_jobs\n\n    def _log_and_find(self, log_str, find_only):\n        logging.info(log_str)\n        self._find_and_configure_jobs_recursion(self.dic_tree, depth=self.starting_depth - 1, find_only=find_only)\n        self.all_jobs_found = True\n</code></pre>"},{"location":"reference/study_da/submit/config_jobs.html#study_da.submit.config_jobs.ConfigJobs.__init__","title":"<code>__init__(dic_tree, starting_depth=0)</code>","text":"<p>Initializes the ConfigJobs class.</p> <p>Parameters:</p> Name Type Description Default <code>dic_tree</code> <code>dict</code> <p>The dictionary representing the job tree.</p> required Source code in <code>study_da/submit/config_jobs.py</code> <pre><code>def __init__(self, dic_tree: dict,starting_depth: int = 0):\n    \"\"\"\n    Initializes the ConfigJobs class.\n\n    Args:\n        dic_tree (dict): The dictionary representing the job tree.\n\n    \"\"\"\n    self.dic_tree: dict = dic_tree\n\n    # Flag set to True if self.find_all_jobs() has been called at least once\n    self.all_jobs_found: bool = False\n\n    # Variables to store the jobs and their configuration\n    self.dic_all_jobs: dict[str, Any] = {}\n\n    # Variable to adjust the depth of the tree \n    self.starting_depth: int = starting_depth\n</code></pre>"},{"location":"reference/study_da/submit/config_jobs.html#study_da.submit.config_jobs.ConfigJobs.check_config_jobs","title":"<code>check_config_jobs(job_name)</code>","text":"<p>Check the configuration of a job and ensure that it is properly set. Useful when the dic_config_jobs is provided externally.</p> <p>Parameters:</p> Name Type Description Default <code>job_name</code> <code>str</code> <p>The name of the job to be configured.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>None</code> <p>The updated job configuration.</p> Source code in <code>study_da/submit/config_jobs.py</code> <pre><code>def check_config_jobs(self, job_name: str) -&gt; None:\n    \"\"\"\n    Check the configuration of a job and ensure that it is properly set.\n    Useful when the dic_config_jobs is provided externally.\n\n    Args:\n        job_name (str): The name of the job to be configured.\n\n    Returns:\n        dict: The updated job configuration.\n    \"\"\"\n\n    # Ensure flavour is set for htc jobs\n    if (\n        self.dic_config_jobs[job_name][\"submission_type\"] in [\"htc\", \"htc_docker\"]\n        and \"htc_flavor\" not in self.dic_config_jobs[job_name]\n    ):\n        raise ValueError(\n            f\"Job {job_name} is not properly configured. Please set the htc_flavor.\"\n        )\n\n    # Set status to to_submit if not already set\n    if \"status\" not in self.dic_config_jobs[job_name]:\n        self.dic_config_jobs[job_name][\"status\"] = \"to_submit\"\n</code></pre>"},{"location":"reference/study_da/submit/config_jobs.html#study_da.submit.config_jobs.ConfigJobs.find_all_jobs","title":"<code>find_all_jobs()</code>","text":"<p>Finds all jobs in the tree.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing all jobs and their details.</p> Source code in <code>study_da/submit/config_jobs.py</code> <pre><code>def find_all_jobs(self) -&gt; dict:\n    \"\"\"\n    Finds all jobs in the tree.\n\n    Returns:\n        dict: A dictionary containing all jobs and their details.\n    \"\"\"\n    if not self.all_jobs_found:\n        self._log_and_find(\"Finding all jobs in the tree\", True)\n    else:\n        logging.info(\"All jobs have already been found. Returning the existing dictionary.\")\n\n    return self.dic_all_jobs\n</code></pre>"},{"location":"reference/study_da/submit/config_jobs.html#study_da.submit.config_jobs.ConfigJobs.find_and_configure_jobs","title":"<code>find_and_configure_jobs(dic_config_jobs=None)</code>","text":"<p>Finds and configures all jobs in the tree.</p> <p>Parameters:</p> Name Type Description Default <code>dic_config_jobs</code> <code>dict[str, dict[str, Any]]</code> <p>A dictionary containing the configuration of the jobs. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The updated job tree with configurations.</p> Source code in <code>study_da/submit/config_jobs.py</code> <pre><code>def find_and_configure_jobs(\n    self, dic_config_jobs: Optional[dict[str, dict[str, Any]]] = None\n) -&gt; dict:\n    \"\"\"\n    Finds and configures all jobs in the tree.\n\n    Args:\n        dic_config_jobs (dict[str, dict[str, Any]], optional): A dictionary containing the\n            configuration of the jobs. Defaults to None.\n\n    Returns:\n        dict: The updated job tree with configurations.\n    \"\"\"\n    # Variables to store the jobs and their configuration\n    self.dic_config_jobs = dic_config_jobs if dic_config_jobs is not None else {}\n    self.skip_configured_jobs = None\n    self._log_and_find(\"Finding and configuring jobs in the tree\", False)\n    return self.dic_tree\n</code></pre>"},{"location":"reference/study_da/submit/dependency_graph.html","title":"dependency_graph","text":"<p>This module contains the DependencyGraph class to manage the dependencies between jobs</p>"},{"location":"reference/study_da/submit/dependency_graph.html#study_da.submit.dependency_graph.DependencyGraph","title":"<code>DependencyGraph</code>","text":"<p>A class to manage the dependencies between jobs.</p> <p>Attributes:</p> Name Type Description <code>dic_tree</code> <code>dict</code> <p>The dictionary representing the job tree.</p> <code>dic_all_jobs</code> <code>dict</code> <p>The dictionary containing all jobs and their details.</p> <code>dependency_graph</code> <code>dict</code> <p>The dictionary representing the dependency graph.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initializes the DependencyGraph class.</p> <code>build_full_dependency_graph</code> <p>Builds the full dependency graph.</p> <code>get_unfinished_dependency</code> <p>Gets the list of unfinished dependencies for a given job.</p> Source code in <code>study_da/submit/dependency_graph.py</code> <pre><code>class DependencyGraph:\n    \"\"\"\n    A class to manage the dependencies between jobs.\n\n    Attributes:\n        dic_tree (dict): The dictionary representing the job tree.\n        dic_all_jobs (dict): The dictionary containing all jobs and their details.\n        dependency_graph (dict): The dictionary representing the dependency graph.\n\n    Methods:\n        __init__(dic_tree, dic_all_jobs): Initializes the DependencyGraph class.\n        build_full_dependency_graph(): Builds the full dependency graph.\n        get_unfinished_dependency(job): Gets the list of unfinished dependencies for a given job.\n    \"\"\"\n\n    def __init__(self, dic_tree: dict, dic_all_jobs: dict):\n        \"\"\"\n        Initializes the DependencyGraph class.\n\n        Args:\n            dic_tree (dict): The dictionary representing the job tree.\n            dic_all_jobs (dict): The dictionary containing all jobs and their details.\n        \"\"\"\n        self.dic_tree = dic_tree\n        self.dic_all_jobs = dic_all_jobs\n        self.dependency_graph = {}\n\n    def build_full_dependency_graph(self) -&gt; dict:\n        \"\"\"\n        Builds the full dependency graph.\n\n        Returns:\n            dict: The full dependency graph.\n        \"\"\"\n        self.set_l_keys = {\n            tuple(self.dic_all_jobs[job][\"l_keys\"][:-1]) for job in self.dic_all_jobs\n        }\n        for job in self.dic_all_jobs:\n            l_keys = self.dic_all_jobs[job][\"l_keys\"]\n            self.dependency_graph[job] = set()\n            # Add all parents to the dependency graph\n            for i in range(len(l_keys) - 1):\n                l_keys_parent = l_keys[:i]\n                if tuple(l_keys_parent) in self.set_l_keys:\n                    parent = nested_get(self.dic_tree, l_keys_parent)\n                    # Look for all the jobs in the parent (but not the generations below)\n                    for name_parent, sub_dict in parent.items():\n                        if \"file\" in sub_dict:\n                            self.dependency_graph[job].add(sub_dict[\"file\"])\n        return self.dependency_graph\n\n    def get_unfinished_dependency(self, job: str) -&gt; list:\n        \"\"\"\n        Gets the list of unfinished dependencies for a given job.\n\n        Args:\n            job (str): The name of the job.\n\n        Returns:\n            list: The list of unfinished dependencies.\n        \"\"\"\n        # Ensure the dependency graph is built\n        if self.dependency_graph == {}:\n            self.build_full_dependency_graph()\n\n        # Get the list of dependencies\n        l_dependencies = self.dependency_graph[job]\n\n        # Get the corresponding list of l_keys\n        ll_keys = [self.dic_all_jobs[dep][\"l_keys\"] for dep in l_dependencies]\n\n        return [\n            dep\n            for dep, l_keys in zip(l_dependencies, ll_keys)\n            if nested_get(self.dic_tree, l_keys + [\"status\"]) not in [\"finished\", \"failed\"]\n        ]\n\n    def get_failed_dependency(self, job: str) -&gt; list:\n        \"\"\"\n        Gets the list of failed dependencies for a given job.\n\n        Args:\n            job (str): The name of the job.\n\n        Returns:\n            list: The list of failed dependencies.\n        \"\"\"\n        # Ensure the dependency graph is built\n        if self.dependency_graph == {}:\n            self.build_full_dependency_graph()\n\n        # Get the list of dependencies\n        l_dependencies = self.dependency_graph[job]\n\n        # Get the corresponding list of l_keys\n        ll_keys = [self.dic_all_jobs[dep][\"l_keys\"] for dep in l_dependencies]\n\n        return [\n            dep\n            for dep, l_keys in zip(l_dependencies, ll_keys)\n            if nested_get(self.dic_tree, l_keys + [\"status\"]) == \"failed\"\n        ]\n</code></pre>"},{"location":"reference/study_da/submit/dependency_graph.html#study_da.submit.dependency_graph.DependencyGraph.__init__","title":"<code>__init__(dic_tree, dic_all_jobs)</code>","text":"<p>Initializes the DependencyGraph class.</p> <p>Parameters:</p> Name Type Description Default <code>dic_tree</code> <code>dict</code> <p>The dictionary representing the job tree.</p> required <code>dic_all_jobs</code> <code>dict</code> <p>The dictionary containing all jobs and their details.</p> required Source code in <code>study_da/submit/dependency_graph.py</code> <pre><code>def __init__(self, dic_tree: dict, dic_all_jobs: dict):\n    \"\"\"\n    Initializes the DependencyGraph class.\n\n    Args:\n        dic_tree (dict): The dictionary representing the job tree.\n        dic_all_jobs (dict): The dictionary containing all jobs and their details.\n    \"\"\"\n    self.dic_tree = dic_tree\n    self.dic_all_jobs = dic_all_jobs\n    self.dependency_graph = {}\n</code></pre>"},{"location":"reference/study_da/submit/dependency_graph.html#study_da.submit.dependency_graph.DependencyGraph.build_full_dependency_graph","title":"<code>build_full_dependency_graph()</code>","text":"<p>Builds the full dependency graph.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The full dependency graph.</p> Source code in <code>study_da/submit/dependency_graph.py</code> <pre><code>def build_full_dependency_graph(self) -&gt; dict:\n    \"\"\"\n    Builds the full dependency graph.\n\n    Returns:\n        dict: The full dependency graph.\n    \"\"\"\n    self.set_l_keys = {\n        tuple(self.dic_all_jobs[job][\"l_keys\"][:-1]) for job in self.dic_all_jobs\n    }\n    for job in self.dic_all_jobs:\n        l_keys = self.dic_all_jobs[job][\"l_keys\"]\n        self.dependency_graph[job] = set()\n        # Add all parents to the dependency graph\n        for i in range(len(l_keys) - 1):\n            l_keys_parent = l_keys[:i]\n            if tuple(l_keys_parent) in self.set_l_keys:\n                parent = nested_get(self.dic_tree, l_keys_parent)\n                # Look for all the jobs in the parent (but not the generations below)\n                for name_parent, sub_dict in parent.items():\n                    if \"file\" in sub_dict:\n                        self.dependency_graph[job].add(sub_dict[\"file\"])\n    return self.dependency_graph\n</code></pre>"},{"location":"reference/study_da/submit/dependency_graph.html#study_da.submit.dependency_graph.DependencyGraph.get_failed_dependency","title":"<code>get_failed_dependency(job)</code>","text":"<p>Gets the list of failed dependencies for a given job.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>str</code> <p>The name of the job.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>The list of failed dependencies.</p> Source code in <code>study_da/submit/dependency_graph.py</code> <pre><code>def get_failed_dependency(self, job: str) -&gt; list:\n    \"\"\"\n    Gets the list of failed dependencies for a given job.\n\n    Args:\n        job (str): The name of the job.\n\n    Returns:\n        list: The list of failed dependencies.\n    \"\"\"\n    # Ensure the dependency graph is built\n    if self.dependency_graph == {}:\n        self.build_full_dependency_graph()\n\n    # Get the list of dependencies\n    l_dependencies = self.dependency_graph[job]\n\n    # Get the corresponding list of l_keys\n    ll_keys = [self.dic_all_jobs[dep][\"l_keys\"] for dep in l_dependencies]\n\n    return [\n        dep\n        for dep, l_keys in zip(l_dependencies, ll_keys)\n        if nested_get(self.dic_tree, l_keys + [\"status\"]) == \"failed\"\n    ]\n</code></pre>"},{"location":"reference/study_da/submit/dependency_graph.html#study_da.submit.dependency_graph.DependencyGraph.get_unfinished_dependency","title":"<code>get_unfinished_dependency(job)</code>","text":"<p>Gets the list of unfinished dependencies for a given job.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>str</code> <p>The name of the job.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>The list of unfinished dependencies.</p> Source code in <code>study_da/submit/dependency_graph.py</code> <pre><code>def get_unfinished_dependency(self, job: str) -&gt; list:\n    \"\"\"\n    Gets the list of unfinished dependencies for a given job.\n\n    Args:\n        job (str): The name of the job.\n\n    Returns:\n        list: The list of unfinished dependencies.\n    \"\"\"\n    # Ensure the dependency graph is built\n    if self.dependency_graph == {}:\n        self.build_full_dependency_graph()\n\n    # Get the list of dependencies\n    l_dependencies = self.dependency_graph[job]\n\n    # Get the corresponding list of l_keys\n    ll_keys = [self.dic_all_jobs[dep][\"l_keys\"] for dep in l_dependencies]\n\n    return [\n        dep\n        for dep, l_keys in zip(l_dependencies, ll_keys)\n        if nested_get(self.dic_tree, l_keys + [\"status\"]) not in [\"finished\", \"failed\"]\n    ]\n</code></pre>"},{"location":"reference/study_da/submit/generate_run.html","title":"generate_run","text":"<p>This module provides functions to generate run files for jobs in different environments (local/Slurm and HTCondor). It includes functions to create shell script snippets for tagging jobs as finished and generating run files with appropriate configurations.</p> <p>Functions:</p> Name Description <code>tag_str</code> <p>str) -&gt; str:</p> <code>generate_run_file</code> <p>str, job_name: str, setup_env_script: str, generation_number: int, tree_path: str, l_keys: list[str], htc: bool = False, additionnal_command: str = \"\", l_dependencies: list[str] | None = None, name_config: str = \"config.yaml\") -&gt; str:</p> <code>_generate_run_file</code> <p>str, job_name: str, setup_env_script: str, tree_path: str, l_keys: list[str], additionnal_command: str = \"\") -&gt; str:</p> <code>_generate_run_file_htc</code> <p>str, job_name: str, setup_env_script: str, generation_number: int, tree_path: str, l_keys: list[str], additionnal_command: str = \"\", l_dependencies: list[str] | None = None, name_config: str = \"config.yaml\") -&gt; str:</p>"},{"location":"reference/study_da/submit/generate_run.html#study_da.submit.generate_run.generate_run_file","title":"<code>generate_run_file(abs_job_folder, job_name, setup_env_script, htc=False, slurm_docker_fix=True, additionnal_command='', **kwargs_htc_run_files)</code>","text":"<p>Generates a run file for a job, either for local/Slurm or HTC environments.</p> <p>Parameters:</p> Name Type Description Default <code>abs_job_folder</code> <code>str</code> <p>The (absolute) folder where the job is located.</p> required <code>job_name</code> <code>str</code> <p>The name of the job script.</p> required <code>setup_env_script</code> <code>str</code> <p>The script to set up the environment.</p> required <code>htc</code> <code>bool</code> <p>Whether the job shoud be run on HTCondor. Defaults to False.</p> <code>False</code> <code>slurm_docker_fix</code> <code>bool</code> <p>Whether to fix the Docker issue with recovery path on Slurm. Defaults to False.</p> <code>True</code> <code>additionnal_command</code> <code>str</code> <p>Additional command to run. Defaults to \"\".</p> <code>''</code> <code>**kwargs_htc_run_files</code> <code>Any</code> <p>Additional keyword arguments for the generate_run_files method     when submitting HTC jobs.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The generated run file content.</p> Source code in <code>study_da/submit/generate_run.py</code> <pre><code>def generate_run_file(\n    abs_job_folder: str,\n    job_name: str,\n    setup_env_script: str,\n    htc: bool = False,\n    slurm_docker_fix: bool = True,\n    additionnal_command: str = \"\",\n    **kwargs_htc_run_files: Any,\n) -&gt; str:\n    \"\"\"\n    Generates a run file for a job, either for local/Slurm or HTC environments.\n\n    Args:\n        abs_job_folder (str): The (absolute) folder where the job is located.\n        job_name (str): The name of the job script.\n        setup_env_script (str): The script to set up the environment.\n        htc (bool, optional): Whether the job shoud be run on HTCondor. Defaults to False.\n        slurm_docker_fix (bool, optional): Whether to fix the Docker issue with recovery path on\n            Slurm. Defaults to False.\n        additionnal_command (str, optional): Additional command to run. Defaults to \"\".\n        **kwargs_htc_run_files (Any): Additional keyword arguments for the generate_run_files method\n                when submitting HTC jobs.\n\n    Returns:\n        str: The generated run file content.\n    \"\"\"\n    if not htc:\n        return _generate_run_file(\n            abs_job_folder,\n            job_name,\n            setup_env_script,\n            additionnal_command,\n            slurm_docker_fix=slurm_docker_fix,\n        )\n    if kwargs_htc_run_files[\"l_dependencies\"] is None:\n        kwargs_htc_run_files[\"l_dependencies\"] = []\n    return _generate_run_file_htc(\n        abs_job_folder,\n        job_name,\n        setup_env_script,\n        additionnal_command,\n        **kwargs_htc_run_files,\n    )\n</code></pre>"},{"location":"reference/study_da/submit/generate_run.html#study_da.submit.generate_run.tag_str","title":"<code>tag_str(abs_job_folder)</code>","text":"<p>\" Generates a shell script snippet to tag a job as finished if it was successful. Args:     abs_job_folder (str): The (absolute) folder where the job is located. Returns:     str: A formatted string containing the shell script snippet.</p> Source code in <code>study_da/submit/generate_run.py</code> <pre><code>def tag_str(abs_job_folder: str) -&gt; str:\n    \"\"\" \"\n    Generates a shell script snippet to tag a job as finished if it was successful.\n    Args:\n        abs_job_folder (str): The (absolute) folder where the job is located.\n    Returns:\n        str: A formatted string containing the shell script snippet.\n    \"\"\"\n    return f\"\"\"\n# Ensure job run was successful and tag as finished, or as failed otherwise\nif [ $? -eq 0 ]; then\n    touch {abs_job_folder}/.finished\nelse\n    touch {abs_job_folder}/.failed\nfi\\n\n\"\"\"\n</code></pre>"},{"location":"reference/study_da/submit/submit_scan.html","title":"submit_scan","text":"<p>This module contains the SubmitScan class, which is used to submit jobs either locally or on a cluster.</p>"},{"location":"reference/study_da/submit/submit_scan.html#study_da.submit.submit_scan.SubmitScan","title":"<code>SubmitScan</code>","text":"Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>class SubmitScan:\n    def __init__(\n        self,\n        path_tree: str,\n        path_python_environment: str = \"\",\n        path_python_environment_container: str = \"\",\n        path_container_image: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the SubmitScan class.\n\n        Args:\n            path_tree (str): The path to the tree structure.\n            path_python_environment (str): The path to the Python environment. Defaults to \"\".\n            path_python_environment_container (str, optional): The path to the Python environment\n                in the container. Defaults to \"\".\n            path_container_image (Optional[str], optional): The path to the container image.\n                Defaults to None.\n        \"\"\"\n        # Path to study files\n        self.path_tree = path_tree\n\n        # Absolute path to the tree\n        self.abs_path_tree = os.path.abspath(path_tree)\n\n        # Name of the study folder\n        self.study_name = os.path.dirname(path_tree)\n\n        # Absolute path to the study folder (get from the path_tree)\n        self.abs_path = os.path.abspath(self.study_name).split(f\"/{self.study_name}\")[0]\n\n        # Check that the current working directory is one step above the study folder\n        if os.getcwd() != self.abs_path:\n            raise ValueError(\n                \"The current working directory must be the parent folder of the study folder, \"\n                \"i.e. the folder from which the study was generated. \"\n                \"Please submit from there.\"\n            )\n\n        # Container image (Docker or Singularity, if any)\n        # Turn to absolute path if it is not already\n        if path_container_image is None:\n            self.path_container_image = None\n        elif not os.path.isabs(path_container_image):\n            self.path_container_image = os.path.abspath(path_container_image)\n        else:\n            self.path_container_image = path_container_image\n\n        # Python environment for the container\n        self.path_python_environment_container = path_python_environment_container\n\n        # Ensure that the container image is set if the python environment is set\n        if self.path_container_image and not self.path_python_environment_container:\n            raise ValueError(\n                \"The path to the python environment in the container must be set if the container\"\n                \"image is set.\"\n            )\n\n        # Add /bin/activate to the path_python_environment if needed\n        if not self.path_python_environment_container.endswith(\"/bin/activate\"):\n            # Remove potential / at the end of the path\n            if (\n                self.path_python_environment_container\n                and self.path_python_environment_container[-1] == \"/\"\n            ):\n                self.path_python_environment_container = self.path_python_environment_container[:-1]\n            self.path_python_environment_container += \"/bin/activate\"\n\n        # Ensure the path to the python environment is not \"\" if the container image is not set\n        if not self.path_container_image and not path_python_environment:\n            raise ValueError(\n                \"The path to the python environment must be set if the container image is not set.\"\n            )\n\n        # Path to the python environment, activate with `source path_python_environment`\n        if not path_python_environment:\n            logging.warning(\"No local python environment provided.\")\n            self.path_python_environment = \"\"\n\n        else:\n            # Ensure that the path is not of the form path/bin/activate environment_name\n            split_path = path_python_environment.split(\" \")\n            real_path = split_path[0]\n            env_name = split_path[1] if len(split_path) &gt; 1 else \"\"\n\n            # Turn to absolute path if it is not already\n            self.path_python_environment = (\n                real_path if os.path.isabs(real_path) else os.path.abspath(real_path)\n            )\n\n            # Add /bin/activate to the path_python_environment if needed\n            if \"bin/activate\" not in self.path_python_environment:\n                # Ensure there's no / at the end of the path\n                if self.path_python_environment and self.path_python_environment[-1] == \"/\":\n                    self.path_python_environment = self.path_python_environment[:-1]\n                self.path_python_environment += \"/bin/activate\"\n\n            # Add environment name to the path_python_environment if needed\n            if env_name:\n                self.path_python_environment += f\" {env_name}\"\n        # Lock file to avoid concurrent access (softlock as several platforms are used)\n        self.lock = SoftFileLock(f\"{self.path_tree}.lock\", timeout=60)\n\n    # dic_tree as a property so that it is reloaded every time it is accessed\n    @property\n    def dic_tree(self) -&gt; dict:\n        \"\"\"\n        Loads the dictionary tree from the path.\n\n        Returns:\n            dict: The loaded dictionary tree.\n        \"\"\"\n        logging.info(f\"Loading tree from {self.path_tree}\")\n        return load_dic_from_path(self.path_tree)[0]\n\n    # Setter for the dic_tree property\n    @dic_tree.setter\n    def dic_tree(self, value: dict) -&gt; None:\n        \"\"\"\n        Writes the dictionary tree to the path.\n\n        Args:\n            value (dict): The dictionary tree to write.\n        \"\"\"\n        logging.info(f\"Writing tree to {self.path_tree}\")\n        write_dic_to_path(value, self.path_tree)\n\n    def configure_jobs(\n        self,\n        force_configure: bool = False,\n        dic_config_jobs: Optional[dict[str, dict[str, Any]]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Configures the jobs by modifying the tree structure and creating the run files for each job.\n\n        Args:\n            force_configure (bool, optional): Whether to force reconfiguration. Defaults to False.\n            dic_config_jobs (Optional[dict[str, dict[str, Any]]], optional): A dictionary containing\n                the configuration of the jobs. Defaults to None.\n        \"\"\"\n        # Lock since we are modifying the tree\n        logging.info(\"Acquiring lock to configure jobs\")\n        with self.lock:\n            # Get the tree\n            dic_tree = self.dic_tree\n\n            # Ensure jobs have not been configured already\n            if (\"configured\" in dic_tree and dic_tree[\"configured\"]) and not force_configure:\n                logging.warning(\"Jobs have already been configured. Skipping.\")\n                return\n\n            # Configure the jobs (add generation and job keys, set status to \"To finish\")\n            dic_tree = ConfigJobs(dic_tree,starting_depth=-len(Path(self.path_tree).parts) + 2).find_and_configure_jobs(dic_config_jobs)\n\n            # Add the python environment, container image and absolute path of the study to the tree\n            dic_tree[\"python_environment\"] = self.path_python_environment\n            dic_tree[\"container_image\"] = self.path_container_image\n            dic_tree[\"absolute_path\"] = self.abs_path\n            dic_tree[\"status\"] = \"to_finish\"\n            dic_tree[\"configured\"] = True\n\n            # Explicitly set the dic_tree property to force rewrite\n            self.dic_tree = dic_tree\n\n        logging.info(\"Jobs have been configured. Lock released.\")\n\n    def get_all_jobs(self) -&gt; dict:\n        \"\"\"\n        Retrieves all jobs from the configuration, without modifying the tree.\n\n        Returns:\n            dict: A dictionary containing all jobs.\n        \"\"\"\n        # Get a copy of the tree as it's safer\n        with self.lock:\n            dic_tree = self.dic_tree\n        return ConfigJobs(dic_tree,starting_depth=-len(Path(self.path_tree).parts) + 2).find_all_jobs()\n\n    def generate_run_files(\n        self,\n        dic_tree: dict[str, Any],\n        l_jobs: list[str],\n        dic_additional_commands_per_gen: dict[int, str],\n        dic_dependencies_per_gen: dict[int, list[str]],\n        dic_copy_back_per_gen: dict[int, dict[str, bool]],\n        name_config: str,\n    ) -&gt; dict:\n        \"\"\"\n        Generates run files for the specified jobs.\n\n        Args:\n            dic_tree (dict): The dictionary tree structure.\n            l_jobs (list[str]): List of jobs to submit.\n            dic_additional_commands_per_gen (dict[int, str], optional): Additional commands per\n                generation. Defaults to {}.\n            dic_dependencies_per_gen (dict[int, list[str]], optional): Dependencies per generation.\n                Only used when doing a HTC submission.\n            dic_copy_back_per_gen (Optional[dict[int, dict[str, bool]]], optional): A dictionary\n                containing the files to copy back per generation. Accepted keys are \"parquet\",\n                \"yaml\", \"txt\", \"json\", \"zip\" and \"all\".\n            name_config (str, optional): The name of the configuration file for the study.\n\n        Returns:\n            dict: The updated dictionary tree structure.\n        \"\"\"\n\n        logging.info(\"Generating run files for the jobs to submit\")\n        # Generate the run files for the jobs to submit\n        dic_all_jobs = self.get_all_jobs()\n        for job in l_jobs:\n            l_keys = dic_all_jobs[job][\"l_keys\"]\n            job_name = os.path.basename(job)\n            relative_job_folder = os.path.dirname(job)\n            absolute_job_folder = f\"{self.abs_path}/{relative_job_folder}\"\n            generation_number = dic_all_jobs[job][\"gen\"]\n            submission_type = nested_get(dic_tree, l_keys + [\"submission_type\"])\n            singularity = \"docker\" in submission_type\n            path_python_environment = (\n                self.path_python_environment_container\n                if singularity\n                else self.path_python_environment\n            )\n\n            # Ensure that the run file does not already exist\n            if \"path_run\" in nested_get(dic_tree, l_keys):\n                path_run_curr = nested_get(dic_tree, l_keys + [\"path_run\"])\n                if path_run_curr is not None and os.path.exists(path_run_curr):\n                    logging.info(f\"Run file already exists for job {job}. Skipping.\")\n                    continue\n\n            # Build l_dependencies and add to the kwargs\n            l_dependencies = dic_dependencies_per_gen.get(generation_number, [])\n\n            # Get arguments of current generation\n            dic_args = dic_copy_back_per_gen.get(generation_number, {})\n\n            # Mutate the keys\n            dic_args = {f\"copy_back_{key}\": value for key, value in dic_args.items()}\n\n            # Build kwargs for the run file\n            kwargs_htc = {\n                \"l_dependencies\": l_dependencies,\n                \"name_config\": name_config,\n            } | dic_args\n\n            run_str = generate_run_file(\n                absolute_job_folder,\n                job_name,\n                path_python_environment,\n                htc=\"htc\" in submission_type,\n                additionnal_command=dic_additional_commands_per_gen.get(generation_number, \"\"),\n                **kwargs_htc,\n            )\n            # Write the run file\n            path_run_job = f\"{absolute_job_folder}/run.sh\"\n            with open(path_run_job, \"w\") as f:\n                f.write(run_str)\n\n            # Change permissions to make the file executable\n            os.chmod(path_run_job, 0o755)\n\n            # Record the path to the run file in the tree\n            nested_set(dic_tree, l_keys + [\"path_run\"], path_run_job)\n\n        return dic_tree\n\n    def check_and_update_all_jobs_status(self) -&gt; tuple[dict[str, Any], str]:\n        \"\"\"\n        Checks the status of all jobs and updates their status in the job dictionary.\n\n        This method iterates through all jobs, checks if a \".finished\" or a \".failed\" file exists in\n        the job's folder, and updates the job's status accordingly. If at least one job is not\n        finished or failed, the overall status is set to \"to_finish\". If all jobs are finished or\n        failed, the overall status is set to \"finished\".\n\n        Returns:\n            tuple[dict[str, Any], str]: A tuple containing:\n            - A dictionary with all jobs and their updated statuses.\n            - A string representing the final status (\"to_finish\" or \"finished\").\n        \"\"\"\n        dic_all_jobs = self.get_all_jobs()\n        at_least_one_job_to_finish = False\n        final_status = \"to_finish\"\n        with self.lock:\n            # Get dic tree once to avoid reloading it for every job\n            dic_tree = self.dic_tree\n\n            # First pass to update the state of the tree\n            for job in dic_all_jobs:\n                # Skip jobs that are already finished, failed or unsubmittable\n                if nested_get(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"]) in [\n                    \"finished\",\n                    \"failed\",\n                    \"unsubmittable\",\n                ]:\n                    continue\n\n                # Check the state of the others\n                relative_job_folder = os.path.dirname(job)\n                absolute_job_folder = f\"{self.abs_path}/{relative_job_folder}\"\n                if os.path.exists(f\"{absolute_job_folder}/.finished\"):\n                    nested_set(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"], \"finished\")\n                # Check if the job failed otherwise (not to resubmit it again)\n                elif os.path.exists(f\"{absolute_job_folder}/.failed\"):\n                    nested_set(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"], \"failed\")\n                # else:\n                #     at_least_one_job_to_finish = True\n\n            # Second pass to update the state of the tree with unreachable jobs\n            dependency_graph = DependencyGraph(dic_tree, dic_all_jobs)\n            for job in dic_all_jobs:\n                # Get all failed dependencies across the tree\n                l_dep_failed = dependency_graph.get_failed_dependency(job)\n                if len(l_dep_failed) &gt; 0:\n                    nested_set(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"], \"unsubmittable\")\n                elif nested_get(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"]) == \"to_submit\":\n                    at_least_one_job_to_finish = True\n\n            if not at_least_one_job_to_finish:\n                # No more jobs to submit so finished\n                dic_tree[\"status\"] = final_status = \"finished\"\n                # Last pass to check if all jobs are properly finished\n                for job in dic_all_jobs:\n                    if nested_get(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"]) != \"finished\":\n                        dic_tree[\"status\"] = final_status = \"finished with issues\"\n                        break\n\n            # Update dic_tree from cluster_submission\n            self.dic_tree = dic_tree\n\n        return dic_all_jobs, final_status\n\n    def reset_failed_jobs(self, dic_tree: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"\n        Resets the status of jobs that have failed to \"to_submit\".\n\n        Args:\n            dic_tree (dict[str, Any]): The dictionary tree structure.\n\n        Returns:\n            dict[str, Any]: The updated dictionary tree structure.\n        \"\"\"\n\n        dic_all_jobs = self.get_all_jobs()\n        # First pass to update the state of the tree\n        for job in dic_all_jobs:\n            # Skip jobs that are not failed\n            if nested_get(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"]) != \"failed\":\n                continue\n\n            # Reset the state of the others\n            relative_job_folder = os.path.dirname(job)\n            absolute_job_folder = f\"{self.abs_path}/{relative_job_folder}\"\n\n            # Remove failed tag\n            if os.path.exists(f\"{absolute_job_folder}/.failed\"):\n                os.remove(f\"{absolute_job_folder}/.failed\")\n            else:\n                logging.warning(f\"Failed file not found for job {job}.\")\n\n            # Remove run file\n            if \"path_run\" in nested_get(dic_tree, dic_all_jobs[job][\"l_keys\"]):\n                path_run_curr = nested_get(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"path_run\"])\n                if path_run_curr is not None and os.path.exists(path_run_curr):\n                    os.remove(path_run_curr)\n                else:\n                    logging.warning(f\"Run file not found for job {job}.\")\n\n            # Reset the status of the job\n            nested_set(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"], \"to_submit\")\n\n        return dic_tree\n\n    def submit(\n        self,\n        one_generation_at_a_time: bool = False,\n        dic_additional_commands_per_gen: Optional[dict[int, str]] = None,\n        dic_dependencies_per_gen: Optional[dict[int, list[str]]] = None,\n        dic_copy_back_per_gen: Optional[dict[int, dict[str, bool]]] = None,\n        name_config: str = \"config.yaml\",\n        force_submit: bool = False,\n    ) -&gt; str:\n        \"\"\"\n        Submits the jobs to the cluster. Note that copying back large files (e.g. json colliders)\n        can trigger a throttling mechanism in AFS.\n\n        The following arguments are only used for HTC jobs submission:\n        - dic_additional_commands_per_gen\n        - dic_dependencies_per_gen\n        - dic_copy_back_per_gen\n        - name_config\n\n        Args:\n            one_generation_at_a_time (bool, optional): Whether to submit one full generation at a\n                time. Defaults to False.\n            dic_additional_commands_per_gen (dict[int, str], optional): Additional commands per\n                generation. Defaults to None.\n            dic_dependencies_per_gen (dict[int, list[str]], optional): Dependencies per generation.\n                Only used when doing a HTC submission. Defaults to None.\n            dic_copy_back_per_gen (Optional[dict[int, dict[str, bool]]], optional): A dictionary\n                containing the files to copy back per generation. Accepted keys are \"parquet\",\n                \"yaml\", \"txt\", \"json\", \"zip\" and \"all\". Defaults to None, corresponding to copying\n                back only \"light\" files, i.e. parquet, yaml and txt.\n            name_config (str, optional): The name of the configuration file for the study.\n                Defaults to \"config.yaml\".\n            force_submit (bool, optional): If True, jobs are resubmitted even though they failed.\n                Defaults to False.\n\n        Returns:\n            str: The final status of the jobs.\n        \"\"\"\n        # Handle mutable default arguments\n        if dic_additional_commands_per_gen is None:\n            dic_additional_commands_per_gen = {}\n        if dic_dependencies_per_gen is None:\n            dic_dependencies_per_gen = {}\n        if dic_copy_back_per_gen is None:\n            dic_copy_back_per_gen = {}\n\n        # Handle force submit\n        if force_submit:\n            logging.warning(\"Forcing resubmission of all failed jobs.\")\n            with self.lock:\n                # Acquire tree from disk\n                dic_tree = self.dic_tree\n\n                # Reset the tree by deleting the failed tags\n                dic_tree = self.reset_failed_jobs(dic_tree)\n                dic_tree[\"status\"] = \"to_finish\"\n                # Write the tree back to disk\n                self.dic_tree = dic_tree\n\n        # Update the status of all jobs before submitting\n        dic_all_jobs, final_status = self.check_and_update_all_jobs_status()\n        if final_status == \"finished\":\n            print(\"All jobs are finished.\")\n            return final_status\n        elif final_status == \"finished with issues\":\n            print(\"All jobs are finished but some did not run properly.\")\n            return final_status\n\n        logging.info(\"Acquiring lock to submit jobs\")\n        with self.lock:\n            # Get dic tree once to avoid reloading it for every job\n            dic_tree = self.dic_tree\n\n            # Submit the jobs\n            self._submit(\n                dic_tree,\n                dic_all_jobs,\n                one_generation_at_a_time,\n                dic_additional_commands_per_gen,\n                dic_dependencies_per_gen,\n                dic_copy_back_per_gen,\n                name_config,\n            )\n\n            # Update dic_tree from cluster_submission\n            self.dic_tree = dic_tree\n        logging.info(\"Jobs have been submitted. Lock released.\")\n        return final_status\n\n    def _submit(\n        self,\n        dic_tree: dict[str, Any],\n        dic_all_jobs: dict[str, dict[str, Any]],\n        one_generation_at_a_time: bool,\n        dic_additional_commands_per_gen: dict[int, str],\n        dic_dependencies_per_gen: dict[int, list[str]],\n        dic_copy_back_per_gen: dict[int, dict[str, bool]],\n        name_config: str,\n    ) -&gt; None:\n        \"\"\"\n        Submits the jobs to the cluster.\n\n        Args:\n            dic_tree (dict[str, Any]): The dictionary tree structure.\n            dic_all_jobs (dict[str, dict[str,Any]]): A dictionary containing all jobs.\n            one_generation_at_a_time (bool): Whether to submit one full generation at a time.\n            dic_additional_commands_per_gen (dict[int, str], optional): Additional commands per\n                generation.\n\n            The following arguments are only used for HTC jobs submission:\n\n            dic_dependencies_per_gen (dict[int, list[str]], optional): Dependencies per generation.\n                Only used when doing a HTC submission. Defaults to None.\n            dic_copy_back_per_gen (Optional[dict[int, dict[str, bool]]], optional): A dictionary\n                containing the files to copy back per generation.\n            name_config (str, optional): The name of the configuration file for the study.\n        \"\"\"\n        # Collect dict of list of unfinished jobs for every tree branch and every gen\n        dic_to_submit_by_gen = {}\n        dic_summary_by_gen = {}\n        dependency_graph = DependencyGraph(dic_tree, dic_all_jobs)\n        for job in dic_all_jobs:\n            dic_to_submit_by_gen, dic_summary_by_gen = self._check_job_submit_status(\n                job,\n                dic_tree,\n                dic_all_jobs,\n                dic_to_submit_by_gen,\n                dic_summary_by_gen,\n                dependency_graph,\n            )\n\n        # Only keep the topmost generation if one_generation_at_a_time is True\n        if one_generation_at_a_time:\n            logging.info(\n                \"Cropping list of jobs to submit to ensure only one generation is submitted at \"\n                \"a time.\"\n            )\n            min_gen = min(k for k, l_jobs in dic_to_submit_by_gen.items() if l_jobs)\n            dic_to_submit_by_gen = {min_gen: dic_to_submit_by_gen[min_gen]}\n\n        # Convert dic_to_submit_by_gen to contain all requested information\n        l_jobs_to_submit = [job for dic_gen in dic_to_submit_by_gen.values() for job in dic_gen]\n\n        # Generate run files for the jobs to submit\n        # ! Run files are generated at submit and not at configuration as the configuration\n        # ! files are created at the end of each generation\n        dic_tree = self.generate_run_files(\n            dic_tree,\n            l_jobs_to_submit,\n            dic_additional_commands_per_gen,\n            dic_dependencies_per_gen=dic_dependencies_per_gen,\n            dic_copy_back_per_gen=dic_copy_back_per_gen,\n            name_config=name_config,\n        )\n\n        # Create the ClusterSubmission object\n        path_submission_file = f\"{self.abs_path}/{self.study_name}/submission/submission_file.sub\"\n        cluster_submission = ClusterSubmission(\n            self.study_name,\n            l_jobs_to_submit,\n            dic_all_jobs,\n            dic_tree,\n            path_submission_file,\n            self.abs_path,\n        )\n\n        # Write and submit the submission files\n        logging.info(\"Writing and submitting submission files\")\n        dic_submission_files = cluster_submission.write_sub_files(dic_summary_by_gen)\n\n        # Log the state of the jobs\n        self.log_jobs_state(dic_summary_by_gen)\n        for submission_type, (\n            list_of_jobs,\n            l_submission_filenames,\n        ) in dic_submission_files.items():\n            cluster_submission.submit(list_of_jobs, l_submission_filenames, submission_type)\n\n    @staticmethod\n    def log_jobs_state(dic_summary_by_gen: dict[int, dict[str, int]]) -&gt; None:\n        \"\"\"\n        Logs the state of jobs for each generation.\n\n        Args:\n            dic_summary_by_gen (dict): A dictionary where the keys are generation numbers\n                and the values are dictionaries summarizing job states.\n                Each summary dictionary should contain the following keys:\n                - 'to_submit_later': int, number of jobs left to submit later\n                - 'running_or_queuing': int, number of jobs running or queuing\n                - 'submitted_now': int, number of jobs submitted now\n                - 'finished': int, number of jobs finished\n                - 'failed': int, number of jobs failed\n                - 'dependency_failed': int, number of jobs on hold due to failed dependencies\n\n        Returns:\n            None\n        \"\"\"\n        print(\"State of the jobs:\")\n        for gen, dic_summary in dic_summary_by_gen.items():\n            print(\"********************************\")\n            print(f\"Generation {gen}\")\n            print(f\"Jobs left to submit later: {dic_summary['to_submit_later']}\")\n            print(f\"Jobs running or queuing: {dic_summary['running_or_queuing']}\")\n            print(f\"Jobs submitted now: {dic_summary['submitted_now']}\")\n            print(f\"Jobs finished: {dic_summary['finished']}\")\n            print(f\"Jobs failed: {dic_summary['failed']}\")\n            print(f\"Jobs on hold due to failed dependencies: {dic_summary['dependency_failed']}\")\n            print(\"********************************\")\n\n    @staticmethod\n    def _check_job_submit_status(\n        job: str,\n        dic_tree: dict[str, Any],\n        dic_all_jobs: dict[str, dict[str, Any]],\n        dic_to_submit_by_gen: dict[int, list[str]],\n        dic_summary_by_gen: dict[int, dict[str, int]],\n        dependency_graph: DependencyGraph,\n    ) -&gt; tuple[dict[int, list[str]], dict[int, dict[str, int]]]:\n        \"\"\"\n        Checks the status and dependencies of a job and updates the submission and summary\n        dictionaries.\n\n        Args:\n            job (str): The job identifier.\n            dic_tree (dict[str, Any]): The dictionary tree structure.\n            dic_all_jobs (dict[str, dict[str,Any]]): A dictionary containing all jobs.\n            dic_to_submit_by_gen (dict[int, list[str]]): A dictionary where keys are generation\n                numbers and values are lists of jobs to submit for each generation.\n            dic_summary_by_gen (dict[int, dict[str, int]]): A dictionary where keys are generation\n                numbers and values are dictionaries summarizing job states.\n            dependency_graph (DependencyGraph): An object to check job dependencies.\n\n        Returns:\n            tuple[dict[int, list[str]], dict[int, dict[str, int]]]: Updated dictionaries for jobs to\n                submit and job summaries.\n        \"\"\"\n        gen = dic_all_jobs[job][\"gen\"]\n        if gen not in dic_to_submit_by_gen:\n            dic_to_submit_by_gen[gen] = []\n            dic_summary_by_gen[gen] = {\n                \"finished\": 0,\n                \"failed\": 0,\n                \"dependency_failed\": 0,\n                \"running_or_queuing\": 0,\n                \"submitted_now\": 0,\n                \"to_submit_later\": 0,\n            }\n        logging.info(f\"Checking job {job} dependencies and status in tree\")\n        l_dep = dependency_graph.get_unfinished_dependency(job)\n        l_dep_failed = dependency_graph.get_failed_dependency(job)\n\n        # Job will be on hold as it has failed dependencies\n        if len(l_dep_failed) &gt; 0:\n            logging.warning(\n                f\"Job {job} has failed dependencies: {l_dep_failed}, it won't be submitted.\"\n            )\n            dic_summary_by_gen[gen][\"dependency_failed\"] += 1\n\n        # Jobs is waiting for dependencies to finish\n        elif len(l_dep) &gt; 0:\n            dic_summary_by_gen[gen][\"to_submit_later\"] += 1\n\n        # Job dependencies are ok\n        elif len(l_dep) == 0:\n            # But job has failed already\n            if nested_get(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"]) == \"failed\":\n                dic_summary_by_gen[gen][\"failed\"] += 1\n\n            # Or job has finished already\n            elif nested_get(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"]) == \"finished\":\n                dic_summary_by_gen[gen][\"finished\"] += 1\n\n            # Else everything is ok, added to the submit dict\n            else:\n                logging.info(f\"Job {job} is added for submission.\")\n                dic_to_submit_by_gen[gen].append(job)\n                # We'll determine which jobs actually have to be submitted and which jobs\n                # are running at the end of the function, after querying the cluster or the local pc\n\n        return dic_to_submit_by_gen, dic_summary_by_gen\n\n    def keep_submit_until_done(\n        self,\n        one_generation_at_a_time: bool = False,\n        wait_time: float = 30,\n        max_try=100,\n        dic_additional_commands_per_gen: Optional[dict[int, str]] = None,\n        dic_dependencies_per_gen: Optional[dict[int, list[str]]] = None,\n        dic_copy_back_per_gen: Optional[dict[int, dict[str, bool]]] = None,\n        name_config: str = \"config.yaml\",\n        force_submit: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Keeps submitting jobs until all jobs are finished or failed.\n\n        The following arguments are only used for HTC jobs submission:\n        - dic_additional_commands_per_gen\n        - dic_dependencies_per_gen\n        - dic_copy_back_per_gen\n        - name_config\n\n        Args:\n            one_generation_at_a_time (bool, optional): Whether to submit one full generation at a\n                time. Defaults to False.\n            wait_time (float, optional): The wait time between submissions in minutes.\n                Defaults to 30.\n            max_try (int, optional): The maximum number of tries before stopping the submission.\n            dic_additional_commands_per_gen (dict[int, str], optional): Additional commands per\n                generation. Defaults to None.\n            dic_dependencies_per_gen (dict[int, list[str]], optional): Dependencies per generation.\n                Only used when doing a HTC submission. Defaults to None.\n            dic_copy_back_per_gen (Optional[dict[int, dict[str, bool]]], optional): A dictionary\n                containing the files to copy back per generation. Accepted keys are \"parquet\",\n                \"yaml\", \"txt\", \"json\", \"zip\" and \"all\". Defaults to None, corresponding to copying\n                back only \"light\" files, i.e. parquet, yaml and txt.\n            name_config (str, optional): The name of the configuration file for the study.\n                Defaults to \"config.yaml\".\n            force_submit (bool, optional): If True, jobs are resubmitted even though they failed.\n                Defaults to False.\n\n\n        Returns:\n            None\n        \"\"\"\n        # Handle mutable default arguments\n        if dic_additional_commands_per_gen is None:\n            dic_additional_commands_per_gen = {}\n        if dic_dependencies_per_gen is None:\n            dic_dependencies_per_gen = {}\n\n        if wait_time &lt; 1 / 20:\n            logging.warning(\"Wait time should be at least 10 seconds to prevent locking errors.\")\n            logging.warning(\"Setting wait time to 10 seconds.\")\n            wait_time = 10 / 60\n\n        # I don't need to lock the tree here since the status cheking is read only and\n        # the lock is acquired in the submit method for the submission\n        while (\n            self.submit(\n                one_generation_at_a_time,\n                dic_additional_commands_per_gen,\n                dic_dependencies_per_gen,\n                dic_copy_back_per_gen,\n                name_config,\n                force_submit=force_submit,\n            )\n            not in [\"finished\", \"finished with issues\"]\n            and max_try &gt; 0\n        ):\n            # Wait for a certain amount of time before checking again\n            logging.info(f\"Waiting {wait_time} minutes before checking again.\")\n            time.sleep(wait_time * 60)\n            max_try -= 1\n\n        if max_try == 0:\n            print(\"Maximum number of tries reached. Stopping submission.\")\n</code></pre>"},{"location":"reference/study_da/submit/submit_scan.html#study_da.submit.submit_scan.SubmitScan.dic_tree","title":"<code>dic_tree: dict</code>  <code>property</code> <code>writable</code>","text":"<p>Loads the dictionary tree from the path.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The loaded dictionary tree.</p>"},{"location":"reference/study_da/submit/submit_scan.html#study_da.submit.submit_scan.SubmitScan.__init__","title":"<code>__init__(path_tree, path_python_environment='', path_python_environment_container='', path_container_image=None)</code>","text":"<p>Initializes the SubmitScan class.</p> <p>Parameters:</p> Name Type Description Default <code>path_tree</code> <code>str</code> <p>The path to the tree structure.</p> required <code>path_python_environment</code> <code>str</code> <p>The path to the Python environment. Defaults to \"\".</p> <code>''</code> <code>path_python_environment_container</code> <code>str</code> <p>The path to the Python environment in the container. Defaults to \"\".</p> <code>''</code> <code>path_container_image</code> <code>Optional[str]</code> <p>The path to the container image. Defaults to None.</p> <code>None</code> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>def __init__(\n    self,\n    path_tree: str,\n    path_python_environment: str = \"\",\n    path_python_environment_container: str = \"\",\n    path_container_image: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes the SubmitScan class.\n\n    Args:\n        path_tree (str): The path to the tree structure.\n        path_python_environment (str): The path to the Python environment. Defaults to \"\".\n        path_python_environment_container (str, optional): The path to the Python environment\n            in the container. Defaults to \"\".\n        path_container_image (Optional[str], optional): The path to the container image.\n            Defaults to None.\n    \"\"\"\n    # Path to study files\n    self.path_tree = path_tree\n\n    # Absolute path to the tree\n    self.abs_path_tree = os.path.abspath(path_tree)\n\n    # Name of the study folder\n    self.study_name = os.path.dirname(path_tree)\n\n    # Absolute path to the study folder (get from the path_tree)\n    self.abs_path = os.path.abspath(self.study_name).split(f\"/{self.study_name}\")[0]\n\n    # Check that the current working directory is one step above the study folder\n    if os.getcwd() != self.abs_path:\n        raise ValueError(\n            \"The current working directory must be the parent folder of the study folder, \"\n            \"i.e. the folder from which the study was generated. \"\n            \"Please submit from there.\"\n        )\n\n    # Container image (Docker or Singularity, if any)\n    # Turn to absolute path if it is not already\n    if path_container_image is None:\n        self.path_container_image = None\n    elif not os.path.isabs(path_container_image):\n        self.path_container_image = os.path.abspath(path_container_image)\n    else:\n        self.path_container_image = path_container_image\n\n    # Python environment for the container\n    self.path_python_environment_container = path_python_environment_container\n\n    # Ensure that the container image is set if the python environment is set\n    if self.path_container_image and not self.path_python_environment_container:\n        raise ValueError(\n            \"The path to the python environment in the container must be set if the container\"\n            \"image is set.\"\n        )\n\n    # Add /bin/activate to the path_python_environment if needed\n    if not self.path_python_environment_container.endswith(\"/bin/activate\"):\n        # Remove potential / at the end of the path\n        if (\n            self.path_python_environment_container\n            and self.path_python_environment_container[-1] == \"/\"\n        ):\n            self.path_python_environment_container = self.path_python_environment_container[:-1]\n        self.path_python_environment_container += \"/bin/activate\"\n\n    # Ensure the path to the python environment is not \"\" if the container image is not set\n    if not self.path_container_image and not path_python_environment:\n        raise ValueError(\n            \"The path to the python environment must be set if the container image is not set.\"\n        )\n\n    # Path to the python environment, activate with `source path_python_environment`\n    if not path_python_environment:\n        logging.warning(\"No local python environment provided.\")\n        self.path_python_environment = \"\"\n\n    else:\n        # Ensure that the path is not of the form path/bin/activate environment_name\n        split_path = path_python_environment.split(\" \")\n        real_path = split_path[0]\n        env_name = split_path[1] if len(split_path) &gt; 1 else \"\"\n\n        # Turn to absolute path if it is not already\n        self.path_python_environment = (\n            real_path if os.path.isabs(real_path) else os.path.abspath(real_path)\n        )\n\n        # Add /bin/activate to the path_python_environment if needed\n        if \"bin/activate\" not in self.path_python_environment:\n            # Ensure there's no / at the end of the path\n            if self.path_python_environment and self.path_python_environment[-1] == \"/\":\n                self.path_python_environment = self.path_python_environment[:-1]\n            self.path_python_environment += \"/bin/activate\"\n\n        # Add environment name to the path_python_environment if needed\n        if env_name:\n            self.path_python_environment += f\" {env_name}\"\n    # Lock file to avoid concurrent access (softlock as several platforms are used)\n    self.lock = SoftFileLock(f\"{self.path_tree}.lock\", timeout=60)\n</code></pre>"},{"location":"reference/study_da/submit/submit_scan.html#study_da.submit.submit_scan.SubmitScan.check_and_update_all_jobs_status","title":"<code>check_and_update_all_jobs_status()</code>","text":"<p>Checks the status of all jobs and updates their status in the job dictionary.</p> <p>This method iterates through all jobs, checks if a \".finished\" or a \".failed\" file exists in the job's folder, and updates the job's status accordingly. If at least one job is not finished or failed, the overall status is set to \"to_finish\". If all jobs are finished or failed, the overall status is set to \"finished\".</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>tuple[dict[str, Any], str]: A tuple containing:</p> <code>str</code> <ul> <li>A dictionary with all jobs and their updated statuses.</li> </ul> <code>tuple[dict[str, Any], str]</code> <ul> <li>A string representing the final status (\"to_finish\" or \"finished\").</li> </ul> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>def check_and_update_all_jobs_status(self) -&gt; tuple[dict[str, Any], str]:\n    \"\"\"\n    Checks the status of all jobs and updates their status in the job dictionary.\n\n    This method iterates through all jobs, checks if a \".finished\" or a \".failed\" file exists in\n    the job's folder, and updates the job's status accordingly. If at least one job is not\n    finished or failed, the overall status is set to \"to_finish\". If all jobs are finished or\n    failed, the overall status is set to \"finished\".\n\n    Returns:\n        tuple[dict[str, Any], str]: A tuple containing:\n        - A dictionary with all jobs and their updated statuses.\n        - A string representing the final status (\"to_finish\" or \"finished\").\n    \"\"\"\n    dic_all_jobs = self.get_all_jobs()\n    at_least_one_job_to_finish = False\n    final_status = \"to_finish\"\n    with self.lock:\n        # Get dic tree once to avoid reloading it for every job\n        dic_tree = self.dic_tree\n\n        # First pass to update the state of the tree\n        for job in dic_all_jobs:\n            # Skip jobs that are already finished, failed or unsubmittable\n            if nested_get(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"]) in [\n                \"finished\",\n                \"failed\",\n                \"unsubmittable\",\n            ]:\n                continue\n\n            # Check the state of the others\n            relative_job_folder = os.path.dirname(job)\n            absolute_job_folder = f\"{self.abs_path}/{relative_job_folder}\"\n            if os.path.exists(f\"{absolute_job_folder}/.finished\"):\n                nested_set(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"], \"finished\")\n            # Check if the job failed otherwise (not to resubmit it again)\n            elif os.path.exists(f\"{absolute_job_folder}/.failed\"):\n                nested_set(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"], \"failed\")\n            # else:\n            #     at_least_one_job_to_finish = True\n\n        # Second pass to update the state of the tree with unreachable jobs\n        dependency_graph = DependencyGraph(dic_tree, dic_all_jobs)\n        for job in dic_all_jobs:\n            # Get all failed dependencies across the tree\n            l_dep_failed = dependency_graph.get_failed_dependency(job)\n            if len(l_dep_failed) &gt; 0:\n                nested_set(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"], \"unsubmittable\")\n            elif nested_get(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"]) == \"to_submit\":\n                at_least_one_job_to_finish = True\n\n        if not at_least_one_job_to_finish:\n            # No more jobs to submit so finished\n            dic_tree[\"status\"] = final_status = \"finished\"\n            # Last pass to check if all jobs are properly finished\n            for job in dic_all_jobs:\n                if nested_get(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"]) != \"finished\":\n                    dic_tree[\"status\"] = final_status = \"finished with issues\"\n                    break\n\n        # Update dic_tree from cluster_submission\n        self.dic_tree = dic_tree\n\n    return dic_all_jobs, final_status\n</code></pre>"},{"location":"reference/study_da/submit/submit_scan.html#study_da.submit.submit_scan.SubmitScan.configure_jobs","title":"<code>configure_jobs(force_configure=False, dic_config_jobs=None)</code>","text":"<p>Configures the jobs by modifying the tree structure and creating the run files for each job.</p> <p>Parameters:</p> Name Type Description Default <code>force_configure</code> <code>bool</code> <p>Whether to force reconfiguration. Defaults to False.</p> <code>False</code> <code>dic_config_jobs</code> <code>Optional[dict[str, dict[str, Any]]]</code> <p>A dictionary containing the configuration of the jobs. Defaults to None.</p> <code>None</code> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>def configure_jobs(\n    self,\n    force_configure: bool = False,\n    dic_config_jobs: Optional[dict[str, dict[str, Any]]] = None,\n) -&gt; None:\n    \"\"\"\n    Configures the jobs by modifying the tree structure and creating the run files for each job.\n\n    Args:\n        force_configure (bool, optional): Whether to force reconfiguration. Defaults to False.\n        dic_config_jobs (Optional[dict[str, dict[str, Any]]], optional): A dictionary containing\n            the configuration of the jobs. Defaults to None.\n    \"\"\"\n    # Lock since we are modifying the tree\n    logging.info(\"Acquiring lock to configure jobs\")\n    with self.lock:\n        # Get the tree\n        dic_tree = self.dic_tree\n\n        # Ensure jobs have not been configured already\n        if (\"configured\" in dic_tree and dic_tree[\"configured\"]) and not force_configure:\n            logging.warning(\"Jobs have already been configured. Skipping.\")\n            return\n\n        # Configure the jobs (add generation and job keys, set status to \"To finish\")\n        dic_tree = ConfigJobs(dic_tree,starting_depth=-len(Path(self.path_tree).parts) + 2).find_and_configure_jobs(dic_config_jobs)\n\n        # Add the python environment, container image and absolute path of the study to the tree\n        dic_tree[\"python_environment\"] = self.path_python_environment\n        dic_tree[\"container_image\"] = self.path_container_image\n        dic_tree[\"absolute_path\"] = self.abs_path\n        dic_tree[\"status\"] = \"to_finish\"\n        dic_tree[\"configured\"] = True\n\n        # Explicitly set the dic_tree property to force rewrite\n        self.dic_tree = dic_tree\n\n    logging.info(\"Jobs have been configured. Lock released.\")\n</code></pre>"},{"location":"reference/study_da/submit/submit_scan.html#study_da.submit.submit_scan.SubmitScan.generate_run_files","title":"<code>generate_run_files(dic_tree, l_jobs, dic_additional_commands_per_gen, dic_dependencies_per_gen, dic_copy_back_per_gen, name_config)</code>","text":"<p>Generates run files for the specified jobs.</p> <p>Parameters:</p> Name Type Description Default <code>dic_tree</code> <code>dict</code> <p>The dictionary tree structure.</p> required <code>l_jobs</code> <code>list[str]</code> <p>List of jobs to submit.</p> required <code>dic_additional_commands_per_gen</code> <code>dict[int, str]</code> <p>Additional commands per generation. Defaults to {}.</p> required <code>dic_dependencies_per_gen</code> <code>dict[int, list[str]]</code> <p>Dependencies per generation. Only used when doing a HTC submission.</p> required <code>dic_copy_back_per_gen</code> <code>Optional[dict[int, dict[str, bool]]]</code> <p>A dictionary containing the files to copy back per generation. Accepted keys are \"parquet\", \"yaml\", \"txt\", \"json\", \"zip\" and \"all\".</p> required <code>name_config</code> <code>str</code> <p>The name of the configuration file for the study.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The updated dictionary tree structure.</p> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>def generate_run_files(\n    self,\n    dic_tree: dict[str, Any],\n    l_jobs: list[str],\n    dic_additional_commands_per_gen: dict[int, str],\n    dic_dependencies_per_gen: dict[int, list[str]],\n    dic_copy_back_per_gen: dict[int, dict[str, bool]],\n    name_config: str,\n) -&gt; dict:\n    \"\"\"\n    Generates run files for the specified jobs.\n\n    Args:\n        dic_tree (dict): The dictionary tree structure.\n        l_jobs (list[str]): List of jobs to submit.\n        dic_additional_commands_per_gen (dict[int, str], optional): Additional commands per\n            generation. Defaults to {}.\n        dic_dependencies_per_gen (dict[int, list[str]], optional): Dependencies per generation.\n            Only used when doing a HTC submission.\n        dic_copy_back_per_gen (Optional[dict[int, dict[str, bool]]], optional): A dictionary\n            containing the files to copy back per generation. Accepted keys are \"parquet\",\n            \"yaml\", \"txt\", \"json\", \"zip\" and \"all\".\n        name_config (str, optional): The name of the configuration file for the study.\n\n    Returns:\n        dict: The updated dictionary tree structure.\n    \"\"\"\n\n    logging.info(\"Generating run files for the jobs to submit\")\n    # Generate the run files for the jobs to submit\n    dic_all_jobs = self.get_all_jobs()\n    for job in l_jobs:\n        l_keys = dic_all_jobs[job][\"l_keys\"]\n        job_name = os.path.basename(job)\n        relative_job_folder = os.path.dirname(job)\n        absolute_job_folder = f\"{self.abs_path}/{relative_job_folder}\"\n        generation_number = dic_all_jobs[job][\"gen\"]\n        submission_type = nested_get(dic_tree, l_keys + [\"submission_type\"])\n        singularity = \"docker\" in submission_type\n        path_python_environment = (\n            self.path_python_environment_container\n            if singularity\n            else self.path_python_environment\n        )\n\n        # Ensure that the run file does not already exist\n        if \"path_run\" in nested_get(dic_tree, l_keys):\n            path_run_curr = nested_get(dic_tree, l_keys + [\"path_run\"])\n            if path_run_curr is not None and os.path.exists(path_run_curr):\n                logging.info(f\"Run file already exists for job {job}. Skipping.\")\n                continue\n\n        # Build l_dependencies and add to the kwargs\n        l_dependencies = dic_dependencies_per_gen.get(generation_number, [])\n\n        # Get arguments of current generation\n        dic_args = dic_copy_back_per_gen.get(generation_number, {})\n\n        # Mutate the keys\n        dic_args = {f\"copy_back_{key}\": value for key, value in dic_args.items()}\n\n        # Build kwargs for the run file\n        kwargs_htc = {\n            \"l_dependencies\": l_dependencies,\n            \"name_config\": name_config,\n        } | dic_args\n\n        run_str = generate_run_file(\n            absolute_job_folder,\n            job_name,\n            path_python_environment,\n            htc=\"htc\" in submission_type,\n            additionnal_command=dic_additional_commands_per_gen.get(generation_number, \"\"),\n            **kwargs_htc,\n        )\n        # Write the run file\n        path_run_job = f\"{absolute_job_folder}/run.sh\"\n        with open(path_run_job, \"w\") as f:\n            f.write(run_str)\n\n        # Change permissions to make the file executable\n        os.chmod(path_run_job, 0o755)\n\n        # Record the path to the run file in the tree\n        nested_set(dic_tree, l_keys + [\"path_run\"], path_run_job)\n\n    return dic_tree\n</code></pre>"},{"location":"reference/study_da/submit/submit_scan.html#study_da.submit.submit_scan.SubmitScan.get_all_jobs","title":"<code>get_all_jobs()</code>","text":"<p>Retrieves all jobs from the configuration, without modifying the tree.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing all jobs.</p> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>def get_all_jobs(self) -&gt; dict:\n    \"\"\"\n    Retrieves all jobs from the configuration, without modifying the tree.\n\n    Returns:\n        dict: A dictionary containing all jobs.\n    \"\"\"\n    # Get a copy of the tree as it's safer\n    with self.lock:\n        dic_tree = self.dic_tree\n    return ConfigJobs(dic_tree,starting_depth=-len(Path(self.path_tree).parts) + 2).find_all_jobs()\n</code></pre>"},{"location":"reference/study_da/submit/submit_scan.html#study_da.submit.submit_scan.SubmitScan.keep_submit_until_done","title":"<code>keep_submit_until_done(one_generation_at_a_time=False, wait_time=30, max_try=100, dic_additional_commands_per_gen=None, dic_dependencies_per_gen=None, dic_copy_back_per_gen=None, name_config='config.yaml', force_submit=False)</code>","text":"<p>Keeps submitting jobs until all jobs are finished or failed.</p> <p>The following arguments are only used for HTC jobs submission: - dic_additional_commands_per_gen - dic_dependencies_per_gen - dic_copy_back_per_gen - name_config</p> <p>Parameters:</p> Name Type Description Default <code>one_generation_at_a_time</code> <code>bool</code> <p>Whether to submit one full generation at a time. Defaults to False.</p> <code>False</code> <code>wait_time</code> <code>float</code> <p>The wait time between submissions in minutes. Defaults to 30.</p> <code>30</code> <code>max_try</code> <code>int</code> <p>The maximum number of tries before stopping the submission.</p> <code>100</code> <code>dic_additional_commands_per_gen</code> <code>dict[int, str]</code> <p>Additional commands per generation. Defaults to None.</p> <code>None</code> <code>dic_dependencies_per_gen</code> <code>dict[int, list[str]]</code> <p>Dependencies per generation. Only used when doing a HTC submission. Defaults to None.</p> <code>None</code> <code>dic_copy_back_per_gen</code> <code>Optional[dict[int, dict[str, bool]]]</code> <p>A dictionary containing the files to copy back per generation. Accepted keys are \"parquet\", \"yaml\", \"txt\", \"json\", \"zip\" and \"all\". Defaults to None, corresponding to copying back only \"light\" files, i.e. parquet, yaml and txt.</p> <code>None</code> <code>name_config</code> <code>str</code> <p>The name of the configuration file for the study. Defaults to \"config.yaml\".</p> <code>'config.yaml'</code> <code>force_submit</code> <code>bool</code> <p>If True, jobs are resubmitted even though they failed. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>def keep_submit_until_done(\n    self,\n    one_generation_at_a_time: bool = False,\n    wait_time: float = 30,\n    max_try=100,\n    dic_additional_commands_per_gen: Optional[dict[int, str]] = None,\n    dic_dependencies_per_gen: Optional[dict[int, list[str]]] = None,\n    dic_copy_back_per_gen: Optional[dict[int, dict[str, bool]]] = None,\n    name_config: str = \"config.yaml\",\n    force_submit: bool = False,\n) -&gt; None:\n    \"\"\"\n    Keeps submitting jobs until all jobs are finished or failed.\n\n    The following arguments are only used for HTC jobs submission:\n    - dic_additional_commands_per_gen\n    - dic_dependencies_per_gen\n    - dic_copy_back_per_gen\n    - name_config\n\n    Args:\n        one_generation_at_a_time (bool, optional): Whether to submit one full generation at a\n            time. Defaults to False.\n        wait_time (float, optional): The wait time between submissions in minutes.\n            Defaults to 30.\n        max_try (int, optional): The maximum number of tries before stopping the submission.\n        dic_additional_commands_per_gen (dict[int, str], optional): Additional commands per\n            generation. Defaults to None.\n        dic_dependencies_per_gen (dict[int, list[str]], optional): Dependencies per generation.\n            Only used when doing a HTC submission. Defaults to None.\n        dic_copy_back_per_gen (Optional[dict[int, dict[str, bool]]], optional): A dictionary\n            containing the files to copy back per generation. Accepted keys are \"parquet\",\n            \"yaml\", \"txt\", \"json\", \"zip\" and \"all\". Defaults to None, corresponding to copying\n            back only \"light\" files, i.e. parquet, yaml and txt.\n        name_config (str, optional): The name of the configuration file for the study.\n            Defaults to \"config.yaml\".\n        force_submit (bool, optional): If True, jobs are resubmitted even though they failed.\n            Defaults to False.\n\n\n    Returns:\n        None\n    \"\"\"\n    # Handle mutable default arguments\n    if dic_additional_commands_per_gen is None:\n        dic_additional_commands_per_gen = {}\n    if dic_dependencies_per_gen is None:\n        dic_dependencies_per_gen = {}\n\n    if wait_time &lt; 1 / 20:\n        logging.warning(\"Wait time should be at least 10 seconds to prevent locking errors.\")\n        logging.warning(\"Setting wait time to 10 seconds.\")\n        wait_time = 10 / 60\n\n    # I don't need to lock the tree here since the status cheking is read only and\n    # the lock is acquired in the submit method for the submission\n    while (\n        self.submit(\n            one_generation_at_a_time,\n            dic_additional_commands_per_gen,\n            dic_dependencies_per_gen,\n            dic_copy_back_per_gen,\n            name_config,\n            force_submit=force_submit,\n        )\n        not in [\"finished\", \"finished with issues\"]\n        and max_try &gt; 0\n    ):\n        # Wait for a certain amount of time before checking again\n        logging.info(f\"Waiting {wait_time} minutes before checking again.\")\n        time.sleep(wait_time * 60)\n        max_try -= 1\n\n    if max_try == 0:\n        print(\"Maximum number of tries reached. Stopping submission.\")\n</code></pre>"},{"location":"reference/study_da/submit/submit_scan.html#study_da.submit.submit_scan.SubmitScan.log_jobs_state","title":"<code>log_jobs_state(dic_summary_by_gen)</code>  <code>staticmethod</code>","text":"<p>Logs the state of jobs for each generation.</p> <p>Parameters:</p> Name Type Description Default <code>dic_summary_by_gen</code> <code>dict</code> <p>A dictionary where the keys are generation numbers and the values are dictionaries summarizing job states. Each summary dictionary should contain the following keys: - 'to_submit_later': int, number of jobs left to submit later - 'running_or_queuing': int, number of jobs running or queuing - 'submitted_now': int, number of jobs submitted now - 'finished': int, number of jobs finished - 'failed': int, number of jobs failed - 'dependency_failed': int, number of jobs on hold due to failed dependencies</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>@staticmethod\ndef log_jobs_state(dic_summary_by_gen: dict[int, dict[str, int]]) -&gt; None:\n    \"\"\"\n    Logs the state of jobs for each generation.\n\n    Args:\n        dic_summary_by_gen (dict): A dictionary where the keys are generation numbers\n            and the values are dictionaries summarizing job states.\n            Each summary dictionary should contain the following keys:\n            - 'to_submit_later': int, number of jobs left to submit later\n            - 'running_or_queuing': int, number of jobs running or queuing\n            - 'submitted_now': int, number of jobs submitted now\n            - 'finished': int, number of jobs finished\n            - 'failed': int, number of jobs failed\n            - 'dependency_failed': int, number of jobs on hold due to failed dependencies\n\n    Returns:\n        None\n    \"\"\"\n    print(\"State of the jobs:\")\n    for gen, dic_summary in dic_summary_by_gen.items():\n        print(\"********************************\")\n        print(f\"Generation {gen}\")\n        print(f\"Jobs left to submit later: {dic_summary['to_submit_later']}\")\n        print(f\"Jobs running or queuing: {dic_summary['running_or_queuing']}\")\n        print(f\"Jobs submitted now: {dic_summary['submitted_now']}\")\n        print(f\"Jobs finished: {dic_summary['finished']}\")\n        print(f\"Jobs failed: {dic_summary['failed']}\")\n        print(f\"Jobs on hold due to failed dependencies: {dic_summary['dependency_failed']}\")\n        print(\"********************************\")\n</code></pre>"},{"location":"reference/study_da/submit/submit_scan.html#study_da.submit.submit_scan.SubmitScan.reset_failed_jobs","title":"<code>reset_failed_jobs(dic_tree)</code>","text":"<p>Resets the status of jobs that have failed to \"to_submit\".</p> <p>Parameters:</p> Name Type Description Default <code>dic_tree</code> <code>dict[str, Any]</code> <p>The dictionary tree structure.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: The updated dictionary tree structure.</p> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>def reset_failed_jobs(self, dic_tree: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"\n    Resets the status of jobs that have failed to \"to_submit\".\n\n    Args:\n        dic_tree (dict[str, Any]): The dictionary tree structure.\n\n    Returns:\n        dict[str, Any]: The updated dictionary tree structure.\n    \"\"\"\n\n    dic_all_jobs = self.get_all_jobs()\n    # First pass to update the state of the tree\n    for job in dic_all_jobs:\n        # Skip jobs that are not failed\n        if nested_get(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"]) != \"failed\":\n            continue\n\n        # Reset the state of the others\n        relative_job_folder = os.path.dirname(job)\n        absolute_job_folder = f\"{self.abs_path}/{relative_job_folder}\"\n\n        # Remove failed tag\n        if os.path.exists(f\"{absolute_job_folder}/.failed\"):\n            os.remove(f\"{absolute_job_folder}/.failed\")\n        else:\n            logging.warning(f\"Failed file not found for job {job}.\")\n\n        # Remove run file\n        if \"path_run\" in nested_get(dic_tree, dic_all_jobs[job][\"l_keys\"]):\n            path_run_curr = nested_get(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"path_run\"])\n            if path_run_curr is not None and os.path.exists(path_run_curr):\n                os.remove(path_run_curr)\n            else:\n                logging.warning(f\"Run file not found for job {job}.\")\n\n        # Reset the status of the job\n        nested_set(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"], \"to_submit\")\n\n    return dic_tree\n</code></pre>"},{"location":"reference/study_da/submit/submit_scan.html#study_da.submit.submit_scan.SubmitScan.submit","title":"<code>submit(one_generation_at_a_time=False, dic_additional_commands_per_gen=None, dic_dependencies_per_gen=None, dic_copy_back_per_gen=None, name_config='config.yaml', force_submit=False)</code>","text":"<p>Submits the jobs to the cluster. Note that copying back large files (e.g. json colliders) can trigger a throttling mechanism in AFS.</p> <p>The following arguments are only used for HTC jobs submission: - dic_additional_commands_per_gen - dic_dependencies_per_gen - dic_copy_back_per_gen - name_config</p> <p>Parameters:</p> Name Type Description Default <code>one_generation_at_a_time</code> <code>bool</code> <p>Whether to submit one full generation at a time. Defaults to False.</p> <code>False</code> <code>dic_additional_commands_per_gen</code> <code>dict[int, str]</code> <p>Additional commands per generation. Defaults to None.</p> <code>None</code> <code>dic_dependencies_per_gen</code> <code>dict[int, list[str]]</code> <p>Dependencies per generation. Only used when doing a HTC submission. Defaults to None.</p> <code>None</code> <code>dic_copy_back_per_gen</code> <code>Optional[dict[int, dict[str, bool]]]</code> <p>A dictionary containing the files to copy back per generation. Accepted keys are \"parquet\", \"yaml\", \"txt\", \"json\", \"zip\" and \"all\". Defaults to None, corresponding to copying back only \"light\" files, i.e. parquet, yaml and txt.</p> <code>None</code> <code>name_config</code> <code>str</code> <p>The name of the configuration file for the study. Defaults to \"config.yaml\".</p> <code>'config.yaml'</code> <code>force_submit</code> <code>bool</code> <p>If True, jobs are resubmitted even though they failed. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The final status of the jobs.</p> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>def submit(\n    self,\n    one_generation_at_a_time: bool = False,\n    dic_additional_commands_per_gen: Optional[dict[int, str]] = None,\n    dic_dependencies_per_gen: Optional[dict[int, list[str]]] = None,\n    dic_copy_back_per_gen: Optional[dict[int, dict[str, bool]]] = None,\n    name_config: str = \"config.yaml\",\n    force_submit: bool = False,\n) -&gt; str:\n    \"\"\"\n    Submits the jobs to the cluster. Note that copying back large files (e.g. json colliders)\n    can trigger a throttling mechanism in AFS.\n\n    The following arguments are only used for HTC jobs submission:\n    - dic_additional_commands_per_gen\n    - dic_dependencies_per_gen\n    - dic_copy_back_per_gen\n    - name_config\n\n    Args:\n        one_generation_at_a_time (bool, optional): Whether to submit one full generation at a\n            time. Defaults to False.\n        dic_additional_commands_per_gen (dict[int, str], optional): Additional commands per\n            generation. Defaults to None.\n        dic_dependencies_per_gen (dict[int, list[str]], optional): Dependencies per generation.\n            Only used when doing a HTC submission. Defaults to None.\n        dic_copy_back_per_gen (Optional[dict[int, dict[str, bool]]], optional): A dictionary\n            containing the files to copy back per generation. Accepted keys are \"parquet\",\n            \"yaml\", \"txt\", \"json\", \"zip\" and \"all\". Defaults to None, corresponding to copying\n            back only \"light\" files, i.e. parquet, yaml and txt.\n        name_config (str, optional): The name of the configuration file for the study.\n            Defaults to \"config.yaml\".\n        force_submit (bool, optional): If True, jobs are resubmitted even though they failed.\n            Defaults to False.\n\n    Returns:\n        str: The final status of the jobs.\n    \"\"\"\n    # Handle mutable default arguments\n    if dic_additional_commands_per_gen is None:\n        dic_additional_commands_per_gen = {}\n    if dic_dependencies_per_gen is None:\n        dic_dependencies_per_gen = {}\n    if dic_copy_back_per_gen is None:\n        dic_copy_back_per_gen = {}\n\n    # Handle force submit\n    if force_submit:\n        logging.warning(\"Forcing resubmission of all failed jobs.\")\n        with self.lock:\n            # Acquire tree from disk\n            dic_tree = self.dic_tree\n\n            # Reset the tree by deleting the failed tags\n            dic_tree = self.reset_failed_jobs(dic_tree)\n            dic_tree[\"status\"] = \"to_finish\"\n            # Write the tree back to disk\n            self.dic_tree = dic_tree\n\n    # Update the status of all jobs before submitting\n    dic_all_jobs, final_status = self.check_and_update_all_jobs_status()\n    if final_status == \"finished\":\n        print(\"All jobs are finished.\")\n        return final_status\n    elif final_status == \"finished with issues\":\n        print(\"All jobs are finished but some did not run properly.\")\n        return final_status\n\n    logging.info(\"Acquiring lock to submit jobs\")\n    with self.lock:\n        # Get dic tree once to avoid reloading it for every job\n        dic_tree = self.dic_tree\n\n        # Submit the jobs\n        self._submit(\n            dic_tree,\n            dic_all_jobs,\n            one_generation_at_a_time,\n            dic_additional_commands_per_gen,\n            dic_dependencies_per_gen,\n            dic_copy_back_per_gen,\n            name_config,\n        )\n\n        # Update dic_tree from cluster_submission\n        self.dic_tree = dic_tree\n    logging.info(\"Jobs have been submitted. Lock released.\")\n    return final_status\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/index.html","title":"cluster_submission","text":""},{"location":"reference/study_da/submit/cluster_submission/index.html#study_da.submit.cluster_submission.ClusterSubmission","title":"<code>ClusterSubmission</code>","text":"<p>A class to handle the submission of jobs to various cluster systems such as local PC, HTC, and Slurm.</p> <p>Attributes:</p> Name Type Description <code>study_name</code> <code>str</code> <p>The name of the study.</p> <code>l_jobs_to_submit</code> <code>list[str]</code> <p>A list of jobs to submit.</p> <code>dic_all_jobs</code> <code>dict</code> <p>A dictionary containing all jobs.</p> <code>dic_tree</code> <code>dict</code> <p>A dictionary representing the job tree.</p> <code>path_submission_file</code> <code>str</code> <p>The path to the submission file.</p> <code>abs_path_study</code> <code>str</code> <p>The absolute path to the study.</p> <code>dic_submission</code> <code>dict</code> <p>A dictionary mapping submission types to their corresponding classes.</p> <p>Methods:</p> Name Description <code>dic_id_to_path_job</code> <p>Getter for the dictionary mapping job IDs to their paths.</p> <code>dic_id_to_path_job</code> <p>dict[int, str]): Setter for the dictionary mapping job IDs to their paths.</p> <code>_update_dic_id_to_path_job</code> <p>list[str], queuing_jobs: list[str]) -&gt; None: Updates the dictionary mapping job IDs to their paths based on the current running and queuing jobs.</p> <code>_check_submission_type</code> <p>Checks the submission type for the jobs and ensures that HTC and Slurm submissions are not mixed.</p> <code>_get_state_jobs</code> <p>bool = True) -&gt; tuple[list[str], list[str]]: Gets the current state of the jobs (running and queuing).</p> <code>_test_job</code> <p>str, path_job: str, running_jobs: list[str], queuing_jobs: list[str]) -&gt; bool: Tests if a job is completed, running, or queuing.</p> <code>_return_htc_flavour</code> <p>str) -&gt; str: Returns the HTC flavor for a given job.</p> <code>_return_abs_path_job</code> <p>str) -&gt; tuple[str, str]: Returns the absolute path of a job.</p> <code>_write_sub_files_slurm_docker</code> <p>str, running_jobs: list[str], queuing_jobs: list[str], list_of_jobs: list[str]) -&gt; tuple[list[str], list[str]]: Writes submission files for Slurm Docker jobs.</p> <code>_get_Sub</code> <p>str, submission_type: str, sub_filename: str, abs_path_job: str, gpu: bool) -&gt; LocalPC | HTC | HTCDocker | Slurm | SlurmDocker: Returns the appropriate submission object based on the submission type.</p> <code>_write_sub_file</code> <p>str, running_jobs: list[str], queuing_jobs: list[str], list_of_jobs: list[str], submission_type: str) -&gt; tuple[list[str], list[str]]: Writes a submission file for the given jobs.</p> <code>_write_sub_files</code> <p>str, running_jobs: list[str], queuing_jobs: list[str], list_of_jobs: list[str], submission_type: str) -&gt; tuple[list[str], list[str]]: Writes submission files for the given jobs based on the submission type.</p> <code>write_sub_files</code> <p>Writes submission files for all jobs to be submitted and returns a dictionary of submission files.</p> <code>_update_job_status_from_hpc_output</code> <p>str, submission_type: str, dic_id_to_path_job_temp: dict, list_of_jobs: list[str], idx_submission: int = 0)     -&gt; tuple[dict, int]: Updates the job status from the HPC output.</p> <code>submit</code> <p>list[str], l_submission_filenames: list[str], submission_type: str) -&gt; None: Submits the jobs to the appropriate cluster system.</p> <code>_get_local_jobs</code> <p>Gets the list of local jobs.</p> <code>_get_condor_jobs</code> <p>str, force_query_individually: bool = False) -&gt; list[str]: Gets the list of Condor jobs based on the status.</p> <code>_get_slurm_jobs</code> <p>str, force_query_individually: bool = False) -&gt; list[str]: Gets the list of Slurm jobs based on the status.</p> <code>querying_jobs</code> <p>bool, check_htc: bool, check_slurm: bool, status: str = \"running\") -&gt; list[str]: Queries the jobs based on the submission type and status.</p> Source code in <code>study_da/submit/cluster_submission/cluster_submission.py</code> <pre><code>class ClusterSubmission:\n    \"\"\"\n    A class to handle the submission of jobs to various cluster systems such as local PC, HTC, and\n    Slurm.\n\n    Attributes:\n        study_name (str): The name of the study.\n        l_jobs_to_submit (list[str]): A list of jobs to submit.\n        dic_all_jobs (dict): A dictionary containing all jobs.\n        dic_tree (dict): A dictionary representing the job tree.\n        path_submission_file (str): The path to the submission file.\n        abs_path_study (str): The absolute path to the study.\n        dic_submission (dict): A dictionary mapping submission types to their corresponding classes.\n\n    Methods:\n        dic_id_to_path_job() -&gt; dict | None:\n            Getter for the dictionary mapping job IDs to their paths.\n        dic_id_to_path_job(dic_id_to_path_job: dict[int, str]):\n            Setter for the dictionary mapping job IDs to their paths.\n        _update_dic_id_to_path_job(running_jobs: list[str], queuing_jobs: list[str]) -&gt; None:\n            Updates the dictionary mapping job IDs to their paths based on the current running and\n            queuing jobs.\n        _check_submission_type() -&gt; tuple[bool, bool, bool]:\n            Checks the submission type for the jobs and ensures that HTC and Slurm submissions are\n            not mixed.\n        _get_state_jobs(verbose: bool = True) -&gt; tuple[list[str], list[str]]:\n            Gets the current state of the jobs (running and queuing).\n        _test_job(job: str, path_job: str, running_jobs: list[str], queuing_jobs: list[str]) -&gt; bool:\n            Tests if a job is completed, running, or queuing.\n        _return_htc_flavour(job: str) -&gt; str:\n            Returns the HTC flavor for a given job.\n        _return_abs_path_job(job: str) -&gt; tuple[str, str]:\n            Returns the absolute path of a job.\n        _write_sub_files_slurm_docker(sub_filename: str, running_jobs: list[str],\n            queuing_jobs: list[str], list_of_jobs: list[str]) -&gt; tuple[list[str], list[str]]:\n            Writes submission files for Slurm Docker jobs.\n        _get_Sub(job: str, submission_type: str, sub_filename: str, abs_path_job: str,\n            gpu: bool) -&gt; LocalPC | HTC | HTCDocker | Slurm | SlurmDocker:\n            Returns the appropriate submission object based on the submission type.\n        _write_sub_file(sub_filename: str, running_jobs: list[str], queuing_jobs: list[str],\n            list_of_jobs: list[str], submission_type: str) -&gt; tuple[list[str], list[str]]:\n            Writes a submission file for the given jobs.\n        _write_sub_files(sub_filename: str, running_jobs: list[str], queuing_jobs: list[str],\n            list_of_jobs: list[str], submission_type: str) -&gt; tuple[list[str], list[str]]:\n            Writes submission files for the given jobs based on the submission type.\n        write_sub_files() -&gt; dict:\n            Writes submission files for all jobs to be submitted and returns a dictionary of\n            submission files.\n        _update_job_status_from_hpc_output(submit_command: str, submission_type: str,\n            dic_id_to_path_job_temp: dict, list_of_jobs: list[str], idx_submission: int = 0)\n                -&gt; tuple[dict, int]:\n            Updates the job status from the HPC output.\n        submit(list_of_jobs: list[str], l_submission_filenames: list[str], submission_type: str)\n            -&gt; None:\n            Submits the jobs to the appropriate cluster system.\n        _get_local_jobs() -&gt; list[str]:\n            Gets the list of local jobs.\n        _get_condor_jobs(status: str, force_query_individually: bool = False) -&gt; list[str]:\n            Gets the list of Condor jobs based on the status.\n        _get_slurm_jobs(status: str, force_query_individually: bool = False) -&gt; list[str]:\n            Gets the list of Slurm jobs based on the status.\n        querying_jobs(check_local: bool, check_htc: bool, check_slurm: bool,\n            status: str = \"running\") -&gt; list[str]:\n            Queries the jobs based on the submission type and status.\n    \"\"\"\n\n    def __init__(\n        self,\n        study_name: str,\n        l_jobs_to_submit: list[str],\n        dic_all_jobs: dict,\n        dic_tree: dict,\n        path_submission_file: str,\n        abs_path_study: str,\n    ):\n        self.study_name: str = study_name\n        self.l_jobs_to_submit: list[str] = l_jobs_to_submit\n        self.dic_all_jobs: dict = dic_all_jobs\n        self.dic_tree: dict = dic_tree\n        self.path_submission_file: str = path_submission_file\n        self.abs_path_study: str = abs_path_study\n        self.dic_submission: dict = {\n            \"local\": LocalPC,\n            \"htc\": HTC,\n            \"htc_docker\": HTCDocker,\n            \"slurm\": Slurm,\n            \"slurm_docker\": SlurmDocker,\n        }\n        \"\"\"\n        Initialize the ClusterSubmission class.\n\n        Args:\n            study_name (str): The name of the study.\n            l_jobs_to_submit (list[str]): A list of job names to submit.\n            dic_all_jobs (dict): A dictionary containing all jobs.\n            dic_tree (dict): A dictionary representing the job tree structure.\n            path_submission_file (str): The path to the submission file.\n            abs_path_study (str): The absolute path to the study.\n        \"\"\"\n\n    # Getter for dic_id_to_path_job\n    @property\n    def dic_id_to_path_job(self) -&gt; dict | None:\n        \"\"\"\n        Generates a dictionary mapping job IDs to their respective job paths.\n\n        This method iterates over the list of jobs to submit and constructs a dictionary\n        where the keys are job IDs and the values are the absolute paths to the jobs.\n        If no job IDs are found, the method returns None.\n\n        Returns:\n            dict | None: A dictionary mapping job IDs to job paths, or None if no job IDs are found.\n        \"\"\"\n        dic_id_to_path_job = {}\n        found_at_least_one = False\n        for job in self.l_jobs_to_submit:\n            l_keys = self.dic_all_jobs[job][\"l_keys\"]\n            subdic_job = nested_get(self.dic_tree, l_keys)\n            if \"id_sub\" in subdic_job:\n                dic_id_to_path_job[subdic_job[\"id_sub\"]] = self._return_abs_path_job(job)[0]\n                found_at_least_one = True\n\n        return dic_id_to_path_job if found_at_least_one else None\n\n    # Setter for dic_id_to_path_job\n    @dic_id_to_path_job.setter\n    def dic_id_to_path_job(self, dic_id_to_path_job: dict[int, str]):\n        \"\"\"\n        Updates the internal job submission tree with job IDs and their corresponding paths.\n\n        Args:\n            dic_id_to_path_job (dict[int, str]): A dictionary mapping job IDs (integers) to their\n            respective paths (strings).\n\n        Raises:\n            AssertionError: If dic_id_to_path_job is not a dictionary.\n\n        Notes:\n            - Ensures all job IDs are integers.\n            - Updates the internal job submission tree by adding or removing job IDs based on the\n                provided dictionary.\n            - If a job's path is found in the dictionary, its ID is updated in the tree.\n            - If a job's ID is not found in the dictionary, it is removed from the tree.\n        \"\"\"\n        assert isinstance(dic_id_to_path_job, dict)\n        # Ensure all ids are integers\n        dic_id_to_path_job = {\n            int(id_job): path_job for id_job, path_job in dic_id_to_path_job.items()\n        }\n        dic_job_to_id = {path_job: int(id_job) for id_job, path_job in dic_id_to_path_job.items()}\n\n        # Update the tree\n        for job in self.l_jobs_to_submit:\n            path_job = self._return_abs_path_job(job)[0]\n            l_keys = self.dic_all_jobs[job][\"l_keys\"]\n            subdic_job = nested_get(self.dic_tree, l_keys)\n            if \"id_sub\" in subdic_job and int(subdic_job[\"id_sub\"]) not in dic_id_to_path_job:\n                del subdic_job[\"id_sub\"]\n            elif \"id_sub\" not in subdic_job and path_job in dic_job_to_id:\n                subdic_job[\"id_sub\"] = dic_job_to_id[path_job]\n            # Else all is consistent\n\n    def _update_dic_id_to_path_job(self, running_jobs: list[str], queuing_jobs: list[str]) -&gt; None:\n        \"\"\"\n        Updates the dictionary `dic_id_to_path_job` by removing jobs that are no longer running or\n        queuing.\n\n        Args:\n            running_jobs (list[str]): A list of job identifiers that are currently running.\n            queuing_jobs (list[str]): A list of job identifiers that are currently queuing.\n\n        Returns:\n            None\n        \"\"\"\n        # Look for jobs in the dictionnary that are not running or queuing anymore\n        set_current_jobs = set(running_jobs + queuing_jobs)\n        if self.dic_id_to_path_job is not None:\n            dic_id_to_path_job = self.dic_id_to_path_job\n            for id_job, job in self.dic_id_to_path_job.items():\n                if job not in set_current_jobs:\n                    del dic_id_to_path_job[id_job]\n\n            # Update dic_id_to_path_job\n            self.dic_id_to_path_job = dic_id_to_path_job\n\n    def _check_submission_type(self) -&gt; tuple[bool, bool, bool]:\n        \"\"\"\n        Checks the types of job submissions in the current batch and ensures that\n        there is no mixing of HTC and Slurm submission types.\n\n        Args:\n            None\n\n        Returns:\n            tuple[bool, bool, bool]: A tuple containing three boolean values indicating\n            whether there are local, HTC, and Slurm submissions respectively.\n\n        Raises:\n            ValueError: If both HTC and Slurm submission types are found in the jobs to submit.\n        \"\"\"\n        check_local = False\n        check_htc = False\n        check_slurm = False\n        for job in self.l_jobs_to_submit:\n            l_keys = self.dic_all_jobs[job][\"l_keys\"]\n            submission_type = nested_get(self.dic_tree, l_keys + [\"submission_type\"])\n            if submission_type == \"local\":\n                check_local = True\n            elif submission_type in [\"htc\", \"htc_docker\"]:\n                check_htc = True\n            elif submission_type in [\"slurm\", \"slurm_docker\"]:\n                check_slurm = True\n\n        if check_htc and check_slurm:\n            raise ValueError(\"Error: Mixing htc and slurm submission is not allowed\")\n\n        return check_local, check_htc, check_slurm\n\n    def _get_state_jobs(self, verbose: bool = True) -&gt; tuple[list[str], list[str]]:\n        \"\"\"\n        Retrieves the state of jobs (running and queuing) based on the submission type.\n\n        This method first determines the submission type (local, HTC, or Slurm) and then queries\n        the jobs accordingly. It updates the internal dictionary mapping job IDs to their paths\n        and optionally prints the running and queuing jobs.\n\n        Args:\n            verbose (bool): If True, prints the running and queuing jobs. Default is True.\n\n        Returns:\n            tuple[list[str], list[str]]: A tuple containing two lists:\n                - The first list contains the IDs of running jobs.\n                - The second list contains the IDs of queuing jobs.\n        \"\"\"\n        # First check whether the jobs are submitted on local, htc or slurm\n        check_local, check_htc, check_slurm = self._check_submission_type()\n\n        # Then query accordingly\n        running_jobs = self.querying_jobs(check_local, check_htc, check_slurm, status=\"running\")\n        queuing_jobs = self.querying_jobs(check_local, check_htc, check_slurm, status=\"queuing\")\n        self._update_dic_id_to_path_job(running_jobs, queuing_jobs)\n        if verbose:\n            logging.info(\"Running: \\n\" + \"\\n\".join(running_jobs))\n            logging.info(\"queuing: \\n\" + \"\\n\".join(queuing_jobs))\n        return running_jobs, queuing_jobs\n\n    def _test_job(\n        self, job: str, path_job: str, running_jobs: list[str], queuing_jobs: list[str]\n    ) -&gt; bool:\n        \"\"\"\n        Tests the status of a job and determines if it needs to be (re)submitted.\n\n        Args:\n            job (str): The job identifier.\n            path_job (str): The path to the job.\n            running_jobs (list[str]): A list of currently running jobs.\n            queuing_jobs (list[str]): A list of currently queuing jobs.\n\n        Returns:\n            bool: True if the job must be (re)submitted, False otherwise.\n        \"\"\"\n        # Test if job is completed\n        l_keys = self.dic_all_jobs[job][\"l_keys\"]\n        completed = nested_get(self.dic_tree, l_keys + [\"status\"]) == \"finished\"\n        failed = nested_get(self.dic_tree, l_keys + [\"status\"]) == \"failed\"\n        if completed:\n            logging.info(f\"{path_job} is already completed.\")\n\n        # Test if job has failed\n        if failed:\n            logging.info(f\"{path_job} has failed.\")\n\n        # Test if job is running\n        elif path_job in running_jobs:\n            logging.info(f\"{path_job} is already running.\")\n\n        # Test if job is queuing\n        elif path_job in queuing_jobs:\n            logging.info(f\"{path_job} is already queuing.\")\n\n        # True if job must be (re)submitted\n        else:\n            # ? If some jobs finish in the second-long step between the tree update and the\n            # ? submission, they will be considered as needed to be submitted...\n            # ? Should happend rarely but still a problem... And no easy way to fix thiss\n            return True\n        return False\n\n    def _return_htc_flavour(self, job: str) -&gt; str:\n        \"\"\"\n        Retrieve the HTC flavor for a given job.\n\n        Args:\n            job (str): The job identifier.\n\n        Returns:\n            str: The HTC flavor associated with the job.\n        \"\"\"\n        l_keys = self.dic_all_jobs[job][\"l_keys\"]\n        return nested_get(self.dic_tree, l_keys + [\"htc_flavor\"])\n\n    def _return_abs_path_job(self, job: str) -&gt; tuple[str, str]:\n        \"\"\"\n        Generate the relative and absolute paths for a given job.\n\n        Args:\n            job (str): The job string containing the path to the job file.\n\n        Returns:\n            tuple[str, str]: A tuple containing:\n            - path_job (str): The relative path to the job directory.\n            - abs_path_job (str): The absolute path to the job directory.\n        \"\"\"\n        # Get corresponding path job (remove the python file name)\n        path_job = \"/\".join(job.split(\"/\")[:-1]) + \"/\"\n        abs_path_job = f\"{self.abs_path_study}/{path_job}\"\n        return path_job, abs_path_job\n\n    def _write_sub_files_slurm_docker(\n        self,\n        sub_filename: str,\n        running_jobs: list[str],\n        queuing_jobs: list[str],\n        list_of_jobs: list[str],\n    ) -&gt; tuple[list[str], list[str]]:\n        \"\"\"\n        Generates SLURM submission files for Docker jobs and writes them to disk.\n\n        This method iterates over a list of jobs, checks their status (running, queuing, or\n        completed), and writes the corresponding SLURM submission files for Docker jobs. The\n        submission files are written to disk with a specific naming convention.\n\n        Args:\n            sub_filename (str): The base name for the submission files.\n            running_jobs (list[str]): A list of job identifiers that are currently running.\n            queuing_jobs (list[str]): A list of job identifiers that are currently queuing.\n            list_of_jobs (list[str]): A list of job identifiers to process.\n\n        Returns:\n            tuple[list[str], list[str]]: A tuple containing two lists:\n            - A list of filenames for the generated submission files.\n            - A list of job identifiers that were updated.\n        \"\"\"\n        l_filenames = []\n        list_of_jobs_updated = []\n        for idx_job, job in enumerate(list_of_jobs):\n            path_job, abs_path_job = self._return_abs_path_job(job)\n\n            # Test if job is running, queuing or completed\n            if self._test_job(job, path_job, running_jobs, queuing_jobs):\n                filename_sub = f\"{sub_filename.split('.sub')[0]}_{idx_job}.sub\"\n\n                # Get job GPU request\n                l_keys = self.dic_all_jobs[job][\"l_keys\"]\n                # Ensure GPU is defined and set it to False if not\n                if \"request_gpu\" not in nested_get(self.dic_tree, l_keys):\n                    gpu = nested_set(self.dic_tree, l_keys + [\"request_gpu\"], False)\n                gpu = nested_get(self.dic_tree, l_keys + [\"request_gpu\"])\n\n                # Write the submission files\n                # ! Careful, I implemented a fix for path due to the temporary home recovery folder\n                logging.info(f'Writing submission file for node \"{abs_path_job}\"')\n                fix = True\n                Sub = self.dic_submission[\"slurm_docker\"](\n                    filename_sub, abs_path_job, gpu, self.dic_tree[\"container_image\"], fix=fix\n                )\n                # Create folder if it does not exist\n                folder = \"/\".join(Sub.sub_filename.split(\"/\")[:-1])\n                Path(folder).mkdir(parents=True, exist_ok=True)\n                with open(Sub.sub_filename, \"w\") as fid:\n                    fid.write(Sub.head + \"\\n\")\n                    fid.write(Sub.body + \"\\n\")\n                    fid.write(Sub.tail + \"\\n\")\n\n                l_filenames.append(Sub.sub_filename)\n                list_of_jobs_updated.append(job)\n        return l_filenames, list_of_jobs_updated\n\n    def _get_Sub(\n        self, job: str, submission_type: str, sub_filename: str, abs_path_job: str, gpu: bool\n    ) -&gt; LocalPC | HTC | HTCDocker | Slurm | SlurmDocker:\n        \"\"\"\n        Generate a submission object based on the specified submission type.\n\n        Args:\n            job (str): The job identifier.\n            submission_type (str): The type of submission (e.g., \"slurm\", \"htc\", \"htc_docker\",\n                \"slurm_docker\", \"local\").\n            sub_filename (str): The submission filename.\n            abs_path_job (str): The absolute path to the job.\n            gpu (bool): Sets if a GPU must be requested for the submission.\n\n        Returns:\n            LocalPC | HTC | HTCDocker | Slurm | SlurmDocker: An instance of the appropriate\n                submission class.\n\n        Raises:\n            ValueError: If the submission type is not valid or if the container_image is not defined\n                in the tree for docker submissions.\n        \"\"\"\n        match submission_type:\n            case \"slurm\":\n                return self.dic_submission[submission_type](sub_filename, abs_path_job, gpu)\n            case \"htc\":\n                return self.dic_submission[submission_type](\n                    sub_filename, abs_path_job, gpu, self._return_htc_flavour(job)\n                )\n            case w if w in [\"htc_docker\", \"slurm_docker\"]:\n                # Path to singularity image\n                if (\n                    \"container_image\" in self.dic_tree\n                    and self.dic_tree[\"container_image\"] is not None\n                ):\n                    self.path_image = self.dic_tree[\"container_image\"]\n                else:\n                    raise ValueError(\n                        \"Error: container_image is not defined in the tree. Please define it in the\"\n                        \" config.yaml file.\"\n                    )\n\n                if submission_type == \"htc_docker\":\n                    return self.dic_submission[submission_type](\n                        sub_filename,\n                        abs_path_job,\n                        gpu,\n                        self.path_image,\n                        self._return_htc_flavour(job),\n                    )\n                else:\n                    return self.dic_submission[submission_type](\n                        sub_filename, abs_path_job, gpu, self.path_image\n                    )\n            case \"local\":\n                return self.dic_submission[submission_type](sub_filename, abs_path_job)\n            case _:\n                raise ValueError(f\"Error: {submission_type} is not a valid submission mode\")\n\n    def _write_sub_file(\n        self,\n        sub_filename: str,\n        running_jobs: list[str],\n        queuing_jobs: list[str],\n        list_of_jobs: list[str],\n        submission_type: str,\n    ) -&gt; tuple[list[str], list[str]]:\n        \"\"\"\n        Writes a submission file for a list of jobs and returns the updated list of jobs.\n\n        Args:\n            sub_filename (str): The filename for the submission file.\n            running_jobs (list[str]): List of currently running jobs.\n            queuing_jobs (list[str]): List of currently queuing jobs.\n            list_of_jobs (list[str]): List of jobs to be submitted.\n            submission_type (str): The type of submission.\n\n        Returns:\n            tuple[list[str], list[str]]: A tuple containing:\n            - A list with the submission filename if the file was created, otherwise an empty list.\n            - An updated list of jobs that were included in the submission file.\n        \"\"\"\n        # Flag to know if the file can be submitted (at least one job in it)\n        ok_to_submit = False\n\n        # Flat to know if the header has been written\n        header_written = False\n\n        # Create folder to the submission file if it does not exist\n        os.makedirs(\"/\".join(sub_filename.split(\"/\")[:-1]), exist_ok=True)\n\n        # Updated list of jobs (without unsubmitted jobs)\n        list_of_jobs_updated = []\n\n        # Write the submission file\n        Sub = None\n        with open(sub_filename, \"w\") as fid:\n            for job in list_of_jobs:\n                # Get corresponding path job (remove the python file name)\n                path_job, abs_path_job = self._return_abs_path_job(job)\n\n                # Test if job is running, queuing or completed\n                if self._test_job(job, path_job, running_jobs, queuing_jobs):\n                    logging.info(f'Writing submission command for node \"{abs_path_job}\"')\n\n                    # Get job GPU request\n                    l_keys = self.dic_all_jobs[job][\"l_keys\"]\n                    # Ensure GPU is defined, and set it to False if not\n                    if \"request_gpu\" not in nested_get(self.dic_tree, l_keys):\n                        gpu = nested_set(self.dic_tree, l_keys + [\"request_gpu\"], False)\n                    gpu = nested_get(self.dic_tree, l_keys + [\"request_gpu\"])\n\n                    # Get Submission object\n                    Sub = self._get_Sub(job, submission_type, sub_filename, abs_path_job, gpu)\n\n                    # Take the first job as reference for head\n                    if not header_written:\n                        fid.write(Sub.head + \"\\n\")\n                        header_written = True\n\n                    # Write instruction for submission\n                    fid.write(Sub.body + \"\\n\")\n\n                    # Append job to list_of_jobs_updated\n                    list_of_jobs_updated.append(job)\n\n            # Tail instruction\n            if Sub is not None:\n                fid.write(Sub.tail + \"\\n\")\n                ok_to_submit = True\n\n        if not ok_to_submit:\n            os.remove(sub_filename)\n\n        return ([sub_filename], list_of_jobs_updated) if ok_to_submit else ([], [])\n\n    def _write_sub_files(\n        self,\n        sub_filename: str,\n        running_jobs: list[str],\n        queuing_jobs: list[str],\n        list_of_jobs: list[str],\n        submission_type: str,\n    ) -&gt; tuple[list[str], list[str]]:\n        \"\"\"\n        Writes submission files based on the specified submission type.\n\n        Args:\n            sub_filename (str): The name of the submission file to be created.\n            running_jobs (list[str]): A list of currently running jobs.\n            queuing_jobs (list[str]): A list of jobs that are queued.\n            list_of_jobs (list[str]): A list of all jobs to be submitted.\n            submission_type (str): The type of submission system being used (e.g., \"slurm_docker\").\n\n        Returns:\n            tuple[list[str], list[str]]: A tuple containing two lists:\n                - Updated list of running jobs.\n                - Updated list of queuing jobs.\n        \"\"\"\n        # Slurm docker is a peculiar case as one submission file must be created per job\n        if submission_type == \"slurm_docker\":\n            return self._write_sub_files_slurm_docker(\n                sub_filename, running_jobs, queuing_jobs, list_of_jobs\n            )\n\n        else:\n            return self._write_sub_file(\n                sub_filename,\n                running_jobs,\n                queuing_jobs,\n                list_of_jobs,\n                submission_type,\n            )\n\n    def write_sub_files(\n        self, dic_summary_by_gen: Optional[dict[int, dict[str, int]]] = None\n    ) -&gt; dict:\n        \"\"\"\n        Generates and writes submission files for jobs based on their submission type.\n\n        This method categorizes jobs to be submitted by their submission type, writes the\n        corresponding submission files, and returns a dictionary containing the submission\n        files and their associated job GPU requests.\n\n        Returns:\n            dict: A dictionary where keys are submission types and values are tuples containing\n                a list of updated jobs and a list of submission filenames.\n        \"\"\"\n        running_jobs, queuing_jobs = self._get_state_jobs(verbose=False)\n\n        # Make a dict of all jobs to submit depending on the submission type\n        dic_jobs_to_submit = {key: [] for key in self.dic_submission.keys()}\n        for job in self.l_jobs_to_submit:\n            l_keys = self.dic_all_jobs[job][\"l_keys\"]\n            submission_type = nested_get(self.dic_tree, l_keys + [\"submission_type\"])\n            dic_jobs_to_submit[submission_type].append(job)  # type: ignore\n\n        # Write submission files for each submission type\n        dic_submission_files = {}\n        for submission_type, list_of_jobs in dic_jobs_to_submit.items():\n            if len(list_of_jobs) &gt; 0:\n                # Write submission files\n                l_submission_filenames, list_of_jobs_updated = self._write_sub_files(\n                    self.path_submission_file,\n                    running_jobs,\n                    queuing_jobs,\n                    copy.copy(list_of_jobs),\n                    submission_type,\n                )\n\n                # Record submission files and and GPU requests\n                dic_submission_files[submission_type] = (\n                    list_of_jobs_updated,\n                    l_submission_filenames,\n                )\n\n                # Update dic_summary_by_gen inplace\n                if dic_summary_by_gen is not None:\n                    for job in list_of_jobs:\n                        gen = self.dic_all_jobs[job][\"gen\"]\n                        if job in list_of_jobs_updated:\n                            dic_summary_by_gen[gen][\"submitted_now\"] += 1\n                        else:\n                            dic_summary_by_gen[gen][\"running_or_queuing\"] += 1\n\n        return dic_submission_files\n\n    def _update_job_status_from_hpc_output(\n        self,\n        submit_command: str,\n        submission_type: str,\n        dic_id_to_path_job_temp: dict,\n        list_of_jobs: list[str],\n        idx_submission: int = 0,\n    ) -&gt; tuple[dict, int]:\n        \"\"\"\n        Updates the job status from the HPC output.\n\n        This method parses the output of a job submission command to update the job status\n        in a dictionary mapping job IDs to their respective paths. It supports both HTC and\n        SLURM submission types.\n\n        Args:\n            submit_command (str): The command used to submit the job.\n            submission_type (str): The type of submission system ('htc' or 'slurm').\n            dic_id_to_path_job_temp (dict): A dictionary mapping job IDs to their paths.\n            list_of_jobs (list[str]): A list of job paths.\n            idx_submission (int, optional): The index of the current submission. Defaults to 0.\n\n        Returns:\n            tuple[dict, int]: A tuple containing the updated dictionary and the updated index of\n                submission.\n\n        Raises:\n            RuntimeError: If there is an error in the submission process.\n        \"\"\"\n        process = subprocess.run(\n            submit_command.split(\" \"),\n            capture_output=True,\n        )\n\n        output = process.stdout.decode(\"utf-8\")\n        output_error = process.stderr.decode(\"utf-8\")\n        if \"ERROR\" in output_error:\n            raise RuntimeError(f\"Error in submission: {output_error}\")\n        for line in output.split(\"\\n\"):\n            if \"htc\" in submission_type:\n                if \"cluster\" in line:\n                    cluster_id = int(line.split(\"cluster \")[1][:-1])\n                    dic_id_to_path_job_temp[cluster_id] = self._return_abs_path_job(\n                        list_of_jobs[idx_submission]\n                    )[0]\n                    idx_submission += 1\n            elif \"slurm\" in submission_type:\n                if \"Submitted\" in line:\n                    job_id = int(line.split(\" \")[3])\n                    dic_id_to_path_job_temp[job_id] = self._return_abs_path_job(\n                        list_of_jobs[idx_submission]\n                    )[0]\n                    idx_submission += 1\n\n        return dic_id_to_path_job_temp, idx_submission\n\n    def submit(\n        self, list_of_jobs: list[str], l_submission_filenames: list[str], submission_type: str\n    ) -&gt; None:\n        \"\"\"\n        Submits a list of jobs to the specified submission system.\n\n        Args:\n            list_of_jobs (list[str]): List of job identifiers to be submitted.\n            l_submission_filenames (list[str]): List of filenames containing submission scripts.\n            submission_type (str): Type of submission system to use. Valid options are \"local\",\n                \"htc\", \"slurm\", \"htc_docker\", and \"slurm_docker\".\n\n        Raises:\n            ValueError: If multiple submission files are provided for a non-\"slurm_docker\"\n                submission type.\n            ValueError: If the submission type is not valid.\n\n        Returns:\n            None\n        \"\"\"\n        # Check that the submission file(s) is/are appropriate for the submission mode\n        if len(l_submission_filenames) &gt; 1 and submission_type != \"slurm_docker\":\n            raise ValueError(\n                \"Error: Multiple submission files should not be implemented for this submission\"\n                \" mode\"\n            )\n\n        # Check that at least one job is being submitted\n        if not l_submission_filenames:\n            logging.info(\"No job being submitted.\")\n\n        # Submit\n        dic_id_to_path_job_temp = {}\n        idx_submission = 0\n        for sub_filename in l_submission_filenames:\n            if submission_type == \"local\":\n                os.system(self.dic_submission[submission_type].get_submit_command(sub_filename))\n            elif submission_type in {\"htc\", \"slurm\", \"htc_docker\", \"slurm_docker\"}:\n                submit_command = self.dic_submission[submission_type].get_submit_command(\n                    sub_filename\n                )\n                dic_id_to_path_job_temp, idx_submission = self._update_job_status_from_hpc_output(\n                    submit_command,\n                    submission_type,\n                    dic_id_to_path_job_temp,\n                    list_of_jobs,\n                    idx_submission,\n                )\n            else:\n                raise ValueError(f\"Error: {submission_type} is not a valid submission mode\")\n\n        # Update and write the id-job file\n        if dic_id_to_path_job_temp:\n            assert len(dic_id_to_path_job_temp) == len(list_of_jobs)\n\n        # Merge with the previous id-job file\n        dic_id_to_path_job = self.dic_id_to_path_job\n\n        # Update and write on disk\n        if dic_id_to_path_job is not None:\n            dic_id_to_path_job.update(dic_id_to_path_job_temp)\n            self.dic_id_to_path_job = dic_id_to_path_job\n        elif dic_id_to_path_job_temp:\n            dic_id_to_path_job = dic_id_to_path_job_temp\n            self.dic_id_to_path_job = dic_id_to_path_job\n\n        logging.info(\"Jobs status after submission:\")\n        _, _ = self._get_state_jobs(verbose=True)\n\n    def _get_local_jobs(self) -&gt; list[str]:\n        \"\"\"\n        Retrieves a list of local job paths.\n        This method scans the current processes to identify jobs that are running\n        a script named 'run.sh'. It extracts the job paths and filters them to\n        include only the paths that are relevant to the current study.\n\n        Args:\n            None\n\n        Returns:\n            list[str]: A list of job paths that are currently running and relevant\n            to the study.\n        \"\"\"\n\n        l_path_jobs = []\n        # Warning, does not work at the moment in lxplus...\n        for ps in psutil.pids():\n            try:\n                aux = psutil.Process(ps).cmdline()\n            except Exception:\n                aux = []\n            if len(aux) &gt; 1 and \"run.sh\" in aux[-1]:\n                job = str(Path(aux[-1]).parent)\n\n                # Only get path after name of the study\n                try:\n                    job = job.split(self.study_name)[1]\n                    l_path_jobs.append(f\"{self.study_name}{job}/\")\n                except IndexError:\n                    logging.warning(\n                        \"Some jobs from another study are running. Acquiring the full path as the \"\n                        \"study name is unknown.\"\n                    )\n                    l_path_jobs.append(job)\n        return l_path_jobs\n\n    def _get_condor_jobs(self, status: str, force_query_individually: bool = False) -&gt; list[str]:\n        \"\"\"\n        Retrieve the paths of Condor jobs based on their status.\n\n        Args:\n            status (str): The status of the jobs to retrieve. Can be \"running\" or \"queuing\".\n            force_query_individually (bool, optional): If True, query each job individually if the\n                id-job file is missing. Defaults to False.\n\n        Returns:\n            list[str]: A list of paths to the jobs that match the specified status.\n\n        Raises:\n            ValueError: If the status provided is not \"running\" or \"queuing\".\n\n        Notes:\n            - The method relies on the `condor_q` command to retrieve job information.\n            - If the id-job file is missing and `force_query_individually` is False, jobs not in\n                `dic_id_to_path_job` will be ignored.\n            - If `force_query_individually` is True, the method will query each job individually to\n                retrieve its path.\n            - Warnings are printed if jobs are found that are not in the id-job file or if the\n                id-job file is missing.\n        \"\"\"\n        l_path_jobs = []\n        dic_status = {\"running\": 1, \"queuing\": 2}\n        condor_output = subprocess.run([\"condor_q\"], capture_output=True).stdout.decode(\"utf-8\")\n\n        # Check which jobs are running\n        first_line = True\n        first_missing_job = True\n        for line in condor_output.split(\"\\n\")[4:]:\n            if line == \"\":\n                break\n            jobid = int(line.split(\"ID:\")[1][1:8])\n            condor_status = line.split(\"      \")[1:5]  # Done, Run, Idle, and potentially Hold\n\n            # If job is running/queuing, get the path to the job\n            if condor_status[dic_status[status]] == \"1\":\n                # Get path from dic_id_to_path_job if available\n                if self.dic_id_to_path_job is not None:\n                    if jobid in self.dic_id_to_path_job:\n                        l_path_jobs.append(self.dic_id_to_path_job[jobid])\n                    elif first_missing_job:\n                        logging.warning(\n                            \"Warning, some jobs are queuing/running and are not in the id-job\"\n                            \" file. They may come from another study. Ignoring them.\"\n                        )\n                        first_missing_job = False\n\n                elif force_query_individually:\n                    if first_line:\n                        logging.warning(\n                            \"Warning, some jobs are queuing/running and the id-job file is\"\n                            \" missing... Querying them individually.\"\n                        )\n                        first_line = False\n                    job_details = subprocess.run(\n                        [\"condor_q\", \"-l\", f\"{jobid}\"], capture_output=True\n                    ).stdout.decode(\"utf-8\")\n                    job = job_details.split('Cmd = \"')[1].split(\"run.sh\")[0]\n\n                    # Only get path after master_study\n                    job = job.split(self.study_name)[1]\n                    l_path_jobs.append(f\"{self.study_name}{job}\")\n\n                elif first_line:\n                    logging.warning(\n                        \"Warning, some jobs are queuing/running and the id-job file is\"\n                        \" missing... Ignoring them.\"\n                    )\n                    first_line = False\n\n        return l_path_jobs\n\n    def _get_slurm_jobs(self, status: str, force_query_individually: bool = False) -&gt; list[str]:\n        \"\"\"\n        Retrieve a list of SLURM job paths based on their status.\n\n        This method queries SLURM to get the job IDs and their statuses for the current user.\n        It then attempts to map these job IDs to their corresponding paths using an internal\n        dictionary. If the dictionary is not available or does not contain the job ID, it can\n        optionally query each job individually for its details.\n\n        Args:\n            status (str): The status of the jobs to retrieve. Expected values are \"running\" or\n                \"queuing\".\n            force_query_individually (bool, optional): If True, query each job individually for its\n                details when the job ID is not found in the internal dictionary. Defaults to False.\n\n        Returns:\n            list[str]: A list of job paths corresponding to the specified status.\n        \"\"\"\n        l_path_jobs = []\n        dic_status = {\"running\": \"RUNNING\", \"queuing\": \"PENDING\"}\n        username = (\n            subprocess.run([\"id\", \"-u\", \"-n\"], capture_output=True).stdout.decode(\"utf-8\").strip()\n        )\n        slurm_output = subprocess.run(\n            [\"squeue\", \"-u\", f\"{username}\", \"-t\", dic_status[status]], capture_output=True\n        ).stdout.decode(\"utf-8\")\n\n        # Get job id and details\n        first_line = True\n        first_missing_job = True\n        for line in slurm_output.split(\"\\n\")[1:]:\n            l_split = line.split()\n            if len(l_split) == 0:\n                break\n            jobid = int(l_split[0])\n            slurm_status = l_split[4]  # R or PD  # noqa: F841\n\n            # Get path from dic_id_to_path_job if available\n            if self.dic_id_to_path_job is not None:\n                if jobid in self.dic_id_to_path_job:\n                    l_path_jobs.append(self.dic_id_to_path_job[jobid])\n                elif first_missing_job:\n                    logging.warning(\n                        \"Warning, some jobs are queuing/running and are not in the id-job\"\n                        \" file. They may come from another study. Ignoring them.\"\n                    )\n                    first_missing_job = False\n\n            elif force_query_individually:\n                if first_line:\n                    logging.warning(\n                        \"Warning, some jobs are queuing/running and the id-job file is\"\n                        \" missing... Querying them individually.\"\n                    )\n                    first_line = False\n                job_details = subprocess.run(\n                    [\"scontrol\", \"show\", \"jobid\", \"-dd\", f\"{jobid}\"], capture_output=True\n                ).stdout.decode(\"utf-8\")\n                job = (\n                    job_details.split(\"Command=\")[1].split(\"run.sh\")[0]\n                    if \"run.sh\" in job_details\n                    else job_details.split(\"StdOut=\")[1].split(\"output.txt\")[0]\n                )\n                # Only get path after study_name\n                job = job.split(self.study_name)[1]\n                l_path_jobs.append(f\"{self.study_name}{job}\")\n\n            elif first_line:\n                logging.warning(\n                    \"Warning, some jobs are queuing/running and the id-job file is\"\n                    \" missing... Ignoring them.\"\n                )\n                first_line = False\n\n        return l_path_jobs\n\n    def querying_jobs(\n        self, check_local: bool, check_htc: bool, check_slurm: bool, status: str = \"running\"\n    ) -&gt; list[str]:\n        \"\"\"\n        Queries jobs from different job management systems based on the provided flags and status.\n\n        Args:\n            check_local (bool): If True, check for local jobs.\n            check_htc (bool): If True, check for HTC (High Throughput Computing) jobs.\n            check_slurm (bool): If True, check for SLURM jobs.\n            status (str, optional): The status of the jobs to query. Defaults to \"running\".\n\n        Returns:\n            list[str]: A list of job paths that match the query criteria.\n        \"\"\"\n        # sourcery skip: remove-redundant-if, remove-redundant-pass, swap-nested-ifs\n        l_path_jobs = []\n        if check_local:\n            if status == \"running\":\n                l_path_jobs.extend(self._get_local_jobs())\n            else:\n                # Always empty return as there is no queuing in local pc\n                pass\n\n        if check_htc:\n            l_path_jobs.extend(self._get_condor_jobs(status))\n\n        if check_slurm:\n            l_path_jobs.extend(self._get_slurm_jobs(status))\n\n        return l_path_jobs\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/index.html#study_da.submit.cluster_submission.ClusterSubmission.dic_id_to_path_job","title":"<code>dic_id_to_path_job: dict | None</code>  <code>property</code> <code>writable</code>","text":"<p>Generates a dictionary mapping job IDs to their respective job paths.</p> <p>This method iterates over the list of jobs to submit and constructs a dictionary where the keys are job IDs and the values are the absolute paths to the jobs. If no job IDs are found, the method returns None.</p> <p>Returns:</p> Type Description <code>dict | None</code> <p>dict | None: A dictionary mapping job IDs to job paths, or None if no job IDs are found.</p>"},{"location":"reference/study_da/submit/cluster_submission/index.html#study_da.submit.cluster_submission.ClusterSubmission.dic_submission","title":"<code>dic_submission: dict = {'local': LocalPC, 'htc': HTC, 'htc_docker': HTCDocker, 'slurm': Slurm, 'slurm_docker': SlurmDocker}</code>  <code>instance-attribute</code>","text":"<p>Initialize the ClusterSubmission class.</p> <p>Parameters:</p> Name Type Description Default <code>study_name</code> <code>str</code> <p>The name of the study.</p> required <code>l_jobs_to_submit</code> <code>list[str]</code> <p>A list of job names to submit.</p> required <code>dic_all_jobs</code> <code>dict</code> <p>A dictionary containing all jobs.</p> required <code>dic_tree</code> <code>dict</code> <p>A dictionary representing the job tree structure.</p> required <code>path_submission_file</code> <code>str</code> <p>The path to the submission file.</p> required <code>abs_path_study</code> <code>str</code> <p>The absolute path to the study.</p> required"},{"location":"reference/study_da/submit/cluster_submission/index.html#study_da.submit.cluster_submission.ClusterSubmission.querying_jobs","title":"<code>querying_jobs(check_local, check_htc, check_slurm, status='running')</code>","text":"<p>Queries jobs from different job management systems based on the provided flags and status.</p> <p>Parameters:</p> Name Type Description Default <code>check_local</code> <code>bool</code> <p>If True, check for local jobs.</p> required <code>check_htc</code> <code>bool</code> <p>If True, check for HTC (High Throughput Computing) jobs.</p> required <code>check_slurm</code> <code>bool</code> <p>If True, check for SLURM jobs.</p> required <code>status</code> <code>str</code> <p>The status of the jobs to query. Defaults to \"running\".</p> <code>'running'</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of job paths that match the query criteria.</p> Source code in <code>study_da/submit/cluster_submission/cluster_submission.py</code> <pre><code>def querying_jobs(\n    self, check_local: bool, check_htc: bool, check_slurm: bool, status: str = \"running\"\n) -&gt; list[str]:\n    \"\"\"\n    Queries jobs from different job management systems based on the provided flags and status.\n\n    Args:\n        check_local (bool): If True, check for local jobs.\n        check_htc (bool): If True, check for HTC (High Throughput Computing) jobs.\n        check_slurm (bool): If True, check for SLURM jobs.\n        status (str, optional): The status of the jobs to query. Defaults to \"running\".\n\n    Returns:\n        list[str]: A list of job paths that match the query criteria.\n    \"\"\"\n    # sourcery skip: remove-redundant-if, remove-redundant-pass, swap-nested-ifs\n    l_path_jobs = []\n    if check_local:\n        if status == \"running\":\n            l_path_jobs.extend(self._get_local_jobs())\n        else:\n            # Always empty return as there is no queuing in local pc\n            pass\n\n    if check_htc:\n        l_path_jobs.extend(self._get_condor_jobs(status))\n\n    if check_slurm:\n        l_path_jobs.extend(self._get_slurm_jobs(status))\n\n    return l_path_jobs\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/index.html#study_da.submit.cluster_submission.ClusterSubmission.submit","title":"<code>submit(list_of_jobs, l_submission_filenames, submission_type)</code>","text":"<p>Submits a list of jobs to the specified submission system.</p> <p>Parameters:</p> Name Type Description Default <code>list_of_jobs</code> <code>list[str]</code> <p>List of job identifiers to be submitted.</p> required <code>l_submission_filenames</code> <code>list[str]</code> <p>List of filenames containing submission scripts.</p> required <code>submission_type</code> <code>str</code> <p>Type of submission system to use. Valid options are \"local\", \"htc\", \"slurm\", \"htc_docker\", and \"slurm_docker\".</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If multiple submission files are provided for a non-\"slurm_docker\" submission type.</p> <code>ValueError</code> <p>If the submission type is not valid.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/submit/cluster_submission/cluster_submission.py</code> <pre><code>def submit(\n    self, list_of_jobs: list[str], l_submission_filenames: list[str], submission_type: str\n) -&gt; None:\n    \"\"\"\n    Submits a list of jobs to the specified submission system.\n\n    Args:\n        list_of_jobs (list[str]): List of job identifiers to be submitted.\n        l_submission_filenames (list[str]): List of filenames containing submission scripts.\n        submission_type (str): Type of submission system to use. Valid options are \"local\",\n            \"htc\", \"slurm\", \"htc_docker\", and \"slurm_docker\".\n\n    Raises:\n        ValueError: If multiple submission files are provided for a non-\"slurm_docker\"\n            submission type.\n        ValueError: If the submission type is not valid.\n\n    Returns:\n        None\n    \"\"\"\n    # Check that the submission file(s) is/are appropriate for the submission mode\n    if len(l_submission_filenames) &gt; 1 and submission_type != \"slurm_docker\":\n        raise ValueError(\n            \"Error: Multiple submission files should not be implemented for this submission\"\n            \" mode\"\n        )\n\n    # Check that at least one job is being submitted\n    if not l_submission_filenames:\n        logging.info(\"No job being submitted.\")\n\n    # Submit\n    dic_id_to_path_job_temp = {}\n    idx_submission = 0\n    for sub_filename in l_submission_filenames:\n        if submission_type == \"local\":\n            os.system(self.dic_submission[submission_type].get_submit_command(sub_filename))\n        elif submission_type in {\"htc\", \"slurm\", \"htc_docker\", \"slurm_docker\"}:\n            submit_command = self.dic_submission[submission_type].get_submit_command(\n                sub_filename\n            )\n            dic_id_to_path_job_temp, idx_submission = self._update_job_status_from_hpc_output(\n                submit_command,\n                submission_type,\n                dic_id_to_path_job_temp,\n                list_of_jobs,\n                idx_submission,\n            )\n        else:\n            raise ValueError(f\"Error: {submission_type} is not a valid submission mode\")\n\n    # Update and write the id-job file\n    if dic_id_to_path_job_temp:\n        assert len(dic_id_to_path_job_temp) == len(list_of_jobs)\n\n    # Merge with the previous id-job file\n    dic_id_to_path_job = self.dic_id_to_path_job\n\n    # Update and write on disk\n    if dic_id_to_path_job is not None:\n        dic_id_to_path_job.update(dic_id_to_path_job_temp)\n        self.dic_id_to_path_job = dic_id_to_path_job\n    elif dic_id_to_path_job_temp:\n        dic_id_to_path_job = dic_id_to_path_job_temp\n        self.dic_id_to_path_job = dic_id_to_path_job\n\n    logging.info(\"Jobs status after submission:\")\n    _, _ = self._get_state_jobs(verbose=True)\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/index.html#study_da.submit.cluster_submission.ClusterSubmission.write_sub_files","title":"<code>write_sub_files(dic_summary_by_gen=None)</code>","text":"<p>Generates and writes submission files for jobs based on their submission type.</p> <p>This method categorizes jobs to be submitted by their submission type, writes the corresponding submission files, and returns a dictionary containing the submission files and their associated job GPU requests.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary where keys are submission types and values are tuples containing a list of updated jobs and a list of submission filenames.</p> Source code in <code>study_da/submit/cluster_submission/cluster_submission.py</code> <pre><code>def write_sub_files(\n    self, dic_summary_by_gen: Optional[dict[int, dict[str, int]]] = None\n) -&gt; dict:\n    \"\"\"\n    Generates and writes submission files for jobs based on their submission type.\n\n    This method categorizes jobs to be submitted by their submission type, writes the\n    corresponding submission files, and returns a dictionary containing the submission\n    files and their associated job GPU requests.\n\n    Returns:\n        dict: A dictionary where keys are submission types and values are tuples containing\n            a list of updated jobs and a list of submission filenames.\n    \"\"\"\n    running_jobs, queuing_jobs = self._get_state_jobs(verbose=False)\n\n    # Make a dict of all jobs to submit depending on the submission type\n    dic_jobs_to_submit = {key: [] for key in self.dic_submission.keys()}\n    for job in self.l_jobs_to_submit:\n        l_keys = self.dic_all_jobs[job][\"l_keys\"]\n        submission_type = nested_get(self.dic_tree, l_keys + [\"submission_type\"])\n        dic_jobs_to_submit[submission_type].append(job)  # type: ignore\n\n    # Write submission files for each submission type\n    dic_submission_files = {}\n    for submission_type, list_of_jobs in dic_jobs_to_submit.items():\n        if len(list_of_jobs) &gt; 0:\n            # Write submission files\n            l_submission_filenames, list_of_jobs_updated = self._write_sub_files(\n                self.path_submission_file,\n                running_jobs,\n                queuing_jobs,\n                copy.copy(list_of_jobs),\n                submission_type,\n            )\n\n            # Record submission files and and GPU requests\n            dic_submission_files[submission_type] = (\n                list_of_jobs_updated,\n                l_submission_filenames,\n            )\n\n            # Update dic_summary_by_gen inplace\n            if dic_summary_by_gen is not None:\n                for job in list_of_jobs:\n                    gen = self.dic_all_jobs[job][\"gen\"]\n                    if job in list_of_jobs_updated:\n                        dic_summary_by_gen[gen][\"submitted_now\"] += 1\n                    else:\n                        dic_summary_by_gen[gen][\"running_or_queuing\"] += 1\n\n    return dic_submission_files\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/cluster_submission.html","title":"cluster_submission","text":"<p>This module contains a class for submitting jobs on a cluster (or locally).</p>"},{"location":"reference/study_da/submit/cluster_submission/cluster_submission.html#study_da.submit.cluster_submission.cluster_submission.ClusterSubmission","title":"<code>ClusterSubmission</code>","text":"<p>A class to handle the submission of jobs to various cluster systems such as local PC, HTC, and Slurm.</p> <p>Attributes:</p> Name Type Description <code>study_name</code> <code>str</code> <p>The name of the study.</p> <code>l_jobs_to_submit</code> <code>list[str]</code> <p>A list of jobs to submit.</p> <code>dic_all_jobs</code> <code>dict</code> <p>A dictionary containing all jobs.</p> <code>dic_tree</code> <code>dict</code> <p>A dictionary representing the job tree.</p> <code>path_submission_file</code> <code>str</code> <p>The path to the submission file.</p> <code>abs_path_study</code> <code>str</code> <p>The absolute path to the study.</p> <code>dic_submission</code> <code>dict</code> <p>A dictionary mapping submission types to their corresponding classes.</p> <p>Methods:</p> Name Description <code>dic_id_to_path_job</code> <p>Getter for the dictionary mapping job IDs to their paths.</p> <code>dic_id_to_path_job</code> <p>dict[int, str]): Setter for the dictionary mapping job IDs to their paths.</p> <code>_update_dic_id_to_path_job</code> <p>list[str], queuing_jobs: list[str]) -&gt; None: Updates the dictionary mapping job IDs to their paths based on the current running and queuing jobs.</p> <code>_check_submission_type</code> <p>Checks the submission type for the jobs and ensures that HTC and Slurm submissions are not mixed.</p> <code>_get_state_jobs</code> <p>bool = True) -&gt; tuple[list[str], list[str]]: Gets the current state of the jobs (running and queuing).</p> <code>_test_job</code> <p>str, path_job: str, running_jobs: list[str], queuing_jobs: list[str]) -&gt; bool: Tests if a job is completed, running, or queuing.</p> <code>_return_htc_flavour</code> <p>str) -&gt; str: Returns the HTC flavor for a given job.</p> <code>_return_abs_path_job</code> <p>str) -&gt; tuple[str, str]: Returns the absolute path of a job.</p> <code>_write_sub_files_slurm_docker</code> <p>str, running_jobs: list[str], queuing_jobs: list[str], list_of_jobs: list[str]) -&gt; tuple[list[str], list[str]]: Writes submission files for Slurm Docker jobs.</p> <code>_get_Sub</code> <p>str, submission_type: str, sub_filename: str, abs_path_job: str, gpu: bool) -&gt; LocalPC | HTC | HTCDocker | Slurm | SlurmDocker: Returns the appropriate submission object based on the submission type.</p> <code>_write_sub_file</code> <p>str, running_jobs: list[str], queuing_jobs: list[str], list_of_jobs: list[str], submission_type: str) -&gt; tuple[list[str], list[str]]: Writes a submission file for the given jobs.</p> <code>_write_sub_files</code> <p>str, running_jobs: list[str], queuing_jobs: list[str], list_of_jobs: list[str], submission_type: str) -&gt; tuple[list[str], list[str]]: Writes submission files for the given jobs based on the submission type.</p> <code>write_sub_files</code> <p>Writes submission files for all jobs to be submitted and returns a dictionary of submission files.</p> <code>_update_job_status_from_hpc_output</code> <p>str, submission_type: str, dic_id_to_path_job_temp: dict, list_of_jobs: list[str], idx_submission: int = 0)     -&gt; tuple[dict, int]: Updates the job status from the HPC output.</p> <code>submit</code> <p>list[str], l_submission_filenames: list[str], submission_type: str) -&gt; None: Submits the jobs to the appropriate cluster system.</p> <code>_get_local_jobs</code> <p>Gets the list of local jobs.</p> <code>_get_condor_jobs</code> <p>str, force_query_individually: bool = False) -&gt; list[str]: Gets the list of Condor jobs based on the status.</p> <code>_get_slurm_jobs</code> <p>str, force_query_individually: bool = False) -&gt; list[str]: Gets the list of Slurm jobs based on the status.</p> <code>querying_jobs</code> <p>bool, check_htc: bool, check_slurm: bool, status: str = \"running\") -&gt; list[str]: Queries the jobs based on the submission type and status.</p> Source code in <code>study_da/submit/cluster_submission/cluster_submission.py</code> <pre><code>class ClusterSubmission:\n    \"\"\"\n    A class to handle the submission of jobs to various cluster systems such as local PC, HTC, and\n    Slurm.\n\n    Attributes:\n        study_name (str): The name of the study.\n        l_jobs_to_submit (list[str]): A list of jobs to submit.\n        dic_all_jobs (dict): A dictionary containing all jobs.\n        dic_tree (dict): A dictionary representing the job tree.\n        path_submission_file (str): The path to the submission file.\n        abs_path_study (str): The absolute path to the study.\n        dic_submission (dict): A dictionary mapping submission types to their corresponding classes.\n\n    Methods:\n        dic_id_to_path_job() -&gt; dict | None:\n            Getter for the dictionary mapping job IDs to their paths.\n        dic_id_to_path_job(dic_id_to_path_job: dict[int, str]):\n            Setter for the dictionary mapping job IDs to their paths.\n        _update_dic_id_to_path_job(running_jobs: list[str], queuing_jobs: list[str]) -&gt; None:\n            Updates the dictionary mapping job IDs to their paths based on the current running and\n            queuing jobs.\n        _check_submission_type() -&gt; tuple[bool, bool, bool]:\n            Checks the submission type for the jobs and ensures that HTC and Slurm submissions are\n            not mixed.\n        _get_state_jobs(verbose: bool = True) -&gt; tuple[list[str], list[str]]:\n            Gets the current state of the jobs (running and queuing).\n        _test_job(job: str, path_job: str, running_jobs: list[str], queuing_jobs: list[str]) -&gt; bool:\n            Tests if a job is completed, running, or queuing.\n        _return_htc_flavour(job: str) -&gt; str:\n            Returns the HTC flavor for a given job.\n        _return_abs_path_job(job: str) -&gt; tuple[str, str]:\n            Returns the absolute path of a job.\n        _write_sub_files_slurm_docker(sub_filename: str, running_jobs: list[str],\n            queuing_jobs: list[str], list_of_jobs: list[str]) -&gt; tuple[list[str], list[str]]:\n            Writes submission files for Slurm Docker jobs.\n        _get_Sub(job: str, submission_type: str, sub_filename: str, abs_path_job: str,\n            gpu: bool) -&gt; LocalPC | HTC | HTCDocker | Slurm | SlurmDocker:\n            Returns the appropriate submission object based on the submission type.\n        _write_sub_file(sub_filename: str, running_jobs: list[str], queuing_jobs: list[str],\n            list_of_jobs: list[str], submission_type: str) -&gt; tuple[list[str], list[str]]:\n            Writes a submission file for the given jobs.\n        _write_sub_files(sub_filename: str, running_jobs: list[str], queuing_jobs: list[str],\n            list_of_jobs: list[str], submission_type: str) -&gt; tuple[list[str], list[str]]:\n            Writes submission files for the given jobs based on the submission type.\n        write_sub_files() -&gt; dict:\n            Writes submission files for all jobs to be submitted and returns a dictionary of\n            submission files.\n        _update_job_status_from_hpc_output(submit_command: str, submission_type: str,\n            dic_id_to_path_job_temp: dict, list_of_jobs: list[str], idx_submission: int = 0)\n                -&gt; tuple[dict, int]:\n            Updates the job status from the HPC output.\n        submit(list_of_jobs: list[str], l_submission_filenames: list[str], submission_type: str)\n            -&gt; None:\n            Submits the jobs to the appropriate cluster system.\n        _get_local_jobs() -&gt; list[str]:\n            Gets the list of local jobs.\n        _get_condor_jobs(status: str, force_query_individually: bool = False) -&gt; list[str]:\n            Gets the list of Condor jobs based on the status.\n        _get_slurm_jobs(status: str, force_query_individually: bool = False) -&gt; list[str]:\n            Gets the list of Slurm jobs based on the status.\n        querying_jobs(check_local: bool, check_htc: bool, check_slurm: bool,\n            status: str = \"running\") -&gt; list[str]:\n            Queries the jobs based on the submission type and status.\n    \"\"\"\n\n    def __init__(\n        self,\n        study_name: str,\n        l_jobs_to_submit: list[str],\n        dic_all_jobs: dict,\n        dic_tree: dict,\n        path_submission_file: str,\n        abs_path_study: str,\n    ):\n        self.study_name: str = study_name\n        self.l_jobs_to_submit: list[str] = l_jobs_to_submit\n        self.dic_all_jobs: dict = dic_all_jobs\n        self.dic_tree: dict = dic_tree\n        self.path_submission_file: str = path_submission_file\n        self.abs_path_study: str = abs_path_study\n        self.dic_submission: dict = {\n            \"local\": LocalPC,\n            \"htc\": HTC,\n            \"htc_docker\": HTCDocker,\n            \"slurm\": Slurm,\n            \"slurm_docker\": SlurmDocker,\n        }\n        \"\"\"\n        Initialize the ClusterSubmission class.\n\n        Args:\n            study_name (str): The name of the study.\n            l_jobs_to_submit (list[str]): A list of job names to submit.\n            dic_all_jobs (dict): A dictionary containing all jobs.\n            dic_tree (dict): A dictionary representing the job tree structure.\n            path_submission_file (str): The path to the submission file.\n            abs_path_study (str): The absolute path to the study.\n        \"\"\"\n\n    # Getter for dic_id_to_path_job\n    @property\n    def dic_id_to_path_job(self) -&gt; dict | None:\n        \"\"\"\n        Generates a dictionary mapping job IDs to their respective job paths.\n\n        This method iterates over the list of jobs to submit and constructs a dictionary\n        where the keys are job IDs and the values are the absolute paths to the jobs.\n        If no job IDs are found, the method returns None.\n\n        Returns:\n            dict | None: A dictionary mapping job IDs to job paths, or None if no job IDs are found.\n        \"\"\"\n        dic_id_to_path_job = {}\n        found_at_least_one = False\n        for job in self.l_jobs_to_submit:\n            l_keys = self.dic_all_jobs[job][\"l_keys\"]\n            subdic_job = nested_get(self.dic_tree, l_keys)\n            if \"id_sub\" in subdic_job:\n                dic_id_to_path_job[subdic_job[\"id_sub\"]] = self._return_abs_path_job(job)[0]\n                found_at_least_one = True\n\n        return dic_id_to_path_job if found_at_least_one else None\n\n    # Setter for dic_id_to_path_job\n    @dic_id_to_path_job.setter\n    def dic_id_to_path_job(self, dic_id_to_path_job: dict[int, str]):\n        \"\"\"\n        Updates the internal job submission tree with job IDs and their corresponding paths.\n\n        Args:\n            dic_id_to_path_job (dict[int, str]): A dictionary mapping job IDs (integers) to their\n            respective paths (strings).\n\n        Raises:\n            AssertionError: If dic_id_to_path_job is not a dictionary.\n\n        Notes:\n            - Ensures all job IDs are integers.\n            - Updates the internal job submission tree by adding or removing job IDs based on the\n                provided dictionary.\n            - If a job's path is found in the dictionary, its ID is updated in the tree.\n            - If a job's ID is not found in the dictionary, it is removed from the tree.\n        \"\"\"\n        assert isinstance(dic_id_to_path_job, dict)\n        # Ensure all ids are integers\n        dic_id_to_path_job = {\n            int(id_job): path_job for id_job, path_job in dic_id_to_path_job.items()\n        }\n        dic_job_to_id = {path_job: int(id_job) for id_job, path_job in dic_id_to_path_job.items()}\n\n        # Update the tree\n        for job in self.l_jobs_to_submit:\n            path_job = self._return_abs_path_job(job)[0]\n            l_keys = self.dic_all_jobs[job][\"l_keys\"]\n            subdic_job = nested_get(self.dic_tree, l_keys)\n            if \"id_sub\" in subdic_job and int(subdic_job[\"id_sub\"]) not in dic_id_to_path_job:\n                del subdic_job[\"id_sub\"]\n            elif \"id_sub\" not in subdic_job and path_job in dic_job_to_id:\n                subdic_job[\"id_sub\"] = dic_job_to_id[path_job]\n            # Else all is consistent\n\n    def _update_dic_id_to_path_job(self, running_jobs: list[str], queuing_jobs: list[str]) -&gt; None:\n        \"\"\"\n        Updates the dictionary `dic_id_to_path_job` by removing jobs that are no longer running or\n        queuing.\n\n        Args:\n            running_jobs (list[str]): A list of job identifiers that are currently running.\n            queuing_jobs (list[str]): A list of job identifiers that are currently queuing.\n\n        Returns:\n            None\n        \"\"\"\n        # Look for jobs in the dictionnary that are not running or queuing anymore\n        set_current_jobs = set(running_jobs + queuing_jobs)\n        if self.dic_id_to_path_job is not None:\n            dic_id_to_path_job = self.dic_id_to_path_job\n            for id_job, job in self.dic_id_to_path_job.items():\n                if job not in set_current_jobs:\n                    del dic_id_to_path_job[id_job]\n\n            # Update dic_id_to_path_job\n            self.dic_id_to_path_job = dic_id_to_path_job\n\n    def _check_submission_type(self) -&gt; tuple[bool, bool, bool]:\n        \"\"\"\n        Checks the types of job submissions in the current batch and ensures that\n        there is no mixing of HTC and Slurm submission types.\n\n        Args:\n            None\n\n        Returns:\n            tuple[bool, bool, bool]: A tuple containing three boolean values indicating\n            whether there are local, HTC, and Slurm submissions respectively.\n\n        Raises:\n            ValueError: If both HTC and Slurm submission types are found in the jobs to submit.\n        \"\"\"\n        check_local = False\n        check_htc = False\n        check_slurm = False\n        for job in self.l_jobs_to_submit:\n            l_keys = self.dic_all_jobs[job][\"l_keys\"]\n            submission_type = nested_get(self.dic_tree, l_keys + [\"submission_type\"])\n            if submission_type == \"local\":\n                check_local = True\n            elif submission_type in [\"htc\", \"htc_docker\"]:\n                check_htc = True\n            elif submission_type in [\"slurm\", \"slurm_docker\"]:\n                check_slurm = True\n\n        if check_htc and check_slurm:\n            raise ValueError(\"Error: Mixing htc and slurm submission is not allowed\")\n\n        return check_local, check_htc, check_slurm\n\n    def _get_state_jobs(self, verbose: bool = True) -&gt; tuple[list[str], list[str]]:\n        \"\"\"\n        Retrieves the state of jobs (running and queuing) based on the submission type.\n\n        This method first determines the submission type (local, HTC, or Slurm) and then queries\n        the jobs accordingly. It updates the internal dictionary mapping job IDs to their paths\n        and optionally prints the running and queuing jobs.\n\n        Args:\n            verbose (bool): If True, prints the running and queuing jobs. Default is True.\n\n        Returns:\n            tuple[list[str], list[str]]: A tuple containing two lists:\n                - The first list contains the IDs of running jobs.\n                - The second list contains the IDs of queuing jobs.\n        \"\"\"\n        # First check whether the jobs are submitted on local, htc or slurm\n        check_local, check_htc, check_slurm = self._check_submission_type()\n\n        # Then query accordingly\n        running_jobs = self.querying_jobs(check_local, check_htc, check_slurm, status=\"running\")\n        queuing_jobs = self.querying_jobs(check_local, check_htc, check_slurm, status=\"queuing\")\n        self._update_dic_id_to_path_job(running_jobs, queuing_jobs)\n        if verbose:\n            logging.info(\"Running: \\n\" + \"\\n\".join(running_jobs))\n            logging.info(\"queuing: \\n\" + \"\\n\".join(queuing_jobs))\n        return running_jobs, queuing_jobs\n\n    def _test_job(\n        self, job: str, path_job: str, running_jobs: list[str], queuing_jobs: list[str]\n    ) -&gt; bool:\n        \"\"\"\n        Tests the status of a job and determines if it needs to be (re)submitted.\n\n        Args:\n            job (str): The job identifier.\n            path_job (str): The path to the job.\n            running_jobs (list[str]): A list of currently running jobs.\n            queuing_jobs (list[str]): A list of currently queuing jobs.\n\n        Returns:\n            bool: True if the job must be (re)submitted, False otherwise.\n        \"\"\"\n        # Test if job is completed\n        l_keys = self.dic_all_jobs[job][\"l_keys\"]\n        completed = nested_get(self.dic_tree, l_keys + [\"status\"]) == \"finished\"\n        failed = nested_get(self.dic_tree, l_keys + [\"status\"]) == \"failed\"\n        if completed:\n            logging.info(f\"{path_job} is already completed.\")\n\n        # Test if job has failed\n        if failed:\n            logging.info(f\"{path_job} has failed.\")\n\n        # Test if job is running\n        elif path_job in running_jobs:\n            logging.info(f\"{path_job} is already running.\")\n\n        # Test if job is queuing\n        elif path_job in queuing_jobs:\n            logging.info(f\"{path_job} is already queuing.\")\n\n        # True if job must be (re)submitted\n        else:\n            # ? If some jobs finish in the second-long step between the tree update and the\n            # ? submission, they will be considered as needed to be submitted...\n            # ? Should happend rarely but still a problem... And no easy way to fix thiss\n            return True\n        return False\n\n    def _return_htc_flavour(self, job: str) -&gt; str:\n        \"\"\"\n        Retrieve the HTC flavor for a given job.\n\n        Args:\n            job (str): The job identifier.\n\n        Returns:\n            str: The HTC flavor associated with the job.\n        \"\"\"\n        l_keys = self.dic_all_jobs[job][\"l_keys\"]\n        return nested_get(self.dic_tree, l_keys + [\"htc_flavor\"])\n\n    def _return_abs_path_job(self, job: str) -&gt; tuple[str, str]:\n        \"\"\"\n        Generate the relative and absolute paths for a given job.\n\n        Args:\n            job (str): The job string containing the path to the job file.\n\n        Returns:\n            tuple[str, str]: A tuple containing:\n            - path_job (str): The relative path to the job directory.\n            - abs_path_job (str): The absolute path to the job directory.\n        \"\"\"\n        # Get corresponding path job (remove the python file name)\n        path_job = \"/\".join(job.split(\"/\")[:-1]) + \"/\"\n        abs_path_job = f\"{self.abs_path_study}/{path_job}\"\n        return path_job, abs_path_job\n\n    def _write_sub_files_slurm_docker(\n        self,\n        sub_filename: str,\n        running_jobs: list[str],\n        queuing_jobs: list[str],\n        list_of_jobs: list[str],\n    ) -&gt; tuple[list[str], list[str]]:\n        \"\"\"\n        Generates SLURM submission files for Docker jobs and writes them to disk.\n\n        This method iterates over a list of jobs, checks their status (running, queuing, or\n        completed), and writes the corresponding SLURM submission files for Docker jobs. The\n        submission files are written to disk with a specific naming convention.\n\n        Args:\n            sub_filename (str): The base name for the submission files.\n            running_jobs (list[str]): A list of job identifiers that are currently running.\n            queuing_jobs (list[str]): A list of job identifiers that are currently queuing.\n            list_of_jobs (list[str]): A list of job identifiers to process.\n\n        Returns:\n            tuple[list[str], list[str]]: A tuple containing two lists:\n            - A list of filenames for the generated submission files.\n            - A list of job identifiers that were updated.\n        \"\"\"\n        l_filenames = []\n        list_of_jobs_updated = []\n        for idx_job, job in enumerate(list_of_jobs):\n            path_job, abs_path_job = self._return_abs_path_job(job)\n\n            # Test if job is running, queuing or completed\n            if self._test_job(job, path_job, running_jobs, queuing_jobs):\n                filename_sub = f\"{sub_filename.split('.sub')[0]}_{idx_job}.sub\"\n\n                # Get job GPU request\n                l_keys = self.dic_all_jobs[job][\"l_keys\"]\n                # Ensure GPU is defined and set it to False if not\n                if \"request_gpu\" not in nested_get(self.dic_tree, l_keys):\n                    gpu = nested_set(self.dic_tree, l_keys + [\"request_gpu\"], False)\n                gpu = nested_get(self.dic_tree, l_keys + [\"request_gpu\"])\n\n                # Write the submission files\n                # ! Careful, I implemented a fix for path due to the temporary home recovery folder\n                logging.info(f'Writing submission file for node \"{abs_path_job}\"')\n                fix = True\n                Sub = self.dic_submission[\"slurm_docker\"](\n                    filename_sub, abs_path_job, gpu, self.dic_tree[\"container_image\"], fix=fix\n                )\n                # Create folder if it does not exist\n                folder = \"/\".join(Sub.sub_filename.split(\"/\")[:-1])\n                Path(folder).mkdir(parents=True, exist_ok=True)\n                with open(Sub.sub_filename, \"w\") as fid:\n                    fid.write(Sub.head + \"\\n\")\n                    fid.write(Sub.body + \"\\n\")\n                    fid.write(Sub.tail + \"\\n\")\n\n                l_filenames.append(Sub.sub_filename)\n                list_of_jobs_updated.append(job)\n        return l_filenames, list_of_jobs_updated\n\n    def _get_Sub(\n        self, job: str, submission_type: str, sub_filename: str, abs_path_job: str, gpu: bool\n    ) -&gt; LocalPC | HTC | HTCDocker | Slurm | SlurmDocker:\n        \"\"\"\n        Generate a submission object based on the specified submission type.\n\n        Args:\n            job (str): The job identifier.\n            submission_type (str): The type of submission (e.g., \"slurm\", \"htc\", \"htc_docker\",\n                \"slurm_docker\", \"local\").\n            sub_filename (str): The submission filename.\n            abs_path_job (str): The absolute path to the job.\n            gpu (bool): Sets if a GPU must be requested for the submission.\n\n        Returns:\n            LocalPC | HTC | HTCDocker | Slurm | SlurmDocker: An instance of the appropriate\n                submission class.\n\n        Raises:\n            ValueError: If the submission type is not valid or if the container_image is not defined\n                in the tree for docker submissions.\n        \"\"\"\n        match submission_type:\n            case \"slurm\":\n                return self.dic_submission[submission_type](sub_filename, abs_path_job, gpu)\n            case \"htc\":\n                return self.dic_submission[submission_type](\n                    sub_filename, abs_path_job, gpu, self._return_htc_flavour(job)\n                )\n            case w if w in [\"htc_docker\", \"slurm_docker\"]:\n                # Path to singularity image\n                if (\n                    \"container_image\" in self.dic_tree\n                    and self.dic_tree[\"container_image\"] is not None\n                ):\n                    self.path_image = self.dic_tree[\"container_image\"]\n                else:\n                    raise ValueError(\n                        \"Error: container_image is not defined in the tree. Please define it in the\"\n                        \" config.yaml file.\"\n                    )\n\n                if submission_type == \"htc_docker\":\n                    return self.dic_submission[submission_type](\n                        sub_filename,\n                        abs_path_job,\n                        gpu,\n                        self.path_image,\n                        self._return_htc_flavour(job),\n                    )\n                else:\n                    return self.dic_submission[submission_type](\n                        sub_filename, abs_path_job, gpu, self.path_image\n                    )\n            case \"local\":\n                return self.dic_submission[submission_type](sub_filename, abs_path_job)\n            case _:\n                raise ValueError(f\"Error: {submission_type} is not a valid submission mode\")\n\n    def _write_sub_file(\n        self,\n        sub_filename: str,\n        running_jobs: list[str],\n        queuing_jobs: list[str],\n        list_of_jobs: list[str],\n        submission_type: str,\n    ) -&gt; tuple[list[str], list[str]]:\n        \"\"\"\n        Writes a submission file for a list of jobs and returns the updated list of jobs.\n\n        Args:\n            sub_filename (str): The filename for the submission file.\n            running_jobs (list[str]): List of currently running jobs.\n            queuing_jobs (list[str]): List of currently queuing jobs.\n            list_of_jobs (list[str]): List of jobs to be submitted.\n            submission_type (str): The type of submission.\n\n        Returns:\n            tuple[list[str], list[str]]: A tuple containing:\n            - A list with the submission filename if the file was created, otherwise an empty list.\n            - An updated list of jobs that were included in the submission file.\n        \"\"\"\n        # Flag to know if the file can be submitted (at least one job in it)\n        ok_to_submit = False\n\n        # Flat to know if the header has been written\n        header_written = False\n\n        # Create folder to the submission file if it does not exist\n        os.makedirs(\"/\".join(sub_filename.split(\"/\")[:-1]), exist_ok=True)\n\n        # Updated list of jobs (without unsubmitted jobs)\n        list_of_jobs_updated = []\n\n        # Write the submission file\n        Sub = None\n        with open(sub_filename, \"w\") as fid:\n            for job in list_of_jobs:\n                # Get corresponding path job (remove the python file name)\n                path_job, abs_path_job = self._return_abs_path_job(job)\n\n                # Test if job is running, queuing or completed\n                if self._test_job(job, path_job, running_jobs, queuing_jobs):\n                    logging.info(f'Writing submission command for node \"{abs_path_job}\"')\n\n                    # Get job GPU request\n                    l_keys = self.dic_all_jobs[job][\"l_keys\"]\n                    # Ensure GPU is defined, and set it to False if not\n                    if \"request_gpu\" not in nested_get(self.dic_tree, l_keys):\n                        gpu = nested_set(self.dic_tree, l_keys + [\"request_gpu\"], False)\n                    gpu = nested_get(self.dic_tree, l_keys + [\"request_gpu\"])\n\n                    # Get Submission object\n                    Sub = self._get_Sub(job, submission_type, sub_filename, abs_path_job, gpu)\n\n                    # Take the first job as reference for head\n                    if not header_written:\n                        fid.write(Sub.head + \"\\n\")\n                        header_written = True\n\n                    # Write instruction for submission\n                    fid.write(Sub.body + \"\\n\")\n\n                    # Append job to list_of_jobs_updated\n                    list_of_jobs_updated.append(job)\n\n            # Tail instruction\n            if Sub is not None:\n                fid.write(Sub.tail + \"\\n\")\n                ok_to_submit = True\n\n        if not ok_to_submit:\n            os.remove(sub_filename)\n\n        return ([sub_filename], list_of_jobs_updated) if ok_to_submit else ([], [])\n\n    def _write_sub_files(\n        self,\n        sub_filename: str,\n        running_jobs: list[str],\n        queuing_jobs: list[str],\n        list_of_jobs: list[str],\n        submission_type: str,\n    ) -&gt; tuple[list[str], list[str]]:\n        \"\"\"\n        Writes submission files based on the specified submission type.\n\n        Args:\n            sub_filename (str): The name of the submission file to be created.\n            running_jobs (list[str]): A list of currently running jobs.\n            queuing_jobs (list[str]): A list of jobs that are queued.\n            list_of_jobs (list[str]): A list of all jobs to be submitted.\n            submission_type (str): The type of submission system being used (e.g., \"slurm_docker\").\n\n        Returns:\n            tuple[list[str], list[str]]: A tuple containing two lists:\n                - Updated list of running jobs.\n                - Updated list of queuing jobs.\n        \"\"\"\n        # Slurm docker is a peculiar case as one submission file must be created per job\n        if submission_type == \"slurm_docker\":\n            return self._write_sub_files_slurm_docker(\n                sub_filename, running_jobs, queuing_jobs, list_of_jobs\n            )\n\n        else:\n            return self._write_sub_file(\n                sub_filename,\n                running_jobs,\n                queuing_jobs,\n                list_of_jobs,\n                submission_type,\n            )\n\n    def write_sub_files(\n        self, dic_summary_by_gen: Optional[dict[int, dict[str, int]]] = None\n    ) -&gt; dict:\n        \"\"\"\n        Generates and writes submission files for jobs based on their submission type.\n\n        This method categorizes jobs to be submitted by their submission type, writes the\n        corresponding submission files, and returns a dictionary containing the submission\n        files and their associated job GPU requests.\n\n        Returns:\n            dict: A dictionary where keys are submission types and values are tuples containing\n                a list of updated jobs and a list of submission filenames.\n        \"\"\"\n        running_jobs, queuing_jobs = self._get_state_jobs(verbose=False)\n\n        # Make a dict of all jobs to submit depending on the submission type\n        dic_jobs_to_submit = {key: [] for key in self.dic_submission.keys()}\n        for job in self.l_jobs_to_submit:\n            l_keys = self.dic_all_jobs[job][\"l_keys\"]\n            submission_type = nested_get(self.dic_tree, l_keys + [\"submission_type\"])\n            dic_jobs_to_submit[submission_type].append(job)  # type: ignore\n\n        # Write submission files for each submission type\n        dic_submission_files = {}\n        for submission_type, list_of_jobs in dic_jobs_to_submit.items():\n            if len(list_of_jobs) &gt; 0:\n                # Write submission files\n                l_submission_filenames, list_of_jobs_updated = self._write_sub_files(\n                    self.path_submission_file,\n                    running_jobs,\n                    queuing_jobs,\n                    copy.copy(list_of_jobs),\n                    submission_type,\n                )\n\n                # Record submission files and and GPU requests\n                dic_submission_files[submission_type] = (\n                    list_of_jobs_updated,\n                    l_submission_filenames,\n                )\n\n                # Update dic_summary_by_gen inplace\n                if dic_summary_by_gen is not None:\n                    for job in list_of_jobs:\n                        gen = self.dic_all_jobs[job][\"gen\"]\n                        if job in list_of_jobs_updated:\n                            dic_summary_by_gen[gen][\"submitted_now\"] += 1\n                        else:\n                            dic_summary_by_gen[gen][\"running_or_queuing\"] += 1\n\n        return dic_submission_files\n\n    def _update_job_status_from_hpc_output(\n        self,\n        submit_command: str,\n        submission_type: str,\n        dic_id_to_path_job_temp: dict,\n        list_of_jobs: list[str],\n        idx_submission: int = 0,\n    ) -&gt; tuple[dict, int]:\n        \"\"\"\n        Updates the job status from the HPC output.\n\n        This method parses the output of a job submission command to update the job status\n        in a dictionary mapping job IDs to their respective paths. It supports both HTC and\n        SLURM submission types.\n\n        Args:\n            submit_command (str): The command used to submit the job.\n            submission_type (str): The type of submission system ('htc' or 'slurm').\n            dic_id_to_path_job_temp (dict): A dictionary mapping job IDs to their paths.\n            list_of_jobs (list[str]): A list of job paths.\n            idx_submission (int, optional): The index of the current submission. Defaults to 0.\n\n        Returns:\n            tuple[dict, int]: A tuple containing the updated dictionary and the updated index of\n                submission.\n\n        Raises:\n            RuntimeError: If there is an error in the submission process.\n        \"\"\"\n        process = subprocess.run(\n            submit_command.split(\" \"),\n            capture_output=True,\n        )\n\n        output = process.stdout.decode(\"utf-8\")\n        output_error = process.stderr.decode(\"utf-8\")\n        if \"ERROR\" in output_error:\n            raise RuntimeError(f\"Error in submission: {output_error}\")\n        for line in output.split(\"\\n\"):\n            if \"htc\" in submission_type:\n                if \"cluster\" in line:\n                    cluster_id = int(line.split(\"cluster \")[1][:-1])\n                    dic_id_to_path_job_temp[cluster_id] = self._return_abs_path_job(\n                        list_of_jobs[idx_submission]\n                    )[0]\n                    idx_submission += 1\n            elif \"slurm\" in submission_type:\n                if \"Submitted\" in line:\n                    job_id = int(line.split(\" \")[3])\n                    dic_id_to_path_job_temp[job_id] = self._return_abs_path_job(\n                        list_of_jobs[idx_submission]\n                    )[0]\n                    idx_submission += 1\n\n        return dic_id_to_path_job_temp, idx_submission\n\n    def submit(\n        self, list_of_jobs: list[str], l_submission_filenames: list[str], submission_type: str\n    ) -&gt; None:\n        \"\"\"\n        Submits a list of jobs to the specified submission system.\n\n        Args:\n            list_of_jobs (list[str]): List of job identifiers to be submitted.\n            l_submission_filenames (list[str]): List of filenames containing submission scripts.\n            submission_type (str): Type of submission system to use. Valid options are \"local\",\n                \"htc\", \"slurm\", \"htc_docker\", and \"slurm_docker\".\n\n        Raises:\n            ValueError: If multiple submission files are provided for a non-\"slurm_docker\"\n                submission type.\n            ValueError: If the submission type is not valid.\n\n        Returns:\n            None\n        \"\"\"\n        # Check that the submission file(s) is/are appropriate for the submission mode\n        if len(l_submission_filenames) &gt; 1 and submission_type != \"slurm_docker\":\n            raise ValueError(\n                \"Error: Multiple submission files should not be implemented for this submission\"\n                \" mode\"\n            )\n\n        # Check that at least one job is being submitted\n        if not l_submission_filenames:\n            logging.info(\"No job being submitted.\")\n\n        # Submit\n        dic_id_to_path_job_temp = {}\n        idx_submission = 0\n        for sub_filename in l_submission_filenames:\n            if submission_type == \"local\":\n                os.system(self.dic_submission[submission_type].get_submit_command(sub_filename))\n            elif submission_type in {\"htc\", \"slurm\", \"htc_docker\", \"slurm_docker\"}:\n                submit_command = self.dic_submission[submission_type].get_submit_command(\n                    sub_filename\n                )\n                dic_id_to_path_job_temp, idx_submission = self._update_job_status_from_hpc_output(\n                    submit_command,\n                    submission_type,\n                    dic_id_to_path_job_temp,\n                    list_of_jobs,\n                    idx_submission,\n                )\n            else:\n                raise ValueError(f\"Error: {submission_type} is not a valid submission mode\")\n\n        # Update and write the id-job file\n        if dic_id_to_path_job_temp:\n            assert len(dic_id_to_path_job_temp) == len(list_of_jobs)\n\n        # Merge with the previous id-job file\n        dic_id_to_path_job = self.dic_id_to_path_job\n\n        # Update and write on disk\n        if dic_id_to_path_job is not None:\n            dic_id_to_path_job.update(dic_id_to_path_job_temp)\n            self.dic_id_to_path_job = dic_id_to_path_job\n        elif dic_id_to_path_job_temp:\n            dic_id_to_path_job = dic_id_to_path_job_temp\n            self.dic_id_to_path_job = dic_id_to_path_job\n\n        logging.info(\"Jobs status after submission:\")\n        _, _ = self._get_state_jobs(verbose=True)\n\n    def _get_local_jobs(self) -&gt; list[str]:\n        \"\"\"\n        Retrieves a list of local job paths.\n        This method scans the current processes to identify jobs that are running\n        a script named 'run.sh'. It extracts the job paths and filters them to\n        include only the paths that are relevant to the current study.\n\n        Args:\n            None\n\n        Returns:\n            list[str]: A list of job paths that are currently running and relevant\n            to the study.\n        \"\"\"\n\n        l_path_jobs = []\n        # Warning, does not work at the moment in lxplus...\n        for ps in psutil.pids():\n            try:\n                aux = psutil.Process(ps).cmdline()\n            except Exception:\n                aux = []\n            if len(aux) &gt; 1 and \"run.sh\" in aux[-1]:\n                job = str(Path(aux[-1]).parent)\n\n                # Only get path after name of the study\n                try:\n                    job = job.split(self.study_name)[1]\n                    l_path_jobs.append(f\"{self.study_name}{job}/\")\n                except IndexError:\n                    logging.warning(\n                        \"Some jobs from another study are running. Acquiring the full path as the \"\n                        \"study name is unknown.\"\n                    )\n                    l_path_jobs.append(job)\n        return l_path_jobs\n\n    def _get_condor_jobs(self, status: str, force_query_individually: bool = False) -&gt; list[str]:\n        \"\"\"\n        Retrieve the paths of Condor jobs based on their status.\n\n        Args:\n            status (str): The status of the jobs to retrieve. Can be \"running\" or \"queuing\".\n            force_query_individually (bool, optional): If True, query each job individually if the\n                id-job file is missing. Defaults to False.\n\n        Returns:\n            list[str]: A list of paths to the jobs that match the specified status.\n\n        Raises:\n            ValueError: If the status provided is not \"running\" or \"queuing\".\n\n        Notes:\n            - The method relies on the `condor_q` command to retrieve job information.\n            - If the id-job file is missing and `force_query_individually` is False, jobs not in\n                `dic_id_to_path_job` will be ignored.\n            - If `force_query_individually` is True, the method will query each job individually to\n                retrieve its path.\n            - Warnings are printed if jobs are found that are not in the id-job file or if the\n                id-job file is missing.\n        \"\"\"\n        l_path_jobs = []\n        dic_status = {\"running\": 1, \"queuing\": 2}\n        condor_output = subprocess.run([\"condor_q\"], capture_output=True).stdout.decode(\"utf-8\")\n\n        # Check which jobs are running\n        first_line = True\n        first_missing_job = True\n        for line in condor_output.split(\"\\n\")[4:]:\n            if line == \"\":\n                break\n            jobid = int(line.split(\"ID:\")[1][1:8])\n            condor_status = line.split(\"      \")[1:5]  # Done, Run, Idle, and potentially Hold\n\n            # If job is running/queuing, get the path to the job\n            if condor_status[dic_status[status]] == \"1\":\n                # Get path from dic_id_to_path_job if available\n                if self.dic_id_to_path_job is not None:\n                    if jobid in self.dic_id_to_path_job:\n                        l_path_jobs.append(self.dic_id_to_path_job[jobid])\n                    elif first_missing_job:\n                        logging.warning(\n                            \"Warning, some jobs are queuing/running and are not in the id-job\"\n                            \" file. They may come from another study. Ignoring them.\"\n                        )\n                        first_missing_job = False\n\n                elif force_query_individually:\n                    if first_line:\n                        logging.warning(\n                            \"Warning, some jobs are queuing/running and the id-job file is\"\n                            \" missing... Querying them individually.\"\n                        )\n                        first_line = False\n                    job_details = subprocess.run(\n                        [\"condor_q\", \"-l\", f\"{jobid}\"], capture_output=True\n                    ).stdout.decode(\"utf-8\")\n                    job = job_details.split('Cmd = \"')[1].split(\"run.sh\")[0]\n\n                    # Only get path after master_study\n                    job = job.split(self.study_name)[1]\n                    l_path_jobs.append(f\"{self.study_name}{job}\")\n\n                elif first_line:\n                    logging.warning(\n                        \"Warning, some jobs are queuing/running and the id-job file is\"\n                        \" missing... Ignoring them.\"\n                    )\n                    first_line = False\n\n        return l_path_jobs\n\n    def _get_slurm_jobs(self, status: str, force_query_individually: bool = False) -&gt; list[str]:\n        \"\"\"\n        Retrieve a list of SLURM job paths based on their status.\n\n        This method queries SLURM to get the job IDs and their statuses for the current user.\n        It then attempts to map these job IDs to their corresponding paths using an internal\n        dictionary. If the dictionary is not available or does not contain the job ID, it can\n        optionally query each job individually for its details.\n\n        Args:\n            status (str): The status of the jobs to retrieve. Expected values are \"running\" or\n                \"queuing\".\n            force_query_individually (bool, optional): If True, query each job individually for its\n                details when the job ID is not found in the internal dictionary. Defaults to False.\n\n        Returns:\n            list[str]: A list of job paths corresponding to the specified status.\n        \"\"\"\n        l_path_jobs = []\n        dic_status = {\"running\": \"RUNNING\", \"queuing\": \"PENDING\"}\n        username = (\n            subprocess.run([\"id\", \"-u\", \"-n\"], capture_output=True).stdout.decode(\"utf-8\").strip()\n        )\n        slurm_output = subprocess.run(\n            [\"squeue\", \"-u\", f\"{username}\", \"-t\", dic_status[status]], capture_output=True\n        ).stdout.decode(\"utf-8\")\n\n        # Get job id and details\n        first_line = True\n        first_missing_job = True\n        for line in slurm_output.split(\"\\n\")[1:]:\n            l_split = line.split()\n            if len(l_split) == 0:\n                break\n            jobid = int(l_split[0])\n            slurm_status = l_split[4]  # R or PD  # noqa: F841\n\n            # Get path from dic_id_to_path_job if available\n            if self.dic_id_to_path_job is not None:\n                if jobid in self.dic_id_to_path_job:\n                    l_path_jobs.append(self.dic_id_to_path_job[jobid])\n                elif first_missing_job:\n                    logging.warning(\n                        \"Warning, some jobs are queuing/running and are not in the id-job\"\n                        \" file. They may come from another study. Ignoring them.\"\n                    )\n                    first_missing_job = False\n\n            elif force_query_individually:\n                if first_line:\n                    logging.warning(\n                        \"Warning, some jobs are queuing/running and the id-job file is\"\n                        \" missing... Querying them individually.\"\n                    )\n                    first_line = False\n                job_details = subprocess.run(\n                    [\"scontrol\", \"show\", \"jobid\", \"-dd\", f\"{jobid}\"], capture_output=True\n                ).stdout.decode(\"utf-8\")\n                job = (\n                    job_details.split(\"Command=\")[1].split(\"run.sh\")[0]\n                    if \"run.sh\" in job_details\n                    else job_details.split(\"StdOut=\")[1].split(\"output.txt\")[0]\n                )\n                # Only get path after study_name\n                job = job.split(self.study_name)[1]\n                l_path_jobs.append(f\"{self.study_name}{job}\")\n\n            elif first_line:\n                logging.warning(\n                    \"Warning, some jobs are queuing/running and the id-job file is\"\n                    \" missing... Ignoring them.\"\n                )\n                first_line = False\n\n        return l_path_jobs\n\n    def querying_jobs(\n        self, check_local: bool, check_htc: bool, check_slurm: bool, status: str = \"running\"\n    ) -&gt; list[str]:\n        \"\"\"\n        Queries jobs from different job management systems based on the provided flags and status.\n\n        Args:\n            check_local (bool): If True, check for local jobs.\n            check_htc (bool): If True, check for HTC (High Throughput Computing) jobs.\n            check_slurm (bool): If True, check for SLURM jobs.\n            status (str, optional): The status of the jobs to query. Defaults to \"running\".\n\n        Returns:\n            list[str]: A list of job paths that match the query criteria.\n        \"\"\"\n        # sourcery skip: remove-redundant-if, remove-redundant-pass, swap-nested-ifs\n        l_path_jobs = []\n        if check_local:\n            if status == \"running\":\n                l_path_jobs.extend(self._get_local_jobs())\n            else:\n                # Always empty return as there is no queuing in local pc\n                pass\n\n        if check_htc:\n            l_path_jobs.extend(self._get_condor_jobs(status))\n\n        if check_slurm:\n            l_path_jobs.extend(self._get_slurm_jobs(status))\n\n        return l_path_jobs\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/cluster_submission.html#study_da.submit.cluster_submission.cluster_submission.ClusterSubmission.dic_id_to_path_job","title":"<code>dic_id_to_path_job: dict | None</code>  <code>property</code> <code>writable</code>","text":"<p>Generates a dictionary mapping job IDs to their respective job paths.</p> <p>This method iterates over the list of jobs to submit and constructs a dictionary where the keys are job IDs and the values are the absolute paths to the jobs. If no job IDs are found, the method returns None.</p> <p>Returns:</p> Type Description <code>dict | None</code> <p>dict | None: A dictionary mapping job IDs to job paths, or None if no job IDs are found.</p>"},{"location":"reference/study_da/submit/cluster_submission/cluster_submission.html#study_da.submit.cluster_submission.cluster_submission.ClusterSubmission.dic_submission","title":"<code>dic_submission: dict = {'local': LocalPC, 'htc': HTC, 'htc_docker': HTCDocker, 'slurm': Slurm, 'slurm_docker': SlurmDocker}</code>  <code>instance-attribute</code>","text":"<p>Initialize the ClusterSubmission class.</p> <p>Parameters:</p> Name Type Description Default <code>study_name</code> <code>str</code> <p>The name of the study.</p> required <code>l_jobs_to_submit</code> <code>list[str]</code> <p>A list of job names to submit.</p> required <code>dic_all_jobs</code> <code>dict</code> <p>A dictionary containing all jobs.</p> required <code>dic_tree</code> <code>dict</code> <p>A dictionary representing the job tree structure.</p> required <code>path_submission_file</code> <code>str</code> <p>The path to the submission file.</p> required <code>abs_path_study</code> <code>str</code> <p>The absolute path to the study.</p> required"},{"location":"reference/study_da/submit/cluster_submission/cluster_submission.html#study_da.submit.cluster_submission.cluster_submission.ClusterSubmission.querying_jobs","title":"<code>querying_jobs(check_local, check_htc, check_slurm, status='running')</code>","text":"<p>Queries jobs from different job management systems based on the provided flags and status.</p> <p>Parameters:</p> Name Type Description Default <code>check_local</code> <code>bool</code> <p>If True, check for local jobs.</p> required <code>check_htc</code> <code>bool</code> <p>If True, check for HTC (High Throughput Computing) jobs.</p> required <code>check_slurm</code> <code>bool</code> <p>If True, check for SLURM jobs.</p> required <code>status</code> <code>str</code> <p>The status of the jobs to query. Defaults to \"running\".</p> <code>'running'</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of job paths that match the query criteria.</p> Source code in <code>study_da/submit/cluster_submission/cluster_submission.py</code> <pre><code>def querying_jobs(\n    self, check_local: bool, check_htc: bool, check_slurm: bool, status: str = \"running\"\n) -&gt; list[str]:\n    \"\"\"\n    Queries jobs from different job management systems based on the provided flags and status.\n\n    Args:\n        check_local (bool): If True, check for local jobs.\n        check_htc (bool): If True, check for HTC (High Throughput Computing) jobs.\n        check_slurm (bool): If True, check for SLURM jobs.\n        status (str, optional): The status of the jobs to query. Defaults to \"running\".\n\n    Returns:\n        list[str]: A list of job paths that match the query criteria.\n    \"\"\"\n    # sourcery skip: remove-redundant-if, remove-redundant-pass, swap-nested-ifs\n    l_path_jobs = []\n    if check_local:\n        if status == \"running\":\n            l_path_jobs.extend(self._get_local_jobs())\n        else:\n            # Always empty return as there is no queuing in local pc\n            pass\n\n    if check_htc:\n        l_path_jobs.extend(self._get_condor_jobs(status))\n\n    if check_slurm:\n        l_path_jobs.extend(self._get_slurm_jobs(status))\n\n    return l_path_jobs\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/cluster_submission.html#study_da.submit.cluster_submission.cluster_submission.ClusterSubmission.submit","title":"<code>submit(list_of_jobs, l_submission_filenames, submission_type)</code>","text":"<p>Submits a list of jobs to the specified submission system.</p> <p>Parameters:</p> Name Type Description Default <code>list_of_jobs</code> <code>list[str]</code> <p>List of job identifiers to be submitted.</p> required <code>l_submission_filenames</code> <code>list[str]</code> <p>List of filenames containing submission scripts.</p> required <code>submission_type</code> <code>str</code> <p>Type of submission system to use. Valid options are \"local\", \"htc\", \"slurm\", \"htc_docker\", and \"slurm_docker\".</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If multiple submission files are provided for a non-\"slurm_docker\" submission type.</p> <code>ValueError</code> <p>If the submission type is not valid.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/submit/cluster_submission/cluster_submission.py</code> <pre><code>def submit(\n    self, list_of_jobs: list[str], l_submission_filenames: list[str], submission_type: str\n) -&gt; None:\n    \"\"\"\n    Submits a list of jobs to the specified submission system.\n\n    Args:\n        list_of_jobs (list[str]): List of job identifiers to be submitted.\n        l_submission_filenames (list[str]): List of filenames containing submission scripts.\n        submission_type (str): Type of submission system to use. Valid options are \"local\",\n            \"htc\", \"slurm\", \"htc_docker\", and \"slurm_docker\".\n\n    Raises:\n        ValueError: If multiple submission files are provided for a non-\"slurm_docker\"\n            submission type.\n        ValueError: If the submission type is not valid.\n\n    Returns:\n        None\n    \"\"\"\n    # Check that the submission file(s) is/are appropriate for the submission mode\n    if len(l_submission_filenames) &gt; 1 and submission_type != \"slurm_docker\":\n        raise ValueError(\n            \"Error: Multiple submission files should not be implemented for this submission\"\n            \" mode\"\n        )\n\n    # Check that at least one job is being submitted\n    if not l_submission_filenames:\n        logging.info(\"No job being submitted.\")\n\n    # Submit\n    dic_id_to_path_job_temp = {}\n    idx_submission = 0\n    for sub_filename in l_submission_filenames:\n        if submission_type == \"local\":\n            os.system(self.dic_submission[submission_type].get_submit_command(sub_filename))\n        elif submission_type in {\"htc\", \"slurm\", \"htc_docker\", \"slurm_docker\"}:\n            submit_command = self.dic_submission[submission_type].get_submit_command(\n                sub_filename\n            )\n            dic_id_to_path_job_temp, idx_submission = self._update_job_status_from_hpc_output(\n                submit_command,\n                submission_type,\n                dic_id_to_path_job_temp,\n                list_of_jobs,\n                idx_submission,\n            )\n        else:\n            raise ValueError(f\"Error: {submission_type} is not a valid submission mode\")\n\n    # Update and write the id-job file\n    if dic_id_to_path_job_temp:\n        assert len(dic_id_to_path_job_temp) == len(list_of_jobs)\n\n    # Merge with the previous id-job file\n    dic_id_to_path_job = self.dic_id_to_path_job\n\n    # Update and write on disk\n    if dic_id_to_path_job is not None:\n        dic_id_to_path_job.update(dic_id_to_path_job_temp)\n        self.dic_id_to_path_job = dic_id_to_path_job\n    elif dic_id_to_path_job_temp:\n        dic_id_to_path_job = dic_id_to_path_job_temp\n        self.dic_id_to_path_job = dic_id_to_path_job\n\n    logging.info(\"Jobs status after submission:\")\n    _, _ = self._get_state_jobs(verbose=True)\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/cluster_submission.html#study_da.submit.cluster_submission.cluster_submission.ClusterSubmission.write_sub_files","title":"<code>write_sub_files(dic_summary_by_gen=None)</code>","text":"<p>Generates and writes submission files for jobs based on their submission type.</p> <p>This method categorizes jobs to be submitted by their submission type, writes the corresponding submission files, and returns a dictionary containing the submission files and their associated job GPU requests.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary where keys are submission types and values are tuples containing a list of updated jobs and a list of submission filenames.</p> Source code in <code>study_da/submit/cluster_submission/cluster_submission.py</code> <pre><code>def write_sub_files(\n    self, dic_summary_by_gen: Optional[dict[int, dict[str, int]]] = None\n) -&gt; dict:\n    \"\"\"\n    Generates and writes submission files for jobs based on their submission type.\n\n    This method categorizes jobs to be submitted by their submission type, writes the\n    corresponding submission files, and returns a dictionary containing the submission\n    files and their associated job GPU requests.\n\n    Returns:\n        dict: A dictionary where keys are submission types and values are tuples containing\n            a list of updated jobs and a list of submission filenames.\n    \"\"\"\n    running_jobs, queuing_jobs = self._get_state_jobs(verbose=False)\n\n    # Make a dict of all jobs to submit depending on the submission type\n    dic_jobs_to_submit = {key: [] for key in self.dic_submission.keys()}\n    for job in self.l_jobs_to_submit:\n        l_keys = self.dic_all_jobs[job][\"l_keys\"]\n        submission_type = nested_get(self.dic_tree, l_keys + [\"submission_type\"])\n        dic_jobs_to_submit[submission_type].append(job)  # type: ignore\n\n    # Write submission files for each submission type\n    dic_submission_files = {}\n    for submission_type, list_of_jobs in dic_jobs_to_submit.items():\n        if len(list_of_jobs) &gt; 0:\n            # Write submission files\n            l_submission_filenames, list_of_jobs_updated = self._write_sub_files(\n                self.path_submission_file,\n                running_jobs,\n                queuing_jobs,\n                copy.copy(list_of_jobs),\n                submission_type,\n            )\n\n            # Record submission files and and GPU requests\n            dic_submission_files[submission_type] = (\n                list_of_jobs_updated,\n                l_submission_filenames,\n            )\n\n            # Update dic_summary_by_gen inplace\n            if dic_summary_by_gen is not None:\n                for job in list_of_jobs:\n                    gen = self.dic_all_jobs[job][\"gen\"]\n                    if job in list_of_jobs_updated:\n                        dic_summary_by_gen[gen][\"submitted_now\"] += 1\n                    else:\n                        dic_summary_by_gen[gen][\"running_or_queuing\"] += 1\n\n    return dic_submission_files\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html","title":"submission_statements","text":"<p>This module contains the classes for the submission statements for the different cluster systems.</p>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.HTC","title":"<code>HTC</code>","text":"<p>               Bases: <code>SubmissionStatement</code></p> <p>A class to represent an HTCondor submission statement.</p> <p>Attributes:</p> Name Type Description <code>head</code> <code>str</code> <p>The header of the submission script.</p> <code>body</code> <code>str</code> <p>The body of the submission script.</p> <code>tail</code> <code>str</code> <p>The tail of the submission script.</p> <code>submit_command</code> <code>str</code> <p>The command to submit the job.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initializes the HTCondor submission statement.</p> <code>get_submit_command</code> <p>Returns the command to submit the job.</p> Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>class HTC(SubmissionStatement):\n    \"\"\"\n    A class to represent an HTCondor submission statement.\n\n    Attributes:\n        head (str): The header of the submission script.\n        body (str): The body of the submission script.\n        tail (str): The tail of the submission script.\n        submit_command (str): The command to submit the job.\n\n    Methods:\n        __init__(sub_filename, path_job_folder, gpu, htc_flavor='espresso'):\n            Initializes the HTCondor submission statement.\n        get_submit_command(sub_filename): Returns the command to submit the job.\n    \"\"\"\n\n    def __init__(\n        self, sub_filename: str, path_job_folder: str, gpu: bool, htc_flavor: str = \"espresso\"\n    ):\n        \"\"\"\n        Initializes the HTCondor submission statement.\n\n        Args:\n            sub_filename (str): The name of the submission file.\n            path_job_folder (str): The path to the job folder.\n            gpu (bool): If a GPU must be requested for the submission.\n            htc_flavor (str, optional): The flavor of the HTCondor job. Defaults to \"espresso\".\n        \"\"\"\n        super().__init__(sub_filename, path_job_folder, gpu)\n\n        self.head: str = (\n            \"# This is a HTCondor submission file\\n\"\n            + \"error  = error.txt\\n\"\n            + \"output = output.txt\\n\"\n            + \"log  = log.txt\"\n        )\n        self.body: str = (\n            f\"initialdir = {self.path_job_folder}\\n\"\n            + f\"executable = {self.path_job_folder}/run.sh\\n\"\n            + f\"request_GPUs = {self.request_GPUs}\\n\"\n            + f'+JobFlavour  = \"{htc_flavor}\"\\n'\n            + \"queue\"\n        )\n        self.tail: str = \"# HTC\"\n        self.submit_command: str = self.get_submit_command(sub_filename)\n\n    @staticmethod\n    def get_submit_command(sub_filename: str) -&gt; str:\n        \"\"\"\n        Returns the command to submit the job.\n\n        Args:\n            sub_filename (str): The name of the submission file.\n\n        Returns:\n            str: The command to submit the job.\n        \"\"\"\n        return f\"condor_submit {sub_filename}\"\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.HTC.__init__","title":"<code>__init__(sub_filename, path_job_folder, gpu, htc_flavor='espresso')</code>","text":"<p>Initializes the HTCondor submission statement.</p> <p>Parameters:</p> Name Type Description Default <code>sub_filename</code> <code>str</code> <p>The name of the submission file.</p> required <code>path_job_folder</code> <code>str</code> <p>The path to the job folder.</p> required <code>gpu</code> <code>bool</code> <p>If a GPU must be requested for the submission.</p> required <code>htc_flavor</code> <code>str</code> <p>The flavor of the HTCondor job. Defaults to \"espresso\".</p> <code>'espresso'</code> Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>def __init__(\n    self, sub_filename: str, path_job_folder: str, gpu: bool, htc_flavor: str = \"espresso\"\n):\n    \"\"\"\n    Initializes the HTCondor submission statement.\n\n    Args:\n        sub_filename (str): The name of the submission file.\n        path_job_folder (str): The path to the job folder.\n        gpu (bool): If a GPU must be requested for the submission.\n        htc_flavor (str, optional): The flavor of the HTCondor job. Defaults to \"espresso\".\n    \"\"\"\n    super().__init__(sub_filename, path_job_folder, gpu)\n\n    self.head: str = (\n        \"# This is a HTCondor submission file\\n\"\n        + \"error  = error.txt\\n\"\n        + \"output = output.txt\\n\"\n        + \"log  = log.txt\"\n    )\n    self.body: str = (\n        f\"initialdir = {self.path_job_folder}\\n\"\n        + f\"executable = {self.path_job_folder}/run.sh\\n\"\n        + f\"request_GPUs = {self.request_GPUs}\\n\"\n        + f'+JobFlavour  = \"{htc_flavor}\"\\n'\n        + \"queue\"\n    )\n    self.tail: str = \"# HTC\"\n    self.submit_command: str = self.get_submit_command(sub_filename)\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.HTC.get_submit_command","title":"<code>get_submit_command(sub_filename)</code>  <code>staticmethod</code>","text":"<p>Returns the command to submit the job.</p> <p>Parameters:</p> Name Type Description Default <code>sub_filename</code> <code>str</code> <p>The name of the submission file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The command to submit the job.</p> Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>@staticmethod\ndef get_submit_command(sub_filename: str) -&gt; str:\n    \"\"\"\n    Returns the command to submit the job.\n\n    Args:\n        sub_filename (str): The name of the submission file.\n\n    Returns:\n        str: The command to submit the job.\n    \"\"\"\n    return f\"condor_submit {sub_filename}\"\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.HTCDocker","title":"<code>HTCDocker</code>","text":"<p>               Bases: <code>SubmissionStatement</code></p> <p>A class to represent an HTCondor submission statement using Docker.</p> <p>Attributes:</p> Name Type Description <code>head</code> <code>str</code> <p>The header of the submission script.</p> <code>body</code> <code>str</code> <p>The body of the submission script.</p> <code>tail</code> <code>str</code> <p>The tail of the submission script.</p> <code>submit_command</code> <code>str</code> <p>The command to submit the job.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initializes the HTCondor Docker submission statement.</p> <code>get_submit_command</code> <p>Returns the command to submit the job.</p> Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>class HTCDocker(SubmissionStatement):\n    \"\"\"\n    A class to represent an HTCondor submission statement using Docker.\n\n    Attributes:\n        head (str): The header of the submission script.\n        body (str): The body of the submission script.\n        tail (str): The tail of the submission script.\n        submit_command (str): The command to submit the job.\n\n    Methods:\n        __init__(sub_filename, path_job_folder, gpu, path_image, htc_flavor='espresso'):\n            Initializes the HTCondor Docker submission statement.\n        get_submit_command(sub_filename): Returns the command to submit the job.\n    \"\"\"\n\n    def __init__(\n        self,\n        sub_filename: str,\n        path_job_folder: str,\n        gpu: bool,\n        path_image: str,\n        htc_flavor: str = \"espresso\",\n    ):\n        \"\"\"\n        Initializes the HTCondor Docker submission statement.\n\n        Args:\n            sub_filename (str): The name of the submission file.\n            path_job_folder (str): The path to the job folder.\n            gpu (bool): If a GPU must be requested for the submission.\n            path_image (str): The path to the Docker image.\n            htc_flavor (str, optional): The flavor of the HTCondor job. Defaults to \"espresso\".\n        \"\"\"\n        super().__init__(sub_filename, path_job_folder, gpu)\n\n        self.head: str = (\n            \"# This is a HTCondor submission file using Docker\\n\"\n            + \"error  = error.txt\\n\"\n            + \"output = output.txt\\n\"\n            + \"log  = log.txt\\n\"\n            + \"universe = vanilla\\n\"\n            + \"+SingularityImage =\"\n            + f' \"{path_image}\"'\n        )\n        self.body: str = (\n            f\"initialdir = {self.path_job_folder}\\n\"\n            + f\"executable = {self.path_job_folder}/run.sh\\n\"\n            + f\"request_GPUs = {self.request_GPUs}\\n\"\n            + f'+JobFlavour  = \"{htc_flavor}\"\\n'\n            + \"queue\"\n        )\n        self.tail: str = \"# HTC Docker\"\n        self.submit_command: str = self.get_submit_command(sub_filename)\n\n    @staticmethod\n    def get_submit_command(sub_filename: str) -&gt; str:\n        \"\"\"\n        Returns the command to submit the job.\n\n        Args:\n            sub_filename (str): The name of the submission file.\n\n        Returns:\n            str: The command to submit the job.\n        \"\"\"\n        return f\"condor_submit {sub_filename}\"\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.HTCDocker.__init__","title":"<code>__init__(sub_filename, path_job_folder, gpu, path_image, htc_flavor='espresso')</code>","text":"<p>Initializes the HTCondor Docker submission statement.</p> <p>Parameters:</p> Name Type Description Default <code>sub_filename</code> <code>str</code> <p>The name of the submission file.</p> required <code>path_job_folder</code> <code>str</code> <p>The path to the job folder.</p> required <code>gpu</code> <code>bool</code> <p>If a GPU must be requested for the submission.</p> required <code>path_image</code> <code>str</code> <p>The path to the Docker image.</p> required <code>htc_flavor</code> <code>str</code> <p>The flavor of the HTCondor job. Defaults to \"espresso\".</p> <code>'espresso'</code> Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>def __init__(\n    self,\n    sub_filename: str,\n    path_job_folder: str,\n    gpu: bool,\n    path_image: str,\n    htc_flavor: str = \"espresso\",\n):\n    \"\"\"\n    Initializes the HTCondor Docker submission statement.\n\n    Args:\n        sub_filename (str): The name of the submission file.\n        path_job_folder (str): The path to the job folder.\n        gpu (bool): If a GPU must be requested for the submission.\n        path_image (str): The path to the Docker image.\n        htc_flavor (str, optional): The flavor of the HTCondor job. Defaults to \"espresso\".\n    \"\"\"\n    super().__init__(sub_filename, path_job_folder, gpu)\n\n    self.head: str = (\n        \"# This is a HTCondor submission file using Docker\\n\"\n        + \"error  = error.txt\\n\"\n        + \"output = output.txt\\n\"\n        + \"log  = log.txt\\n\"\n        + \"universe = vanilla\\n\"\n        + \"+SingularityImage =\"\n        + f' \"{path_image}\"'\n    )\n    self.body: str = (\n        f\"initialdir = {self.path_job_folder}\\n\"\n        + f\"executable = {self.path_job_folder}/run.sh\\n\"\n        + f\"request_GPUs = {self.request_GPUs}\\n\"\n        + f'+JobFlavour  = \"{htc_flavor}\"\\n'\n        + \"queue\"\n    )\n    self.tail: str = \"# HTC Docker\"\n    self.submit_command: str = self.get_submit_command(sub_filename)\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.HTCDocker.get_submit_command","title":"<code>get_submit_command(sub_filename)</code>  <code>staticmethod</code>","text":"<p>Returns the command to submit the job.</p> <p>Parameters:</p> Name Type Description Default <code>sub_filename</code> <code>str</code> <p>The name of the submission file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The command to submit the job.</p> Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>@staticmethod\ndef get_submit_command(sub_filename: str) -&gt; str:\n    \"\"\"\n    Returns the command to submit the job.\n\n    Args:\n        sub_filename (str): The name of the submission file.\n\n    Returns:\n        str: The command to submit the job.\n    \"\"\"\n    return f\"condor_submit {sub_filename}\"\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.LocalPC","title":"<code>LocalPC</code>","text":"<p>               Bases: <code>SubmissionStatement</code></p> <p>A class to represent a local PC submission statement.</p> <p>Attributes:</p> Name Type Description <code>head</code> <code>str</code> <p>The header of the submission script.</p> <code>body</code> <code>str</code> <p>The body of the submission script.</p> <code>tail</code> <code>str</code> <p>The tail of the submission script.</p> <code>submit_command</code> <code>str</code> <p>The command to submit the job.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initializes the LocalPC submission statement.</p> <code>get_submit_command</code> <p>Returns the command to submit the job.</p> Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>class LocalPC(SubmissionStatement):\n    \"\"\"\n    A class to represent a local PC submission statement.\n\n    Attributes:\n        head (str): The header of the submission script.\n        body (str): The body of the submission script.\n        tail (str): The tail of the submission script.\n        submit_command (str): The command to submit the job.\n\n    Methods:\n        __init__(sub_filename, path_job_folder, gpu=None): Initializes the LocalPC submission\n            statement.\n        get_submit_command(sub_filename): Returns the command to submit the job.\n    \"\"\"\n\n    def __init__(self, sub_filename: str, path_job_folder: str, gpu: bool | None = None):\n        \"\"\"\n        Initializes the LocalPC submission statement.\n\n        Args:\n            sub_filename (str): The name of the submission file.\n            path_job_folder (str): The path to the job folder.\n            gpu (bool, optional): If a GPU must be requested for the submission. Defaults to None.\n        \"\"\"\n        super().__init__(sub_filename, path_job_folder, gpu)\n\n        self.head: str = \"# Running on local pc\"\n        self.body: str = f\"bash {self.path_job_folder}/run.sh &amp;\"\n        self.tail: str = \"# Local pc\"\n        self.submit_command: str = self.get_submit_command(sub_filename)\n\n    @staticmethod\n    def get_submit_command(sub_filename: str) -&gt; str:\n        \"\"\"\n        Returns the command to submit the job.\n\n        Args:\n            sub_filename (str): The name of the submission file.\n\n        Returns:\n            str: The command to submit the job.\n        \"\"\"\n        return f\"bash {sub_filename}\"\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.LocalPC.__init__","title":"<code>__init__(sub_filename, path_job_folder, gpu=None)</code>","text":"<p>Initializes the LocalPC submission statement.</p> <p>Parameters:</p> Name Type Description Default <code>sub_filename</code> <code>str</code> <p>The name of the submission file.</p> required <code>path_job_folder</code> <code>str</code> <p>The path to the job folder.</p> required <code>gpu</code> <code>bool</code> <p>If a GPU must be requested for the submission. Defaults to None.</p> <code>None</code> Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>def __init__(self, sub_filename: str, path_job_folder: str, gpu: bool | None = None):\n    \"\"\"\n    Initializes the LocalPC submission statement.\n\n    Args:\n        sub_filename (str): The name of the submission file.\n        path_job_folder (str): The path to the job folder.\n        gpu (bool, optional): If a GPU must be requested for the submission. Defaults to None.\n    \"\"\"\n    super().__init__(sub_filename, path_job_folder, gpu)\n\n    self.head: str = \"# Running on local pc\"\n    self.body: str = f\"bash {self.path_job_folder}/run.sh &amp;\"\n    self.tail: str = \"# Local pc\"\n    self.submit_command: str = self.get_submit_command(sub_filename)\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.LocalPC.get_submit_command","title":"<code>get_submit_command(sub_filename)</code>  <code>staticmethod</code>","text":"<p>Returns the command to submit the job.</p> <p>Parameters:</p> Name Type Description Default <code>sub_filename</code> <code>str</code> <p>The name of the submission file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The command to submit the job.</p> Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>@staticmethod\ndef get_submit_command(sub_filename: str) -&gt; str:\n    \"\"\"\n    Returns the command to submit the job.\n\n    Args:\n        sub_filename (str): The name of the submission file.\n\n    Returns:\n        str: The command to submit the job.\n    \"\"\"\n    return f\"bash {sub_filename}\"\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.Slurm","title":"<code>Slurm</code>","text":"<p>               Bases: <code>SubmissionStatement</code></p> <p>A class to represent a SLURM submission statement.</p> <p>Attributes:</p> Name Type Description <code>head</code> <code>str</code> <p>The header of the submission script.</p> <code>body</code> <code>str</code> <p>The body of the submission script.</p> <code>tail</code> <code>str</code> <p>The tail of the submission script.</p> <code>submit_command</code> <code>str</code> <p>The command to submit the job.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initializes the SLURM submission statement.</p> <code>get_submit_command</code> <p>Returns the command to submit the job.</p> Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>class Slurm(SubmissionStatement):\n    \"\"\"\n    A class to represent a SLURM submission statement.\n\n    Attributes:\n        head (str): The header of the submission script.\n        body (str): The body of the submission script.\n        tail (str): The tail of the submission script.\n        submit_command (str): The command to submit the job.\n\n    Methods:\n        __init__(sub_filename, path_job_folder, gpu): Initializes the SLURM submission statement.\n        get_submit_command(sub_filename): Returns the command to submit the job.\n    \"\"\"\n\n    def __init__(self, sub_filename: str, path_job_folder: str, gpu: bool | None):\n        \"\"\"\n        Initializes the SLURM submission statement.\n\n        Args:\n            sub_filename (str): The name of the submission file.\n            path_job_folder (str): The path to the job folder.\n            gpu (bool|None): If a GPU must be requested for the submission.\n        \"\"\"\n        super().__init__(sub_filename, path_job_folder, gpu)\n\n        self.head: str = \"# Running on SLURM \"\n        if self.slurm_queue_statement != \"\":\n            queue_statement = self.slurm_queue_statement.split(\" \")[1]\n        else:\n            queue_statement = self.slurm_queue_statement\n        self.body: str = (\n            f\"sbatch --ntasks=2 {queue_statement} \"\n            f\"--output=output.txt --error=error.txt \"\n            f\"--gres=gpu:{self.request_GPUs} {self.path_job_folder}/run.sh\"\n        )\n        self.tail: str = \"# SLURM\"\n        self.submit_command: str = self.get_submit_command(sub_filename)\n\n    @staticmethod\n    def get_submit_command(sub_filename: str) -&gt; str:\n        \"\"\"\n        Returns the command to submit the job.\n\n        Args:\n            sub_filename (str): The name of the submission file.\n\n        Returns:\n            str: The command to submit the job.\n        \"\"\"\n        return f\"bash {sub_filename}\"\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.Slurm.__init__","title":"<code>__init__(sub_filename, path_job_folder, gpu)</code>","text":"<p>Initializes the SLURM submission statement.</p> <p>Parameters:</p> Name Type Description Default <code>sub_filename</code> <code>str</code> <p>The name of the submission file.</p> required <code>path_job_folder</code> <code>str</code> <p>The path to the job folder.</p> required <code>gpu</code> <code>bool | None</code> <p>If a GPU must be requested for the submission.</p> required Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>def __init__(self, sub_filename: str, path_job_folder: str, gpu: bool | None):\n    \"\"\"\n    Initializes the SLURM submission statement.\n\n    Args:\n        sub_filename (str): The name of the submission file.\n        path_job_folder (str): The path to the job folder.\n        gpu (bool|None): If a GPU must be requested for the submission.\n    \"\"\"\n    super().__init__(sub_filename, path_job_folder, gpu)\n\n    self.head: str = \"# Running on SLURM \"\n    if self.slurm_queue_statement != \"\":\n        queue_statement = self.slurm_queue_statement.split(\" \")[1]\n    else:\n        queue_statement = self.slurm_queue_statement\n    self.body: str = (\n        f\"sbatch --ntasks=2 {queue_statement} \"\n        f\"--output=output.txt --error=error.txt \"\n        f\"--gres=gpu:{self.request_GPUs} {self.path_job_folder}/run.sh\"\n    )\n    self.tail: str = \"# SLURM\"\n    self.submit_command: str = self.get_submit_command(sub_filename)\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.Slurm.get_submit_command","title":"<code>get_submit_command(sub_filename)</code>  <code>staticmethod</code>","text":"<p>Returns the command to submit the job.</p> <p>Parameters:</p> Name Type Description Default <code>sub_filename</code> <code>str</code> <p>The name of the submission file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The command to submit the job.</p> Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>@staticmethod\ndef get_submit_command(sub_filename: str) -&gt; str:\n    \"\"\"\n    Returns the command to submit the job.\n\n    Args:\n        sub_filename (str): The name of the submission file.\n\n    Returns:\n        str: The command to submit the job.\n    \"\"\"\n    return f\"bash {sub_filename}\"\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.SlurmDocker","title":"<code>SlurmDocker</code>","text":"<p>               Bases: <code>SubmissionStatement</code></p> <p>A class to represent a SLURM submission statement using Docker.</p> <p>Attributes:</p> Name Type Description <code>head</code> <code>str</code> <p>The header of the submission script.</p> <code>body</code> <code>str</code> <p>The body of the submission script.</p> <code>tail</code> <code>str</code> <p>The tail of the submission script.</p> <code>submit_command</code> <code>str</code> <p>The command to submit the job.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initializes the SLURM Docker submission statement.</p> <code>get_submit_command</code> <p>Returns the command to submit the job.</p> Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>class SlurmDocker(SubmissionStatement):\n    \"\"\"\n    A class to represent a SLURM submission statement using Docker.\n\n    Attributes:\n        head (str): The header of the submission script.\n        body (str): The body of the submission script.\n        tail (str): The tail of the submission script.\n        submit_command (str): The command to submit the job.\n\n    Methods:\n        __init__(sub_filename, path_job_folder, gpu, path_image, fix=False): Initializes the\n            SLURM Docker submission statement.\n        get_submit_command(sub_filename): Returns the command to submit the job.\n    \"\"\"\n\n    def __init__(\n        self,\n        sub_filename: str,\n        path_job_folder: str,\n        gpu: bool,\n        path_image: str,  # type: ignore\n        fix: bool = False,\n    ):\n        \"\"\"\n        Initializes the SLURM Docker submission statement.\n\n        Args:\n            sub_filename (str): The name of the submission file.\n            path_job_folder (str): The path to the job folder.\n            gpu (bool): If a GPU must be requested for the submission.\n            path_image (str): The path to the Docker image.\n            fix (bool, optional): A flag to apply a fix for INFN. Defaults to False.\n        \"\"\"\n        super().__init__(sub_filename, path_job_folder, gpu)\n\n        # ! Ugly fix, will need to be removed when INFN is fixed\n        if fix:\n            to_replace = \"/storage-hpc/gpfs_data/HPC/home_recovery\"\n            replacement = \"/home/HPC\"\n            self.path_job_folder: str = self.path_job_folder.replace(to_replace, replacement)\n            path_image: str = path_image.replace(to_replace, replacement)\n            self.sub_filename: str = self.sub_filename.replace(to_replace, replacement)\n\n        self.head: str = (\n            \"#!/bin/bash\\n\"\n            + \"# This is a SLURM submission file using Docker\\n\"\n            + self.slurm_queue_statement\n            + \"\\n\"\n            + f\"#SBATCH --output={self.path_job_folder}/output.txt\\n\"\n            + f\"#SBATCH --error={self.path_job_folder}/error.txt\\n\"\n            + \"#SBATCH --ntasks=2\\n\"\n            + f\"#SBATCH --gres=gpu:{self.request_GPUs}\"\n        )\n        self.body: str = f\"singularity exec {path_image} {self.path_job_folder}/run.sh\"\n        self.tail: str = \"# SLURM Docker\"\n        self.submit_command: str = self.get_submit_command(sub_filename)\n\n    @staticmethod\n    def get_submit_command(sub_filename: str) -&gt; str:\n        \"\"\"\n        Returns the command to submit the job.\n\n        Args:\n            sub_filename (str): The name of the submission file.\n\n        Returns:\n            str: The command to submit the job.\n        \"\"\"\n        return f\"sbatch {sub_filename}\"\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.SlurmDocker.__init__","title":"<code>__init__(sub_filename, path_job_folder, gpu, path_image, fix=False)</code>","text":"<p>Initializes the SLURM Docker submission statement.</p> <p>Parameters:</p> Name Type Description Default <code>sub_filename</code> <code>str</code> <p>The name of the submission file.</p> required <code>path_job_folder</code> <code>str</code> <p>The path to the job folder.</p> required <code>gpu</code> <code>bool</code> <p>If a GPU must be requested for the submission.</p> required <code>path_image</code> <code>str</code> <p>The path to the Docker image.</p> required <code>fix</code> <code>bool</code> <p>A flag to apply a fix for INFN. Defaults to False.</p> <code>False</code> Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>def __init__(\n    self,\n    sub_filename: str,\n    path_job_folder: str,\n    gpu: bool,\n    path_image: str,  # type: ignore\n    fix: bool = False,\n):\n    \"\"\"\n    Initializes the SLURM Docker submission statement.\n\n    Args:\n        sub_filename (str): The name of the submission file.\n        path_job_folder (str): The path to the job folder.\n        gpu (bool): If a GPU must be requested for the submission.\n        path_image (str): The path to the Docker image.\n        fix (bool, optional): A flag to apply a fix for INFN. Defaults to False.\n    \"\"\"\n    super().__init__(sub_filename, path_job_folder, gpu)\n\n    # ! Ugly fix, will need to be removed when INFN is fixed\n    if fix:\n        to_replace = \"/storage-hpc/gpfs_data/HPC/home_recovery\"\n        replacement = \"/home/HPC\"\n        self.path_job_folder: str = self.path_job_folder.replace(to_replace, replacement)\n        path_image: str = path_image.replace(to_replace, replacement)\n        self.sub_filename: str = self.sub_filename.replace(to_replace, replacement)\n\n    self.head: str = (\n        \"#!/bin/bash\\n\"\n        + \"# This is a SLURM submission file using Docker\\n\"\n        + self.slurm_queue_statement\n        + \"\\n\"\n        + f\"#SBATCH --output={self.path_job_folder}/output.txt\\n\"\n        + f\"#SBATCH --error={self.path_job_folder}/error.txt\\n\"\n        + \"#SBATCH --ntasks=2\\n\"\n        + f\"#SBATCH --gres=gpu:{self.request_GPUs}\"\n    )\n    self.body: str = f\"singularity exec {path_image} {self.path_job_folder}/run.sh\"\n    self.tail: str = \"# SLURM Docker\"\n    self.submit_command: str = self.get_submit_command(sub_filename)\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.SlurmDocker.get_submit_command","title":"<code>get_submit_command(sub_filename)</code>  <code>staticmethod</code>","text":"<p>Returns the command to submit the job.</p> <p>Parameters:</p> Name Type Description Default <code>sub_filename</code> <code>str</code> <p>The name of the submission file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The command to submit the job.</p> Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>@staticmethod\ndef get_submit_command(sub_filename: str) -&gt; str:\n    \"\"\"\n    Returns the command to submit the job.\n\n    Args:\n        sub_filename (str): The name of the submission file.\n\n    Returns:\n        str: The command to submit the job.\n    \"\"\"\n    return f\"sbatch {sub_filename}\"\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.SubmissionStatement","title":"<code>SubmissionStatement</code>","text":"<p>A master class to represent a submission statement for job scheduling.</p> <p>Attributes:</p> Name Type Description <code>sub_filename</code> <code>str</code> <p>The name of the submission file.</p> <code>path_job_folder</code> <code>str</code> <p>The path to the job folder, ensuring no trailing slash.</p> <code>request_GPUs</code> <code>int</code> <p>The number of GPUs requested.</p> <code>slurm_queue_statement</code> <code>str</code> <p>The SLURM queue statement.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>str, path_job_folder: str, gpu: bool | None): Initializes the SubmissionStatement with the given filename, job folder path, and gpu request.</p> Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>class SubmissionStatement:\n    \"\"\"\n    A master class to represent a submission statement for job scheduling.\n\n    Attributes:\n        sub_filename (str): The name of the submission file.\n        path_job_folder (str): The path to the job folder, ensuring no trailing slash.\n        request_GPUs (int): The number of GPUs requested.\n        slurm_queue_statement (str): The SLURM queue statement.\n\n    Methods:\n        __init__(sub_filename: str, path_job_folder: str, gpu: bool | None):\n            Initializes the SubmissionStatement with the given filename, job folder path, and\n            gpu request.\n    \"\"\"\n\n    def __init__(self, sub_filename: str, path_job_folder: str, gpu: bool | None):\n        \"\"\"\n        Initialize the submission statement configuration.\n\n        Args:\n            sub_filename (str): The name of the submission file.\n            path_job_folder (str): The path to the job folder. Trailing slash will be removed if\n                present.\n            gpu (bool | None): If a GPU must be requested.\n\n        Attributes:\n            sub_filename (str): The name of the submission file.\n            path_job_folder (str): The path to the job folder without trailing slash.\n            request_GPUs (int): Number of GPUs requested. 1 if gpu is True, 0 otherwise.\n            slurm_queue_statement (str): SLURM queue statement. Empty if gpu requested, otherwise\n                set to '#SBATCH --partition=slurm_hpc_acc'.\n        \"\"\"\n        self.sub_filename: str = sub_filename\n        self.path_job_folder: str = (\n            path_job_folder[:-1] if path_job_folder[-1] == \"/\" else path_job_folder\n        )\n\n        # GPU configuration\n        if gpu:\n            self.request_GPUs: int = 1\n            self.slurm_queue_statement: str = \"\"\n        else:\n            self.request_GPUs: int = 0\n            self.slurm_queue_statement: str = \"#SBATCH --partition=slurm_hpc_acc\"\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.SubmissionStatement.__init__","title":"<code>__init__(sub_filename, path_job_folder, gpu)</code>","text":"<p>Initialize the submission statement configuration.</p> <p>Parameters:</p> Name Type Description Default <code>sub_filename</code> <code>str</code> <p>The name of the submission file.</p> required <code>path_job_folder</code> <code>str</code> <p>The path to the job folder. Trailing slash will be removed if present.</p> required <code>gpu</code> <code>bool | None</code> <p>If a GPU must be requested.</p> required <p>Attributes:</p> Name Type Description <code>sub_filename</code> <code>str</code> <p>The name of the submission file.</p> <code>path_job_folder</code> <code>str</code> <p>The path to the job folder without trailing slash.</p> <code>request_GPUs</code> <code>int</code> <p>Number of GPUs requested. 1 if gpu is True, 0 otherwise.</p> <code>slurm_queue_statement</code> <code>str</code> <p>SLURM queue statement. Empty if gpu requested, otherwise set to '#SBATCH --partition=slurm_hpc_acc'.</p> Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>def __init__(self, sub_filename: str, path_job_folder: str, gpu: bool | None):\n    \"\"\"\n    Initialize the submission statement configuration.\n\n    Args:\n        sub_filename (str): The name of the submission file.\n        path_job_folder (str): The path to the job folder. Trailing slash will be removed if\n            present.\n        gpu (bool | None): If a GPU must be requested.\n\n    Attributes:\n        sub_filename (str): The name of the submission file.\n        path_job_folder (str): The path to the job folder without trailing slash.\n        request_GPUs (int): Number of GPUs requested. 1 if gpu is True, 0 otherwise.\n        slurm_queue_statement (str): SLURM queue statement. Empty if gpu requested, otherwise\n            set to '#SBATCH --partition=slurm_hpc_acc'.\n    \"\"\"\n    self.sub_filename: str = sub_filename\n    self.path_job_folder: str = (\n        path_job_folder[:-1] if path_job_folder[-1] == \"/\" else path_job_folder\n    )\n\n    # GPU configuration\n    if gpu:\n        self.request_GPUs: int = 1\n        self.slurm_queue_statement: str = \"\"\n    else:\n        self.request_GPUs: int = 0\n        self.slurm_queue_statement: str = \"#SBATCH --partition=slurm_hpc_acc\"\n</code></pre>"},{"location":"reference/study_da/utils/index.html","title":"utils","text":""},{"location":"reference/study_da/utils/index.html#study_da.utils.clean_dic","title":"<code>clean_dic(o)</code>","text":"<p>Convert numpy types to standard types in a nested dictionary containing number and lists.</p> <p>Parameters:</p> Name Type Description Default <code>o</code> <code>Any</code> <p>The object to convert.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def clean_dic(o: Any) -&gt; None:\n    \"\"\"Convert numpy types to standard types in a nested dictionary containing number and lists.\n\n    Args:\n        o (Any): The object to convert.\n\n    Returns:\n        None\n    \"\"\"\n    if not isinstance(o, dict):\n        return\n    for k, v in o.items():\n        if isinstance(v, np.generic):\n            o[k] = v.item()\n        elif isinstance(v, list):\n            for i, x in enumerate(v):\n                if isinstance(x, np.generic):\n                    v[i] = x.item()\n                if isinstance(x, dict):\n                    clean_dic(x)\n        else:\n            clean_dic(v)\n</code></pre>"},{"location":"reference/study_da/utils/index.html#study_da.utils.find_item_in_dic","title":"<code>find_item_in_dic(obj, key)</code>","text":"<p>Find an item in a nested dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>dict</code> <p>The nested dictionary.</p> required <code>key</code> <code>str</code> <p>The key to find in the nested dictionary.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The value corresponding to the key in the nested dictionary.</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def find_item_in_dic(obj: dict, key: str) -&gt; Any:\n    \"\"\"Find an item in a nested dictionary.\n\n    Args:\n        obj (dict): The nested dictionary.\n        key (str): The key to find in the nested dictionary.\n\n    Returns:\n        Any: The value corresponding to the key in the nested dictionary.\n\n    \"\"\"\n    if key in obj:\n        return obj[key]\n    for v in obj.values():\n        if isinstance(v, dict):\n            item = find_item_in_dic(v, key)\n            if item is not None:\n                return item\n</code></pre>"},{"location":"reference/study_da/utils/index.html#study_da.utils.load_dic_from_path","title":"<code>load_dic_from_path(path, ryaml=None)</code>","text":"<p>Load a dictionary from a yaml file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the yaml file.</p> required <code>ryaml</code> <code>YAML</code> <p>The yaml reader.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[dict, YAML]</code> <p>tuple[dict, ruamel.yaml.YAML]: The dictionary and the yaml reader.</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def load_dic_from_path(\n    path: str, ryaml: ruamel.yaml.YAML | None = None\n) -&gt; tuple[dict, ruamel.yaml.YAML]:\n    \"\"\"Load a dictionary from a yaml file.\n\n    Args:\n        path (str): The path to the yaml file.\n        ryaml (ruamel.yaml.YAML): The yaml reader.\n\n    Returns:\n        tuple[dict, ruamel.yaml.YAML]: The dictionary and the yaml reader.\n\n    \"\"\"\n\n    if ryaml is None:\n        # Initialize yaml reader\n        ryaml = ruamel.yaml.YAML()\n\n    # Load dic\n    with open(path, \"r\") as fid:\n        dic = ryaml.load(fid)\n\n    return dic, ryaml\n</code></pre>"},{"location":"reference/study_da/utils/index.html#study_da.utils.load_template_configuration_as_dic","title":"<code>load_template_configuration_as_dic(template_configuration_name)</code>","text":"<p>Load a template configuration as a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>template_configuration_name</code> <code>str</code> <p>The name of the template configuration.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>tuple[dict, YAML]</code> <p>The template dictionary.</p> Source code in <code>study_da/utils/template_utils.py</code> <pre><code>def load_template_configuration_as_dic(\n    template_configuration_name: str,\n) -&gt; tuple[dict, ruamel.yaml.YAML]:\n    \"\"\"Load a template configuration as a dictionary.\n\n    Args:\n        template_configuration_name (str): The name of the template configuration.\n\n    Returns:\n        dict: The template dictionary.\n\n    \"\"\"\n    path_local_template_configurations = (\n        f\"{os.path.dirname(inspect.getfile(GenerateScan))}/../assets/configurations/\"\n    )\n\n    # Add .yaml extension to template name\n    if not template_configuration_name.endswith(\".yaml\"):\n        template_configuration_name = f\"{template_configuration_name}.yaml\"\n\n    # Get path to template\n    path_template_config = f\"{path_local_template_configurations}{template_configuration_name}\"\n\n    # Load template\n    return load_dic_from_path(path_template_config)\n</code></pre>"},{"location":"reference/study_da/utils/index.html#study_da.utils.load_template_script_as_str","title":"<code>load_template_script_as_str(template_script_name)</code>","text":"<p>Load a template script as a string.</p> <p>Parameters:</p> Name Type Description Default <code>template_script_name</code> <code>str</code> <p>The name of the template script.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The template script as a string.</p> Source code in <code>study_da/utils/template_utils.py</code> <pre><code>def load_template_script_as_str(template_script_name: str) -&gt; str:\n    \"\"\"Load a template script as a string.\n\n    Args:\n        template_script_name (str): The name of the template script.\n\n    Returns:\n        str: The template script as a string.\n\n    \"\"\"\n    path_local_template_scripts = (\n        f\"{os.path.dirname(inspect.getfile(GenerateScan))}/../assets/template_scripts/\"\n    )\n\n    # Get path to template\n    path_template_script = f\"{path_local_template_scripts}{template_script_name}\"\n\n    # Load template\n    with open(path_template_script, \"r\") as fid:\n        template_script = fid.read()\n\n    return template_script\n</code></pre>"},{"location":"reference/study_da/utils/index.html#study_da.utils.nested_get","title":"<code>nested_get(dic, keys)</code>","text":"<p>Get the value from a nested dictionary using a list of keys.</p> <p>Parameters:</p> Name Type Description Default <code>dic</code> <code>dict</code> <p>The nested dictionary.</p> required <code>keys</code> <code>list</code> <p>The list of keys to traverse the nested dictionary.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The value corresponding to the keys in the nested dictionary.</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def nested_get(dic: dict, keys: list) -&gt; Any:\n    # Adapted from https://stackoverflow.com/questions/14692690/access-nested-dictionary-items-via-a-list-of-keys\n    \"\"\"Get the value from a nested dictionary using a list of keys.\n\n    Args:\n        dic (dict): The nested dictionary.\n        keys (list): The list of keys to traverse the nested dictionary.\n\n    Returns:\n        Any: The value corresponding to the keys in the nested dictionary.\n\n    \"\"\"\n    for key in keys:\n        dic = dic[key]\n    return dic\n</code></pre>"},{"location":"reference/study_da/utils/index.html#study_da.utils.nested_set","title":"<code>nested_set(dic, keys, value)</code>","text":"<p>Set a value in a nested dictionary using a list of keys.</p> <p>Parameters:</p> Name Type Description Default <code>dic</code> <code>dict</code> <p>The nested dictionary.</p> required <code>keys</code> <code>list</code> <p>The list of keys to traverse the nested dictionary.</p> required <code>value</code> <code>Any</code> <p>The value to set in the nested dictionary.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def nested_set(dic: dict, keys: list, value: Any) -&gt; None:\n    \"\"\"Set a value in a nested dictionary using a list of keys.\n\n    Args:\n        dic (dict): The nested dictionary.\n        keys (list): The list of keys to traverse the nested dictionary.\n        value (Any): The value to set in the nested dictionary.\n\n    Returns:\n        None\n\n    \"\"\"\n    for key in keys[:-1]:\n        dic = dic.setdefault(key, {})\n    dic[keys[-1]] = value\n</code></pre>"},{"location":"reference/study_da/utils/index.html#study_da.utils.set_item_in_dic","title":"<code>set_item_in_dic(obj, key, value, found=False)</code>","text":"<p>Set an item in a nested dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>dict</code> <p>The nested dictionary.</p> required <code>key</code> <code>str</code> <p>The key to set in the nested dictionary.</p> required <code>value</code> <code>Any</code> <p>The value to set in the nested dictionary.</p> required <code>found</code> <code>bool</code> <p>Whether the key has been found in the nested dictionary.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def set_item_in_dic(obj: dict, key: str, value: Any, found: bool = False) -&gt; None:\n    \"\"\"Set an item in a nested dictionary.\n\n    Args:\n        obj (dict): The nested dictionary.\n        key (str): The key to set in the nested dictionary.\n        value (Any): The value to set in the nested dictionary.\n        found (bool): Whether the key has been found in the nested dictionary.\n\n    Returns:\n        None\n\n    \"\"\"\n    if key in obj:\n        if found:\n            raise ValueError(f\"Key {key} found more than once in the nested dictionary.\")\n\n        obj[key] = value\n        found = True\n    for v in obj.values():\n        if isinstance(v, dict):\n            set_item_in_dic(v, key, value, found)\n</code></pre>"},{"location":"reference/study_da/utils/index.html#study_da.utils.write_dic_to_path","title":"<code>write_dic_to_path(dic, path, ryaml=None)</code>","text":"<p>Write a dictionary to a yaml file.</p> <p>Parameters:</p> Name Type Description Default <code>dic</code> <code>dict</code> <p>The dictionary to write.</p> required <code>path</code> <code>str</code> <p>The path to the yaml file.</p> required <code>ryaml</code> <code>YAML</code> <p>The yaml reader.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def write_dic_to_path(dic: dict, path: str, ryaml: ruamel.yaml.YAML | None = None) -&gt; None:\n    \"\"\"Write a dictionary to a yaml file.\n\n    Args:\n        dic (dict): The dictionary to write.\n        path (str): The path to the yaml file.\n        ryaml (ruamel.yaml.YAML): The yaml reader.\n\n    Returns:\n        None\n\n    \"\"\"\n\n    if ryaml is None:\n        # Initialize yaml reader\n        ryaml = ruamel.yaml.YAML()\n\n    # Write dic\n    with open(path, \"w\") as fid:\n        ryaml.dump(dic, fid)\n        # Force os to write to disk now, to avoid race conditions\n        fid.flush()\n        os.fsync(fid.fileno())\n</code></pre>"},{"location":"reference/study_da/utils/dic_utils.html","title":"dic_utils","text":"<p>This module provides utility functions for handling nested dictionaries and YAML files.</p> <p>Functions:</p> Name Description <code>load_dic_from_path</code> <p>str, ryaml: ruamel.yaml.YAML | None = None) -&gt; tuple[dict, ruamel.yaml.YAML]: Load a dictionary from a YAML file.</p> <code>write_dic_to_path</code> <p>dict, path: str, ryaml: ruamel.yaml.YAML | None = None) -&gt; None: Write a dictionary to a YAML file.</p> <code>nested_get</code> <p>dict, keys: list) -&gt; Any: Get the value from a nested dictionary using a list of keys.</p> <code>nested_set</code> <p>dict, keys: list, value: Any) -&gt; None: Set a value in a nested dictionary using a list of keys.</p> <code>find_item_in_dic</code> <p>dict, key: str) -&gt; Any: Find an item in a nested dictionary.</p> <code>set_item_in_dic</code> <p>dict, key: str, value: Any, found: bool = False) -&gt; None: Set an item in a nested dictionary.</p> <code>clean_dic</code> <p>Any) -&gt; None: Convert numpy types to standard types in a nested dictionary containing numbers and lists.</p>"},{"location":"reference/study_da/utils/dic_utils.html#study_da.utils.dic_utils.clean_dic","title":"<code>clean_dic(o)</code>","text":"<p>Convert numpy types to standard types in a nested dictionary containing number and lists.</p> <p>Parameters:</p> Name Type Description Default <code>o</code> <code>Any</code> <p>The object to convert.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def clean_dic(o: Any) -&gt; None:\n    \"\"\"Convert numpy types to standard types in a nested dictionary containing number and lists.\n\n    Args:\n        o (Any): The object to convert.\n\n    Returns:\n        None\n    \"\"\"\n    if not isinstance(o, dict):\n        return\n    for k, v in o.items():\n        if isinstance(v, np.generic):\n            o[k] = v.item()\n        elif isinstance(v, list):\n            for i, x in enumerate(v):\n                if isinstance(x, np.generic):\n                    v[i] = x.item()\n                if isinstance(x, dict):\n                    clean_dic(x)\n        else:\n            clean_dic(v)\n</code></pre>"},{"location":"reference/study_da/utils/dic_utils.html#study_da.utils.dic_utils.find_item_in_dic","title":"<code>find_item_in_dic(obj, key)</code>","text":"<p>Find an item in a nested dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>dict</code> <p>The nested dictionary.</p> required <code>key</code> <code>str</code> <p>The key to find in the nested dictionary.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The value corresponding to the key in the nested dictionary.</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def find_item_in_dic(obj: dict, key: str) -&gt; Any:\n    \"\"\"Find an item in a nested dictionary.\n\n    Args:\n        obj (dict): The nested dictionary.\n        key (str): The key to find in the nested dictionary.\n\n    Returns:\n        Any: The value corresponding to the key in the nested dictionary.\n\n    \"\"\"\n    if key in obj:\n        return obj[key]\n    for v in obj.values():\n        if isinstance(v, dict):\n            item = find_item_in_dic(v, key)\n            if item is not None:\n                return item\n</code></pre>"},{"location":"reference/study_da/utils/dic_utils.html#study_da.utils.dic_utils.load_dic_from_path","title":"<code>load_dic_from_path(path, ryaml=None)</code>","text":"<p>Load a dictionary from a yaml file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the yaml file.</p> required <code>ryaml</code> <code>YAML</code> <p>The yaml reader.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[dict, YAML]</code> <p>tuple[dict, ruamel.yaml.YAML]: The dictionary and the yaml reader.</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def load_dic_from_path(\n    path: str, ryaml: ruamel.yaml.YAML | None = None\n) -&gt; tuple[dict, ruamel.yaml.YAML]:\n    \"\"\"Load a dictionary from a yaml file.\n\n    Args:\n        path (str): The path to the yaml file.\n        ryaml (ruamel.yaml.YAML): The yaml reader.\n\n    Returns:\n        tuple[dict, ruamel.yaml.YAML]: The dictionary and the yaml reader.\n\n    \"\"\"\n\n    if ryaml is None:\n        # Initialize yaml reader\n        ryaml = ruamel.yaml.YAML()\n\n    # Load dic\n    with open(path, \"r\") as fid:\n        dic = ryaml.load(fid)\n\n    return dic, ryaml\n</code></pre>"},{"location":"reference/study_da/utils/dic_utils.html#study_da.utils.dic_utils.nested_get","title":"<code>nested_get(dic, keys)</code>","text":"<p>Get the value from a nested dictionary using a list of keys.</p> <p>Parameters:</p> Name Type Description Default <code>dic</code> <code>dict</code> <p>The nested dictionary.</p> required <code>keys</code> <code>list</code> <p>The list of keys to traverse the nested dictionary.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The value corresponding to the keys in the nested dictionary.</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def nested_get(dic: dict, keys: list) -&gt; Any:\n    # Adapted from https://stackoverflow.com/questions/14692690/access-nested-dictionary-items-via-a-list-of-keys\n    \"\"\"Get the value from a nested dictionary using a list of keys.\n\n    Args:\n        dic (dict): The nested dictionary.\n        keys (list): The list of keys to traverse the nested dictionary.\n\n    Returns:\n        Any: The value corresponding to the keys in the nested dictionary.\n\n    \"\"\"\n    for key in keys:\n        dic = dic[key]\n    return dic\n</code></pre>"},{"location":"reference/study_da/utils/dic_utils.html#study_da.utils.dic_utils.nested_set","title":"<code>nested_set(dic, keys, value)</code>","text":"<p>Set a value in a nested dictionary using a list of keys.</p> <p>Parameters:</p> Name Type Description Default <code>dic</code> <code>dict</code> <p>The nested dictionary.</p> required <code>keys</code> <code>list</code> <p>The list of keys to traverse the nested dictionary.</p> required <code>value</code> <code>Any</code> <p>The value to set in the nested dictionary.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def nested_set(dic: dict, keys: list, value: Any) -&gt; None:\n    \"\"\"Set a value in a nested dictionary using a list of keys.\n\n    Args:\n        dic (dict): The nested dictionary.\n        keys (list): The list of keys to traverse the nested dictionary.\n        value (Any): The value to set in the nested dictionary.\n\n    Returns:\n        None\n\n    \"\"\"\n    for key in keys[:-1]:\n        dic = dic.setdefault(key, {})\n    dic[keys[-1]] = value\n</code></pre>"},{"location":"reference/study_da/utils/dic_utils.html#study_da.utils.dic_utils.set_item_in_dic","title":"<code>set_item_in_dic(obj, key, value, found=False)</code>","text":"<p>Set an item in a nested dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>dict</code> <p>The nested dictionary.</p> required <code>key</code> <code>str</code> <p>The key to set in the nested dictionary.</p> required <code>value</code> <code>Any</code> <p>The value to set in the nested dictionary.</p> required <code>found</code> <code>bool</code> <p>Whether the key has been found in the nested dictionary.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def set_item_in_dic(obj: dict, key: str, value: Any, found: bool = False) -&gt; None:\n    \"\"\"Set an item in a nested dictionary.\n\n    Args:\n        obj (dict): The nested dictionary.\n        key (str): The key to set in the nested dictionary.\n        value (Any): The value to set in the nested dictionary.\n        found (bool): Whether the key has been found in the nested dictionary.\n\n    Returns:\n        None\n\n    \"\"\"\n    if key in obj:\n        if found:\n            raise ValueError(f\"Key {key} found more than once in the nested dictionary.\")\n\n        obj[key] = value\n        found = True\n    for v in obj.values():\n        if isinstance(v, dict):\n            set_item_in_dic(v, key, value, found)\n</code></pre>"},{"location":"reference/study_da/utils/dic_utils.html#study_da.utils.dic_utils.write_dic_to_path","title":"<code>write_dic_to_path(dic, path, ryaml=None)</code>","text":"<p>Write a dictionary to a yaml file.</p> <p>Parameters:</p> Name Type Description Default <code>dic</code> <code>dict</code> <p>The dictionary to write.</p> required <code>path</code> <code>str</code> <p>The path to the yaml file.</p> required <code>ryaml</code> <code>YAML</code> <p>The yaml reader.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def write_dic_to_path(dic: dict, path: str, ryaml: ruamel.yaml.YAML | None = None) -&gt; None:\n    \"\"\"Write a dictionary to a yaml file.\n\n    Args:\n        dic (dict): The dictionary to write.\n        path (str): The path to the yaml file.\n        ryaml (ruamel.yaml.YAML): The yaml reader.\n\n    Returns:\n        None\n\n    \"\"\"\n\n    if ryaml is None:\n        # Initialize yaml reader\n        ryaml = ruamel.yaml.YAML()\n\n    # Write dic\n    with open(path, \"w\") as fid:\n        ryaml.dump(dic, fid)\n        # Force os to write to disk now, to avoid race conditions\n        fid.flush()\n        os.fsync(fid.fileno())\n</code></pre>"},{"location":"reference/study_da/utils/template_utils.html","title":"template_utils","text":""},{"location":"reference/study_da/utils/template_utils.html#study_da.utils.template_utils.load_template_configuration_as_dic","title":"<code>load_template_configuration_as_dic(template_configuration_name)</code>","text":"<p>Load a template configuration as a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>template_configuration_name</code> <code>str</code> <p>The name of the template configuration.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>tuple[dict, YAML]</code> <p>The template dictionary.</p> Source code in <code>study_da/utils/template_utils.py</code> <pre><code>def load_template_configuration_as_dic(\n    template_configuration_name: str,\n) -&gt; tuple[dict, ruamel.yaml.YAML]:\n    \"\"\"Load a template configuration as a dictionary.\n\n    Args:\n        template_configuration_name (str): The name of the template configuration.\n\n    Returns:\n        dict: The template dictionary.\n\n    \"\"\"\n    path_local_template_configurations = (\n        f\"{os.path.dirname(inspect.getfile(GenerateScan))}/../assets/configurations/\"\n    )\n\n    # Add .yaml extension to template name\n    if not template_configuration_name.endswith(\".yaml\"):\n        template_configuration_name = f\"{template_configuration_name}.yaml\"\n\n    # Get path to template\n    path_template_config = f\"{path_local_template_configurations}{template_configuration_name}\"\n\n    # Load template\n    return load_dic_from_path(path_template_config)\n</code></pre>"},{"location":"reference/study_da/utils/template_utils.html#study_da.utils.template_utils.load_template_script_as_str","title":"<code>load_template_script_as_str(template_script_name)</code>","text":"<p>Load a template script as a string.</p> <p>Parameters:</p> Name Type Description Default <code>template_script_name</code> <code>str</code> <p>The name of the template script.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The template script as a string.</p> Source code in <code>study_da/utils/template_utils.py</code> <pre><code>def load_template_script_as_str(template_script_name: str) -&gt; str:\n    \"\"\"Load a template script as a string.\n\n    Args:\n        template_script_name (str): The name of the template script.\n\n    Returns:\n        str: The template script as a string.\n\n    \"\"\"\n    path_local_template_scripts = (\n        f\"{os.path.dirname(inspect.getfile(GenerateScan))}/../assets/template_scripts/\"\n    )\n\n    # Get path to template\n    path_template_script = f\"{path_local_template_scripts}{template_script_name}\"\n\n    # Load template\n    with open(path_template_script, \"r\") as fid:\n        template_script = fid.read()\n\n    return template_script\n</code></pre>"},{"location":"template_files/index.html","title":"Templates and assets","text":"<p>This page contains the template files for the HL-LHC and run III configurations, as well as the template scripts to build a Xsuite collider from a MAD-X sequence (generation 1), and the scripts to configure the Xsuite collider and do the tracking (generation 2).</p> <p>In addition, this page contains assets used for the simulations, such as the filling schemes (other assets might be added in the future).</p> <p>Please do not hesitate to add any template you deem interesting for the project, through a pull request.</p>"},{"location":"template_files/index.html#moving-from-one-hl-lhc-version-to-another","title":"Moving from one (HL-)LHC version to another","text":"<p>Overall, going from one (HL-)LHC version to another should be very transparent: the templates are identical. The only difference worth mentionning are the following:</p> <ul> <li>The run III optics (ions or not) are usually taken directly from CERN AFS, as you can see in the <code>acc-models-lhc</code> field in the configurations. This means that they're only accessible if you're at CERN, or if you have a VPN connection to CERN (note that you can probably pull them from the CERN gitlab if really needed). Conversely, it is assumed that the HL-LHC optics are available locally (they should get cloned along with the repo if you install the package in editable mode from GitHub). However, if you install the package from PyPI, you will need to download the optics from the CERN GitHub (or elsewhere) and put the appropriate paths in the configuration file.</li> <li>The ions runs levelling are usually done by varying the separation (see generation_2_level_by_sep), while the protons runs levelling are done by varying the beta functions. However, in the latter case, since the beta functions can't be changed with a single knob, you will have to update the optics file for each levelling step and optimize the bunch intensity for each optics (see generation_2_level_nb).</li> </ul>"},{"location":"template_files/configurations/config_hllhc13.html","title":"HL-LHC v1.3 configuration","text":"config_hllhc13.yaml<pre><code>config_particles:\n  r_min: 4.0\n  r_max: 8.0\n  n_r: 256\n  n_angles: 5\n  n_split: 5\n  path_distribution_folder_output: particles\n\nconfig_mad:\n  # Links to be made for tools and scripts\n  links:\n    acc-models-lhc: ../../../../external_dependencies/acc-models-lhc-v13\n\n  # Optics file\n  optics_file: acc-models-lhc/opt_500_500_500_500_thin.madx\n\n  # Beam parameters\n  beam_config:\n    lhcb1:\n      beam_energy_tot: 7000 # [GeV]\n    lhcb2:\n      beam_energy_tot: 7000 # [GeV]\n\n  # Ions being simulated\n  ions: false\n\n  # Enable machine imperfections\n  enable_imperfections: false\n\n  # Enable knob synthesis (for coupling correction, if no imperfections)\n  enable_knob_synthesis: true\n\n  # Rename the coupling knobs to avoid conflict between b1 and b2\n  # (for hllhc using old fortran code to generate the knobs)\n  rename_coupling_knobs: true\n\n  # Optics version, for choice of correction algorithms\n  # (ver_lhc_run or ver_hllhc_optics)\n  ver_hllhc_optics: 1.3\n  ver_lhc_run:\n\n  # Parameters for machine imperfections\n  pars_for_imperfections:\n    par_myseed: 1\n    par_correct_for_D2: 0\n    par_correct_for_MCBX: 0\n    par_on_errors_LHC: 1\n    par_off_errors_Q4_inIP15: 0\n    par_off_errors_Q5_inIP15: 0\n    par_on_errors_MBH: 1\n    par_on_errors_Q4: 1\n    par_on_errors_D2: 1\n    par_on_errors_D1: 1\n    par_on_errors_IT: 1\n    par_on_errors_MCBRD: 0\n    par_on_errors_MCBXF: 0\n    par_on_errors_NLC: 0\n    par_write_errortable: 1\n\n  phasing:\n    # RF voltage and phases\n    vrf400: 16.0 # [MV]\n    lagrf400.b1: 0.5 # [rad]\n    lagrf400.b2: 0. # [rad]\n\n  # To make some specifics checks\n  sanity_checks: true\n\n  # Path of the collider file to be saved (usually at the end of the first generation)\n  path_collider_file_for_configuration_as_output: collider_file_for_configuration.json\n  compress: true # will compress the collider file, filename will end with .zip\n\n# Configuration for tuning of the collider\nconfig_collider:\n  # Even though the file doesn't end with .zip, scrip will first try to load it as a zip file\n  path_collider_file_for_configuration_as_input: ../collider_file_for_configuration.json\n  config_knobs_and_tuning:\n    knob_settings:\n      # Orbit knobs\n      on_x1: 250 # [urad]\n      on_sep1: 0 # [mm]\n      on_x2: -170 # [urad]\n      on_sep2: 0.138 # 0.1443593672910653 # 0.138 # [mm]\n      on_x5: 250 # [urad]\n      on_sep5: 0 # [mm]\n      on_x8h: 0.0\n      on_x8v: 170\n      on_sep8h: -0.01 # different from 0 so that the levelling algorithm is not stuck\n      on_sep8v: 0.01 # idem\n      on_a1: 0 # [urad]\n      on_o1: 0 # [mm]\n      on_a2: 0 # [urad]\n      on_o2: 0 # [mm]\n      on_a5: 0 # [urad]\n      on_o5: 0 # [mm]\n      on_a8: 0 # [urad]\n      on_o8: 0 # [mm]\n      on_disp: 1 # Value to choose could be optics-dependent\n\n      # Crab cavities\n      on_crab1: 0 # [urad]\n      on_crab5: 0 # [urad]\n\n      # Magnets of the experiments\n      on_alice_normalized: 1\n      on_lhcb_normalized: 1\n      on_sol_atlas: 0\n      on_sol_cms: 0\n      on_sol_alice: 0\n\n      # Octupoles\n      i_oct_b1: -60. # [A]\n      i_oct_b2: -60. # [A]\n\n    # Tunes and chromaticities\n    qx:\n      lhcb1: 62.31\n      lhcb2: 62.31\n    qy:\n      lhcb1: 60.32\n      lhcb2: 60.32\n    dqx:\n      lhcb1: 15\n      lhcb2: 15\n    dqy:\n      lhcb1: 15\n      lhcb2: 15\n\n    # Linear coupling\n    delta_cmr: 0.001\n    delta_cmi: 0.0\n\n    knob_names:\n      lhcb1:\n        q_knob_1: kqtf.b1\n        q_knob_2: kqtd.b1\n        dq_knob_1: ksf.b1\n        dq_knob_2: ksd.b1\n        c_minus_knob_1: c_minus_re_b1\n        c_minus_knob_2: c_minus_im_b1\n      lhcb2:\n        q_knob_1: kqtf.b2\n        q_knob_2: kqtd.b2\n        dq_knob_1: ksf.b2\n        dq_knob_2: ksd.b2\n        c_minus_knob_1: c_minus_re_b2\n        c_minus_knob_2: c_minus_im_b2\n\n  config_beambeam:\n    skip_beambeam: false\n    bunch_spacing_buckets: 10\n    num_slices_head_on: 11\n    num_long_range_encounters_per_side:\n      ip1: 25\n      ip2: 20\n      ip5: 25\n      ip8: 20\n    sigma_z: 0.0761\n    num_particles_per_bunch: 140000000000.0\n    nemitt_x: 2.5e-6\n    nemitt_y: 2.5e-6\n    mask_with_filling_pattern:\n      # If not already existing in the study-da package, pattern must have an absolute path or be\n      # added as a dependency for the run file\n      pattern_fname: 25ns_2760b_2748_2492_2574_288bpi_13inj_800ns_bs200ns.json\n      i_bunch_b1:      # If not specified, the bunch with the worst schedule is chosen\n      i_bunch_b2:      # Same. A value for i_bunch_b1 and i_bunch_b2 must be specified if pattern_fname is specified\n    cross_section: 81e-27\n\n  config_lumi_leveling_ip1_5:\n    skip_leveling: false\n    luminosity: 5.0e+34\n    num_colliding_bunches:      # This will be set automatically according to the filling scheme\n    vary:\n    - num_particles_per_bunch\n    constraints:\n      max_intensity: 2.3e11\n      max_PU: 160\n\n  skip_leveling: false\n  config_lumi_leveling:\n    ip2:\n      separation_in_sigmas: 5\n      plane: x\n      impose_separation_orthogonal_to_crossing: false\n      knobs:\n      - on_sep2\n      bump_range:\n        lhcb1:\n        - e.ds.l2.b1\n        - s.ds.r2.b1\n        lhcb2:\n        - s.ds.r2.b2\n        - e.ds.l2.b2\n      preserve_angles_at_ip: true\n      preserve_bump_closure: true\n      corrector_knob_names:\n        # to preserve angles at ip\n      - corr_co_acbyvs4.l2b1\n      - corr_co_acbyhs4.l2b1\n      - corr_co_acbyvs4.r2b2\n      - corr_co_acbyhs4.r2b2\n          # to close the bumps\n      - corr_co_acbyvs4.l2b2\n      - corr_co_acbyhs4.l2b2\n      - corr_co_acbyvs4.r2b1\n      - corr_co_acbyhs4.r2b1\n      - corr_co_acbyhs5.l2b2\n      - corr_co_acbyvs5.l2b2\n      - corr_co_acbchs5.r2b1\n      - corr_co_acbcvs5.r2b1\n    ip8:\n      luminosity: 2.0e+33\n      num_colliding_bunches:      # This will be set automatically according to the filling scheme\n      impose_separation_orthogonal_to_crossing: true\n      knobs:\n      - on_sep8h\n      - on_sep8v\n      bump_range:\n        lhcb1:\n        - e.ds.l8.b1\n        - s.ds.r8.b1\n        lhcb2:\n        - s.ds.r8.b2\n        - e.ds.l8.b2\n      preserve_angles_at_ip: true\n      preserve_bump_closure: true\n      corrector_knob_names:\n        # to preserve angles at ip\n      - corr_co_acbyvs4.l8b1\n      - corr_co_acbyhs4.l8b1\n      - corr_co_acbyvs4.r8b2\n      - corr_co_acbyhs4.r8b2\n          # to close the bumps\n      - corr_co_acbyvs4.l8b2\n      - corr_co_acbyhs4.l8b2\n      - corr_co_acbyvs4.r8b1\n      - corr_co_acbyhs4.r8b1\n      - corr_co_acbcvs5.l8b2\n      - corr_co_acbchs5.l8b2\n      - corr_co_acbyvs5.r8b1\n      - corr_co_acbyhs5.r8b1\n\n  # Save collider or not (usually at the end of the collider tuning)\n  save_output_collider: false\n  path_collider_file_for_tracking_as_output: collider_file_for_tracking.json\n  compress: true # will compress the collider file, filename will end with .zip\n\nconfig_simulation:\n  # Collider file to load for the tracking\n  path_collider_file_for_tracking_as_input: ../collider_file_for_tracking.json\n\n  # Distribution in the normalized xy space\n  path_distribution_folder_input: ../particles\n  distribution_file: 00.parquet\n\n  # Output particle file\n  path_distribution_file_output: output_particles.parquet\n\n  # Initial off-momentum\n  delta_max: 27.e-5\n\n  # Tracking\n  n_turns: 1000000 # number of turns to track\n\n  # Beam to track\n  beam: lhcb1 #lhcb1 or lhcb2\n\n  # Context for the simulation\n  context: cpu   # 'cupy' # opencl\n\n  # Device number for GPU simulation\n  device_number: # 0\n</code></pre>"},{"location":"template_files/configurations/config_hllhc16.html","title":"HL-LHC v1.6 configuration","text":"config_hllhc16.yaml<pre><code>config_particles:\n  r_min: 4.0\n  r_max: 8.0\n  n_r: 256\n  n_angles: 5\n  n_split: 5\n  path_distribution_folder_output: particles\n\nconfig_mad:\n  # Links to be made for tools and scripts\n  links:\n    acc-models-lhc: ../../../../external_dependencies/acc-models-lhc\n\n  # Optics file\n  optics_file: acc-models-lhc/strengths/round/opt_round_150_1500_optphases_thin.madx\n\n  # Beam parameters\n  beam_config:\n    lhcb1:\n      beam_energy_tot: 7000 # [GeV]\n    lhcb2:\n      beam_energy_tot: 7000 # [GeV]\n\n  # Ions being simulated\n  ions: false\n\n  # Enable machine imperfections\n  enable_imperfections: false\n\n  # Enable knob synthesis (for coupling correction, if no imperfections)\n  enable_knob_synthesis: true\n\n  # Rename the coupling knobs to avoid conflict between b1 and b2\n  # (for hllhc using old fortran code to generate the knobs)\n  rename_coupling_knobs: true\n\n  # Optics version, for choice of correction algorithms\n  # (ver_lhc_run or ver_hllhc_optics)\n  ver_hllhc_optics: 1.6\n  ver_lhc_run:\n\n  # Parameters for machine imperfections\n  pars_for_imperfections:\n    par_myseed: 1\n    par_correct_for_D2: 0\n    par_correct_for_MCBX: 0\n    par_on_errors_LHC: 1\n    par_off_errors_Q4_inIP15: 0\n    par_off_errors_Q5_inIP15: 0\n    par_on_errors_MBH: 1\n    par_on_errors_Q4: 1\n    par_on_errors_D2: 1\n    par_on_errors_D1: 1\n    par_on_errors_IT: 1\n    par_on_errors_MCBRD: 0\n    par_on_errors_MCBXF: 0\n    par_on_errors_NLC: 0\n    par_write_errortable: 1\n\n  phasing:\n    # RF voltage and phases\n    vrf400: 16.0 # [MV]\n    lagrf400.b1: 0.5 # [rad]\n    lagrf400.b2: 0.5 # [rad]\n\n  # To make some specifics checks\n  sanity_checks: true\n\n  # Path of the collider file to be saved (usually at the end of the first generation)\n  path_collider_file_for_configuration_as_output: collider_file_for_configuration.json\n  compress: true # will compress the collider file, filename will end with .zip\n\n# Configuration for tuning of the collider\nconfig_collider:\n  # Even though the file doesn't end with .zip, scrip will first try to load it as a zip file\n  path_collider_file_for_configuration_as_input: ../collider_file_for_configuration.json\n  config_knobs_and_tuning:\n    knob_settings:\n      # Orbit knobs\n      on_x1: 250 # [urad]\n      on_sep1: 0 # [mm]\n      on_x2: -170 # [urad]\n      on_sep2: 0.138 # 0.1443593672910653 # 0.138 # [mm]\n      on_x5: 250 # [urad]\n      on_sep5: 0 # [mm]\n      on_x8h: 0.0\n      on_x8v: 170\n      on_sep8h: -0.01 # different from 0 so that the levelling algorithm is not stuck\n      on_sep8v: 0.01 # idem\n      on_a1: 0 # [urad]\n      on_o1: 0 # [mm]\n      on_a2: 0 # [urad]\n      on_o2: 0 # [mm]\n      on_a5: 0 # [urad]\n      on_o5: 0 # [mm]\n      on_a8: 0 # [urad]\n      on_o8: 0 # [mm]\n      on_disp: 1 # Value to choose could be optics-dependent\n\n      # Crab cavities\n      on_crab1: 0 # [urad]\n      on_crab5: 0 # [urad]\n\n      # Magnets of the experiments\n      on_alice_normalized: 1\n      on_lhcb_normalized: 1\n      on_sol_atlas: 0\n      on_sol_cms: 0\n      on_sol_alice: 0\n\n      # Octupoles\n      i_oct_b1: -60. # [A]\n      i_oct_b2: -60. # [A]\n\n    # Tunes and chromaticities\n    qx:\n      lhcb1: 62.31\n      lhcb2: 62.31\n    qy:\n      lhcb1: 60.32\n      lhcb2: 60.32\n    dqx:\n      lhcb1: 15\n      lhcb2: 15\n    dqy:\n      lhcb1: 15\n      lhcb2: 15\n\n    # Linear coupling\n    delta_cmr: 0.001\n    delta_cmi: 0.0\n\n    knob_names:\n      lhcb1:\n        q_knob_1: kqtf.b1\n        q_knob_2: kqtd.b1\n        dq_knob_1: ksf.b1\n        dq_knob_2: ksd.b1\n        c_minus_knob_1: c_minus_re_b1\n        c_minus_knob_2: c_minus_im_b1\n      lhcb2:\n        q_knob_1: kqtf.b2\n        q_knob_2: kqtd.b2\n        dq_knob_1: ksf.b2\n        dq_knob_2: ksd.b2\n        c_minus_knob_1: c_minus_re_b2\n        c_minus_knob_2: c_minus_im_b2\n\n  config_beambeam:\n    skip_beambeam: false\n    bunch_spacing_buckets: 10\n    num_slices_head_on: 11\n    num_long_range_encounters_per_side:\n      ip1: 25\n      ip2: 20\n      ip5: 25\n      ip8: 20\n    sigma_z: 0.0761\n    num_particles_per_bunch: 140000000000.0\n    nemitt_x: 2.5e-6\n    nemitt_y: 2.5e-6\n    mask_with_filling_pattern:\n      # If not already existing in the study-da package, pattern must have an absolute path or be\n      # added as a dependency for the run file\n      pattern_fname: 25ns_2760b_2748_2492_2574_288bpi_13inj_800ns_bs200ns.json\n      i_bunch_b1:      # If not specified, the bunch with the worst schedule is chosen\n      i_bunch_b2:      # Same. A value for i_bunch_b1 and i_bunch_b2 must be specified if pattern_fname is specified\n    cross_section: 81e-27\n\n  config_lumi_leveling_ip1_5:\n    skip_leveling: false\n    luminosity: 5.0e+34\n    num_colliding_bunches:      # This will be set automatically according to the filling scheme\n    vary:\n    - num_particles_per_bunch\n    constraints:\n      max_intensity: 2.3e11\n      max_PU: 160\n\n  skip_leveling: false\n  config_lumi_leveling:\n    ip2:\n      separation_in_sigmas: 5\n      plane: x\n      impose_separation_orthogonal_to_crossing: false\n      knobs:\n      - on_sep2\n      bump_range:\n        lhcb1:\n        - e.ds.l2.b1\n        - s.ds.r2.b1\n        lhcb2:\n        - s.ds.r2.b2\n        - e.ds.l2.b2\n      preserve_angles_at_ip: true\n      preserve_bump_closure: true\n      corrector_knob_names:\n        # to preserve angles at ip\n      - corr_co_acbyvs4.l2b1\n      - corr_co_acbyhs4.l2b1\n      - corr_co_acbyvs4.r2b2\n      - corr_co_acbyhs4.r2b2\n          # to close the bumps\n      - corr_co_acbyvs4.l2b2\n      - corr_co_acbyhs4.l2b2\n      - corr_co_acbyvs4.r2b1\n      - corr_co_acbyhs4.r2b1\n      - corr_co_acbyhs5.l2b2\n      - corr_co_acbyvs5.l2b2\n      - corr_co_acbchs5.r2b1\n      - corr_co_acbcvs5.r2b1\n    ip8:\n      luminosity: 2.0e+33\n      num_colliding_bunches:      # This will be set automatically according to the filling scheme\n      impose_separation_orthogonal_to_crossing: true\n      knobs:\n      - on_sep8h\n      - on_sep8v\n      bump_range:\n        lhcb1:\n        - e.ds.l8.b1\n        - s.ds.r8.b1\n        lhcb2:\n        - s.ds.r8.b2\n        - e.ds.l8.b2\n      preserve_angles_at_ip: true\n      preserve_bump_closure: true\n      corrector_knob_names:\n        # to preserve angles at ip\n      - corr_co_acbyvs4.l8b1\n      - corr_co_acbyhs4.l8b1\n      - corr_co_acbyvs4.r8b2\n      - corr_co_acbyhs4.r8b2\n          # to close the bumps\n      - corr_co_acbyvs4.l8b2\n      - corr_co_acbyhs4.l8b2\n      - corr_co_acbyvs4.r8b1\n      - corr_co_acbyhs4.r8b1\n      - corr_co_acbcvs5.l8b2\n      - corr_co_acbchs5.l8b2\n      - corr_co_acbyvs5.r8b1\n      - corr_co_acbyhs5.r8b1\n\n  # Save collider or not (usually at the end of the collider tuning)\n  save_output_collider: false\n  path_collider_file_for_tracking_as_output: collider_file_for_tracking.json\n  compress: true # will compress the collider file, filename will end with .zip\n\nconfig_simulation:\n  # Collider file to load for the tracking\n  path_collider_file_for_tracking_as_input: ../collider_file_for_tracking.json\n\n  # Distribution in the normalized xy space\n  path_distribution_folder_input: ../particles\n  distribution_file: 00.parquet\n\n  # Output particle file\n  path_distribution_file_output: output_particles.parquet\n\n  # Initial off-momentum\n  delta_max: 27.e-5\n\n  # Tracking\n  n_turns: 1000000 # number of turns to track\n\n  # Beam to track\n  beam: lhcb1 #lhcb1 or lhcb2\n\n  # Context for the simulation\n  context: cpu   # 'cupy' # opencl\n\n  # Device number for GPU simulation\n  device_number: # 0\n</code></pre>"},{"location":"template_files/configurations/config_runIII.html","title":"Run III configuration","text":"config_runIII.yaml<pre><code>config_particles:\n  r_min: 4.0\n  r_max: 8.0\n  n_r: 256\n  n_angles: 5\n  n_split: 5\n  path_distribution_folder_output: particles\n\nconfig_mad:\n  # Links to be made for tools and scripts\n  links:\n    acc-models-lhc: /afs/cern.ch/eng/lhc/optics/runIII\n\n  # Optics file\n  optics_file: acc-models-lhc/RunIII_dev/Proton_2024/opticsfile.37 #\n\n  # Beam parameters\n  beam_config:\n    lhcb1:\n      beam_energy_tot: 6800 # [GeV]\n    lhcb2:\n      beam_energy_tot: 6800 # [GeV]\n\n  # Ions being simulated\n  ions: false\n\n  # Enable machine imperfections\n  enable_imperfections: false\n\n  # Enable knob synthesis (for coupling correction, if no imperfections)\n  enable_knob_synthesis: true\n\n  # Rename the coupling knobs to avoid conflict between b1 and b2\n  # (for hllhc using old fortran code to generate the knobs)\n  rename_coupling_knobs: true\n\n  # Optics version, for choice of correction algorithms\n  # (ver_lhc_run or ver_hllhc_optics)\n  ver_hllhc_optics:\n  ver_lhc_run: 3.0\n\n  # Parameters for machine imperfections\n  pars_for_imperfections:\n    par_myseed: 1\n    par_correct_for_D2: 0\n    par_correct_for_MCBX: 0\n    par_on_errors_LHC: 1\n    par_off_errors_Q4_inIP15: 0\n    par_off_errors_Q5_inIP15: 0\n    par_on_errors_MBH: 1\n    par_on_errors_Q4: 1\n    par_on_errors_D2: 1\n    par_on_errors_D1: 1\n    par_on_errors_IT: 1\n    par_on_errors_MCBRD: 0\n    par_on_errors_MCBXF: 0\n    par_on_errors_NLC: 0\n    par_write_errortable: 1\n\n  phasing:\n    # RF voltage and phases\n    vrf400: 12.0 # [MV]\n    lagrf400.b1: 0.5 # [rad]\n    lagrf400.b2: 0. # [rad]\n\n  # To make some specifics checks\n  sanity_checks: true\n\n  # Path of the collider file to be saved (usually at the end of the first generation)\n  path_collider_file_for_configuration_as_output: collider_file_for_configuration.json\n  compress: true # will compress the collider file, filename will end with .zip\n\n# Configuration for tuning of the collider\nconfig_collider:\n  # Even though the file doesn't end with .zip, scrip will first try to load it as a zip file\n  path_collider_file_for_configuration_as_input: ../collider_file_for_configuration.json\n  config_knobs_and_tuning:\n    knob_settings:\n      # Exp. configuration in IR1, IR2, IR5 and IR8***\n      on_x1: -145.000\n      on_sep1: 0.0 #-0.550\n      phi_IR1: 180.000\n\n      on_x2h: 0.000\n      on_sep2h: 1.0 # 1.000\n      on_x2v: 200.000\n      on_sep2v: 0.000\n      phi_IR2: 90.000\n\n      on_x5: 145.000\n      on_sep5: 0.0 # 0.550\n      phi_IR5: 90.000\n\n      on_x8h: 0.000\n      on_sep8h: -0.01 #-1.000\n      on_x8v: 200.000\n      on_sep8v: 0.000\n      phi_IR8: 180.000\n\n      # Spurious dispersion correction\n      on_disp: 1.000\n\n      # Magnets of the experiments\n      on_alice_normalized: 1\n      on_lhcb_normalized: 1\n      on_sol_atlas: 0\n      on_sol_cms: 0\n      on_sol_alice: 0\n\n      # Octupoles\n      i_oct_b1: 300. # [A]\n      i_oct_b2: 300. # [A]\n\n    # Tunes and chromaticities\n    qx:\n      lhcb1: 62.31\n      lhcb2: 62.31\n    qy:\n      lhcb1: 60.32\n      lhcb2: 60.32\n    dqx:\n      lhcb1: 15\n      lhcb2: 15\n    dqy:\n      lhcb1: 15\n      lhcb2: 15\n\n    # Linear coupling\n    delta_cmr: 0.001\n    delta_cmi: 0.0\n\n    knob_names:\n      lhcb1:\n        q_knob_1: dqx.b1_sq\n        q_knob_2: dqy.b1_sq\n        dq_knob_1: dqpx.b1_sq\n        dq_knob_2: dqpy.b1_sq\n        c_minus_knob_1: c_minus_re_b1\n        c_minus_knob_2: c_minus_im_b1\n      lhcb2:\n        q_knob_1: dqx.b2_sq\n        q_knob_2: dqy.b2_sq\n        dq_knob_1: dqpx.b2_sq\n        dq_knob_2: dqpy.b2_sq\n        c_minus_knob_1: c_minus_re_b2\n        c_minus_knob_2: c_minus_im_b2\n\n  config_beambeam:\n    skip_beambeam: false\n    bunch_spacing_buckets: 10\n    num_slices_head_on: 11\n    num_long_range_encounters_per_side:\n      ip1: 25\n      ip2: 20\n      ip5: 25\n      ip8: 20\n    sigma_z: 0.09\n    num_particles_per_bunch: 1.15e11\n    nemitt_x: 2.2e-6\n    nemitt_y: 2.2e-6\n    mask_with_filling_pattern:\n      # If not already existing in the study-da package, pattern must have an absolute path or be\n      # added as a dependency for the run file\n      pattern_fname: 25ns_2604b_2592_2310_2421_4x48bpi_16inj.json\n      i_bunch_b1:      # If not specified, the bunch with the worst schedule is chosen\n      i_bunch_b2:      # Same. A value for i_bunch_b1 and i_bunch_b2 must be specified if pattern_fname is specified\n    cross_section: 81e-27\n\n  config_lumi_leveling_ip1_5:\n    skip_leveling: false\n    luminosity: 2.0e+34\n    num_colliding_bunches:      # This will be set automatically according to the filling scheme\n    vary:\n    - num_particles_per_bunch\n    constraints:\n      max_intensity: 1.8e11\n      max_PU: 70\n\n  skip_leveling: false\n  config_lumi_leveling:\n    ip2:\n      separation_in_sigmas: 5\n      plane: x\n      impose_separation_orthogonal_to_crossing: false\n      knobs:\n      - on_sep2h\n      - on_sep2v\n      bump_range:\n        lhcb1:\n        - e.ds.l2.b1\n        - s.ds.r2.b1\n        lhcb2:\n        - s.ds.r2.b2\n        - e.ds.l2.b2\n      preserve_angles_at_ip: true\n      preserve_bump_closure: true\n      corrector_knob_names:\n        # to preserve angles at ip\n      - corr_co_acbyvs4.l2b1\n      - corr_co_acbyhs4.l2b1\n      - corr_co_acbyvs4.r2b2\n      - corr_co_acbyhs4.r2b2\n        # to close the bumps\n      - corr_co_acbyvs4.l2b2\n      - corr_co_acbyhs4.l2b2\n      - corr_co_acbyvs4.r2b1\n      - corr_co_acbyhs4.r2b1\n      - corr_co_acbyhs5.l2b2\n      - corr_co_acbyvs5.l2b2\n      - corr_co_acbchs5.r2b1\n      - corr_co_acbcvs5.r2b1\n\n    ip8:\n      luminosity: 2.0e+33\n      num_colliding_bunches:      # This will be set automatically according to the filling scheme\n      impose_separation_orthogonal_to_crossing: true\n      knobs:\n      - on_sep8h\n      - on_sep8v\n      bump_range:\n        lhcb1:\n        - e.ds.l8.b1\n        - s.ds.r8.b1\n        lhcb2:\n        - s.ds.r8.b2\n        - e.ds.l8.b2\n      preserve_angles_at_ip: true\n      preserve_bump_closure: true\n      corrector_knob_names:\n        # to preserve angles at ip\n      - corr_co_acbyvs4.l8b1\n      - corr_co_acbyhs4.l8b1\n      - corr_co_acbyvs4.r8b2\n      - corr_co_acbyhs4.r8b2\n        # to close the bumps\n      - corr_co_acbyvs4.l8b2\n      - corr_co_acbyhs4.l8b2\n      - corr_co_acbyvs4.r8b1\n      - corr_co_acbyhs4.r8b1\n      - corr_co_acbcvs5.l8b2\n      - corr_co_acbchs5.l8b2\n      - corr_co_acbyvs5.r8b1\n      - corr_co_acbyhs5.r8b1\n\n  # Save collider or not (usually at the end of the collider tuning)\n  save_output_collider: false\n  path_collider_file_for_tracking_as_output: collider_file_for_tracking.json\n  compress: true # will compress the collider file, filename will end with .zip\n\nconfig_simulation:\n  # Collider file to load for the tracking\n  path_collider_file_for_tracking_as_input: ../collider_file_for_tracking.json\n\n  # Distribution in the normalized xy space\n  path_distribution_folder_input: ../particles\n  distribution_file: 00.parquet\n\n  # Output particle file\n  path_distribution_file_output: output_particles.parquet\n\n  # Initial off-momentum\n  delta_max: 27.e-5\n\n  # Tracking\n  n_turns: 1000000 # number of turns to track\n\n  # Beam to track\n  beam: lhcb1 #lhcb1 or lhcb2\n\n  # Context for the simulation\n  context: cpu   # 'cupy' # opencl\n\n  # Device number for GPU simulation\n  device_number: # 0\n</code></pre>"},{"location":"template_files/configurations/config_runIII_ions.html","title":"Run III ions configuration","text":"config_runIII_ions.yaml<pre><code>config_particles:\n  r_min: 4.0\n  r_max: 20.0\n  n_r: 256\n  n_angles: 5\n  n_split: 5\n  path_distribution_folder_output: particles\n\nconfig_mad:\n  # Links to be made for tools and scripts\n  links:\n    acc-models-lhc: /afs/cern.ch/eng/lhc/optics\n\n  # Optics file\n  optics_file: acc-models-lhc/runIII/RunIII_dev/ION_2024/opticsfile.21\n\n  # Beam parameters\n  beam_config:\n    lhcb1:\n      # Lead ions\n      beam_energy_tot: 557600 # [GeV] corresponds to 6.8 Z TeV\n      particle_mass: 193.6872729 # [GeV]\n      particle_charge: 82\n    lhcb2:\n      # Lead ions\n      beam_energy_tot: 557600 # [GeV] corresponds to 6.8 Z TeV\n      particle_mass: 193.6872729 # [GeV]\n      particle_charge: 82\n\n  # Ions being simulated\n  ions: true\n\n  # Enable machine imperfections\n  enable_imperfections: false\n\n  # Enable knob synthesis (for coupling correction, if no imperfections)\n  enable_knob_synthesis: true\n\n  # Rename the coupling knobs to avoid conflict between b1 and b2\n  # (for hllhc using old fortran code to generate the knobs)\n  rename_coupling_knobs: true\n\n  # Optics version, for choice of correction algorithms\n  # (ver_lhc_run or ver_hllhc_optics)\n  ver_hllhc_optics:\n  ver_lhc_run: 3.0\n\n  # Parameters for machine imperfections\n  pars_for_imperfections:\n    par_myseed: 1\n    par_correct_for_D2: 0\n    par_correct_for_MCBX: 0\n    par_on_errors_LHC: 1\n    par_off_errors_Q4_inIP15: 0\n    par_off_errors_Q5_inIP15: 0\n    par_on_errors_MBH: 1\n    par_on_errors_Q4: 1\n    par_on_errors_D2: 1\n    par_on_errors_D1: 1\n    par_on_errors_IT: 1\n    par_on_errors_MCBRD: 0\n    par_on_errors_MCBXF: 0\n    par_on_errors_NLC: 0\n    par_write_errortable: 1\n\n  phasing:\n    # RF voltage and phases\n    vrf400: 1148.0 # [MV]\n    lagrf400.b1: 0.5 # [rad]\n    lagrf400.b2: 0. # [rad]\n\n  # To make some specifics checks\n  sanity_checks: true\n\n  # Path of the collider file to be saved (usually at the end of the first generation)\n  path_collider_file_for_configuration_as_output: collider_file_for_configuration.json\n  compress: true # will compress the collider file, filename will end with .zip\n\n# Configuration for tuning of the collider\nconfig_collider:\n  # Even though the file doesn't end with .zip, scrip will first try to load it as a zip file\n  path_collider_file_for_configuration_as_input: ../collider_file_for_configuration.json\n  config_knobs_and_tuning:\n    knob_settings:\n      # Orbit knobs\n      on_x1: 170 # [urad]\n      on_sep1: 1e-3 # [mm]\n      on_x2v: -170 # [urad]\n      on_sep2h: 1e-3 # [mm]\n      on_sep2v: 0 # [mm]\n      on_x5: 170 # [urad]\n      on_sep5: 1e-3 # [mm]\n      on_x8h: -135 # [urad]\n      on_sep8h: 0 # [mm]\n      on_sep8v: 1e-10 # [mm]\n      on_ov2: 0 # [mm]\n      on_ov5: 0 # [mm]\n\n      # Spurious dispersion correction\n      on_disp: 1.000\n\n      # Magnets of the experiments\n      on_alice_normalized: 1\n      on_lhcb_normalized: -1\n      on_sol_atlas: 0\n      on_sol_cms: 0\n      on_sol_alice: 0\n\n      # Octupoles\n      i_oct_b1: 100. # [A]\n      i_oct_b2: 100. # [A]\n\n    # Tunes and chromaticities\n    qx:\n      lhcb1: 62.31\n      lhcb2: 62.31\n    qy:\n      lhcb1: 60.32\n      lhcb2: 60.32\n    dqx:\n      lhcb1: 10\n      lhcb2: 10\n    dqy:\n      lhcb1: 10\n      lhcb2: 10\n\n    # Linear coupling\n    delta_cmr: 0.0\n    delta_cmi: 0.0\n\n    knob_names:\n      lhcb1:\n        q_knob_1: dqx.b1_sq\n        q_knob_2: dqy.b1_sq\n        dq_knob_1: dqpx.b1_sq\n        dq_knob_2: dqpy.b1_sq\n        c_minus_knob_1: cmrs.b1_sq\n        c_minus_knob_2: cmis.b1_sq\n\n      lhcb2:\n        q_knob_1: dqx.b2_sq\n        q_knob_2: dqy.b2_sq\n        dq_knob_1: dqpx.b2_sq\n        dq_knob_2: dqpy.b2_sq\n        c_minus_knob_1: cmrs.b2_sq\n        c_minus_knob_2: cmis.b2_sq\n\n  config_beambeam:\n    skip_beambeam: false\n    bunch_spacing_buckets: 10\n    num_slices_head_on: 11\n    num_long_range_encounters_per_side:\n      ip1: 20\n      ip2: 18\n      ip5: 20\n      ip8: 18\n    sigma_z: 0.0824\n    num_particles_per_bunch: 180000000.0\n    nemitt_x: 1.65e-6\n    nemitt_y: 1.65e-6\n    mask_with_filling_pattern:\n      # If not already existing in the study-da package, pattern must have an absolute path or be\n      # added as a dependency for the run file\n      pattern_fname: 50ns_1240b_1088_1088_398_56bpi_PbPb_converted.json\n      i_bunch_b1:      # If not specified, the bunch with the worst schedule is chosen\n      i_bunch_b2:      # Same. A value for i_bunch_b1 and i_bunch_b2 must be specified if pattern_fname is specified\n    cross_section: 281e-24\n\n  skip_leveling: false\n  config_lumi_leveling:\n    ip1:\n      luminosity: 6.4e+27\n      num_colliding_bunches:\n      impose_separation_orthogonal_to_crossing: false\n      knobs:\n      - on_sep1\n      bump_range:\n        lhcb1:\n        - e.ds.l1.b1\n        - s.ds.r1.b1\n        lhcb2:\n        - s.ds.r1.b2\n        - e.ds.l1.b2\n      preserve_angles_at_ip: true\n      preserve_bump_closure: true\n      corrector_knob_names:\n      - corr_co_acbyvs4.l1b1\n      - corr_co_acbyhs4.l1b1\n      - corr_co_acbyvs4.r1b2\n      - corr_co_acbyhs4.r1b2\n      - corr_co_acbyhs4.r1b1\n      - corr_co_acbyvs4.r1b1\n      - corr_co_acbyhs4.l1b2\n      - corr_co_acbyvs4.l1b2\n      - corr_co_acbcv5.l1b1\n      - corr_co_acbch5.r1b1\n      - corr_co_acbcv5.r1b2\n      - corr_co_acbch5.l1b2\n\n    ip5:\n      luminosity: 6.4e+27\n      num_colliding_bunches:\n      impose_separation_orthogonal_to_crossing: false\n      knobs:\n      - on_sep5\n      bump_range:\n        lhcb1:\n        - e.ds.l5.b1\n        - s.ds.r5.b1\n        lhcb2:\n        - s.ds.r5.b2\n        - e.ds.l5.b2\n      preserve_angles_at_ip: true\n      preserve_bump_closure: true\n      corrector_knob_names:\n      - corr_co_acbcv5.l5b1\n      - corr_co_acbch5.r5b1\n      - corr_co_acbyhs4.l5b1\n      - corr_co_acbyhs4.r5b1\n      - corr_co_acbyvs4.l5b1\n      - corr_co_acbyvs4.r5b1\n      - corr_co_acbcv5.r5b2\n      - corr_co_acbch5.l5b2\n      - corr_co_acbyhs4.l5b2\n      - corr_co_acbyhs4.r5b2\n      - corr_co_acbyvs4.l5b2\n      - corr_co_acbyvs4.r5b2\n\n    ip2:\n      luminosity: 6.4e+27\n      num_colliding_bunches:\n      impose_separation_orthogonal_to_crossing: false\n      knobs:\n      - on_sep2h\n      bump_range:\n        lhcb1:\n        - e.ds.l2.b1\n        - s.ds.r2.b1\n        lhcb2:\n        - s.ds.r2.b2\n        - e.ds.l2.b2\n      preserve_angles_at_ip: true\n      preserve_bump_closure: true\n      corrector_knob_names:\n        # to preserve angles at ip\n      - corr_co_acbyvs4.l2b1\n      - corr_co_acbyhs4.l2b1\n      - corr_co_acbyvs4.r2b2\n      - corr_co_acbyhs4.r2b2\n          # to close the bumps\n      - corr_co_acbyvs4.l2b2\n      - corr_co_acbyhs4.l2b2\n      - corr_co_acbyvs4.r2b1\n      - corr_co_acbyhs4.r2b1\n      - corr_co_acbyhs5.l2b2\n      - corr_co_acbyvs5.l2b2\n      - corr_co_acbchs5.r2b1\n      - corr_co_acbcvs5.r2b1\n\n    # ! There seems to be a problem with the lumi optimizer at IP8\n    # ! Not important for the moment as we only consider head-on, but will have to be solved\n    # ip8:\n    #   luminosity: 1.0e+27\n    #   num_colliding_bunches:\n    #   impose_separation_orthogonal_to_crossing: false\n    #   knobs:\n    #     - on_sep8v\n    #   bump_range:\n    #     lhcb1:\n    #       - e.ds.l8.b1\n    #       - s.ds.r8.b1\n    #     lhcb2:\n    #       - s.ds.r8.b2\n    #       - e.ds.l8.b2\n    #   preserve_angles_at_ip: true\n    #   preserve_bump_closure: true\n    #   corrector_knob_names:\n    #     # to preserve angles at ip\n    #     - corr_co_acbyvs4.l8b1\n    #     - corr_co_acbyhs4.l8b1\n    #     - corr_co_acbyvs4.r8b2\n    #     - corr_co_acbyhs4.r8b2\n    #       # to close the bumps\n    #     - corr_co_acbyvs4.l8b2\n    #     - corr_co_acbyhs4.l8b2\n    #     - corr_co_acbyvs4.r8b1\n    #     - corr_co_acbyhs4.r8b1\n    #     - corr_co_acbcvs5.l8b2\n    #     - corr_co_acbchs5.l8b2\n    #     - corr_co_acbyvs5.r8b1\n    #     - corr_co_acbyhs5.r8b1\n\n  # Save collider or not (usually at the end of the collider tuning)\n  save_output_collider: false\n  path_collider_file_for_tracking_as_output: collider_file_for_tracking.json\n  compress: true # will compress the collider file, filename will end with .zip\n\nconfig_simulation:\n  # Collider file to load for the tracking\n  path_collider_file_for_tracking_as_input: ../collider_file_for_tracking.json\n\n  # Distribution in the normalized xy space\n  path_distribution_folder_input: ../particles\n  distribution_file: 00.parquet\n\n  # Output particle file\n  path_distribution_file_output: output_particles.parquet\n\n  # Initial off-momentum\n  delta_max: 0.00024\n\n  # Tracking\n  n_turns: 1000000 # number of turns to track\n\n  # Beam to track\n  beam: lhcb1 #lhcb1 or lhcb2\n\n  # Context for the simulation\n  context: cpu   # 'cupy' # opencl\n\n  # Device number for GPU simulation\n  device_number: # 0\n</code></pre>"},{"location":"template_files/filling_schemes/index.html","title":"List of available filling schemes","text":"<ul> <li> <p>25ns_1886b_1873_1217_1173_236bpi_12inj_hybrid_2INDIV_converted.json</p> </li> <li> <p>25ns_2464b_2452_1842_1821_236bpi_12inj_hybrid_converted.json</p> </li> <li> <p>25ns_2464b_2452_1842_1821_236bpi_12inj_hybrid.json</p> </li> <li> <p>25ns_2228b_2216_1686_2112_hybrid_8b4e_2x56b_25ns_3x48b_12inj_with_identical_bunches.json</p> </li> <li> <p>25ns_2374b_2361_1730_1773_236bpi_13inj_hybrid_2INDIV_converted_with_identical_bunches.json</p> </li> <li> <p>25ns_2228b_2216_1686_2112_hybrid_8b4e_2x56b_25ns_3x48b_12inj.json</p> </li> <li> <p>25ns_2374b_2361_1730_1773_236bpi_13inj_hybrid_2INDIV.json</p> </li> <li> <p>25ns_2604b_2592_2310_2421_4x48bpi_16inj.json</p> </li> <li> <p>50ns_1240b_1088_1088_398_56bpi_PbPb.json</p> </li> <li> <p>50ns_160b_160_80_73_40bpi_PbPb_BBMD.json</p> </li> <li> <p>25ns_2374b_2361_1730_1773_236bpi_13inj_hybrid_2INDIV_converted.json</p> </li> <li> <p>8b4e_1972b_1960_1178_1886_224bpi_12inj_800ns_bs200ns.json</p> </li> <li> <p>25ns_2076b_2064_1327_1928_216bpi_12inj_800ns_bs200ns_with_identical_bunches.json</p> </li> <li> <p>25ns_2464b_2452_1842_1821_236bpi_12inj_hybrid_converted_with_identical_bunches.json</p> </li> <li> <p>25ns_1983b_1970_1657_1684_144bpi_19inj_3INDIVs_converted.json</p> </li> <li> <p>25ns_2358b_2345_1692_1628_236bpi_14inj_hybrid_2INDIV.json</p> </li> <li> <p>25ns_1983b_1970_1657_1684_144bpi_19inj_3INDIVs.json</p> </li> <li> <p>full_scheme.json</p> </li> <li> <p>25ns_2076b_2064_1327_1928_216bpi_12inj_800ns_bs200ns.json</p> </li> <li> <p>25ns_1886b_1873_1217_1173_236bpi_12inj_hybrid_2INDIV.json</p> </li> <li> <p>25ns_2760b_2748_2492_2574_288bpi_13inj_800ns_bs200ns_converted.json</p> </li> <li> <p>50ns_961b_880_880_209_40bpi_25inj_PbPb.json</p> </li> <li> <p>25ns_2464b_2452_1842_1821_236bpi_12inj_hybrid copy.json</p> </li> <li> <p>50ns_1240b_1088_1088_398_56bpi_PbPb_converted.json</p> </li> <li> <p>25ns_2760b_2748_2492_2574_288bpi_13inj_800ns_bs200ns.json</p> </li> <li> <p>25ns_2358b_2345_1692_1628_236bpi_14inj_hybrid_2INDIV_converted.json</p> </li> </ul>"},{"location":"template_files/mapping/parameters_lhc.html","title":"Parameters mapping","text":"parameters_lhc.yaml<pre><code>### This configuration contains the parameters and the keys needed to run the postprocessing of the study.\n\n# Config mad\noptics_file: [config_mad, optics_file]\nbeam_energy_tot_b1: [config_mad, beam_config, lhcb1, beam_energy_tot]\nbeam_energy_tot_b2: [config_mad, beam_config, lhcb2, beam_energy_tot]\nions: [config_mad, ions]\nver_hllhc_optics: [config_mad, ver_hllhc_optics]\nver_lhc_run: [config_mad, ver_lhc_run]\n\n# Knobs\nqx_b1: [config_collider, config_knobs_and_tuning, qx, lhcb1]\nqx_b2: [config_collider, config_knobs_and_tuning, qx, lhcb2]\nqy_b1: [config_collider, config_knobs_and_tuning, qy, lhcb1]\nqy_b2: [config_collider, config_knobs_and_tuning, qy, lhcb2]\ndqx_b1: [config_collider, config_knobs_and_tuning, dqx, lhcb1]\ndqx_b2: [config_collider, config_knobs_and_tuning, dqx, lhcb2]\ndqy_b1: [config_collider, config_knobs_and_tuning, dqy, lhcb1]\ndqy_b2: [config_collider, config_knobs_and_tuning, dqy, lhcb1]\ndelta_cmr: [config_collider, config_knobs_and_tuning, delta_cmr]\ndelta_cmi: [config_collider, config_knobs_and_tuning, delta_cmi]\ni_oct_b1: [config_collider, config_knobs_and_tuning, knob_settings, i_oct_b1]\ni_oct_b2: [config_collider, config_knobs_and_tuning, knob_settings, i_oct_b2]\non_disp: [config_collider, config_knobs_and_tuning, knob_settings, on_disp]\non_crab1: [config_collider, config_knobs_and_tuning, knob_settings, on_crab1]\non_crab5: [config_collider, config_knobs_and_tuning, knob_settings, on_crab5]\non_alice_normalized: [config_collider, config_knobs_and_tuning, knob_settings, on_alice_normalized]\non_lhcb_normalized: [config_collider, config_knobs_and_tuning, knob_settings, on_lhcb_normalized]\n\n# Knobs that might be optimized for levelling purposes\non_x1: [config_collider, config_knobs_and_tuning, knob_settings, on_x1]\nfinal_on_x1: [config_collider, config_knobs_and_tuning, knob_settings, final_on_x1]\non_x2: [config_collider, config_knobs_and_tuning, knob_settings, on_x2]\nfinal_on_x2: [config_collider, config_knobs_and_tuning, knob_settings, final_on_x2]\non_x2h: [config_collider, config_knobs_and_tuning, knob_settings, on_x2h]\nfinal_on_x2h: [config_collider, config_knobs_and_tuning, knob_settings, final_on_x2h]\non_x2v: [config_collider, config_knobs_and_tuning, knob_settings, on_x2v]\nfinal_on_x2v: [config_collider, config_knobs_and_tuning, knob_settings, final_on_x2v]\non_x5: [config_collider, config_knobs_and_tuning, knob_settings, on_x5]\nfinal_on_x5: [config_collider, config_knobs_and_tuning, knob_settings, final_on_x5]\non_x8: [config_collider, config_knobs_and_tuning, knob_settings, on_x8]\nfinal_on_x8: [config_collider, config_knobs_and_tuning, knob_settings, final_on_x8]\non_x8h: [config_collider, config_knobs_and_tuning, knob_settings, on_x8h]\nfinal_on_x8h: [config_collider, config_knobs_and_tuning, knob_settings, final_on_x8h]\non_x8v: [config_collider, config_knobs_and_tuning, knob_settings, on_x8v]\nfinal_on_x8v: [config_collider, config_knobs_and_tuning, knob_settings, final_on_x8v]\non_sep1: [config_collider, config_knobs_and_tuning, knob_settings, on_sep1]\nfinal_on_sep1: [config_collider, config_knobs_and_tuning, knob_settings, final_on_sep1]\non_sep2: [config_collider, config_knobs_and_tuning, knob_settings, on_sep2]\nfinal_on_sep2: [config_collider, config_knobs_and_tuning, knob_settings, final_on_sep2]\non_sep2h: [config_collider, config_knobs_and_tuning, knob_settings, on_sep2h]\nfinal_on_sep2h: [config_collider, config_knobs_and_tuning, knob_settings, final_on_sep2h]\non_sep2v: [config_collider, config_knobs_and_tuning, knob_settings, on_sep2v]\nfinal_on_sep2v: [config_collider, config_knobs_and_tuning, knob_settings, final_on_sep2v]\non_sep5: [config_collider, config_knobs_and_tuning, knob_settings, on_sep5]\nfinal_on_sep5: [config_collider, config_knobs_and_tuning, knob_settings, final_on_sep5]\non_sep8: [config_collider, config_knobs_and_tuning, knob_settings, on_sep8]\nfinal_on_sep8: [config_collider, config_knobs_and_tuning, knob_settings, final_on_sep8]\non_sep8h: [config_collider, config_knobs_and_tuning, knob_settings, on_sep8h]\nfinal_on_sep8h: [config_collider, config_knobs_and_tuning, knob_settings, final_on_sep8h]\non_sep8v: [config_collider, config_knobs_and_tuning, knob_settings, on_sep8v]\nfinal_on_sep8v: [config_collider, config_knobs_and_tuning, knob_settings, final_on_sep8v]\n\n# Beta functions\nbeta_x_ip1: [config_collider, beta_x_ip1]\nbeta_y_ip1: [config_collider, beta_y_ip1]\nbeta_x_ip2: [config_collider, beta_x_ip2]\nbeta_y_ip2: [config_collider, beta_y_ip2]\nbeta_x_ip5: [config_collider, beta_x_ip5]\nbeta_y_ip5: [config_collider, beta_y_ip5]\nbeta_x_ip8: [config_collider, beta_x_ip8]\nbeta_y_ip8: [config_collider, beta_y_ip8]\n\n# Configuration beam-beam\nsigma_z: [config_collider, config_beambeam, sigma_z]\nnemitt_x: [config_collider, config_beambeam, nemitt_x]\nnemitt_y: [config_collider, config_beambeam, nemitt_y]\npattern_fname: [config_collider, config_beambeam, mask_with_filling_pattern, pattern_fname]\ni_bunch_b1: [config_collider, config_beambeam, mask_with_filling_pattern, i_bunch_b1]\ni_bunch_b2: [config_collider, config_beambeam, mask_with_filling_pattern, i_bunch_b2]\ncross_section: [config_collider, config_beambeam, cross_section]\n\n# Variables that are optimized\nnum_particles_per_bunch: [config_collider, config_beambeam, num_particles_per_bunch]\nfinal_num_particles_per_bunch: [config_collider, config_beambeam, final_num_particles_per_bunch]\nluminosity_ip1_without_beam_beam: [config_collider, config_beambeam, luminosity_ip1_without_beam_beam]\nluminosity_ip1_with_beam_beam: [config_collider, config_beambeam, luminosity_ip1_with_beam_beam]\nluminosity_ip2_without_beam_beam: [config_collider, config_beambeam, luminosity_ip2_without_beam_beam]\nluminosity_ip2_with_beam_beam: [config_collider, config_beambeam, luminosity_ip2_with_beam_beam]\nluminosity_ip5_without_beam_beam: [config_collider, config_beambeam, luminosity_ip5_without_beam_beam]\nluminosity_ip5_with_beam_beam: [config_collider, config_beambeam, luminosity_ip5_with_beam_beam]\nluminosity_ip8_without_beam_beam: [config_collider, config_beambeam, luminosity_ip8_without_beam_beam]\nluminosity_ip8_with_beam_beam: [config_collider, config_beambeam, luminosity_ip8_with_beam_beam]\nPile-up_ip1_without_beam_beam: [config_collider, config_beambeam, Pile-up_ip1_without_beam_beam]\nPile-up_ip1_with_beam_beam: [config_collider, config_beambeam, Pile-up_ip1_with_beam_beam]\nPile-up_ip2_without_beam_beam: [config_collider, config_beambeam, Pile-up_ip2_without_beam_beam]\nPile-up_ip2_with_beam_beam: [config_collider, config_beambeam, Pile-up_ip2_with_beam_beam]\nPile-up_ip5_without_beam_beam: [config_collider, config_beambeam, Pile-up_ip5_without_beam_beam]\nPile-up_ip5_with_beam_beam: [config_collider, config_beambeam, Pile-up_ip5_with_beam_beam]\nPile-up_ip8_without_beam_beam: [config_collider, config_beambeam, Pile-up_ip8_without_beam_beam]\nPile-up_ip8_with_beam_beam: [config_collider, config_beambeam, Pile-up_ip8_with_beam_beam]\n\n# Config simulation\nbeam: [config_simulation, beam]\ndelta_max: [config_simulation, delta_max]\nn_turns: [config_simulation, n_turns]\n</code></pre>"},{"location":"template_files/scripts/generation_1.html","title":"Generation 1 (generate xsuite collider from mad sequence) template script","text":"generation_1.py<pre><code>\"\"\"This is a template script for generation 1 of simulation study, in which ones generates a\nparticle distribution and a collider from a MAD-X model.\"\"\"\n\n# ==================================================================================================\n# --- Imports\n# ==================================================================================================\n\n# Import standard library modules\nimport logging\nimport os\nimport sys\n\n# Import third-party modules\n# Import user-defined modules\nfrom study_da.generate import MadCollider, ParticlesDistribution\nfrom study_da.utils import (\n    load_dic_from_path,\n    set_item_in_dic,\n    write_dic_to_path,\n)\n\n# Set up the logger here if needed\n\n\n# ==================================================================================================\n# --- Script functions\n# ==================================================================================================\ndef build_distribution(config_particles):\n    # Build object for generating particle distribution\n    distr = ParticlesDistribution(config_particles)\n\n    # Build particle distribution\n    particle_list = distr.return_distribution_as_list()\n\n    # Write particle distribution to file\n    distr.write_particle_distribution_to_disk(particle_list)\n\n\ndef build_collider(config_mad):\n    # Build object for generating collider from mad\n    mc = MadCollider(config_mad)\n\n    # Build mad model\n    mad_b1b2, mad_b4 = mc.prepare_mad_collider()\n\n    # Build collider from mad model\n    collider = mc.build_collider(mad_b1b2, mad_b4)\n\n    # Twiss to ensure everything is ok\n    mc.activate_RF_and_twiss(collider)\n\n    # Clean temporary files\n    mc.clean_temporary_files()\n\n    # Save collider to json\n    mc.write_collider_to_disk(collider)\n\n\n# ==================================================================================================\n# --- Parameters placeholders definition\n# ==================================================================================================\ndict_mutated_parameters = {}  ###---parameters---###\npath_configuration = \"{}  ###---main_configuration---###\"\npath_root_study = \"{}  ###---path_root_study---###\"\n\n# In case the placeholders have not been replaced, use default path\nif path_configuration.startswith(\"{}\"):\n    path_configuration = \"config.yaml\"\n\nif path_root_study.startswith(\"{}\"):\n    path_root_study = \".\"\n\nsys.path.append(path_root_study)\n# Import modules placed at the root of the study here\n# ==================================================================================================\n# --- Script for execution\n# ==================================================================================================\n\nif __name__ == \"__main__\":\n    logging.info(\"Starting script to build particle distribution and collider\")\n\n    # Load full configuration\n    full_configuration, ryaml = load_dic_from_path(path_configuration)\n\n    # Mutate parameters in configuration\n    for key, value in dict_mutated_parameters.items():\n        set_item_in_dic(full_configuration, key, value)\n\n    # Dump configuration\n    name_configuration = os.path.basename(path_configuration)\n    write_dic_to_path(full_configuration, name_configuration, ryaml)\n\n    # Build and save particle distribution\n    build_distribution(full_configuration[\"config_particles\"])\n\n    # Build and save collider\n    build_collider(full_configuration[\"config_mad\"])\n\n    logging.info(\"Script finished\")\n</code></pre>"},{"location":"template_files/scripts/generation_2_level_by_nb.html","title":"Generation 2 (configure and level by bunch intensity, track) template script","text":"generation_2_level_by_nb.py<pre><code>\"\"\"This is a template script for generation 2 of simulation study, in which ones configures a\na Xsuite collider (including luminosity levelling and beam-beam) and tracks a given particle\ndistribution.\"\"\"\n\n# ==================================================================================================\n# --- Imports\n# ==================================================================================================\n\n# Import standard library modules\nimport argparse\nimport contextlib\nimport logging\nimport os\nimport sys\nimport time\n\n# Import third-party modules\nimport numpy as np\nimport pandas as pd\n\n# Import user-defined modules\nfrom study_da.generate import XsuiteCollider, XsuiteTracking\nfrom study_da.utils import (\n    load_dic_from_path,\n    set_item_in_dic,\n    write_dic_to_path,\n)\n\n# Set up the logger here if needed\n\n\n# ==================================================================================================\n# --- Script functions\n# ==================================================================================================\ndef configure_collider(full_configuration):\n    # Get configuration\n    config_collider = full_configuration[\"config_collider\"]\n    ver_hllhc_optics = full_configuration[\"config_mad\"][\"ver_hllhc_optics\"]\n    ver_lhc_run = full_configuration[\"config_mad\"][\"ver_lhc_run\"]\n    ions = full_configuration[\"config_mad\"][\"ions\"]\n    collider_filepath = full_configuration[\"config_collider\"][\n        \"path_collider_file_for_configuration_as_input\"\n    ]\n\n    # Build object for configuring collider\n    xc = XsuiteCollider(config_collider, collider_filepath, ver_hllhc_optics, ver_lhc_run, ions)\n\n    # Load collider\n    collider = xc.load_collider()\n\n    # Install beam-beam\n    xc.install_beam_beam_wrapper(collider)\n\n    # Build trackers\n    # For now, start with CPU tracker due to a bug with Xsuite\n    # Refer to issue https://github.com/xsuite/xsuite/issues/450\n    collider.build_trackers()  # (_context=context)\n\n    # Set knobs\n    xc.set_knobs(collider)\n\n    # Match tune and chromaticity\n    xc.match_tune_and_chroma(collider, match_linear_coupling_to_zero=True)\n\n    # Set filling scheme\n    xc.set_filling_and_bunch_tracked(ask_worst_bunch=False)\n\n    # Compute the number of collisions in the different IPs\n    n_collisions_ip1_and_5, n_collisions_ip2, n_collisions_ip8 = xc.compute_collision_from_scheme()\n\n    # Do the leveling if requested\n    if \"config_lumi_leveling\" in config_collider and not config_collider[\"skip_leveling\"]:\n        xc.level_ip1_5_by_bunch_intensity(collider, n_collisions_ip1_and_5)\n        xc.level_ip2_8_by_separation(n_collisions_ip2, n_collisions_ip8, collider)\n    else:\n        logging.warning(\n            \"No leveling is done as no configuration has been provided, or skip_leveling\"\n            \" is set to True.\"\n        )\n\n    # Add linear coupling\n    xc.add_linear_coupling(collider)\n\n    # Rematch tune and chromaticity\n    xc.match_tune_and_chroma(collider, match_linear_coupling_to_zero=False)\n\n    # Assert that tune, chromaticity and linear coupling are correct one last time\n    xc.assert_tune_chroma_coupling(collider)\n\n    # Record beta functions in the configuration\n    xc.record_beta_functions(collider)\n\n    # Configure beam-beam if needed\n    if not xc.config_beambeam[\"skip_beambeam\"]:\n        xc.configure_beam_beam(collider)\n\n    # Update configuration with luminosity now that bb is known\n    l_n_collisions = [\n        n_collisions_ip1_and_5,\n        n_collisions_ip2,\n        n_collisions_ip1_and_5,\n        n_collisions_ip8,\n    ]\n    xc.record_final_luminosity(collider, l_n_collisions)\n\n    # Save collider to json (flag to save or not is inside function)\n    xc.write_collider_to_disk(collider, full_configuration)\n\n    # Get fingerprint\n    fingerprint = xc.return_fingerprint(collider)\n\n    return collider, fingerprint\n\n\ndef track_particles(full_configuration, collider, fingerprint):\n    # Get emittances\n    n_emitt_x = full_configuration[\"config_collider\"][\"config_beambeam\"][\"nemitt_x\"]\n    n_emitt_y = full_configuration[\"config_collider\"][\"config_beambeam\"][\"nemitt_y\"]\n    xst = XsuiteTracking(full_configuration[\"config_simulation\"], n_emitt_x, n_emitt_y)\n\n    # Prepare particle distribution\n    particles, particle_id, l_amplitude, l_angle = xst.prepare_particle_distribution_for_tracking(\n        collider\n    )\n\n    # Track\n    particles_dict = xst.track(collider, particles)\n\n    # Convert particles to dataframe\n    particles_df = pd.DataFrame(particles_dict)\n\n    # ! Very important, otherwise the particles will be mixed in each subset\n    # Sort by parent_particle_id\n    particles_df = particles_df.sort_values(\"parent_particle_id\")\n\n    # Assign the old id to the sorted dataframe\n    particles_df[\"particle_id\"] = particle_id\n\n    # Register the amplitude and angle in the dataframe\n    particles_df[\"normalized amplitude in xy-plane\"] = l_amplitude\n    particles_df[\"angle in xy-plane [deg]\"] = l_angle * 180 / np.pi\n\n    # Add some metadata to the output for better interpretability\n    particles_df.attrs[\"hash\"] = hash(fingerprint)\n    particles_df.attrs[\"fingerprint\"] = fingerprint\n    particles_df.attrs[\"configuration\"] = full_configuration\n    particles_df.attrs[\"date\"] = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    # Save output\n    particles_df.to_parquet(\n        full_configuration[\"config_simulation\"][\"path_distribution_file_output\"]\n    )\n\n\ndef clean():\n    # Remote the correction folder, and potential C files remaining\n    with contextlib.suppress(Exception):\n        os.system(\"rm -rf correction\")\n        os.system(\"rm -f *.cc\")\n\n\n# ==================================================================================================\n# --- Parameters placeholders definition\n# ==================================================================================================\ndict_mutated_parameters = {}  ###---parameters---###\npath_configuration = \"{}  ###---main_configuration---###\"\npath_root_study = \"{}  ###---path_root_study---###\"\n\n# In case the placeholders have not been replaced, use default path\nif path_configuration.startswith(\"{}\"):\n    path_configuration = \"config.yaml\"\n\nif path_root_study.startswith(\"{}\"):\n    path_root_study = \".\"\n\nsys.path.append(path_root_study)\n# Import modules placed at the root of the study here\n# ==================================================================================================\n# --- Script for execution\n# ==================================================================================================\n\nif __name__ == \"__main__\":\n    logging.info(\"Starting script to configure collider and track\")\n\n    # Parse potential arguments, e.g. to save output collider\n    aparser = argparse.ArgumentParser()\n    aparser.add_argument(\n        \"-s\", \"--save\", help=\"Save output collider for inspection\", action=\"store_true\"\n    )\n    args = aparser.parse_args()\n    if args.save:\n        dict_mutated_parameters[\"save_output_collider\"] = True\n\n    # Load full configuration\n    full_configuration, ryaml = load_dic_from_path(path_configuration)\n\n    # Mutate parameters in configuration\n    for key, value in dict_mutated_parameters.items():\n        set_item_in_dic(full_configuration, key, value)\n\n    # Configure collider\n    collider, fingerprint = configure_collider(full_configuration)\n\n    # Drop updated configuration\n    name_configuration = os.path.basename(path_configuration)\n    write_dic_to_path(full_configuration, name_configuration, ryaml)\n\n    # Track particles and save to disk\n    track_particles(full_configuration, collider, fingerprint)\n\n    # Clean temporary files\n    clean()\n\n    logging.info(\"Script finished\")\n</code></pre>"},{"location":"template_files/scripts/generation_2_level_by_sep.html","title":"Generation 2 (configure and level by separation, track) template script","text":"generation_2_level_by_sep.py<pre><code>\"\"\"This is a template script for generation 2 of simulation study, in which ones configures a\na Xsuite collider (including luminosity levelling and beam-beam) and tracks a given particle\ndistribution.\"\"\"\n\n# ==================================================================================================\n# --- Imports\n# ==================================================================================================\n\n# Import standard library modules\nimport argparse\nimport contextlib\nimport logging\nimport os\nimport sys\nimport time\n\n# Import third-party modules\nimport numpy as np\nimport pandas as pd\n\n# Import user-defined modules\nfrom study_da.generate import XsuiteCollider, XsuiteTracking\nfrom study_da.utils import (\n    load_dic_from_path,\n    set_item_in_dic,\n    write_dic_to_path,\n)\n\n# Set up the logger here if needed\n\n\n# ==================================================================================================\n# --- Script functions\n# ==================================================================================================\ndef configure_collider(full_configuration):\n    # Get configuration\n    config_collider = full_configuration[\"config_collider\"]\n    ver_hllhc_optics = full_configuration[\"config_mad\"][\"ver_hllhc_optics\"]\n    ver_lhc_run = full_configuration[\"config_mad\"][\"ver_lhc_run\"]\n    ions = full_configuration[\"config_mad\"][\"ions\"]\n    collider_filepath = full_configuration[\"config_collider\"][\n        \"path_collider_file_for_configuration_as_input\"\n    ]\n\n    # Build object for configuring collider\n    xc = XsuiteCollider(config_collider, collider_filepath, ver_hllhc_optics, ver_lhc_run, ions)\n\n    # Load collider\n    collider = xc.load_collider()\n\n    # Install beam-beam\n    xc.install_beam_beam_wrapper(collider)\n\n    # Build trackers\n    # For now, start with CPU tracker due to a bug with Xsuite\n    # Refer to issue https://github.com/xsuite/xsuite/issues/450\n    collider.build_trackers()  # (_context=context)\n\n    # Set knobs\n    xc.set_knobs(collider)\n\n    # Match tune and chromaticity\n    xc.match_tune_and_chroma(collider, match_linear_coupling_to_zero=True)\n\n    # Set filling scheme\n    xc.set_filling_and_bunch_tracked(ask_worst_bunch=False)\n\n    # Compute the number of collisions in the different IPs\n    n_collisions_ip1_and_5, n_collisions_ip2, n_collisions_ip8 = xc.compute_collision_from_scheme()\n\n    # Do the leveling if requested\n    if \"config_lumi_leveling\" in config_collider and not config_collider[\"skip_leveling\"]:\n        xc.level_all_by_separation(\n            n_collisions_ip1_and_5, n_collisions_ip2, n_collisions_ip8, collider\n        )\n    else:\n        logging.warning(\n            \"No leveling is done as no configuration has been provided, or skip_leveling\"\n            \" is set to True.\"\n        )\n\n    # Add linear coupling\n    xc.add_linear_coupling(collider)\n\n    # Rematch tune and chromaticity\n    xc.match_tune_and_chroma(collider, match_linear_coupling_to_zero=False)\n\n    # Assert that tune, chromaticity and linear coupling are correct one last time\n    xc.assert_tune_chroma_coupling(collider)\n\n    # Record beta functions in the configuration\n    xc.record_beta_functions(collider)\n\n    # Configure beam-beam if needed\n    if not xc.config_beambeam[\"skip_beambeam\"]:\n        xc.configure_beam_beam(collider)\n\n    # Update configuration with luminosity now that bb is known\n    l_n_collisions = [\n        n_collisions_ip1_and_5,\n        n_collisions_ip2,\n        n_collisions_ip1_and_5,\n        n_collisions_ip8,\n    ]\n    xc.record_final_luminosity(collider, l_n_collisions)\n\n    # Save collider to json (flag to save or not is inside function)\n    xc.write_collider_to_disk(collider, full_configuration)\n\n    # Get fingerprint\n    fingerprint = xc.return_fingerprint(collider)\n\n    return collider, fingerprint\n\n\ndef track_particles(full_configuration, collider, fingerprint):\n    # Get emittances\n    n_emitt_x = full_configuration[\"config_collider\"][\"config_beambeam\"][\"nemitt_x\"]\n    n_emitt_y = full_configuration[\"config_collider\"][\"config_beambeam\"][\"nemitt_y\"]\n    xst = XsuiteTracking(full_configuration[\"config_simulation\"], n_emitt_x, n_emitt_y)\n\n    # Prepare particle distribution\n    particles, particle_id, l_amplitude, l_angle = xst.prepare_particle_distribution_for_tracking(\n        collider\n    )\n\n    # Track\n    particles_dict = xst.track(collider, particles)\n\n    # Convert particles to dataframe\n    particles_df = pd.DataFrame(particles_dict)\n\n    # ! Very important, otherwise the particles will be mixed in each subset\n    # Sort by parent_particle_id\n    particles_df = particles_df.sort_values(\"parent_particle_id\")\n\n    # Assign the old id to the sorted dataframe\n    particles_df[\"particle_id\"] = particle_id\n\n    # Register the amplitude and angle in the dataframe\n    particles_df[\"normalized amplitude in xy-plane\"] = l_amplitude\n    particles_df[\"angle in xy-plane [deg]\"] = l_angle * 180 / np.pi\n\n    # Add some metadata to the output for better interpretability\n    particles_df.attrs[\"hash\"] = hash(fingerprint)\n    particles_df.attrs[\"fingerprint\"] = fingerprint\n    particles_df.attrs[\"configuration\"] = full_configuration\n    particles_df.attrs[\"date\"] = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    # Save output\n    particles_df.to_parquet(\n        full_configuration[\"config_simulation\"][\"path_distribution_file_output\"]\n    )\n\n\ndef clean():\n    # Remote the correction folder, and potential C files remaining\n    with contextlib.suppress(Exception):\n        os.system(\"rm -rf correction\")\n        os.system(\"rm -f *.cc\")\n\n\n# ==================================================================================================\n# --- Parameters placeholders definition\n# ==================================================================================================\ndict_mutated_parameters = {}  ###---parameters---###\npath_configuration = \"{}  ###---main_configuration---###\"\npath_root_study = \"{}  ###---path_root_study---###\"\n\n# In case the placeholders have not been replaced, use default path\nif path_configuration.startswith(\"{}\"):\n    path_configuration = \"config.yaml\"\n\nif path_root_study.startswith(\"{}\"):\n    path_root_study = \".\"\n\nsys.path.append(path_root_study)\n# Import modules placed at the root of the study here\n# ==================================================================================================\n# --- Script for execution\n# ==================================================================================================\n\nif __name__ == \"__main__\":\n    logging.info(\"Starting script to configure collider and track\")\n\n    # Parse potential arguments, e.g. to save output collider\n    aparser = argparse.ArgumentParser()\n    aparser.add_argument(\n        \"-s\", \"--save\", help=\"Save output collider for inspection\", action=\"store_true\"\n    )\n    args = aparser.parse_args()\n    if args.save:\n        dict_mutated_parameters[\"save_output_collider\"] = True\n\n    # Load full configuration\n    full_configuration, ryaml = load_dic_from_path(path_configuration)\n\n    # Mutate parameters in configuration\n    for key, value in dict_mutated_parameters.items():\n        set_item_in_dic(full_configuration, key, value)\n\n    # Configure collider\n    collider, fingerprint = configure_collider(full_configuration)\n\n    # Drop updated configuration\n    name_configuration = os.path.basename(path_configuration)\n    write_dic_to_path(full_configuration, name_configuration, ryaml)\n\n    # Track particles and save to disk\n    track_particles(full_configuration, collider, fingerprint)\n\n    # Clean temporary files\n    clean()\n\n    logging.info(\"Script finished\")\n</code></pre>"},{"location":"tutorials/index.html","title":"Tutorials","text":"<p>To get started, you can follow the tutorials in this section. They are designed to help you understand how the package works, first without getting into the specifics of colliders (only concepts on a dummy example), and then with a focus on how to use it for an actual collider configuration followed by tracking.</p> <p>However, if you want to start doing tracking studies right away, you might want to go straight to the case studies section. </p> <p>In any case, many tweaks and details are also explained or re-explained in the case studies, so you should definitively check them out at some point.</p>"},{"location":"tutorials/advanced/handling_dependencies.html","title":"Handling dependencies","text":"<p>Two types of dependencies might be required by a given study: data or configuration (i.e. files ending <code>.parquet</code>, <code>.txt</code>, <code>.yaml</code>, etc.), and python modules. There are various ways of handling these dependencies, but we're just going to provide the simpler approach here.</p>"},{"location":"tutorials/advanced/handling_dependencies.html#data-dependencies","title":"Data dependencies","text":"<p>Data dependencies are files that are required by the study to run. In the approach we suggest, these files should be:</p> <ol> <li>Declared as dependencies in the study's scan configuration (optional, but better for portability).</li> <li>Declared as parameters in the study's main configuration, with relative or absolute paths.</li> <li>If running the study on a cluster, declared in the dictionary of dependencies <code>dic_dependencies_per_gen</code> which is passed to the <code>submit()</code> function.</li> </ol> <p>Indeed, dependencies which are declared in the study scan configuration will be automatically copied and placed at the root of the study. This way, the study can access them using relative paths (which must be adapted for each generation, of course. </p> <p>You can do that either by having several paths in the configuration, or by prefixing appropriately the path using <code>../</code> in the template script), or absolute path (simpler, but less portable). In any case, if the study is run on a cluster, the relative paths will be mutated to be absolute paths.</p> <p>For instance, your scan configuration could look like something like this:</p> config_scan.yaml<pre><code># ==================================================================================================\n# --- Structure of the study ---\n# ==================================================================================================\nname: example_tune_scan\n\n# List all useful files that will be used by executable in generations below\n# These files are placed at the root of the study\ndependencies:\n  main_configuration: config_hllhc16.yaml\n  others:\n    - other_config.yaml\n    - input_data.parquet\n\nstructure:\n  # First generation is always at the root of the study\n  # such that config_hllhc16.yaml is accessible as ../config_hllhc16.yaml\n  generation_1:\n    executable: generation_1.py\n\n  # Second generation depends on the config from the first generation\n  generation_2:\n    executable: generation_2_level_by_nb.py\n    scans:\n      qx:\n        subvariables: [lhcb1, lhcb2]\n        linspace: [62.305, 62.330, 26]\n</code></pre> <p>In this case, your main configuration should be adapted to have the following field (in the appropriate section):</p> config_hllhc16.yaml<pre><code># Assuming other config will be called from generation 1\nother_config: ../other_config.yaml\n# Assuming input data will be called from generation 2\ninput_data: ../../input_data.parquet\n</code></pre> <p>If you run this simulation on a cluster, dont forget to declare the dependencies in the dictionary <code>dic_dependencies_per_gen</code>:</p> submit.py<pre><code>dic_dependencies_per_gen = {\n    1: ['other_config.yaml'],\n    2: ['input_data.parquet']\n}\nsubmit(\n    ...\n    dic_dependencies_per_gen=dic_dependencies_per_gen,\n    ...\n)\n</code></pre> <p>Note that we don't need to put the main main_configuration (here <code>config_hllhc16.yaml</code>) in the dictionary of dependencies, as its path doesn't need to be mutated since the file is already copied to the job folder on the node by the <code>run.sh</code> file. In practice, we could copy all the dependencies on the node (avoiding to mutate paths in the config), but it is at the cost of a more complex <code>run.sh</code> file. We chose to keep it simple.</p>"},{"location":"tutorials/advanced/handling_dependencies.html#python-dependencies","title":"Python dependencies","text":"<p>Python dependencies are modules that are required by the study to run. They could be dealt with just like data dependencies, but they are not parameters of the study and don't really belong in the study configuration file.</p> <p>Most likely, the simplest approach is to simply add your python modules as dependencies in the scan configuration, and to import them in your template script. Indeed, the python modules will be copied to the root of the study, and, in the provided template scripts, the path to the root of the study is automatically added to the <code>sys.path</code> of the python script. This way, you can import your modules directly in your template script (after the lines corresponding to adding the path to the root of the study to the <code>sys.path</code>, of course).</p> <p>For instance, your scan configuration could look like something like this:</p> config_scan.yaml<pre><code># ==================================================================================================\n# --- Structure of the study ---\n# ==================================================================================================\nname: example_tune_scan\n\n# List all useful files that will be used by executable in generations below\n# These files are placed at the root of the study\ndependencies:\n  main_configuration: config_hllhc16.yaml\n  others:\n    - modules/my_module.py\n\nstructure:\n  # First generation is always at the root of the study\n  # such that config_hllhc16.yaml is accessible as ../config_hllhc16.yaml\n  generation_1:\n    executable: generation_1.py\n\n  # Second generation depends on the config from the first generation\n  generation_2:\n    executable: generation_2_level_by_nb.py\n    scans:\n      qx:\n        subvariables: [lhcb1, lhcb2]\n        linspace: [62.305, 62.330, 26]\n</code></pre> <p>In this case, you can import your module directly in your template script (providing also the placeholder to be replaced by the path to the root of the study):</p> <pre><code>...\npath_root_study = \"{}  ###---path_root_study---###\"\nsys.path.append(path_root_study)\nimport my_module\n</code></pre> <p>This import will also work from clusters since the path to the root of the study is absolute.</p>"},{"location":"tutorials/advanced/modifying_code.html","title":"Customizing internal code","text":"<p>It can sometimes be needed to customize the code of study-DA to implement a feature that is not natively present. For instance, implementing a new way of doing the luminosity levelling, or even adding some custom checks when building the collider from a MAD-X model.</p> <p>Note that this goes a bit beyond customizing the template scripts, since, in this case, we would like to change the internal code of the tool (although we can do this using only the template scripts, see below).</p>"},{"location":"tutorials/advanced/modifying_code.html#scan-configuration","title":"Scan configuration","text":"<p>We will use an extremely simple scan configuration, with only one generation. Indeed, this tutorial is not about doing a scan or even configure a collider, but simply to illustrate how to modify the template scripts. You should be able to easily adapt this example to a multi-generational scan.</p> config_scan.yaml<pre><code># ==================================================================================================\n# --- Structure of the study ---\n# ==================================================================================================\nname: example_custom_ost\n\n# List all useful files that will be used by executable in generations below\n# These files are placed at the root of the study\ndependencies:\n  main_configuration: config_hllhc16.yaml\n  custom_ost: custom_files/custom_ost.py\n\nstructure:\n  # First generation is always at the root of the study\n  # such that config_hllhc16.yaml is accessible as ../config_hllhc16.yaml\n  generation_1:\n    executable: custom_files/generation_1.py\n</code></pre> <p>What you can already note here is that:</p> <ul> <li>we use a custom script for the first generation, <code>custom_files/generation_1.py</code>. We will show later the modifications brought to this script (with respect to the default), to use custom code.</li> <li>we added a dependency as a custom modules for the OST (optics specific tools), <code>custom_files/custom_ost.py</code>. Indeed, we assume that the internal code to modify is the OST.</li> </ul>"},{"location":"tutorials/advanced/modifying_code.html#study-configuration","title":"Study configuration","text":"<p>The configuration is the template one for HL-LHC v1.6.</p>"},{"location":"tutorials/advanced/modifying_code.html#template-script","title":"Template script","text":"<p>This is where the interesting changes are being made.</p> generation_1.py<pre><code>\"\"\"This is a template script for generation 1 of simulation study, in which ones generates a\nparticle distribution and a collider from a MAD-X model.\"\"\"\n\n# ==================================================================================================\n# --- Imports\n# ==================================================================================================\n\n# Import standard library modules\nimport logging\nimport os\nimport sys\n\n# Import user-defined modules\nfrom study_da.generate import MadCollider, ParticlesDistribution\nfrom study_da.utils import (\n    load_dic_from_path,\n    set_item_in_dic,\n    write_dic_to_path,\n)\n\n# Add path to custom_mad_collider (might need to add ../.., etc. depending on the generation)\nsys.path.append(\"..\")\nimport custom_ost\n\n# Set up the logger here if needed\n\n\n# ==================================================================================================\n# --- Override the MadCollider class\n# ==================================================================================================\nclass MadColliderCustom(MadCollider):\n    def __init__(self, configuration: dict):\n        super().__init__(configuration)\n\n        self._ost = custom_ost\n\n\n# ==================================================================================================\n# --- Script functions\n# ==================================================================================================\ndef build_distribution(config_particles):\n    # Build object for generating particle distribution\n    distr = ParticlesDistribution(config_particles)\n\n    # Build particle distribution\n    particle_list = distr.return_distribution_as_list()\n\n    # Write particle distribution to file\n    distr.write_particle_distribution_to_disk(particle_list)\n\n\ndef build_collider(config_mad):\n    # Build object for generating collider from custom MadCollider class\n    mc = MadColliderCustom(config_mad)\n\n    # Alternatively, you could directly use the MadCollider class and just update the OST\n    # mc = MadCollider(config_mad)\n    # mc._ost = custom_ost\n\n    # Or even more precise, you could define a function yourself and override it in the default ost\n    # ! Note that the number of arguments must be the same as the original function\n    # mc = MadCollider(config_mad)\n    # mc.ost.check_madx_lattices = lambda a: print(\"This is a fake check\")\n\n    # Build mad model\n    mad_b1b2, mad_b4 = mc.prepare_mad_collider()\n\n    # Build collider from mad model\n    collider = mc.build_collider(mad_b1b2, mad_b4)\n\n    # Twiss to ensure everything is ok\n    mc.activate_RF_and_twiss(collider)\n\n    # Clean temporary files\n    mc.clean_temporary_files()\n\n    # Save collider to json\n    mc.write_collider_to_disk(collider)\n\n\n# ==================================================================================================\n# --- Parameters placeholders definition\n# ==================================================================================================\ndict_mutated_parameters = {}  ###---parameters---###\npath_configuration = \"{}  ###---main_configuration---###\"\n# In case the placeholders have not been replaced, use default path\nif path_configuration.startswith(\"{}\"):\n    path_configuration = \"config.yaml\"\n\n# ==================================================================================================\n# --- Script for execution\n# ==================================================================================================\n\nif __name__ == \"__main__\":\n    logging.info(\"Starting script to build particle distribution and collider\")\n\n    # Load full configuration\n    full_configuration, ryaml = load_dic_from_path(path_configuration)\n\n    # Mutate parameters in configuration\n    for key, value in dict_mutated_parameters.items():\n        set_item_in_dic(full_configuration, key, value)\n\n    # Dump configuration\n    name_configuration = os.path.basename(path_configuration)\n    write_dic_to_path(full_configuration, name_configuration, ryaml)\n\n    # Build and save particle distribution\n    build_distribution(full_configuration[\"config_particles\"])\n\n    # Build and save collider\n    build_collider(full_configuration[\"config_mad\"])\n\n    logging.info(\"Script finished\")\n</code></pre> <p>If you look at the script, you will see that several possibilities are shown to customize the internal code of the tool.</p> <p>The most elegant way of proceeding is to inherit from and override the <code>MadCollider</code> class. To this end, we first import our custom module, <code>custom_ost</code>, and then define a new class, <code>MadColliderCustom</code>, that inherits from <code>MadCollider</code>. We then override the <code>_ost</code> attribute of the <code>MadCollider</code> class with our custom module. This is likely the most flexible way of customizing the code, as you can change any method or attribute of the <code>MadCollider</code> class, and solve potential paths or dependencies conflicts.</p> <p>Alternatively, you could directly use the <code>MadCollider</code> class and just update the <code>_ost</code> attribute. This is shown as a comment in the script. This is less flexible than the first method, but it is easier to implement. If you choose this approach, you can of course comment out the MadColliderCustom class.</p> <p>In both of these cases, you just have to modify the initial <code>optics_specific_tools.py</code> (that you can find, for instance, on GitHub) as you please, and put it in the <code>custom_files</code> folder.</p> <p>Finally, you could define a function yourself and override it in the default ost. This is also shown in as commented lines. This is a more surgical way of customizing the code, but note that the new function you define must have a behaviour similar to the original one (same number of arguments, same return type, etc.). If you choose this approach, you don't even have to define a custom OST file.</p> <p>Changes must be consistent with the original code</p> <p>Be careful when customizing the internal code of the tool. If you change the behaviour of the code, you might break the study. Make sure that the changes you make are consistent with the original code.</p> <p>For extensive changes, just fork the repository and modify the internal code</p> <p>If you need to make extensive changes to the internal code, it is probably simpler to fork the repository and modify the classes/methods you need. This might help you avoiding path or import conflicts.</p>"},{"location":"tutorials/advanced/modifying_code.html#study-generation","title":"Study generation","text":"<p>In this case, generation is done as usual, with, for instance, the following script:</p> generate.py<pre><code># ==================================================================================================\n# --- Imports\n# ==================================================================================================\n\n# Import standard library modules\nimport os\n\n# Import third-party modules\n# Import user-defined modules\nfrom study_da import create, submit\nfrom study_da.utils import load_template_configuration_as_dic, write_dic_to_path\n\n# ==================================================================================================\n# --- Script to generate a study\n# ==================================================================================================\n\n# Generate the study in the local directory\npath_tree, name_main_config = create(path_config_scan=\"config_scan.yaml\", force_overwrite=False)\n\n# ==================================================================================================\n# --- Script to submit the study\n# ==================================================================================================\n\n# In case gen_1 is submitted locally\ndic_additional_commands_per_gen = {\n    # To clean up the folder after the first generation if submitted locally\n    1: \"rm -rf final_* modules optics_repository optics_toolkit tools tracking_tools temp mad_collider.log __pycache__ twiss* errors fc* optics_orbit_at* \\n\"\n}\n\n# Submit the study\nsubmit(\n    path_tree=path_tree,\n    path_python_environment=\"/afs/cern.ch/work/c/cdroin/private/study-DA/.venv\",\n    name_config=name_main_config,\n    dic_additional_commands_per_gen=dic_additional_commands_per_gen,\n)\n</code></pre> <p>And that's it!</p>"},{"location":"tutorials/advanced/setting_custom_parameters.html","title":"Setting custom parameters","text":""},{"location":"tutorials/advanced/setting_custom_parameters.html#introduction","title":"Introduction","text":"<p>The possibilities offered by study-DA to scan parameters might sometimes be limited. For instance, you might want to input a parameter as a complex function of another parameter. Or you might want to load parametric values from a file. In such cases, you can set custom parameters instead of using a keyword to scan them in the scan configuration file.</p>"},{"location":"tutorials/advanced/setting_custom_parameters.html#scan-configuration","title":"Scan configuration","text":"<p>For this example, we decide to use a scan configuration that is very simple, based on the template configuration provided for HL-LHC v1.6:</p> config_scan.yaml<pre><code># ==================================================================================================\n# --- Structure of the study ---\n# ==================================================================================================\nname: example_custom_parameters\n\n# List all useful files that will be used by executable in generations below\n# These files are placed at the root of the study\ndependencies:\n  main_configuration: config_hllhc16.yaml\n\nstructure:\n  # First generation is always at the root of the study\n  # such that config_hllhc16.yaml is accessible as ../config_hllhc16.yaml\n  generation_1:\n    executable: generation_1.py\n    common_parameters:\n      # Needs to be redeclared as it's used for parallelization\n      # And re-used ine the second generation\n      n_split: 5\n\n  # Second generation depends on the config from the first generation\n  generation_2:\n    executable: generation_2_level_by_nb.py\n</code></pre>"},{"location":"tutorials/advanced/setting_custom_parameters.html#generating-script","title":"Generating script","text":"<p>We're now going to introduce how to input custom parameters in the generating script. In this example, we want to scan the tunes <code>qx</code> and <code>qy</code> for the LHCb1 and LHCb2 beams. In addition, we have to pay attention not to neglect the <code>n_split</code> parameter used for parallelization.</p> custom_parameters.py<pre><code># ==================================================================================================\n# --- Imports\n# ==================================================================================================\n\nfrom study_da import create\n\n# ==================================================================================================\n# --- Script to generate a study\n# ==================================================================================================\n# Generate the arrays of parameters to scan\nl_qx = []\nl_qy = []\nl_qx_for_naming = []\nl_qy_for_naming = []\nl_n_split = []\nl_n_split_for_naming = []\n\nfor qx in [62.310, 62.311]:\n    for qy in [60.320, 60.321]:\n        for n_split in range(5):\n            l_qx.append({key: qx for key in [\"lhcb1\", \"lhcb2\"]})\n            l_qy.append({key: qy for key in [\"lhcb1\", \"lhcb2\"]})\n            l_qx_for_naming.append(qx)\n            l_qy_for_naming.append(qy)\n            l_n_split.append(f\"{n_split:02d}.parquet\")\n            l_n_split_for_naming.append(n_split)\n\n# Store the parameters in a dictionary\ndic_parameter_all_gen = {\n    \"generation_2\": {\n        \"qx\": l_qx,\n        \"qy\": l_qy,\n        \"n_split\": l_n_split,\n    }\n}\n\ndic_parameter_all_gen_naming = {\n    \"generation_2\": {\n        \"qx\": l_qx_for_naming,\n        \"qy\": l_qy_for_naming,\n        \"n_split\": l_n_split_for_naming,\n    }\n}\n\n# Generate the study in the local directory\npath_tree, name_main_config = create(\n    path_config_scan=\"config_scan.yaml\",\n    dic_parameter_all_gen=dic_parameter_all_gen,\n    dic_parameter_all_gen_naming=dic_parameter_all_gen_naming,\n)\n</code></pre> <p>As you can see, the first part of the script is used to define the parameter space. We basically do the usual cartesian product of the tunes and n_split parameters manually. Because we don't want the <code>lhcb1</code> or <code>lhcb2</code> strings to appear in the filenames when generating the study, we also define a dictionnary of parameters for the naming of the files.</p> <p>In practice, you can customize the parameters as you wish, as long as you respect the structure of the dictionary <code>dic_parameter_all_gen</code> and <code>dic_parameter_all_gen_naming</code> (although the latter is optional, but can lead to very long or inconstistent filenames if not used).</p> <p>You can then generate your study as usual.</p>"},{"location":"tutorials/advanced/tracking_study.html","title":"A dummy tracking study","text":""},{"location":"tutorials/advanced/tracking_study.html#introduction","title":"Introduction","text":"<p>Now that the concepts have been introduced, let's see how to use the package for a dummy tracking study. This will help you understand how the package works and how to use it for an actual collider configuration.</p>"},{"location":"tutorials/advanced/tracking_study.html#creating-the-study","title":"Creating the study","text":""},{"location":"tutorials/advanced/tracking_study.html#template-scripts","title":"Template scripts","text":"<p>The tracking is usually done in two generations to optimize the process. In the first generation, the collider is created from a MAD-X sequence. In the second generation, the tracking is performed. We will use one script per generation, and the same configuration file for both generations (although it will be mutated in the second generation to scan over some parameters).</p>"},{"location":"tutorials/advanced/tracking_study.html#generation-1","title":"Generation 1","text":"<p>Generation 1 usually corresponds to the step in which the Xsuite collider is created from a MAD-X sequence, and the particle distribution is created. At this step, only a few checks are performed, and the collider is not yet ready for tracking studies. For this example, we will work with the template script provided in the package to create the collider.</p> <p>This script should very much be self-explanatory, especially if you have understood the concepts explained in the previous sections, and already have a basic knowledge of Mad-X and Xsuite. Basically, it loads the configuration file, mutates it with the parameters provided in the scan configuration (explained a bit below), and then builds the particle distribution and the collider. You are invited to check the details of the functions (e.g. on the GitHub repository of the package) to better understand what is happening.</p> <p>The collider configuration file is then saved with the mutated parameters, so that the subsequent generation can use it.</p> <p>You'll have to adapt these templates to your needs</p> <p>The template scripts provided in the package are just that: templates. They should be adapted to your needs, especially if you have a more complex collider configuration.</p> <p>For example, you might want to add some checks to ensure that the collider is correctly built, or that the particle distribution is correctly generated. You might also want to add some logging to keep track of what is happening in the script.</p> <p>You're basically free to not use at all the convenience functions provided with the package. </p>"},{"location":"tutorials/advanced/tracking_study.html#generation-2","title":"Generation 2","text":"<p>Generation 2 corresponds to the step in which the collider is configured and the tracking is performed. For this example, we will still work with the template script provided in the package to perform the tracking.</p> <p>Again, this script should be relatively self-explanatory. Basically, it loads the configuration file, mutates it with the parameters provided in the scan configuration, and then configures the collider and tracks the particles.</p> <p>The collider configuration is itself done in several steps:</p> <ul> <li>The collider is loaded from the file created in the first generation</li> <li>The beam-beam elements are installed (but remain inactive for now)</li> <li>The trackers are built for running Xsuite Twiss</li> <li>The knobs are set. This is where the mutation of the parameters usually has some impact, for instance when you scan the tune, you modify the knobs here.</li> <li>The tune and chromaticity are matched</li> <li>The filling scheme is set</li> <li>The number of collisions in the IPs is computed</li> <li>The leveling is done if requested</li> <li>The linear coupling is added</li> <li>The tune and chromaticity are rematched</li> <li>The beam-beam is computed and activated if needed</li> <li>The final luminosity is recorded</li> <li>The collider is saved to disk</li> </ul> <p>Again, you are invited to check the details of the functions if you want to know more. After this configuration step, the particles are tracked and the output is saved to disk.</p>"},{"location":"tutorials/advanced/tracking_study.html#template-configuration","title":"Template configuration","text":"<p>The configuration file is a YAML file that contains all the parameters needed to configure the collider and run the tracking. It is loaded in both generations, and mutated in the second generation to scan over some parameters. In this example, we will work with the template configuration provided for LHC run III.</p> <p>Although it's probably better if you understand the role of each individual parameter in the file, all you have to know for now is that this is a fairly standard configuration for an end-of-levelling tracking. However, you will definitely have to adapt it to your needs for your own scans.</p> <p>You need to properly configure the path to the optics</p> <p>The path to the optics must be properly set in the configuration file (fields <code>acc-models-lhc</code> and <code>optics_file</code>). You need to make sure that the path is correct, and that the optics are available on your machine (or on AFS if you're at CERN). If not, you will get an error when trying to load the collider sequence.</p>"},{"location":"tutorials/advanced/tracking_study.html#scan-configuration","title":"Scan configuration","text":"<p>The scan configuration is not provided as a template since this is completely dependent on the study. We provide here an example in the case of a scan over the horizontal tune for a varying number of turns.</p> config_scan.yaml<pre><code># ==================================================================================================\n# --- Structure of the study ---\n# ==================================================================================================\nname: example_dummy_scan\n\n# List all useful files that will be used by executable in generations below\n# These files are placed at the root of the study\ndependencies:\n  main_configuration: config_runIII.yaml\n\nstructure:\n  # First generation is always at the root of the study\n  # such that config_hllhc16.yaml is accessible as ../config_hllhc16.yaml\n  generation_1:\n    executable: generation_1.py\n    common_parameters:\n      # Needs to be redeclared as it's used for parallelization\n      # And re-used ine the second generation\n      n_split: 5\n\n  # Second generation depends on the config from the first generation\n  generation_2:\n    executable: generation_2_level_by_nb.py\n    scans:\n      distribution_file:\n        # Number of paths is set by n_split in the main config\n        path_list: [\"____.parquet\", n_split]\n      qx:\n        subvariables: [lhcb1, lhcb2]\n        linspace: [62.305, 62.315, 11]\n      n_turns:\n        logspace: [2, 5, 20]\n</code></pre> <p>Scanning the number of turns might not be the wisest choice</p> <p>This is just an example to show you how to scan over some parameters. In practice, scanning the number of turns doesn't really make sense, as you could just do one scan with a large number of turns and regularly save the output to disk (e.g. every 1000 turns).</p> <p>Thre are several things to notice. </p> <p>First, and contrary to the dummy studies presented in the concepts, we don't run an parametric scan in the first generation: we just take the collider with the configuration as it is. However, there is an exception for the <code>n_split</code> parameter. It is declared in the <code>common_parameters</code> of the first generation, and then used in the second generation as a variable. This is because it is used for parallelization and sets how many subsets there will be for the particles distribution (in this case, each job will track one fifth of the particles distribution, which corresponds to the files <code>00.parquet</code>, <code>01.parquet</code>, etc.).</p> <p>Then, in this case, the executables <code>generation_1.py</code> and <code>generation_2_level_by_nb.py</code> are directly provided by the package (we don't need to place them in the same folder as the configuration, altough we could).</p> <p>Finally, you might notice that two parameters are being scanned in the second generation (the horizontal tune and the number of turns). In this case, if no specific keyword is provided (see the case studies for example of keywords), it's the cartesian product of all the parameters that is being scanned.</p> <p>One can then run the following lines of code generate the study:</p> dummy_scan.py<pre><code># ==================================================================================================\n# --- Imports\n# ==================================================================================================\nfrom study_da import create, submit\n\n# ==================================================================================================\n# --- Script to generate a study\n# ==================================================================================================\n\n# Now generate the study in the local directory\npath_tree, name_main_config = create(path_config_scan=\"config_scan.yaml\")\n</code></pre> <p>At this point, the study is created in the local directory. You can check the files that have been created, to better understand how generations and parameter mutations work.</p>"},{"location":"tutorials/advanced/tracking_study.html#submitting-the-study","title":"Submitting the study","text":"<p>The next step is to submit the study. This is done with the following lines of code (continuation of the previous script):</p> dummy_scan.py<pre><code># ==================================================================================================\n# --- Script to submit the study\n# ==================================================================================================\n\n# In case gen_1 is submitted locally\ndic_additional_commands_per_gen = {\n    # To clean up the folder after the first generation if submitted locally\n    1: \"rm -rf final_* modules optics_repository optics_toolkit tools tracking_tools temp mad_collider.log __pycache__ twiss* errors fc* optics_orbit_at* \\n\"\n}\n\n# Dependencies for the executable of each generation. Only needed if one uses HTC or Slurm.\ndic_dependencies_per_gen = {\n    1: [\"acc-models-lhc\"],\n    2: [\"path_collider_file_for_configuration_as_input\", \"path_distribution_folder_input\"],\n}\n\n# Submit the study\nsubmit(\n    path_tree=path_tree,\n    path_python_environment=\"/afs/cern.ch/work/c/cdroin/private/study-DA/.venv\",\n    path_python_environment_container=\"/usr/local/DA_study/miniforge_docker\",\n    path_container_image=\"/cvmfs/unpacked.cern.ch/gitlab-registry.cern.ch/cdroin/da-study-docker:757f55da\",\n    dic_dependencies_per_gen=dic_dependencies_per_gen,\n    name_config=name_main_config,\n    dic_additional_commands_per_gen=dic_additional_commands_per_gen,\n)\n</code></pre> <p>Let's comment a bit on this part of the script.</p> <p>The <code>dic_additional_commands_per_gen</code> is used to clean up the folder after the first generation (since we're going to submit it locally).</p> <p>The <code>dic_dependencies_per_gen</code> is only needed if one uses HTC, and basically specifies which path should be mutated to be absolute in the configuration file of the executable, so that the executable can find the files it needs even if it's being run from a distant node.</p> <p>Finally, the <code>submit</code> function is called with the path to the study, the path to the Python environment (for the first generation that we submit locally), the path to the container image, the path to the environment in the container (for the second generation that we submit on HTC), the name of the main configuration file, and the two dictionaries we just described.</p> <p>And that's it! If you run this file, you will be prompted how to do the submission (although this can also be preconfigured, as explained in the concepts) and the first generation will be submitted.</p> <p>Running the same script a bit later will submit the second generation (although you can try to re-run it right away, to have the package explaining you which jobs are running and why it doesn't submit the second generation). </p> <p>Alternatively, you can just add the argument <code>keep_submit_until_done=True</code> to the <code>submit</code> function to have the package regularly try to re-submit the second generation.</p>"},{"location":"tutorials/advanced/tracking_study.html#analyzing-the-results","title":"Analyzing the results","text":"<p>Once the study is done, re-running the script will just tell you that the study is done. You can then post-process the results and plot them with the following lines of code:</p> postprocess_and_plot.py<pre><code># ==================================================================================================\n# --- Imports\n# ==================================================================================================\nfrom study_da.plot import get_title_from_configuration, plot_heatmap\nfrom study_da.postprocess import aggregate_output_data\n\n# ==================================================================================================\n# --- Postprocess the study\n# ==================================================================================================\n\ndf_final = aggregate_output_data(\n    \"example_dummy_scan/tree.yaml\",\n    l_group_by_parameters=[\"qx_b1\", \"n_turns\"],\n    generation_of_interest=2,\n    name_output=\"output_particles.parquet\",\n    write_output=True,\n    only_keep_lost_particles=True,\n    force_overwrite=False,\n)\n\n# ==================================================================================================\n# --- Plot\n# ==================================================================================================\n\ntitle = get_title_from_configuration(\n    df_final,\n    crossing_type=\"vh\",\n    display_LHC_version=True,\n    display_energy=True,\n    display_bunch_index=True,\n    display_CC_crossing=False,\n    display_bunch_intensity=True,\n    display_beta=True,\n    display_crossing_IP_1=True,\n    display_crossing_IP_2=True,\n    display_crossing_IP_5=True,\n    display_crossing_IP_8=True,\n    display_bunch_length=True,\n    display_polarity_IP_2_8=True,\n    display_emittance=True,\n    display_chromaticity=True,\n    display_octupole_intensity=False,\n    display_coupling=True,\n    display_filling_scheme=True,\n    display_horizontal_tune=False,\n    display_vertical_tune=True,\n    display_luminosity_1=True,\n    display_luminosity_2=True,\n    display_luminosity_5=True,\n    display_luminosity_8=True,\n    display_PU_1=True,\n    display_PU_2=True,\n    display_PU_5=True,\n    display_PU_8=True,\n)\n\nfig, ax = plot_heatmap(\n    df_final,\n    horizontal_variable=\"n_turns\",\n    vertical_variable=\"qx_b1\",\n    color_variable=\"normalized amplitude in xy-plane\",\n    link=\"www.link-to-your-study.com\",\n    position_qr=\"bottom-right\",\n    plot_contours=False,\n    xlabel=\"Number of turns\",\n    ylabel=r\"$Q_x$\",\n    tick_interval=1,\n    round_xticks=0,\n    title=title,\n    vmin=4.0,\n    vmax=8.0,\n    green_contour=6.0,\n    label_cbar=\"Minimum DA (\" + r\"$\\sigma$\" + \")\",\n    output_path=\"qx_turns.png\",\n    vectorize=False,\n    xaxis_ticks_on_top=False,\n    plot_diagonal_lines=False,\n    fill_missing_value_with=8.0,\n)\n</code></pre> <p>The first function being called in this script is the <code>aggregate_output_data</code> function. This function reads the output files (here <code>\"output_particles.parquet\"</code>) of the second generation (since <code>generation_of_interest=2</code>), and aggregates them in a single DataFrame, according to the provided parameters (<code>l_group_by_parameters</code>)  and function (not provided, <code>min</code> by default).</p> <p>Since we've set <code>only_keep_lost_particles=True</code>, only the lost particles are kept in the DataFrame. Therefore, when we aggregate, we will get the minimum amplitude of the lost particles for each combination of the parameters <code>qx_b1</code> and <code>n_turns</code>, i.e. the DA (Dynamic Aperture) of the collider for each combination of the parameters.</p> <p>The DataFrame is then saved to disk (if the argument <code>write_output</code> is set to <code>True</code>). If the file already exists, the function will ask you if you want to overwrite it (unless you set <code>force_overwrite=True</code>).</p> <p>The second function being called, <code>get_title_from_configuration</code>, is used to get a title for the plot. This title is generated from the configuration file of the collider (which is itself stored in the <code>output_particles.parquet</code>), and is used to display the main parameters of the collider in the plot. The arguments should be relatively self-explanatory, but you can check the documentation of the function for more details.</p> <p>In this case, since I'm scanning over the horizontal tune, I'm not displaying it in the title (since it's already displayed in the horizontal axis of the plot). However, I'm displaying the vertical tune. By default, we never display the number of turns in the title (it should always be 1M for DA computations), but you can change this by setting <code>display_number_of_turns=True</code>.</p> <p>Finally, the last function being called, <code>plot_heatmap</code>, is used to plot the heatmap. The function takes the DataFrame, the horizontal and vertical variables, the color variable, and a bunch of other arguments to customize the plot. In particular, the <code>green_contour</code> is to highlight the target DA (very useful to easily detect the islands of viable DA). In this case, we don't plot contours at all since the plot is very small.</p> <p>In addition, it's quite often that some values are missing in the plots, because some jobs failed, or, in this case, because no particles were lost for simulations with low number of turns. In this case, one can fill the missing values with either a number (as in here), or just try to interpolate them (setting <code>fill_missing_value_with='interpolate'</code>).</p> <p>You can add a link as a qrcode to your plot</p> <p>You can add a link to your study as a qrcode in the plot by specifying the <code>link</code> argument in the <code>plot_heatmap</code> function. In this case, the qrcode will be displayed in the bottom right corner of the plot. However, the qrcode will be clickable only if you use a vectorized output format (e.g. pdf).</p> <p>Just for illustration, here is the output of the plot (not vectorized):</p> <p></p> <p>I used a png version as it's lighter to display for the browser but you should probably use a vectorized format for your plots, especially if you want to print them or include them in a presentation (you can always convert them to png afterwards if you need to).</p> <p>Many examples of plotting are available in the case studies section, so you can check them out to see how to plot more realistic types of studies.</p>"},{"location":"tutorials/advanced/tracking_with_GPU.html","title":"Tracking with GPUs","text":"<p>When needing to track large distributions of particles, the computational cost can become a bottleneck for usual CPU tracking. In these cases, it is possible to use GPUs to speed up the process.</p> <p>With study-DA, the use of GPUs is very transparent. There are basically two steps to follow:</p> <ul> <li>modify the configuration file to use a GPU-compatible context (<code>cupy</code> or <code>opencl</code>)</li> <li>request a GPU node when running the simulation, if using a cluster</li> </ul> <p>Array libraries for GPU-accelerated computing must be installed</p> <p>Using <code>cupy</code> or <code>opencl</code> will require you to properly install the corresponding packages, whether locally or in a Docker container. This should be well explained in the Xsuite documentation.</p> <p>Tips for installing opencl</p> <p>If the installation guidelines from the Xsuite website are not enough to get a working simulation, you can try to install these additional packages:</p> <pre><code>```bash\npip install pocl-binary-distribution\npip install mako\n```\n</code></pre>"},{"location":"tutorials/advanced/tracking_with_GPU.html#state-of-gpu-tracking-with-hpc-clusters","title":"State of GPU tracking with HPC clusters","text":"<p>The following configurations have been tested with HTCondor:</p> <ul> <li><code>cupy</code>: works (but it is not very recommended to run HTC simulations without Docker as this can easily clog AFS)</li> <li><code>cupy</code> with Docker: works</li> </ul> <p>The following configurations have been tested with Slurm (CNAF.INFN):</p> <ul> <li><code>cupy</code> with Docker: doesn't work (problem with not up to date CUDA drivers...)</li> <li><code>opencl</code> with Docker: doesn't work (BUILD_PROGRAM_FAILURE)</li> <li><code>cupy</code>: doesn't work (compilation error when tracking)</li> <li><code>opencl</code>: doesn't work (seems to work but is actually tracking on the CPU...)</li> </ul> <p>As you can see, using the GPUs on CNAF.INFN with Slurm is not an easy task... The only simple option left is to ssh connect to a CNAF.INFN machine with a GPU (e.g. <code>pcbe-abp-gpu001.cern.ch</code>) and run the simulation locally with <code>cupy</code> (works, if <code>cupy</code> is properly installed). However, this way, your job won't be handled using the Slurm workload manager (no queue). Therefore, it is your responsability to not overload the machine. At the time of writing, the machine <code>pcbe-abp-gpu001.cern.ch</code> gives you access to 4 GPUs (don't forget to specify the <code>device_number</code> to use the GPU you want).</p>"},{"location":"tutorials/advanced/tracking_with_GPU.html#example-of-a-simple-tracking-study-with-gpus","title":"Example of a simple tracking study with GPUs","text":"<p>We will show how to do a small two-generational tune scan using GPU for the tracking with a very large initial distribution for the particles. We're going to do this tracking on HTCondor, but the procedure would be fairly similar when tracking on a local machine. We use the usual HLLHC v1.6 configuration (with a few tweaks for the GPU tracking, shown later).</p>"},{"location":"tutorials/advanced/tracking_with_GPU.html#scan-configuration","title":"Scan configuration","text":"<p>Since we're going to use <code>cupy</code>, we might want to use a different <code>device_number</code>for each job (to optimally parallelize). This is not possible with our usual scan configuration file, since one variable will evolve with all the others at once. Therefore, we will not define the parameter space in the scan configuration file, but in the script generating the study.</p> config_scan.yaml<pre><code># ==================================================================================================\n# --- Structure of the study ---\n# ==================================================================================================\nname: example_GPU_scan\n\n# List all useful files that will be used by executable in generations below\n# These files are placed at the root of the study\ndependencies:\n  main_configuration: config_hllhc16_GPU.yaml\n\nstructure:\n  # First generation is always at the root of the study\n  # such that config_hllhc16.yaml is accessible as ../config_hllhc16.yaml\n  generation_1:\n    executable: generation_1.py\n\n  # Second generation depends on the config from the first generation\n  generation_2:\n    executable: generation_2_level_by_nb.py\n</code></pre> <p>Note that we don't even define the <code>n_split</code> parameter here, as we won't parallelize the jobs on CPUs by splitting the distribution (the whole point of GPUs is to be able to track many particles in parallel).</p>"},{"location":"tutorials/advanced/tracking_with_GPU.html#study-configuration","title":"Study configuration","text":"<p>First, we want to use a very large initial particles distribution. We use the same type of distribution as in other examples (particles distributed uniformly radially and on a given number of angles), but you're highly encouraged to use a more realistic distribution for your studies (and therefore adapt the configuration for your needs, e.g. define the parameters of a coviariance matrix of a multivariate normal distribution).</p> config_hllhc16_GPU.yaml<pre><code>config_particles:\n  r_min: 4.0\n  r_max: 8.0\n  n_r: 500\n  n_angles: 40\n  n_split: 1\n  path_distribution_folder_output: particles\n</code></pre> <p>Finally, we want to change the context to use <code>cupy</code> (depending on your installation, you might want to use <code>opencl</code> instead):</p> config_hllhc16_GPU.yaml<pre><code>config_simulation:\n  context: \"cupy\" \n</code></pre> <p>And we're done. We can now generate the study.</p>"},{"location":"tutorials/advanced/tracking_with_GPU.html#study-generation","title":"Study generation","text":"<p>The script to generate the study is a bit trickier than usual since, as explain earlier, we want to define the parameter space inside of the script (and not in the scan configuration file, as usual).</p> GPU_scan.py<pre><code># ==================================================================================================\n# --- Imports\n# ==================================================================================================\n\n# Import standard library modules\nimport os\n\n# Import third-party modules\n# Import user-defined modules\nfrom study_da import create, submit\n\n# ==================================================================================================\n# --- Script to generate a study\n# ==================================================================================================\n# Generate the arrays of parameters to scan\nl_device_number = []\nl_qx = []\nl_qy = []\nl_qx_for_naming = []\nl_qy_for_naming = []\ndevice_number = 0\nfor qx in [62.310, 62.311]:\n    for qy in [60.320, 60.321]:\n        # Distribute the simulation over the GPUs, knowing that only 4 GPUs are available\n        l_device_number.append(device_number % 4)\n        l_qx.append({key: qx for key in [\"lhcb1\", \"lhcb2\"]})\n        l_qy.append({key: qy for key in [\"lhcb1\", \"lhcb2\"]})\n        l_qx_for_naming.append(qx)\n        l_qy_for_naming.append(qy)\n        device_number += 1\n\n# Store the parameters in a dictionary\ndic_parameter_all_gen = {\n    \"generation_2\": {\n        # \"device_number\": l_device_number,\n        \"qx\": l_qx,\n        \"qy\": l_qy,\n    }\n}\n\ndic_parameter_all_gen_naming = {\n    \"generation_2\": {\n        # \"device_number\": l_device_number,\n        \"qx\": l_qx_for_naming,\n        \"qy\": l_qy_for_naming,\n    }\n}\n\n# Generate the study in the local directory\npath_tree, name_main_config = create(\n    path_config_scan=\"config_scan.yaml\",\n    force_overwrite=False,\n    dic_parameter_all_gen=dic_parameter_all_gen,\n    dic_parameter_all_gen_naming=dic_parameter_all_gen_naming,\n)\n\n# ==================================================================================================\n# --- Script to submit the study\n# ==================================================================================================\n\n# In case gen_1 is submitted locally\ndic_additional_commands_per_gen = {\n    # To clean up the folder after the first generation if submitted locally\n    1: \"rm -rf final_* modules optics_repository optics_toolkit tools tracking_tools temp mad_collider.log __pycache__ twiss* errors fc* optics_orbit_at* \\n\"\n}\n\n# Dependencies for the executable of each generation. Only needed if one uses HTC or Slurm.\ndic_dependencies_per_gen = {\n    2: [\"path_collider_file_for_configuration_as_input\", \"path_distribution_folder_input\"],\n}\n\n\n# Preconfigure submission to HTC\ndic_config_jobs = {\n    \"generation_1\" + \".py\": {\n        # We leave CPU for gen 1 context as it doesn't do tracking\n        \"request_gpu\": False,\n        \"submission_type\": \"local\",\n    },\n    \"generation_2\" + \".py\": {\n        # We use GPU for gen 2 context as it does tracking\n        # Note that the context is also set in the config file\n        \"request_gpu\": True,\n        \"submission_type\": \"htc_docker\",\n        \"htc_flavor\": \"tomorrow\",\n    },\n}\n\n# Submit the study\nsubmit(\n    path_tree=path_tree,\n    path_python_environment=\"/afs/cern.ch/work/c/cdroin/private/study-DA/.venv\",\n    path_python_environment_container=\"/usr/local/DA_study/miniforge_docker\",\n    path_container_image=\"/cvmfs/unpacked.cern.ch/gitlab-registry.cern.ch/cdroin/da-study-docker:757f55da\",\n    dic_dependencies_per_gen=dic_dependencies_per_gen,\n    name_config=name_main_config,\n    dic_additional_commands_per_gen=dic_additional_commands_per_gen,\n    dic_config_jobs=dic_config_jobs,\n)\n</code></pre> <p>As you can see, the first part of the script is used to define the parameter space. We basically do the usual cartesian product of the tunes manually, but we set a new device number every time. Because we don't want the <code>lhcb1</code> or <code>lhcb2</code> strings to appear in the filenames when generating the study, we also define a dictionnary of parameters for the naming of the files.</p> <p>The device number is commented out</p> <p>As you can see, the <code>device_number</code> is commented out in the <code>dic_parameter_all_gen</code> dictionary. This is because use HTCondor in this simulation, and therefore we don't need to specify the device number in the configuration file. If you were to run this simulation locally, you would need to uncomment this line.</p> <p>And that's it, postprocessing can be done as usual.</p>"},{"location":"tutorials/concepts/1_generation.html","title":"Generation","text":""},{"location":"tutorials/concepts/1_generation.html#introduction","title":"Introduction","text":"<p>Generating studies is one of the main functionalities of the study-da package. To understand how this is done, we will play with some dummy example configuration. In this case, we choose to use only two generations as it's enough to illustrate the main concepts, but you can have as many generations as you want.</p>"},{"location":"tutorials/concepts/1_generation.html#creating-the-study","title":"Creating the study","text":""},{"location":"tutorials/concepts/1_generation.html#template-scripts","title":"Template scripts","text":""},{"location":"tutorials/concepts/1_generation.html#generation-1","title":"Generation 1","text":"<p>First, let's define the scripts from which we would like to generate the jobs. We will start with something very simple and simply add two parameters from the configuration file.</p> generation_1_dummy.py<pre><code># ==================================================================================================\n# --- Imports\n# ==================================================================================================\n\n# Import standard library modules\nimport logging\nimport os\n\n# Import third-party modules\n# Import user-defined modules\nfrom study_da.utils import (\n    load_dic_from_path,\n    set_item_in_dic,\n    write_dic_to_path,\n)\n\n# Set up the logger here if needed\n\n\n# ==================================================================================================\n# --- Script functions\n# ==================================================================================================\ndef add(configuration):\n    x = float(configuration[\"a_random_nest\"][\"x\"])\n    z = float(configuration[\"another_random_nest\"][\"and_another\"][\"z\"])\n    configuration[\"result\"] = {\"x_plus_z\": x + z}\n    return configuration\n\n\n# ==================================================================================================\n# --- Parameters placeholders definition\n# ==================================================================================================\ndict_mutated_parameters = {}  ###---parameters---###\npath_configuration = \"{}  ###---main_configuration---###\"\n# In case the placeholders have not been replaced, use default path\nif path_configuration.startswith(\"{}\"):\n    path_configuration = \"config.yaml\"\n\n# ==================================================================================================\n# --- Script for execution\n# ==================================================================================================\n\nif __name__ == \"__main__\":\n    logging.info(\"Starting custom script\")\n\n    # Load full configuration\n    full_configuration, ryaml = load_dic_from_path(path_configuration)\n\n    # Mutate parameters in configuration\n    for key, value in dict_mutated_parameters.items():\n        set_item_in_dic(full_configuration, key, value)\n\n    # Add x and z and write to configuration\n    full_configuration = add(full_configuration)\n\n    # Dump configuration\n    name_configuration = os.path.basename(path_configuration)\n    write_dic_to_path(full_configuration, name_configuration, ryaml)\n\n    logging.info(\"Script finished\")\n</code></pre>"},{"location":"tutorials/concepts/1_generation.html#jinja2-placeholders","title":"Jinja2 placeholders","text":"<p>The following part of the script is not really made to be used as is, but to be filled in by the study-da package:</p> <pre><code>dict_mutated_parameters = {}  ###---parameters---###\npath_configuration = \"{}  ###---main_configuration---###\"\"\n</code></pre> <p>The <code>{}  ###---</code>and <code>---###</code> are used to indicate placeholders for the actual values that will be filled in by the study-da package, using jinja2 under the hood. In practice, as it will be shown later in this tutorial, the <code>{}  ###---parameters---###</code> will be replaced by a dictionary of parameters (the ones being scanned), and <code>{}  ###---main_configuration---###</code> will be replaced by the path to the main configuration file. This allows to mutate selectively some of the parameters in the configuration file, and to write the modified configuration back to the disk. The parameter values are specific to each generated job.</p> <p>Why these placeholders?</p> <p>You may wonder why we use these weird <code>{}  ###---</code> and <code>---###</code> placeholders, instead of the usual <code>{{</code> and <code>}}</code> from jinja2. The reason is that we want to keep the template script a valid Python executable, and this choice of placeholders allows to do so. This is however quite arbitrary.</p> <p>If you don't understand, no worries, it will get clearer as we go along and actually generate some jobs.</p>"},{"location":"tutorials/concepts/1_generation.html#understanding-the-script","title":"Understanding the script","text":"<p>The script is quite simple. It loads the main configuration file from a path that is not explicitely given yet, mutates the parameters in the configuration, adds two of them, and writes the modified configuration back to the disk.</p> <p>It is assumed that the main configuration is always loaded from the above generation, and written back in the current generation. Therefore, each generation relies on the previous one. This is a simple way to chain the generations, and update the configuration with the mutated parameters every time.</p> Be careful with parameter names <p>As you can see in the script, parameter are accessed only with their names. No key is provided, while the corresponding yaml file might have a nested structure.</p> <p>This is because the <code>set_item_in_dic</code> function is used to set the value of the parameter. This function will look for the parameter in the configuration file (everywhere) and set its value. Now, if two parameters have the same name, but are in different parts of the configuration file, the script will not work as expected. </p> <p>This is the price of making the package as simple as possible. If you happen to have two parameters with the same name, you will have to modify one of them in the configuration file.</p> <p>Why don't you generate all the configuration files directly?</p> <p>You might very legitimately wonder why we don't produce the configuration files for all generations, mutating them appropriately, and then run the jobs. This is because, in a generic workflow, the scripts of each generation will modify the configuration file, and the next generation will depend on the modified configuration file. Therefore, it can't be created in advance.</p>"},{"location":"tutorials/concepts/1_generation.html#generation-2","title":"Generation 2","text":"<p>The second generation script is just as simple:</p> generation_2_dummy.py<pre><code># ==================================================================================================\n# --- Imports\n# ==================================================================================================\n\n# Import standard library modules\nimport logging\nimport os\n\n# Import third-party modules\n# Import user-defined modules\nfrom study_da.utils import (\n    load_dic_from_path,\n    set_item_in_dic,\n    write_dic_to_path,\n)\n\n# Set up the logger here if needed\n\n\n# ==================================================================================================\n# --- Script functions\n# ==================================================================================================\ndef multiply_and_dump(configuration):\n    y = float(configuration[\"y\"])\n    x_plus_z = float(configuration[\"result\"][\"x_plus_z\"])\n\n    # Dump result to txt file\n    with open(\"result.txt\", \"w\") as f:\n        f.write(str(x_plus_z * y))\n\n\n# ==================================================================================================\n# --- Parameters placeholders definition\n# ==================================================================================================\ndict_mutated_parameters = {}  ###---parameters---###\npath_configuration = \"{}  ###---main_configuration---###\"\n# In case the placeholders have not been replaced, use default path\nif path_configuration.startswith(\"{}\"):\n    path_configuration = \"config.yaml\"\n\n# ==================================================================================================\n# --- Script for execution\n# ==================================================================================================\n\nif __name__ == \"__main__\":\n    logging.info(\"Starting custom script\")\n\n    # Load full configuration\n    full_configuration, ryaml = load_dic_from_path(path_configuration)\n\n    # Mutate parameters in configuration\n    for key, value in dict_mutated_parameters.items():\n        set_item_in_dic(full_configuration, key, value)\n\n    # Add x and z and write to configuration\n    multiply_and_dump(full_configuration)\n\n    # Dump configuration\n    name_configuration = os.path.basename(path_configuration)\n    write_dic_to_path(full_configuration, name_configuration, ryaml)\n\n    logging.info(\"Script finished\")\n</code></pre> <p>As you can see, this script multiplies the result of the previous script (stored in the configuration, in the above generation) by a new parameter <code>y</code>, and writes the result to a text file.</p>"},{"location":"tutorials/concepts/1_generation.html#template-configuration","title":"Template configuration","text":"<p>The base configuration is always the same for all generations, although it does get modified (mutated) by the scripts. There's nothing special about it, it's just a simple yaml file:</p> config_dummy.yaml<pre><code>a_random_nest:\n  x: -1\ny: -2\nanother_random_nest:\n  and_another:\n    z: -3\n</code></pre>"},{"location":"tutorials/concepts/1_generation.html#scan-configuration","title":"Scan configuration","text":"<p>The scan configuration is an essential part of the study generation. It defines what are the parameters that will be scanned, and the values they will take. Here is a possible scan configuration for our dummy example:</p> config_scan.yaml<pre><code>name: example_dummy\n\n# List all useful files that will be used by executable in generations below\n# These files are placed at the root of the study\ndependencies:\n  main_configuration: custom_files/config_dummy.yaml\n  # others:\n    # - custom_files/other_file.yaml\n    # - custom_files/module.py\nstructure:\n  # First generation is always at the root of the study\n  # such that config_dummy.yaml is accessible as ../config_dummy.yaml\n  generation_1:\n    executable: custom_files/generation_1_dummy.py\n    scans:\n      x:\n        # Values taken by x using a list\n        list: [1, 2]\n\n  # Second generation depends on the config from the first generation\n  generation_2:\n    executable: custom_files/generation_2_dummy.py\n    scans:\n      y:\n        # Values taken by y using a linspace\n        linspace: [1, 100, 3]\n</code></pre> <p>Let's exlain the different fields in the scan configuration:</p> <ul> <li>name: the name of the study, which will also correspond to the name of the rood folder of the study.</li> <li>dependencies: a list of files that are needed by the executable scripts. These files will be copied to the root of the study, so that they can be accessed by the scripts. Note that some configuration files are already provided by the package, and can be used directly (see e.g. 1_simple_collider  and 2_tune_scan for examples). Understand teh dependencies is not so straightforward, and we will come back to it in the advanced tutorial.</li> <li> <p>structure: the structure of the study, with the different generations:</p> </li> <li> <p>Each generation has an <code>executable</code> field, which is the path to the script that will be executed. These paths can correspond to local files (as in here), or to predefined templates (as, for instance, in the case studies, in which case just the name of the template is enough).</p> </li> <li>The <code>scans</code> field is a dictionary of parameters that will be scanned, and the values they will take. The values can be given as a list, a linspace or a logspace. Other possibilities (e.g. scanning generated string names, using nested variables, defining parameter in terms of other through mathematical expression) are all presented in the Case studies.</li> </ul> <p>By default (if no specific keyword is provided), the cartesian product of all the parameter values will be considered to generate the jobs. This means that the number of jobs will be the product of the number of values for each parameter. In the example above, the number of jobs will be 6 (2 values for <code>x</code> and 3 values for <code>y</code>).</p> <p>Conversely, one can decide to scan two parameters at the same time (useful, for instance, when scanning the tune diagonal in a collider) using the <code>concomitant</code> keyword. This is also used in tune scan case studie section.</p>"},{"location":"tutorials/concepts/1_generation.html#generating-the-study","title":"Generating the study","text":"<p>Everything is now in place to generate our dummy study. A one line-command is enough to do it for us:</p> generate.py<pre><code>from study_da import create\ncreate(path_config_scan=\"config_scan.yaml\")\n</code></pre> <p>You should now see the corresponding study folder appear in the current directory.</p>"},{"location":"tutorials/concepts/1_generation.html#arguments-of-the-create-function","title":"Arguments of the <code>create</code> function","text":"<p>We will detail the structure soon, but let's first have a look at the other possible arguments of the <code>create</code> function. The full signature of the function is:</p> <pre><code>def create(\n    path_config_scan: str = \"config_scan.yaml\",\n    force_overwrite: bool = False,\n    dic_parameter_all_gen: Optional[dict[str, dict[str, Any]]] = None,\n    dic_parameter_all_gen_naming: Optional[dict[str, dict[str, Any]]] = None,\n    add_prefix_to_folder_names: bool = False,\n) -&gt; tuple[str, str]:\n</code></pre> <p>Let's detail the arguments:</p> <ul> <li><code>force_overwrite</code> can be useful if you want to overwrite an existing study. However, in most of the case, we submit the study in the same script, meaning that we might have to run the script several times. In this case, the <code>force_overwrite</code> argument must be set to <code>False</code>, otherwise the study will be overwritten at each run, and you will lose whatever has already been computed.</li> <li><code>dic_parameter_all_gen</code> is a dictionary that allows to specify the parameters that will be scanned for each generation, instead of defining them the scan configuration file. This doesn't free you from defining the structure of the study in the scan configuration file! This can be useful when the way to define your parameters is more complex than a simple list, linspace or logspace, or if your parameters are functions of each other. This is explained in one of the case studies.</li> <li><code>dic_parameter_all_gen_naming</code> is similar to <code>dic_parameter_all_gen</code>, but allows to specify the parameters that will be scanned for each generation, and the way they will be named in the study folder. This is useful when you want to have a specific naming for your parameters, or if parameters are nested. This is also explained in the same case study.</li> <li><code>add_prefix_to_folder_names</code> is a boolean that allows to add a prefix to the folder names (each folder corresponding to a given job) of the study. This can sometimes help browsing the study, especially when the number of jobs is large.</li> </ul>"},{"location":"tutorials/concepts/1_generation.html#the-tree-and-study-structure","title":"The tree and study structure","text":"<p>The following study structure should have been generated (not all files are shown):</p> <pre><code>\ud83d\udcc1 example_dummy/\n\u251c\u2500\u2574\ud83d\udcc1 x_1/\n\u2502   \u251c\u2500\u2500 \ud83d\udcc1 y_1.0/\n\u2502   \u251c\u2500\u2500 \ud83d\udcc1 y_50.5/\n\u2502   \u251c\u2500\u2500 \ud83d\udcc1 y_100.0/\n\u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4 generation_2.py\n\u2502   \u2514\u2500\u2500 \ud83d\udcc4 generation_1.py\n\u251c\u2500\u2574\ud83d\udcc1 x_2/\n\u251c\u2500 \ud83d\udcc4 tree.yaml\n\u2514\u2500 \ud83d\udcc4 config_dummy.yaml\n</code></pre> <p>And similarly, in the <code>tree.yaml</code> file:</p> <pre><code>x_1:\n  generation_1:\n    file: example_dummy/x_1/generation_1.py\n  y_1.0:\n    generation_2:\n      file: example_dummy/x_1/y_1.0/generation_2.py\n  ...\n</code></pre> <p>As you can observe, by default, each folder corresponds to a given generation, and is named after the parameter value it corresponds to. In each folder, an executable script (a <code>.py</code> file) has been created, along with potential subgenerations. </p> <p>If you open a given script, you will see that the placeholders have been replaced by the actual values of the parameters. For instance, for the parameter definition in the <code>generation_1.py</code> script in the <code>x_1</code> folder now looks like:</p> <pre><code>dict_mutated_parameters = {\n    \"x\": 1,\n}\npath_configuration = \"../config_dummy.yaml\"\n</code></pre> <p>This type of structure is very useful to keep track of the different jobs that have been generated, and to easily access the results of each job. Each job has its own executable, and only depends on the previous generation. Therefore, the jobs can be run independently, or in parallel, and can be individually debugged.</p> <p>The tree file will be very useful to keep track of the state of each job, as illustrated in the second part of this tutorial.</p>"},{"location":"tutorials/concepts/2_submission.html","title":"Submission","text":""},{"location":"tutorials/concepts/2_submission.html#introduction","title":"Introduction","text":"<p>Submitting studies is one of the main functionalities of the study-da package. To understand how this is done, we will keep playing with the dummy example presented in the first part of this tutorial.</p>"},{"location":"tutorials/concepts/2_submission.html#submitting-the-study","title":"Submitting the study","text":"<p>Each job can be either submitted locally or on a cluster. Similarly, each job can be submitted to run on the CPU or on the GPU through a given context. When not configured, study-DA will prompt the user about the configuration of job at submission.</p>"},{"location":"tutorials/concepts/2_submission.html#submitting-locally","title":"Submitting locally","text":"<p>To start simple, let's submit the jobs locally. Have a look at the following code:</p> generate_and_submit.py<pre><code>from study_da import create, submit\npath_tree, main_configuration_file = create(path_config_scan=\"config_scan.yaml\", force_overwrite=False)\nsubmit(\n    path_tree=path_tree,\n    path_python_environment=\"/afs/cern.ch/work/u/user/private/study-DA/.venv\",\n    name_config=main_configuration_file,\n    keep_submit_until_done=True,\n    wait_time=0.1,\n)\n</code></pre> <p>The first part of this code is identical to what was done in the first part of this tutorial (except that we now explictely ask not to recreate the study everytime we re-run the script). The second part is the submission of the study. The <code>submit</code> function takes the following arguments:</p> <ul> <li><code>path_tree</code> is the path to the study folder. We get this directly from the <code>create</code> function.</li> <li><code>path_python_environment</code> is the path to the python environment that will be used to run the jobs. You have to configure this path according to your own environment.</li> <li><code>name_config</code> is the name of the main configuration file for your study. This is also directly returned by the <code>create</code> function, from the configuration scan file.</li> <li><code>keep_submit_until_done</code> is a boolean that allows to keep the submission script running until all jobs are done. This is useful when you want to submit a study and wait for it to be completed.</li> <li><code>wait_time</code> is the time in seconds between each check of the status of the jobs. This is useful to avoid overloading the system with too many checks. We asked for 0.1 minutes here, meaning that the jobs will be checked and potentially re-submitted every 6 seconds.</li> </ul> <p>You should always generate and submit from the same folder</p> <p>Although the package leaves you the freedom of separating the generation and submission steps, it is highly recommended to generate and submit in the same folder. This is because the package keeps track of the status of the jobs.</p> <p>If you generate and submit in different folders, the package might have issues locating your job, and worse, wrongly handle the dependencies.</p> <p>When running this script, you get prompted to configure the submission for each job. In this case, we assume that we run all the jobs locally, on the CPU. Directly after the submission, you should get the following output:</p> <pre><code>State of the jobs:\n********************************\nGeneration 1\nJobs left to submit later: 0\nJobs running or queuing: 0\nJobs submitted now: 2\nJobs finished: 0\nJobs failed: 0\nJobs on hold due to failed dependencies: 0\n********************************\n********************************\nGeneration 2\nJobs left to submit later: 6\nJobs running or queuing: 0\nJobs submitted now: 0\nJobs finished: 0\nJobs failed: 0\nJobs on hold due to failed dependencies: 0\n********************************\n</code></pre> <p>No need to explain more here as this is quite explicit, but you can observe that the status of each individual job is tracked and recorded at each (re)-submission. This is very useful when making large, multi-generational studies, in which some of the jobs will inexorably fail for various reasons.</p> <p>You can also observe that, in the study folder, run files have appeared as <code>run.sh</code>, basically instructing the machine that will run the job (here, the local machine) how to proceed:</p> run.sh<pre><code>#!/bin/bash\n# Load the environment\nsource /afs/cern.ch/work/c/cdroin/private/study-DA/.venv/bin/activate\n\n# Move into the job folder\ncd /afs/cern.ch/work/c/cdroin/private/study-DA/tests/generate_and_submit/dummy_custom_template/example_dummy/x_1\n\n# Run the job and tag\npython generation_1.py &gt; output_python.txt 2&gt; error_python.txt\n\n# Ensure job run was successful and tag as finished, or as failed otherwise\nif [ $? -eq 0 ]; then\n    touch /afs/cern.ch/work/c/cdroin/private/study-DA/tests/generate_and_submit/dummy_custom_template/example_dummy/x_1/.finished\nelse\n    touch /afs/cern.ch/work/c/cdroin/private/study-DA/tests/generate_and_submit/dummy_custom_template/example_dummy/x_1/.failed\nfi\n\n# Store abs path as a variable in case it's needed for additional commands\npath_job=$(pwd)\n# Optional user defined command to run\n</code></pre> <p>There's nothing too fancy here: the script loads the python environment, moves to the job folder, runs the python script, and tags the job as finished or failed depending on the return code of the python script. As soon as the job reach completion, some <code>.finished</code> files appear in the study folder (or <code>failed</code>if they fail). This is a way to keep track of the jobs that have been completed, and to avoid re-submitting them.</p> <p>The optional user defined command to run at the end can be provided through an argument called <code>dic_additional_commands_per_gen</code>, which takes the generation number as key, and the additional command as value. This is useful when you want to run some additional commands after the completion of a generation, such as cleaning the output folder from temporary files, copying the results to a specific folder, or sending an email to the user.</p> <p>After a dozen seconds, the script should finish for good. When checking the tree, you should get the following output (cropped for clarity):</p> tree.yaml<pre><code>x_1:\n  generation_1:\n    file: example_dummy/x_1/generation_1.py\n    gpu: false\n    submission_type: local\n    htc_flavor:\n    status: finished\n    path_run: \n      /afs/cern.ch/work/c/cdroin/private/study-DA/tests/generate_and_submit/dummy_custom_template/example_dummy/x_1/run.sh\n  y_1.0:\n    generation_2:\n      file: example_dummy/x_1/y_1.0/generation_2.py\n      gpu: false\n      submission_type: local\n      htc_flavor:\n      status: finished\n      path_run: \n        /afs/cern.ch/work/c/cdroin/private/study-DA/tests/generate_and_submit/dummy_custom_template/example_dummy/x_1/y_1.0/run.sh\n    ...\npython_environment: /afs/cern.ch/work/c/cdroin/private/study-DA/.venv/bin/activate\ncontainer_image:\nabsolute_path: \n  /afs/cern.ch/work/c/cdroin/private/study-DA/tests/generate_and_submit/dummy_custom_template\nstatus: finished\nconfigured: true\n</code></pre> <p>As you can see, the tre file keeps track of everything that has been done, and the status of each job. When the status of each individual job is finished, the tree itself gets tagged as finished. </p>"},{"location":"tutorials/concepts/2_submission.html#submitting-to-htcondor","title":"Submitting to HTCondor","text":"<p>For the following to work, you need to have access to the CERN HTCondor cluster</p> <p>For the following to work, you need to have access to the CERN HTCondor cluster. If you don't have access, you can still run the script locally, but you won't be able to submit the jobs to the cluster (which is usually needed for large scans). You might want to read more about HTCondor here.</p> <p>The procedure for submitting to HTCondor (or Slurm) is fairly similar, but there are a few tricky points to consider:</p> <ul> <li>If you don't use an (externally hosted) Docker distribution, you might want to ensure that your Python environment is available from the cluster. Otherwise, the cluster will not be able to run the scripts.</li> <li>You have to ensure that the dependencies for each file are correctly defined in the configuration file. This is crucial for them to be copied on the cluster node, so that the jobs can run correctly.</li> <li>The relative paths in the main configuration should get mutated to fit the cluster environment. This is done automatically by the <code>submit</code> function for all the dependencies that are specified in the <code>dic_dependencies_per_gen</code> argument (not requested in the current example since the only dependency is the configuration, that is handled by default), but keep in mind that this might be a source of errors.</li> </ul> <p>Here is the same example as before, this time submitting to HTCondor using a Docker container:</p> generate_and_submit.py<pre><code>from study_da import create, submit\n\n# Generate the study in the local directory\npath_tree, main_configuration_file = create(\n    path_config_scan=\"config_scan.yaml\", force_overwrite=False\n)\n\npath_python_environment_container = \"/usr/local/DA_study/miniforge_docker\"\npath_container_image = (\n    \"/cvmfs/unpacked.cern.ch/gitlab-registry.cern.ch/cdroin/da-study-docker:757f55da\"\n)\n\n# Dic copy_back_per_gen (only for HTC)\ndic_copy_back_per_gen = {\n    2: {\"txt\": True},\n}\n\n# Preconfigure submission to HTC\ndic_config_jobs = {\n    \"generation_1\" + \".py\": {\n        \"request_gpu\": False,\n        \"submission_type\": \"htc_docker\",\n        \"htc_flavor\": \"espresso\",\n    },\n    \"generation_2\" + \".py\": {\n        \"request_gpu\": False,\n        \"submission_type\": \"htc_docker\",\n        \"htc_flavor\": \"espresso\",\n    },\n}\n\n# Submit the study\nsubmit(\n    path_tree=path_tree,\n    name_config=main_configuration_file,\n    path_python_environment_container=path_python_environment_container,\n    path_container_image=path_container_image,\n    dic_copy_back_per_gen=dic_copy_back_per_gen,\n    dic_config_jobs=dic_config_jobs,\n    keep_submit_until_done=True,\n    wait_time=2,\n    max_try=100,\n    force_submit=False,\n)\n</code></pre> <p>Some new variables and/or arguments are introduced here:</p> <ul> <li><code>path_python_environment_container</code> is the path to the python environment that will be used to run the jobs. This time, we use a Docker container, so the path is different from the local one.</li> <li><code>path_container_image</code> is the path to the Docker image that will be used to run the jobs. This is a specific image that has been built for the study-da package.</li> <li><code>dic_copy_back_per_gen</code> is a dictionary that allows to specify which files will be copied back from the cluster to the local machine after the completion of the jobs. This is useful when you want to retrieve the results of the study, or some intermediate files that have been generated during the study. In this case, a text file has been produced during the second generation, so we set the value to <code>True</code> for <code>txt</code> for the second generation. Possible file extensions are <code>parquet</code>, <code>yaml</code>, <code>txt</code>, <code>json</code>, <code>zip</code> and <code>all</code> (in which case all files will be copied back).</li> <li><code>dic_config_jobs</code> is a dictionary that allows to preconfigure the submission of the jobs. This is useful when you don't want to get prompted for each job. In this case, we set <code>request_gpu</code> to <code>False</code>, the submission type to <code>htc</code>, and the flavor to <code>espresso</code> for all the jobs, since our scripts are very simple. Note that the <code>request_gpu</code> argument is optional and set to <code>False</code> by default.</li> <li><code>max_try</code> is the maximum number of tries before the submission is considered as failed.Although failed jobs should not be re-submitted, this can prevent infinite loops in case of a problem with the submission. It is set to 100 by default.</li> <li><code>force_submit</code> is a boolean that allows to force the submission of the failed jobs, even if the study is already tagged as finished. This is useful when you want to re-submit the jobs after a failure which, you believe, is not due to the job itself. It is set to <code>False</code> by default.</li> </ul> <p>Keep forcing the resubmission is not a good idea</p> <p>When submitting jobs with the option <code>keep_submit_until_done=True</code>, the package will, by default, keep track of the status of the jobs and will not re-submit the jobs that have been tagged as failed. </p> <p>If you force the submission of the failed jobs with <code>force_submit = True</code>, you might end up in an infinite loop of submission (still limited by the <code>max_try</code> argument). This is not a good idea, and you should always try to understand why the jobs are failing before re-submitting them.</p> <p>When running this script, you will get prompted for the configuration of the jobs, but only for the first generation. The second generation will be submitted automatically. </p> <p>Copying back large file is not recommended</p> <p>Copying back large files on AFS can easily throttle the network, especially when you're running thousands of jobs at the same time.</p> <p>Don't forget to provide an environment if you don't use a Docker container</p> <p>If you submit on HTC but don't use a Docker container (or submit locally), you have to provide the path to the python environment on the cluster using the <code>path_python_environment</code> argument.</p> <p>You should get more or less the same output as before, except that your jobs are now most likely queued on the cluster (for confirmation on HTCondor, you can check the status of your jobs using the <code>condor_q</code> command).</p> <p>In the meanwhile, we can have a look at one of the new run files, for instance for the second generation (if you run the script above, remember that you have to wait for the second generation to be submitted to have the run files created):</p> dummy_custom_template/example_dummy/x_2/y_1.0/run.sh<pre><code>#!/bin/bash\n# Load the environment\nsource /usr/local/DA_study/miniforge_docker/bin/activate\n\n# Copy config in (what will be) the level above\ncp -f /afs/cern.ch/work/c/cdroin/private/study-DA/tests/generate_and_submit/dummy_custom_template/example_dummy/x_2/y_1.0/../config_dummy.yaml .\n\n# Create local directory on node and cd into it\nmkdir y_1.0\ncd y_1.0\n\n# Mutate the paths in config to be absolute\n\n# Run the job and tag\npython /afs/cern.ch/work/c/cdroin/private/study-DA/tests/generate_and_submit/dummy_custom_template/example_dummy/x_2/y_1.0/generation_2.py &gt; output_python.txt 2&gt; error_python.txt\n\n\n# Ensure job run was successful and tag as finished, or as failed otherwise\nif [ $? -eq 0 ]; then\n    touch /afs/cern.ch/work/c/cdroin/private/study-DA/tests/generate_and_submit/dummy_custom_template/example_dummy/x_2/y_1.0/.finished\nelse\n    touch /afs/cern.ch/work/c/cdroin/private/study-DA/tests/generate_and_submit/dummy_custom_template/example_dummy/x_2/y_1.0/.failed\nfi\n\n# Delete the config file from the above directory, otherwise it will be copied back and overwrite the new config\nrm ../config_dummy.yaml\n# Copy back output, including the new config\ncp -f *.parquet *.yaml *.txt /afs/cern.ch/work/c/cdroin/private/study-DA/tests/generate_and_submit/dummy_custom_template/example_dummy/x_2/y_1.0\n\n# Store abs path as a variable in case it's needed for additional commands\npath_job=/afs/cern.ch/work/c/cdroin/private/study-DA/tests/generate_and_submit/dummy_custom_template/example_dummy/x_2/y_1.0\n\n# Optional user defined command to run\n</code></pre> <p>The file should have self-explanatory comments. There are however several difference:</p> <ul> <li>The environment is loaded from the Docker container</li> <li>The configuration file is copied in the job folder on the node (and the output configuration file is copied back to the local machine after the completion of the job)</li> <li>Some paths in the configuration file (declared as <code>dependencies</code>) are mutated to be absolute, so that they can be accessed from the cluster node. In this case, there are no dependencies, but you can find many examples with dependencies in the Case studies section (look for when the <code>dic_dependencies_per_gen</code>is defined in the generating script).</li> <li>The output files are copied back to the local machine after the completion of the job. By default, only light files are copied back (parquet, yaml, txt). In we had set the <code>dic_copy_back_per_gen</code> argument to <code>{\"txt\": False}</code>, the output would not have been copied back.</li> </ul> <p>If all goes well, after a while (this depends on the load on the cluster), the <code>result.txt</code> should be copied back to the local machine for each leaf of the tree that has been tagged as finished (all of them, in theory).</p> <p>We should now have to automatically retrieve all these results. However, the study-da package only provides the tools to do this for tracking studies. You will therefore have to refer directly to the tracking studies section to see how to do this.</p>"}]}